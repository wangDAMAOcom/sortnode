/*
 * Copyright (C) 2013 Great OpenSource Inc. All Rights Reserved.
 */
#include "mysql_plan.h"

#include <async_task_manager.h>
#include <backend.h>
#include <basic.h>
#include <cluster_status.h>
#include <config.h>
#include <cross_node_join_manager.h>
#include <data_space.h>
#include <exception.h>
#include <frontend.h>
#include <log.h>
#include <log_tool.h>
#include <monitor_point_handler.h>
#include <mul_sync_topic.h>
#include <multiple.h>
#include <mysql_migrate_util.h>
#include <mysql_parser.h>
#include <mysql_xa_transaction.h>
#include <mysqld_error.h>
#include <parser.h>
#include <partition_method.h>
#include <plan.h>
#include <socket_base_manager.h>
#include <sql_parser.h>
#include <xa_transaction.h>

#include <boost/algorithm/string.hpp>
#include <boost/date_time/posix_time/posix_time.hpp>
#include <boost/format.hpp>

#include "mysql_connection.h"
#include "mysql_driver.h"
#include "mysql_request.h"
#include "sha1.h"

using namespace dbscale;
using namespace dbscale::sql;
using namespace dbscale::plan;
using namespace dbscale::mysql;

#define DBSCALE_BACKEND_THREADS_NUMS 9
#define CHECK_ERROR_INFO "CHECK TABLE GET ERROR"
#define MAX_DEAD_CONN_PER_MIN 1000

#define STR_LEN_INDEX(start, end) (end) - (start) + 1

struct dbscale_threads_str {
  const char *thread_name;
  const char *desc;
  const char *extra;
};

struct dbscale_threads_str threads_message[DBSCALE_BACKEND_THREADS_NUMS] = {
    {"MySQLXA_purge_thread ",
     "Backend thread used to purge XA transaction meta data", ""},
    {"XA_readonly_conn_handler",
     "Backend thread used to handle read-only connections during an XA "
     "transaction",
     ""},
    {"MigrateCleanThread  ",
     "Backend thread used to do purge tasks after a migration job", ""},
    {"MultipleSyncTool",
     "Backend thread used to handle sync messages when in multiple-dbscale "
     "mode",
     ""},
    {"MultipleManager  ",
     "Backend thread used to handle multiple dbscale related situations", ""},
    {"LicenseCheck", "Backend thread used to check license status", ""},
    {"ManagerThread ", "Backend thread used to do the memory free work", ""},
    {"CrossNodeJoinCleanThread",
     "Backend thread used to purge cross node join related meta data", ""},
    {"MessageHandler ", "Backend thread used to handle different messages", ""},
};

struct dbscale_help_str {
  const char *cmd;
  const char *des;
  const char *extra;
};

// plz use Dictionary order when add new command here
struct dbscale_help_str help_message[] = {
    {"DBSCALE [ADD | REMOVE] AUDIT USER user_name", "add/remove audit user",
     ""},
    {"DBSCALE ADD DEFAULT SESSION VARIABLES para_list",
     "add backend mysql variables which need maintain in dbscale", ""},
    {"DBSCALE BACKEND SERVER EXECUTE sql",
     "send SET, GRANT, DROP USER, FLUSH PRIVILEGES command to backend "
     "dataserver to set variables or user",
     ""},
    {"DBSCALE BLOCK TABLE name_or_full ALL block_time", "", ""},
    {"DBSCALE BLOCK TABLE name_or_full PARTITION", "", ""},
    {"DBSCALE CLEAN FAIL TRANSACTION [xid_str]",
     "clean uncommited xa transaction", ""},
    {"DBSCALE CLEAN MIGRATE int_val", "clean migrate table", ""},
    {"DBSCALE DYNAMIC ADD DATASERVER "
     "server_name=name,server_host=host,server_port=port,server_user=user,"
     "server_password=pwd[,server_alias_host=host]",
     "", ""},
    {"DBSCALE DYNAMIC ADD HASH_PARTITION_TABLE DATASPACE \"schema.table\" "
     "PARTITION_KEY=key_name PARTITION_SCHEME=scheme_name",
     "", ""},
    {"DBSCALE DYNAMIC ADD HASH_TYPE PARTITION_SCHEME scheme_name "
     "PARTITION=par_name [PARTITION ...] [hash_type_str] [IS_SHARD] "
     "[SHARD_NUMS shard_nums]",
     "", ""},
    {"DBSCALE DYNAMIC ADD LOAD_BALANCE DATASOURCE source_name, "
     "SERVER=server_name-min-max-low-high [SERVER...] [GROUP_ID = INTNUM]",
     "dynamic add load_balance datasource", ""},
    {"DBSCALE DYNAMIC ADD MODE_TYPE PARTITION_SCHEME scheme_name "
     "PARTITION=par_name [PARTITION ...] [NOT_SIMPLE] [IS_SHARD] [SHARD_NUMS "
     "shard_nums]",
     "", ""},
    {"DBSCALE DYNAMIC ADD MOD_PARTITION_TABLE DATASPACE \"schema.table\" "
     "PARTITION_KEY=key_name PARTITION_SCHEME=scheme_name",
     "", ""},
    {"DBSCALE DYNAMIC ADD NORMAL_TABLE DATASPACE \"schema.table\" "
     "DATASOURCE=source_name",
     "", ""},
    {"DBSCALE DYNAMIC ADD PARTITION_SCHEME scheme_name PARTITION=ds_name "
     "[PARTITION=ds_name]",
     "", "PARTITION list must bigger than 1"},
    {"DBSCALE DYNAMIC ADD PARTITION_TABLE DATASPACE schema.table "
     "[PARTERN='pattern'] PARTITION_KEY=key_name PARTITION_SCHEME=scheme_name",
     "", ""},
    {"DBSCALE DYNAMIC ADD READ_ONLY DATASOURCE source_name, "
     "server_name-min-max-low-high [GROUP_ID = INTNUM]",
     "dynamic add read_only datasource", ""},
    {"DBSCALE DYNAMIC ADD REPLICATION DATASOURCE source_name, "
     "MASTER=server_name-min-max-low-high SLAVE=server_name-min-max-low-high "
     "[SLAVE...] [GROUP_ID = INTNUM]",
     "dynamic add replication datasource", ""},
    {"DBSCALE DYNAMIC ADD RWSPLIT DATASOURCE source_name, "
     "MASTER=server_name-min-max-low-high SLAVE=server_name-min-max-low-high "
     "[SLAVE ...] [GROUP_ID = INTNUM]",
     "dynamic add rwsplit datasource", ""},
    {"DBSCALE DYNAMIC ADD SCHEMA DATASPACE schema_name DATASOURCE=source_name",
     "", ""},
    {"DBSCALE DYNAMIC ADD SERVER DATASOURCE source_name, "
     "server_name-min-max-low-high [GROUP_ID = INTNUM]",
     "dynamic add server datasource", ""},
    {"DBSCALE DYNAMIC ADD SHARE_DISK DATASOURCE source_name, "
     "ACTIVE=server_name-min-max-low-high "
     "COLD_STANDBY=server_name-min-max-low-high [GROUP_ID = INTNUM]",
     "dynamic add share_disk datasource", ""},
    {"DBSCALE DYNAMIC ADD SLAVE server-min-max-low-high TO "
     "replication_source_name",
     "", ""},
    {"DBSCALE DYNAMIC ADD SLAVE slave_source_name TO replication_source_name",
     "", ""},
    {"DBSCALE DYNAMIC ADD/REMOVE WHITE TO user1@'192.168.0.1' COMMENT 'comment "
     "info'",
     "dynamic update white ip list", ""},
    {"DBSCALE DYNAMIC REMOVE DATASERVER server_name", "remove a dataserver",
     ""},
    {"DBSCALE DYNAMIC REMOVE SCHEMA `schema_name`", "remove a schema dataspace",
     ""},
    {"DBSCALE DYNAMIC REMOVE TABLE `schema_name`.`table_name`",
     "remove a table dataspace", ""},
    {"DBSCALE DYNAMIC REMOVE PARTITION_SCHEME scheme_name", "", ""},
    {"DBSCALE DYNAMIC REMOVE SLAVE slave_name FROM master_name",
     "remove a slave from from replication datasource", ""},
    {"DBSCALE DYNAMIC CHANGE MASTER datasource_name TO new_master",
     "change replcation datasource topu online", ""},
    {"DBSCALE DYNAMIC CHANGE MULTIPLEMASTER mul_master_source ACTIVE TO "
     "mul_master_source_lb_mul_master_server",
     "", ""},
    {"DBSCALE DYNAMIC SET LOAD_BALANCE_STRATEGY = \"val\" FOR data_source_name",
     "", ""},
    {"DBSCALE DYNAMIC SET DATASOURCE datasource_name SERVER server_name WEIGHT "
     "TO intnum",
     "", ""},
    {"DBSCALE DYNAMIC SET name_or_full AUTO_INCREMENT OFFSET TO INTNUM", "",
     ""},
    {"DBSCALE DYNAMIC SET MASTER PRIORITY FOR server_name TO INTNUM", "", ""},
    {"DBSCALE ESTIMATE sql", "", ""},
    {"DBSCALE FLASHBACK DATASERVER name_or_string FORCE ONLINE",
     "set data-server online status to normal when flashback error", ""},
    {"DBSCALE FLUSH [ALL] CONFIG TO FILE [file_name]",
     "flush online dbscale config to file", ""},
    {"DBSCALE FLUSH ACL", "flush current user's ACL info, make ACL up-to-date",
     ""},
    {"DBSCALE FLUSH [FORCE] CONNECTION POOL [pool_name]",
     "flush connection pool version", ""},
    {"DBSCALE GET GLOBAL CONSISTENCE POINT", "", ""},
    {"DBSCALE MIGRATE NO_PARTITION norm_table_name TO source_name,PARTITION "
     "part_table_name vid TO source_name [vid ...],"
     "SHARD part_table_name shard_id TO source_name, SPLIT part_table_name "
     "FROM source_name TO target_source_name [target_source_name ...];",
     "", ""},
    {"DBSCALE HELP [cmd_name]", "", ""},
    {"DBSCALE REMOVE DEFAULT SESSION VARIABLES para_list",
     "remove backend mysql variables no need to maintain in dbscale", ""},
    {"DBSCALE RESET AUTH_INFO opt_user_name", "", ""},
    {"DBSCALE RESET BACKEND THREAD POOL INTNUM", "", ""},
    {"DBSCALE RESET CONNECTION POOL [pool_name] INTNUM", "", ""},
    {"DBSCALE RESET LOGIN HANDLER THREAD POOL INTNUM", "", ""},
    {"DBSCALE RESET pool_type pool_name INTNUM", "",
     "pool_type include [CONNECTION POOL|LOGIN HANDLER THREAD POOL|BACKEND "
     "THREAD POOL|HANDLER THREAD POOL]"},
    {"DBSCALE RESET TMP_TABLE", "clean cross join tmp table", ""},
    {"DBSCALE RESET ZOOKEEPER CONFIG INFO", "clean zookeeper config",
     "only for root user"},
    {"DBSCALE SET PRIORITY number TO USER BY USER_NAME user_name",
     "set user priority", "number can be 1,2,3, the priority 1 > 2 > 3"},
    {"DBSCALE SET SESSION | INSTANCE | GLOBAL option_name = val",
     "set dbscale variables online",
     "check dbscale document for which option can set online"},
    {"DBSCALE SET ACL ON SCHEMA db_name TO user_name [NO_TOUCH | READ_ONLY | "
     "WRITABLE]",
     "set ACL on db_name to user_name", ""},
    {"DBSCALE SET DEFAULT ACL TO user_name [NO_TOUCH | READ_ONLY | WRITABLE]",
     "set default ACL to a user", ""},
    {"DBSCALE RESET ACL ON SCHEMA db_name TO user_name",
     "clean ACL strategy on db_name to user_name", ""},
    {"DBSCALE RESET ACL ON ALL SCHEMA TO user_name",
     "clean all ACL strategy on all schema to user_name", ""},
    {"DBSCALE SET ACL ON TABLE db_name.table_name TO user_name [NO_TOUCH | "
     "READ_ONLY | WRITABLE]",
     "set ACL on table (db_name.table_name) to user_name", ""},
    {"DBSCALE RESET ACL ON TABLE db_name.table_name TO user_name",
     "clean ACL strategy on table (db_name.table_name) to user_name", ""},
    {"DBSCALE RESET ACL ON ALL TABLE IN SCHEMA db_name TO user_name",
     "clean all ACL strategy on all table in schema db_name to user_name", ""},
    {"DBSCALE SET SCHEMA PUSHDOWN_PROCEDURE = INT",
     "set dbscale schema pushdown procedure behavior",
     "check dbscale document for which value for INT"},
    {"DBSCALE SHOW AUDIT USER LIST", "show audit user list", ""},
    {"DBSCALE SHOW AUTO_INCREMENT OFFSET FOR name_or_full", "", ""},
    {"DBSCALE SHOW BACKEND_THREADS", "", ""},
    {"DBSCALE SHOW CATCHUP BINLOG INFO FOR DATASOURCE source_name", "", ""},
    {"DBSCALE SHOW CONNECTION POOL [server_name]", "", ""},
    {"DBSCALE SHOW DEFAULT SESSION VARIABLES para_list",
     "show the backend mysql variables which maintain in dbscale", ""},
    {"DBSCALE SHOW DATASERVERS", "", ""},
    {"DBSCALE SHOW DATASOURCE ds_name", "", ""},
    {"DBSCALE SHOW DATASOURCE TYPE = type_name", "",
     "type name amongs [server,share_disk,load-balance,rwsplit,replication]"},
    {"DBSCALE SHOW SCHEMA DATASPACE [schema_name]", "", ""},
    {"DBSCALE SHOW EXECUTEION PROFILE", "",
     "need to \'DBSCALE SET execute_profile=1\' before"},
    {"DBSCALE SHOW FAIL TRANSACTION [xid]", "", ""},
    {"DBSCALE SHOW INNODB_LOCK_WAITING STATUS", "", ""},
    {"DBSCALE SHOW JOIN STATUS {name}", "", ""},
    {"DBSCALE SHOW MIGRATE CLEAN TABLES",
     "show the tables need clean after migreate", ""},
    {"DBSCALE SHOW LOCK USAGE", "", ""},
    {"DBSCALE SHOW USER SQL COUNT [user_id]",
     "show the user dml sql execution count.", ""},
    {"DBSCALE SHOW OPTION", "show options used by dbscale",
     "DBSCALE SHOW [SESSION | INSTANCE | GLOBAL] OPTION [LIKE "
     "'\%option_name\%']"},
    {"DBSCALE SHOW PARTITION TABLE STATUS FOR table_name", "", ""},
    {"DBSCALE SHOW PARTITION TABLE table_name SHARD MAP", "", ""},
    {"DBSCALE SHOW PARTITION TABLE table_name VIRTUAL MAP", "", ""},
    {"DBSCALE SHOW PARTITION_SCHEME", "", ""},
    {"DBSCALE SHOW PARTITIONS FROM tbl_name", "", ""},
    {"DBSCALE SHOW PATH INFO", "show dbscale install path and log path", ""},
    {"DBSCALE SHOW SHARD PARTITION TABLES [part_name]", "show all shard tables",
     ""},
    {"DBSCALE SHOW STATUS", "show dbscale cluster status", ""},
    {"DBSCALE SHOW TABLE DATASPACE [ [LIKE table_name | table_name] IN SCHEMA "
     "schema_name ]",
     "show table dataspace config info", ""},
    {"DBSCALE SHOW TABLE LOCATION table_name",
     "show table's location, include host, port, data_source and so on", ""},
    {"DBSCALE SHOW THREAD POOL INFO", "", ""},
    {"DBSCALE SHOW TRANSACTION SQLS FOR user_id", "",
     "Option \'record-transaction-sqls\' need turn on"},
    {"DBSCALE SHOW USER MEMORY STATUS", "show all user's memory",
     "only useful when backend server is mariadb"},
    {"DBSCALE SHOW USER STATUS [user_id | FOR user_name | RUNNING]", "", ""},
    {"DBSCALE SHOW VERSIONS", "show dbscale version information", ""},
    {"DBSCALE SHOW VERSION CONNECTION POOL [pool_name]", "", ""},
    {"DBSCALE SHOW WARNINGS", "", ""},
    {"DBSCALE SKIP WAIT CATCHUP BINLOG THREAD FOR DATASOURCE source_name", "",
     ""},
    {"DBSCALE STOP BLOCK TABLE name_or_full", "", ""},
    {"DBSCALE [SHOW |REQUEST] SESSION ID WITH DATASERVER = server_name "
     "CONNECTION = connection_id;",
     "", ""},
    {"DBSCALE EXPLAIN original_stmt", "", ""},
    {"DBSCALE SHOW PROCESSLIST [cluster_id] USER user_id [LOCAL]", "", ""},
};
namespace dbscale {
namespace sql {
void get_aggregate_functions(record_scan *rs, list<AggregateDesc> &aggr_list);
extern void adjust_shard_schema(const char *sql_schema, const char *cur_schema,
                                string &new_schema,
                                unsigned int virtual_machine_id,
                                unsigned int partition_id);
}  // namespace sql
}  // namespace dbscale
namespace dbscale {
Schema *get_or_create_schema(string schema_name);
extern void sync_migrate_change_topo(string topic_name, string param);
Schema *get_migrate_schema(string schema_name) {
  Backend *backend = Backend::instance();
  Schema *schema = backend->find_schema(schema_name.c_str());
  if (schema == NULL) {
    LOG_ERROR("get exception dring get schema[%s], plz dynamic add it\n",
              schema_name.c_str());
    throw Error("get exception dring get schema");
  }
  return schema;
}

bool handle_tokudb_lock_timeout(Connection *conn, Packet *packet) {
  try {
    if (support_tokudb && conn && packet) {
      MySQLErrorResponse error(packet);
      error.unpack();
      if (error.get_error_code() == 1205) {
        string tokudb_last_lock_timeout;
        conn->query_for_one_value("select @@tokudb_last_lock_timeout",
                                  tokudb_last_lock_timeout, 0);
        LOG_INFO("select @@tokudb_last_lock_timeout result is :%s\n",
                 tokudb_last_lock_timeout.c_str());
        // result like below:
        //@@tokudb_last_lock_timeout: {"mysql_thread_id":6052,
        //"dbname":"./wen/t1-main", "requesting_txnid":1558,
        //"blocking_txnid":1554, "key":"0001000000"}
        int pos_start, pos_end;
        pos_start = tokudb_last_lock_timeout.find("\"blocking_txnid\":");
        if (pos_start >= 0) {
          pos_end = tokudb_last_lock_timeout.find(",", pos_start);
          string blocking_txnid = tokudb_last_lock_timeout.substr(
              pos_start + 17, pos_end - pos_start - 17);
          string sql =
              "select locks_mysql_thread_id from "
              "information_schema.TokuDB_locks where locks_trx_id=";
          sql += blocking_txnid;
          LOG_INFO("generate get lock sql:%s\n", sql.c_str());

          vector<string> vec;
          TimeValue tv(backend_sql_net_timeout);
          conn->query_for_one_column(sql.c_str(), 0, &vec, &tv, true);
          vector<string>::iterator it;
          LOG_INFO("plz execute below command:\n");
          for (it = vec.begin(); it != vec.end(); it++) {
            LOG_INFO(
                "DBSCALE SHOW SESSION ID WITH DATASERVER = %s CONNECTION = "
                "%s;\n",
                conn->get_server()->get_name(), it->c_str());
          }
          LOG_INFO(
              "plz kill the session to release lock if necessary by "
              "command:\"kill cluster_id session_id\"\n");
        }
      }
    }
  } catch (exception &e) {
    LOG_ERROR("handle_tokudb_lock_timeout get exception [%s]\n", e.what());
    return true;
  }
  return false;
}
namespace mysql {
#define MAX_PACKET_SIZE ((2 << 23) - 1)

// define the max length of string number, like max_legth=strlen("1234567890");
#define MAX_LENGTH_NUM (10)

static string find_and_replace_quota(const char *sql) {
  string tmp_sql(sql);
  size_t pos = tmp_sql.find("\"");
  while (pos != string::npos) {
    tmp_sql.replace(pos, 1, "\\\"");
    pos = tmp_sql.find("\"", pos + 2);
  }
  return tmp_sql;
}

void transfer_packet(Packet **old_packet, Packet **new_packet) {
  char *p = (*old_packet)->base();
  char *data = (*old_packet)->rd_ptr();
  size_t data_len = Packet::unpack3uint(&p) + PACKET_HEADER_SIZE;
  size_t packet_len =
      data_len > (size_t)row_packet_size ? data_len : row_packet_size;
  *new_packet = Backend::instance()->get_new_packet(packet_len);
  (*new_packet)->rewind();
  (*new_packet)->packdata(data, data_len);
  *(*new_packet)->wr_ptr() = '\0';
  delete *old_packet;
}

void pack_header(Packet *packet, size_t load_length) {
  packet->rewind();
  packet->pack3int(load_length);
  packet->packchar(0);
  packet->wr_ptr(load_length);
  *packet->wr_ptr() = '\0';
}

void check_lines_term_null(into_outfile_item *&into_outfile) {
  // if the LINES TERMINATED BY spectaror is NULL(""),
  // it will use the FIELDS TERMINATED BY replace it.
  if (into_outfile->lines_term_len == 0) {
    unsigned int length = into_outfile->fields_term_len;
    into_outfile->lines_term_len = length;
    for (unsigned int i = 0; i != length; i++) {
      into_outfile->lines_term[i] = into_outfile->fields_term[i];
    }
    into_outfile->lines_term[length] = '\0';
  }
}

ResultType get_result_type_from_column_type(MySQLColumnType col_type) {
  switch (col_type) {
    case MYSQL_TYPE_DECIMAL:
    case MYSQL_TYPE_TINY:
    case MYSQL_TYPE_SHORT:
    case MYSQL_TYPE_LONG:
    case MYSQL_TYPE_FLOAT:
    case MYSQL_TYPE_DOUBLE:
    case MYSQL_TYPE_LONGLONG:
    case MYSQL_TYPE_INT24:
    case MYSQL_TYPE_NEWDECIMAL:
      return RESULT_TYPE_NUM;
      break;

    case MYSQL_TYPE_DATE:
    case MYSQL_TYPE_TIME:
    case MYSQL_TYPE_DATETIME:
    case MYSQL_TYPE_VARCHAR:
    case MYSQL_TYPE_VAR_STRING:
    case MYSQL_TYPE_STRING:
      return RESULT_TYPE_STRING;
      break;

    default:
      throw UnSupportPartitionSQL("Column type in SelectNode is not support.");
  }
}

void rebuild_eof_with_has_more_flag(Packet *packet, MySQLDriver *driver) {
  if (driver->is_eof_packet(packet)) {
    MySQLEOFResponse eof(packet);
    eof.unpack();
    if (eof.has_more_result()) return;
    eof.set_has_more_result();
    // Packet *new_packet = new Packet(row_packet_size);
    eof.pack(packet);
    // delete tmp; *packet = new_packet;
  }
}

void rebuild_ok_with_has_more_flag(Packet *packet, MySQLDriver *driver) {
  if (driver->is_ok_packet(packet)) {
    MySQLOKResponse ok(packet);
    ok.unpack();
    if (ok.has_more_result()) return;
    ok.set_has_more_result();
    ok.pack(packet);
  }
}

void reset_packet_size(Packet *p, size_t len) {
  p->rewind();
  p->base();
  p->size(len);
}

void deal_with_str_column_value(string *str, CharsetType ctype) {
  size_t len = str->length();
  if (!len) return;

  int null_num = 0;
  for (size_t i = 0; i != len - 1; ++i) {
    if ((*str)[i + null_num] == '\0') {
      str->replace(i + null_num, 1, "\\0");
      ++null_num;
    }
  }

  if (ctype == CHARSET_TYPE_OTHER) {
    ctype = charset_type_maps[default_charset_type];
  }
  if (ctype == CHARSET_TYPE_GBK || ctype == CHARSET_TYPE_GB18030) {
    size_t i = 0;
    while (i < str->length()) {
      char c = (*str)[i];
      if ((unsigned char)c > 0x80 && (unsigned char)c < 0xFF) {
        ++i;
      } else {
        if (c == '\\' || c == '\'') {
          str->insert(i, 1, '\\');
          ++i;
        }
      }
      ++i;
    }
  } else {
    find_and_insert_str(str, "\\", "\\");
    find_and_insert_str(str, "'", "\\");
  }
}

void write_result_to_uservar(Session *session, string uservar_name,
                             const char *str, unsigned int length = 0,
                             bool is_number = false) {
  if (!str) {
    session->remove_user_var_by_name(uservar_name);
    LOG_DEBUG("Remove user variable [%s] cause it is NULL.\n",
              uservar_name.c_str());
    return;
  }

  string value(str, length);
  session->add_user_var_value(uservar_name, value, !is_number);
}

void print_warning_infos(Connection *conn) {
  if (!conn) return;
  vector<vector<string> > vec;
  try {
    conn->query_for_all_column("SHOW WARNINGS", &vec);
  } catch (...) {
    LOG_ERROR("Got exception when conn [%@] execute show warnings.\n", conn);
  }
  if (!vec.empty()) {
    string warning_info("SHOW WARNINGS;\n|Level|Code|Message|\n");
    vector<vector<string> >::iterator it = vec.begin();
    for (; it != vec.end(); it++) {
      vector<string>::iterator it1 = it->begin();
      for (; it1 != it->end(); it1++) {
        warning_info.append("|");
        warning_info.append(*it1);
      }
      warning_info.append("|\n");
    }
    LOG_INFO("%s", warning_info.c_str());
  }
}

void deal_with_var(Connection *conn, ExecutePlan *plan, MySQLDriver *driver) {
  Handler *handler = plan->handler;
  Session *session = plan->session;
  if (conn) {
    conn->reset();
    string var_sql = plan->statement->get_select_var_sql();
    if (var_sql.size() == 0) {
      return;
    }

    Packet *packet = Backend::instance()->get_new_packet(row_packet_size);
    Packet exec_packet;
    vector<string> *uservar_list = plan->statement->get_uservar_list();
    vector<string> *sessionvar_list = plan->statement->get_sessionvar_list();
    MySQLQueryRequest query(var_sql.c_str());
    query.set_sql_replace_char(
        plan->session->get_query_sql_replace_null_char());
    query.pack(&exec_packet);

    map<string, string> *conn_session_var_map = conn->get_session_var_map();
    map<string, string> *session_session_var_map =
        session->get_session_var_map();
    try {
      LOG_DEBUG("deal with var send sql [%s]\n", var_sql.c_str());
      handler->send_to_server(conn, &exec_packet);
      handler->receive_from_server(conn, packet);

      if (driver->is_error_packet(packet)) {
        LOG_ERROR("Internal select user variable sql got an error packet.\n");
        throw ErrorPacketException();
      }

      vector<bool> is_str_vec;
      vector<uint32_t> column_len;
      handler->receive_from_server(conn, packet);
      while (!driver->is_eof_packet(packet)) {
        if (driver->is_error_packet(packet)) {
          LOG_ERROR("Internal select user variable sql got an error packet.\n");
          throw ErrorPacketException();
        }
        MySQLColumnResponse column_response(packet);
        column_response.unpack();
        bool is_string = column_response.is_number() ? false : true;
        is_str_vec.push_back(is_string);
        unsigned int length = column_response.get_column_length();
        column_len.push_back(length);
        handler->receive_from_server(conn, packet);
      }

      handler->receive_from_server(conn, packet);
      while (!driver->is_eof_packet(packet)) {
        if (driver->is_error_packet(packet)) {
          LOG_ERROR("Internal select user variable sql got an error packet.\n");
          throw ErrorPacketException();
        }

        MySQLRowResponse row_response(packet);
        unsigned int user_num = uservar_list->size();
        unsigned int session_num = sessionvar_list->size();
        unsigned int fields_num = user_num + session_num;
        for (unsigned int i = 0; i != fields_num; i++) {
          if (i < user_num) {
            if (row_response.field_is_null(i)) {
              write_result_to_uservar(session, uservar_list->at(i), NULL);
            } else {
              uint64_t length;
              const char *str = row_response.get_str(i, &length);
              write_result_to_uservar(session, uservar_list->at(i), str, length,
                                      !is_str_vec[i]);
            }
          } else {
            string var_name = sessionvar_list->at(i - user_num);
            string var_value;
            if (row_response.field_is_null(i)) {
              var_value = "NULL";
              is_str_vec[i] = false;
            } else {
              uint64_t length;
              const char *str = row_response.get_str(i, &length);
              var_value = string(str, length);
            }
            if (!strcasecmp(var_name.c_str(), "CHARACTER_SET_CLIENT")) {
              int number = Backend::instance()->get_charset_number(var_value);
              if (number > 0)
                ((MySQLSession *)session)->set_client_charset(number);
            }
            if (is_str_vec[i]) {
              var_value = "'" + var_value + "'";
            }
            (*conn_session_var_map)[var_name] = var_value;
            (*session_session_var_map)[var_name] = var_value;
          }
        }
        session->set_session_var_map_md5(
            calculate_md5(session_session_var_map));
        conn->set_session_var_map_md5(calculate_md5(conn_session_var_map));
        handler->receive_from_server(conn, packet);
      }
      delete packet;
    } catch (Exception &e) {
      LOG_ERROR("Send select user variable to server got error!\n");
      delete packet;
      throw;
    }
  }
}

inline void handle_warnings_OK_and_eof_packet_inernal(
    MySQLDriver *driver, Packet *packet, Handler *handler, DataSpace *space,
    Connection *conn, bool skip_keep_conn) {
  if (!support_show_warning || skip_keep_conn ||
      (conn && conn->is_changed_user_conn()))  // do not handle keep conn for
                                               // changed_user conn
    return;
  uint16_t warnings = 0;
  if (driver->is_ok_packet(packet)) {
    MySQLOKResponse ok(packet);
    ok.unpack();
    warnings = ok.get_warnings();
  } else if (driver->is_eof_packet(packet)) {
    MySQLEOFResponse eof_response(packet);
    eof_response.unpack();
    warnings = eof_response.get_warnings();
  }
  if (!warnings) return;
  vector<Packet *> *warning_packet_list = NULL;
  uint64_t warning_packet_list_size = 0;
  bool has_add_warning_packet = false;
  Session *session = handler->get_session();
  LOG_DEBUG("session load warning count: [%Q]\n",
            session->get_load_warning_count());
  bool has_saved_too_many_warnings =
      session->get_load_warning_count() > MAX_LOAD_WARNING_PACKET_LIST_SIZE;
  try {
    if (!has_saved_too_many_warnings || support_log_warning_info)
      store_warning_packet(conn, handler, driver, &warning_packet_list,
                           has_add_warning_packet, warning_packet_list_size);
  } catch (std::exception &e) {
    LOG_ERROR("Getting warnings error: [%s]\n", e.what());
    return;
  }
  if (!has_add_warning_packet) return;
  if (support_log_warning_info) {
    MySQLResultSetHeaderResponse result_set(warning_packet_list->front());
    result_set.unpack();
    uint64_t columns_num = result_set.get_columns();
    vector<string> columns_name = vector<string>(0);
    vector<Packet *>::iterator it = warning_packet_list->begin() + 1;
    while (!driver->is_eof_packet(*it)) {
      // unpack FieldPacket
      MySQLFieldListColumnResponse field(*it);
      field.unpack();
      columns_name.push_back(string(field.get_column()));
      ++it;
    }
    ++it;
    string row_data = "query sql: ";
    row_data += handler->get_session()->get_query_sql();
    while (!driver->is_eof_packet(*it)) {
      // unpack RowPacket
      MySQLRowResponse row(*it);
      row.unpack();
      uint64_t field_len = 0;
      row_data += " [SHOW WARNING] ";
      for (unsigned int i = 0; i < columns_num; i++) {
        string cur_data = row.get_str(i, &field_len);
        cur_data = cur_data.substr(0, field_len);
        row_data += columns_name[i] + ": " + cur_data + ". ";
      }
      row_data += "\n";
      ++it;
    }
    LOG_INFO("%s", row_data.c_str());
  }
  if (!has_saved_too_many_warnings && warning_packet_list) {
    session->add_load_warning_packet_list(warning_packet_list,
                                          warning_packet_list_size);
  } else if (warning_packet_list) {
    for (vector<Packet *>::iterator it = warning_packet_list->begin();
         it != warning_packet_list->end(); ++it)
      delete (*it);
    delete warning_packet_list;
  }
  handler->get_session()->get_status()->item_inc(TIMES_WARNINGS_COUNT,
                                                 warnings);
  if (has_saved_too_many_warnings) {
    handler->get_session()->add_warning_dataspace(space, conn);
  }
  LOG_DEBUG("Get warning space %s%@\n", space->get_name(), space);
}

void handle_warnings_OK_and_eof_packet(ExecutePlan *plan, Packet *packet,
                                       Handler *handler, DataSpace *space,
                                       Connection *conn,
                                       bool skip_keep_conn = false) {
  if (plan == NULL) return;
  MySQLDriver *driver = (MySQLDriver *)plan->driver;
  handle_warnings_OK_and_eof_packet_inernal(driver, packet, handler, space,
                                            conn, skip_keep_conn);
}

inline void handle_warnings_OK_packet(Packet *packet, Handler *handler,
                                      DataSpace *space, Connection *conn,
                                      bool skip_keep_conn = false) {
  if (!support_show_warning || skip_keep_conn ||
      (conn && conn->is_changed_user_conn()))  // do not handle keep conn for
                                               // changed_user conn
    return;
  MySQLOKResponse ok(packet);
  ok.unpack();
  uint16_t warnings = ok.get_warnings();

  if (warnings) {
    handler->get_session()->get_status()->item_inc(TIMES_WARNINGS_COUNT,
                                                   warnings);
    handler->get_session()->add_warning_dataspace(space, conn);
    LOG_DEBUG("Get warning space %s%@\n", space->get_name(), space);
  }
}

/** Write the row data to the file.
 */
void MySQLIntoOutfileNode::write_into_outfile(Packet *packet,
                                              into_outfile_item *into_outfile,
                                              ofstream &file,
                                              spawn_param *param) {
  if (param && param->has_got_error()) {
    return;
  }
  string line_data;

  line_data.append(into_outfile->lines_start);

  char enclosed = into_outfile->fields_enclosed[0];
  char escaped = into_outfile->fields_escaped[0];

  MySQLRowResponse row(packet);
  unsigned int size = is_column_number.size();
  for (unsigned int i = 0; i != size; i++) {
    uint64_t row_len;
    const char *row_data = row.get_str(i, &row_len);
    bool is_null = row.field_is_null(i);
    /**
     * If the FIELDS ESCAPED BY is NULL(""), the NULL value will output as
     * "NULL", else the NULL values will output as the ESCAPED char + "N", like
     * the ESCAPED is "\\", output is "\N". But if the FIELDS TERNINATED BY is
     * NULL, it will output its length sapce.
     */
    if (is_null && !plan->statement->insert_select_via_fifo()) {
      if (into_outfile->fields_escaped_len == 0) {
        line_data.append("NULL");
      } else {
        line_data.append(into_outfile->fields_escaped);
        line_data.append("N");
      }

      if (i != size - 1) {
        line_data.append(into_outfile->fields_term);
      }

      continue;
    }

    /**
     * Process the field enclosed.
     * The output will add enclosed char before and after the column,
     * unless set the optionally and the column is a number.
     */
    bool enclosed_output_flag = false;
    if (into_outfile->fields_enclosed_len != 0 &&
        (!into_outfile->is_enclosed_optionally || !is_column_number[i])) {
      enclosed_output_flag = true;
    }

    if (enclosed_output_flag) {
      line_data.append(into_outfile->fields_enclosed);
    }

    char *escaped_str = NULL;
    if (row_len * 2 + 1 > DEFAULT_BUFFER_SIZE) {
      escaped_str = new char[row_len * 2 + 1];
    } else {
      escaped_str = row_buffer;
    }
    char *escaped_begin = escaped_str;
    const char *c = row_data;
    int index = 0;
    unsigned int row_index = 0;
    if (!is_column_number[i]) {
      // set the field escaped code, and the field is not a number
      while (row_index++ != row_len) {
        if (*c == '\0') {
          escaped_str[index++] = escaped;
          escaped_str[index++] = '0';
          c++;
        } else if (*c == escaped || *c == enclosed) {
          escaped_str[index++] = escaped;
          escaped_str[index++] = *c++;
        } else if (into_outfile->fields_enclosed_len == 0 &&
                   (*c == into_outfile->fields_term[0] ||
                    *c == into_outfile->lines_term[0])) {
          bool need_escaped = true;
          if (*c == into_outfile->fields_term[0] &&
              into_outfile->fields_term_len > 1) {
            if (row_len - row_index + 1 < into_outfile->fields_term_len)
              need_escaped = false;
            else if (strncmp((const char *)c,
                             (const char *)into_outfile->fields_term,
                             into_outfile->fields_term_len) != 0)
              need_escaped = false;
          } else if (*c == into_outfile->lines_term[0] &&
                     into_outfile->lines_term_len > 1) {
            if (row_len - row_index + 1 < into_outfile->lines_term_len)
              need_escaped = false;
            else if (strncmp((const char *)c,
                             (const char *)into_outfile->lines_term,
                             into_outfile->lines_term_len) != 0)
              need_escaped = false;
          }
          if (need_escaped) {
            escaped_str[index++] = escaped;
            escaped_str[index++] = *c++;
          } else {
            escaped_str[index++] = *c++;
          }
        } else {
          escaped_str[index++] = *c++;
        }
      }
      if (!is_null && row_len == 0 &&
          plan->statement->insert_select_via_fifo()) {  // for GOS specifically
        escaped_str[index++] = ' ';
      }
    } else {
      while (row_index++ != row_len) escaped_str[index++] = *c++;
    }
    escaped_str[index] = '\0';
    line_data.append(escaped_str);
    if (row_len * 2 + 1 > DEFAULT_BUFFER_SIZE) {
      delete[] escaped_begin;
    }

    if (enclosed_output_flag) line_data.append(into_outfile->fields_enclosed);

    if (i != size - 1) {
      line_data.append(into_outfile->fields_term);
    }
  }

  if (!is_local) {
    line_data.append(into_outfile->lines_term);
  } else {
    line_data.append(into_outfile->lines_term,
                     into_outfile->lines_term_len - 1);
  }
  if (param && param->has_got_error()) {
    return;
  }
  if (!is_local) {
    file << line_data.c_str();
  } else {
    MySQLRowResponse row_res(line_data);
    row_res.pack(packet);
  }
}

bool check_error_packet_found_rows(Packet *packet, MySQLSession *session,
                                   MySQLDriver *driver) {
  if (driver->is_error_packet(packet)) {
    LOG_ERROR("Found_rows get an error packet.\n");
    session->set_error_packet(packet);
    return true;
  }
  return false;
}

void found_rows(MySQLHandler *handler, MySQLDriver *driver, Connection *conn,
                MySQLSession *session) {
  Packet *packet = Backend::instance()->get_new_packet(row_packet_size);
  Packet exec_packet;

  conn->reset();
  MySQLQueryRequest query("SELECT FOUND_ROWS()");
  query.pack(&exec_packet);

  try {
    handler->send_to_server(conn, &exec_packet);
    handler->receive_from_server(conn, packet);

    if (check_error_packet_found_rows(packet, session, driver)) return;

    handler->receive_from_server(conn, packet);
    while (!driver->is_eof_packet(packet)) {
      if (check_error_packet_found_rows(packet, session, driver)) return;
      handler->receive_from_server(conn, packet);
    }

    handler->receive_from_server(conn, packet);
    while (!driver->is_eof_packet(packet)) {
      if (check_error_packet_found_rows(packet, session, driver)) return;
      MySQLRowResponse row(packet);
      session->add_found_rows(row.get_uint(0));
      handler->receive_from_server(conn, packet);
    }
  } catch (Exception &e) {
    LOG_ERROR("Send FOUND_ROWS() to server got error!\n");
    delete packet;
    throw;
  }
  delete packet;
}

void record_modify_server(ExecutePlan *plan, Session *session,
                          const char *server_name, unsigned int vid,
                          bool is_non_modified_conn) {
  if (session->check_for_transaction() &&
      !session->is_in_cluster_xa_transaction()) {
    stmt_type type = plan->statement->get_stmt_node()->type;
    switch (type) {
      case STMT_UPDATE:
      case STMT_DELETE:
      case STMT_INSERT:
      case STMT_REPLACE:
      case STMT_REPLACE_SELECT:
      case STMT_INSERT_SELECT:
      case STMT_LOAD: {
        if (!is_non_modified_conn)
          session->record_modify_server(server_name, vid);
        break;
      }
      default:
        break;
    }
  }
}

void record_xa_modify_sql(ExecutePlan *plan, Session *session,
                          DataSpace *dataspace, const char *sql,
                          bool is_non_modified_conn) {
  if (enable_xa_transaction &&
      session->get_session_option("close_session_xa").int_val == 0 &&
      session->check_for_transaction() &&
      !session->is_in_transaction_consistent()) {
    stmt_type type = plan->statement->get_stmt_node()->type;
    Driver *driver = Driver::get_driver();
    XA_helper *xa_helper = driver->get_xa_helper();
    switch (type) {
      case STMT_UPDATE:
      case STMT_DELETE:
      case STMT_INSERT:
      case STMT_REPLACE:
      case STMT_REPLACE_SELECT:
      case STMT_INSERT_SELECT: {
        if (!is_non_modified_conn)
          xa_helper->record_xa_redo_log(session, dataspace, sql);
        break;
      }
      case STMT_SET:
      case STMT_CHANGE_DB: {
        BackendServerVersion v =
            Backend::instance()->get_backend_server_version();
        if (!(v == MYSQL_VERSION_57 || v == MYSQL_VERSION_8)) {
          xa_helper->record_xa_redo_log(session, dataspace, sql, true);
        }
        break;
      }
      default:
        break;
    }
  }
}

void send_ok_packet_to_client(Handler *handler, uint64_t affected_rows,
                              uint16_t warnings) {
  Packet ok_packet;
  MySQLOKResponse ok(affected_rows, warnings);
  ok.pack(&ok_packet);
  handler->deal_autocommit_with_ok_eof_packet(&ok_packet);
  handler->record_affected_rows(&ok_packet);
  handler->send_to_client(&ok_packet);
}

void record_migrate_error_message(ExecutePlan *plan, Packet *packet, string msg,
                                  bool server_innodb_rollback_on_timeout) {
  if (plan->get_migrate_tool() && packet) {
    try {
      MySQLErrorResponse error(packet);
      error.unpack();
      char message[1000];
      sprintf(message, "%s %d (%s) %s.", msg.c_str(), error.get_error_code(),
              error.get_sqlstate(), error.get_error_message());
      plan->get_migrate_tool()->set_error_message(message);
      plan->get_migrate_tool()->add_select_error_not_rollback_info(
          error.get_error_code(), server_innodb_rollback_on_timeout);
    } catch (exception &e) {
      LOG_ERROR("record_migrate_error_message get exception [%s]\n", e.what());
    }
  }
}
void record_migrate_error_message(ExecutePlan *plan, string msg) {
  if (plan->get_migrate_tool()) {
    plan->get_migrate_tool()->set_error_message(msg);
  }
}

void init_column_types(list<Packet *> *field_packets,
                       vector<MySQLColumnType> &column_types,
                       unsigned int &column_num, bool *column_inited_flag) {
  if (column_inited_flag == NULL || *column_inited_flag == false) {
    list<Packet *>::iterator it_field;
    for (it_field = field_packets->begin(); it_field != field_packets->end();
         it_field++) {
      MySQLColumnResponse col_resp(*it_field);
      col_resp.unpack();
      column_types.push_back(col_resp.get_column_type());
    }

    column_num = field_packets->size();
    if (column_inited_flag) *column_inited_flag = true;
  }
}

void set_select_uservar_by_result(
    Packet *packet, vector<bool> &field_is_num_vec,
    vector<pair<unsigned int, string> > &uservar_vec,
    unsigned int select_field_num, Session *session) {
  MySQLRowResponse row_response(packet);
  unsigned int fields_num = field_is_num_vec.size();

  unsigned int j = 0;
  for (unsigned int i = 0; i != fields_num; i++) {
    if (j == uservar_vec.size()) break;
    pair<unsigned int, string> uservar_pair = uservar_vec[j];
    if (uservar_pair.first != i + select_field_num - fields_num) {
      continue;
    }

    string uservar_name = uservar_pair.second;
    if (row_response.field_is_null(i)) {
      write_result_to_uservar(session, uservar_name, NULL);
      j++;
      continue;
    }

    uint64_t length;
    const char *str = row_response.get_str(i, &length);
    write_result_to_uservar(session, uservar_name, str, length,
                            field_is_num_vec[i]);
    j++;
  }
}

void store_warning_packet(Connection *conn, Handler *handler,
                          MySQLDriver *driver,
                          vector<Packet *> **warning_packet_list,
                          bool &has_add_warning_packet,
                          uint64_t &warning_packet_list_size) {
  Packet exec_packet;
  MySQLQueryRequest show_warning_query("SHOW WARNINGS");
  show_warning_query.pack(&exec_packet);
  conn->reset();
  handler->send_to_server(conn, &exec_packet);
  // receive header
  Packet *packet = NULL;
  packet = Backend::instance()->get_new_packet();
  try {
    handler->receive_from_server(conn, packet);
    if (driver->is_error_packet(packet)) {
      MySQLErrorResponse error(packet);
      error.unpack();
      LOG_ERROR(
          "when try to fetch show warnings info, get an error packet, %d (%s) "
          "%s.\n",
          packet, error.get_error_code(), error.get_sqlstate(),
          error.get_error_message());
      throw Error("when try to fetch show warnings info, get error packet");
    }
    if (!has_add_warning_packet) {
      *warning_packet_list = NULL;
      (*warning_packet_list) = new vector<Packet *>();
      (*warning_packet_list)->push_back(packet);
      packet = NULL;
      packet = Backend::instance()->get_new_packet();
    }
    // receive column packets
    handler->receive_from_server(conn, packet);
    while (!driver->is_eof_packet(packet)) {
      if (driver->is_error_packet(packet)) {
        MySQLErrorResponse error(packet);
        error.unpack();
        LOG_ERROR(
            "when try to fetch show warnings info, get an error packet, %d "
            "(%s) "
            "%s.\n",
            packet, error.get_error_code(), error.get_sqlstate(),
            error.get_error_message());
        throw Error("when try to fetch show warnings info, get error packet");
      }
      if (!has_add_warning_packet) {
        (*warning_packet_list)->push_back(packet);
        packet = NULL;
        packet = Backend::instance()->get_new_packet();
      }
      handler->receive_from_server(conn, packet);
    }
    if (!has_add_warning_packet) {
      (*warning_packet_list)->push_back(packet);
      packet = NULL;
      packet = Backend::instance()->get_new_packet();
      has_add_warning_packet = true;
    } else {
      // remove the previous tailed eof packet in warning_packet_list, then
      // append more warning packet
      Packet *tmp_pkt = (*warning_packet_list)->back();
      (*warning_packet_list)->pop_back();
      delete tmp_pkt;
    }
    // receive row packets
    handler->receive_from_server(conn, packet);
    while (!driver->is_eof_packet(packet)) {
      if (driver->is_error_packet(packet)) {
        MySQLErrorResponse error(packet);
        error.unpack();
        LOG_ERROR(
            "when try to fetch show warnings info, get an error packet, %d "
            "(%s) "
            "%s.\n",
            packet, error.get_error_code(), error.get_sqlstate(),
            error.get_error_message());
        throw Error("when try to fetch show warnings info, get error packet");
      }
      if (load_data_quick_error > 0 &&
          handler->get_session()->get_top_stmt()->get_stmt_node()->type ==
              STMT_LOAD) {
        const char *field_data = NULL;
        uint64_t row_len = 0;
        MySQLRowResponse response(packet);
        // Level, Code, Message
        field_data = response.get_str(0, &row_len);
        string level = response.field_is_null(0) ? string("")
                                                 : string(field_data, row_len);
        field_data = response.get_str(1, &row_len);
        string code = response.field_is_null(1) ? string("")
                                                : string(field_data, row_len);
        field_data = response.get_str(2, &row_len);
        string message = response.field_is_null(2)
                             ? string("")
                             : string(field_data, row_len);
        LOG_ERROR(
            "got error when load data: Level:[%s], Code:[%s], Message:[%s]\n",
            level.c_str(), code.c_str(), message.c_str());
        LOG_ERROR(
            "load_data_quick_error is enabled, cancel current load data task, "
            "this will lead client broken.\n");
        throw Error(
            "got error when load data, load_data_quick_error is enabled, "
            "cancel "
            "current load data task, this will lead client broken, see log for "
            "more infomation");
      }
#ifdef DEBUG
      LOG_DEBUG("store one warning packet during load data.\n");
#endif
      (*warning_packet_list)->push_back(packet);
      packet = NULL;
      warning_packet_list_size++;
      packet = Backend::instance()->get_new_packet();
      handler->receive_from_server(conn, packet);
    }
#ifndef DBSCALE_TEST_DISABLE
    dbscale_test_info *test_info =
        handler->get_session()->get_dbscale_test_info();
    if (test_info->test_case_name == "show warnings" &&
        test_info->test_case_operation == "test memory leak") {
      throw ExecuteNodeError("Show warnings Fake Error!");
    }
#endif
    (*warning_packet_list)->push_back(packet);
  } catch (...) {
    delete packet;
    if (*warning_packet_list) {
      vector<Packet *>::iterator it = (*warning_packet_list)->begin();
      for (; it != (*warning_packet_list)->end(); ++it) {
        delete *it;
      }
      delete *warning_packet_list;
      *warning_packet_list = NULL;
    }
    throw;
  }
}

size_t check_first_row_complete(Packet *packet, string field_terminate,
                                string line_terminate, bool has_field_enclose,
                                char field_enclose, char field_escape,
                                unsigned int table_fields_num) {
  ACE_UNUSED_ARG(table_fields_num);
  const char *pos = packet->base() + PACKET_HEADER_SIZE;
  const char *end = packet->base() + packet->length();
  const char *start = pos;
#ifdef DEBUG
  unsigned int fields_count = 0;
#endif
  bool is_char_escaped = false;
  bool need_enclose = false;
  if (has_field_enclose && field_enclose == *start) need_enclose = true;
  bool enclose_field_end = false;
  int field_term_len = field_terminate.length();
  int line_term_len = line_terminate.length();
  while (pos != end) {
    if (is_char_escaped) {
      is_char_escaped = false;
      pos++;
      continue;
    }

    if (*pos == line_terminate[0]) {
      if (line_term_len == 1) {
        break;
      } else if (end - pos < line_term_len) {
        pos = end;
        break;
      } else if (line_terminate == string(pos, line_term_len)) {
        pos += line_term_len - 1;
        break;
      }
    }

    if (*pos == field_escape) {
      is_char_escaped = true;
      pos++;
      continue;
    }

    if (need_enclose && !enclose_field_end && *pos == field_enclose &&
        pos != start) {
      enclose_field_end = true;
      pos++;
      continue;
    }

    if (enclose_field_end && *pos == field_enclose) {
    } else if (*pos == field_terminate[0]) {
      if (need_enclose && !enclose_field_end) {
        pos++;
        continue;
      }
      if (end - pos < field_term_len) {
        pos++;
        continue;
      }
      if (field_term_len != 1 &&
          field_terminate != string(pos, field_term_len)) {
        pos++;
        continue;
      } else {
        pos += field_term_len - 1;
      }
#ifdef DEBUG
      fields_count++;
#endif
      start = pos + 1;
      if (has_field_enclose && *start == field_enclose) {
        need_enclose = true;
        enclose_field_end = false;
      } else {
        need_enclose = false;
      }
    }
    pos++;
    enclose_field_end = false;
  }
  if (pos == end) {
    /*This packet does not reach the end of packet, so we need to read one
     * more packet.*/
    return packet->length();
  }

#ifdef DEBUG
  if (table_fields_num) ACE_ASSERT(fields_count <= table_fields_num);
#endif
  // TODO: return 0, if it contains the complete row in issue #3675
  /*
  if (*pos == line_terminate && fields_count == table_fields_num - 1)
    return 0;
  */
  return pos - (packet->base() + PACKET_HEADER_SIZE) + 1;
}

void divide_packet_by_first_row(Packet *packet, size_t first_row_len,
                                Packet **first_row_packet,
                                Packet **rest_row_packet) {
  Packet *pf = Backend::instance()->get_new_packet(first_row_len +
                                                   PACKET_HEADER_SIZE + 1);
  memcpy(pf->base() + PACKET_HEADER_SIZE, packet->base() + PACKET_HEADER_SIZE,
         first_row_len);
  char *p = packet->base();
  size_t len = Packet::unpack3uint(&p);
  Packet *pr = Backend::instance()->get_new_packet(len - first_row_len +
                                                   PACKET_HEADER_SIZE + 1);
  memcpy(pr->base() + PACKET_HEADER_SIZE,
         packet->base() + PACKET_HEADER_SIZE + first_row_len,
         len - first_row_len);
  *first_row_packet = pf;
  *rest_row_packet = pr;

  pf->rewind();
  pr->rewind();

  pack_header(pf, first_row_len);
  pack_header(pr, len - first_row_len);

#ifdef DEBUG
  LOG_DEBUG(
      "Find a divided packet with first incomplete row pos %d, reset packet "
      "len %d for pacekt %@.\n",
      first_row_len, len - first_row_len, packet);
#endif
  if (len == first_row_len) {
    LOG_DEBUG("Find a one row packet, should not divide it.\n");
    delete pf;
    delete pr;
    *first_row_packet = NULL;
    *rest_row_packet = NULL;
  }
}

void pack_packet(Packet *packet, string data, unsigned int len) {
  packet->size(len + PACKET_HEADER_SIZE + 1);
  try {
    memcpy(packet->base() + PACKET_HEADER_SIZE, data.c_str(), len);
  } catch (...) {
    LOG_ERROR("pack_packet get exception for pack_packet.\n");
    throw;
  }
  pack_header(packet, len);
}

void redo_load(Connection *load_conn, string sql, Handler *handler,
               MySQLDriver *driver, uint64_t &affected_row_count,
               uint64_t &warning_count, uint64_t &warning_packet_list_size,
               vector<Packet *> **warning_packet_list,
               bool &has_add_warning_packet) {
  LOG_DEBUG("redo_load\n");
  // send empty to tell server load end
  TimeValue timeout =
      TimeValue(redo_load_timeout > 0 ? redo_load_timeout : UINT_MAX, 0);
  Packet *packet = Backend::instance()->get_new_packet();
  pack_packet(packet, "", 0);
  handler->send_to_server(load_conn, packet);
  handler->receive_from_server(load_conn, packet, &timeout);
  if (driver->is_error_packet(packet)) {
    MySQLErrorResponse error(packet);
    error.unpack();
    LOG_ERROR("redo_load get an error packet %d (%s) %s.\n",
              error.get_error_code(), error.get_sqlstate(),
              error.get_error_message());
    char err_msg[256];
    sprintf(err_msg, "redo_load get an error packet, %d (%s) %s.\n",
            error.get_error_code(), error.get_sqlstate(),
            error.get_error_message());

    if (packet) {
      delete packet;
    }
    throw Error(err_msg);
  }

  MySQLOKResponse ok(packet);
  ok.unpack();
  uint64_t warnings = ok.get_warnings();
  uint64_t affected_rows = ok.get_affected_rows();
  affected_row_count += affected_rows;
  warning_count += warnings;
  if (support_show_warning &&
      (warning_packet_list_size < MAX_LOAD_WARNING_PACKET_LIST_SIZE) &&
      warnings)
    store_warning_packet(load_conn, handler, driver, warning_packet_list,
                         has_add_warning_packet, warning_packet_list_size);
  load_conn->reset();

  // do load again
  MySQLQueryRequest query(sql.c_str());
  query.set_sql_replace_char(
      handler->get_session()->get_query_sql_replace_null_char());
  Packet exec_query;
  query.pack(&exec_query);
  handler->send_to_server(load_conn, &exec_query);
  handler->receive_from_server(load_conn, packet, &timeout);
  if (driver->is_error_packet(packet)) {
    MySQLErrorResponse error(packet);
    error.unpack();
    LOG_ERROR("redo_load get an error packet %d (%s) %s.\n",
              error.get_error_code(), error.get_sqlstate(),
              error.get_error_message());
    char err_msg[256];
    sprintf(err_msg, "redo_load get an error packet, %d (%s) %s.\n",
            error.get_error_code(), error.get_sqlstate(),
            error.get_error_message());

    if (packet) {
      delete packet;
    }
    throw Error(err_msg);
  }
  if (packet) {
    delete packet;
  }
}

/* class MySQLExecutePlan */

MySQLExecutePlan::MySQLExecutePlan(Statement *statement, Session *session,
                                   Driver *driver, Handler *handler)
    : ExecutePlan(statement, session, driver, handler) {}

ExecuteNode *MySQLExecutePlan::do_get_fetch_node(DataSpace *dataspace,
                                                 const char *sql) {
  return new MySQLFetchNode(this, dataspace, sql);
}
ExecuteNode *MySQLExecutePlan::do_get_navicat_profile_sql_node() {
  return new MySQLNavicateProfileSqlNode(this);
}

#ifndef DBSCALE_DISABLE_SPARK
ExecuteNode *MySQLExecutePlan::do_get_spark_node(SparkConfigParameter config,
                                                 bool is_insert) {
  return new MySQLSparkNode(this, config, is_insert);
}
#endif

ExecuteNode *MySQLExecutePlan::do_get_send_node() {
  return new MySQLSendNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_dispatch_packet_node(TransferRule *rule) {
  return new MySQLDispatchPacketNode(this, rule);
}

ExecuteNode *MySQLExecutePlan::do_get_into_outfile_node() {
  return new MySQLIntoOutfileNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_modify_limit_node(
    record_scan *rs, PartitionedTable *table, vector<unsigned int> &par_ids,
    unsigned int limit, string &sql) {
  return new MySQLModifyLimitNode(this, rs, table, par_ids, limit, sql);
}

ExecuteNode *MySQLExecutePlan::do_get_select_into_node() {
  return new MySQLSelectIntoNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_sort_node(list<SortDesc> *sort_desc) {
  return new MySQLSortNode(this, sort_desc);
}

ExecuteNode *MySQLExecutePlan::do_get_project_node(unsigned int skip_columns) {
  return new MySQLProjectNode(this, skip_columns);
}

ExecuteNode *MySQLExecutePlan::do_get_aggr_node(
    list<AggregateDesc> &aggregate_desc) {
  return new MySQLAggregateNode(this, aggregate_desc);
}

ExecuteNode *MySQLExecutePlan::do_get_group_node(
    list<SortDesc> *group_desc, list<AggregateDesc> &aggregate_desc) {
  ExecuteNode *node = new MySQLGroupNode(this, group_desc, aggregate_desc);
#ifdef DEBUG
  LOG_DEBUG("in MySQLExecutePlan::do_get_group_node(), get node %@\n", node);
#endif
  return node;
}

ExecuteNode *MySQLExecutePlan::do_get_dbscale_wise_group_node(
    list<SortDesc> *group_desc, list<AggregateDesc> &aggregate_desc) {
  return new MySQLWiseGroupNode(this, group_desc, aggregate_desc);
}

ExecuteNode *MySQLExecutePlan::do_get_dbscale_pages_node(
    list<SortDesc> *group_desc, list<AggregateDesc> &aggregate_desc,
    int page_size) {
  return new MySQLPagesNode(this, group_desc, aggregate_desc, page_size);
}

ExecuteNode *MySQLExecutePlan::do_get_single_sort_node(
    list<SortDesc> *sort_desc) {
  return new MySQLSingleSortNode(this, sort_desc);
}

ExecuteNode *MySQLExecutePlan::do_get_direct_execute_node(DataSpace *dataspace,
                                                          const char *sql) {
  return new MySQLDirectExecuteNode(this, dataspace, sql);
}

ExecuteNode *MySQLExecutePlan::do_get_show_warning_node() {
  return new MySQLShowWarningNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_show_load_warning_node() {
  return new MySQLShowLoadWarningNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_query_one_column_node(
    DataSpace *dataspace, const char *sql, bool only_one_row) {
  return new MySQLQueryForOneColumnNode(this, dataspace, sql, only_one_row);
}
ExecuteNode *MySQLExecutePlan::do_get_query_mul_column_node(
    DataSpace *dataspace, const char *sql) {
  return new MySQLQueryForMulColumnNode(this, dataspace, sql);
}
ExecuteNode *MySQLExecutePlan::do_get_query_one_column_aggr_node(
    DataSpace *dataspace, const char *sql, bool get_min) {
  return new MySQLQueryForOneColumnAggrNode(this, dataspace, sql, get_min);
}
ExecuteNode *MySQLExecutePlan::do_get_query_exists_node(DataSpace *dataspace,
                                                        const char *sql) {
  return new MySQLQueryExistsNode(this, dataspace, sql);
}

ExecuteNode *MySQLExecutePlan::do_get_transaction_unlock_node(
    const char *sql, bool is_send_to_client) {
  return new MySQLTransactionUNLockNode(this, sql, is_send_to_client);
}

ExecuteNode *MySQLExecutePlan::do_get_xatransaction_node(
    const char *sql, bool is_send_to_client) {
  return new MySQLXATransactionNode(this, sql, is_send_to_client);
}

ExecuteNode *MySQLExecutePlan::do_get_cluster_xatransaction_node() {
  return new MySQLClusterXATransactionNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_avg_node(list<AvgDesc> &avg_list) {
  return new MySQLAvgNode(this, avg_list);
}
ExecuteNode *MySQLExecutePlan::do_get_lock_node(
    map<DataSpace *, list<lock_table_item *> *> *lock_table_list) {
  return new MySQLLockNode(this, lock_table_list);
}

ExecuteNode *MySQLExecutePlan::do_get_migrate_node() {
  return new MySQLMigrateNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_limit_node(long offset, long num) {
  return new MySQLLimitNode(this, offset, num);
}

ExecuteNode *MySQLExecutePlan::do_get_ok_node() {
  return new MySQLOKNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_drop_mul_table_node() {
  return new MySQLDropMulTableNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_ok_merge_node() {
  return new MySQLOKMergeNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_modify_node(DataSpace *dataspace,
                                                  const char *sql,
                                                  bool re_parse_shard) {
  MySQLModifyNode *modify_node = new MySQLModifyNode(this, dataspace);
  if (sql) {
    if (re_parse_shard) {
      modify_node->add_sql_during_exec(sql);
    } else {
      modify_node->add_sql(sql);
    }
    modify_node->add_sql("");
  }
  return modify_node;
}

ExecuteNode *MySQLExecutePlan::do_get_modify_select_node(
    const char *modify_sql, vector<string *> *columns, bool can_quick,
    vector<ExecuteNode *> *nodes, bool is_replace_set_value) {
  MySQLModifySelectNode *node = new MySQLModifySelectNode(
      this, modify_sql, columns, nodes, can_quick, is_replace_set_value);
  return node;
}

ExecuteNode *MySQLExecutePlan::do_get_insert_select_node(
    const char *modify_sql, vector<ExecuteNode *> *nodes) {
  MySQLInsertSelectNode *node =
      new MySQLInsertSelectNode(this, modify_sql, nodes);
  return node;
}

ExecuteNode *MySQLExecutePlan::do_get_par_modify_select_node(
    const char *modify_sql, PartitionedTable *par_table,
    PartitionMethod *method, vector<string *> *columns, bool can_quick,
    vector<ExecuteNode *> *nodes) {
  MySQLPartitionModifySelectNode *node = new MySQLPartitionModifySelectNode(
      this, modify_sql, nodes, par_table, method, columns, can_quick);
  return node;
}
ExecuteNode *MySQLExecutePlan::do_get_par_insert_select_node(
    const char *modify_sql, PartitionedTable *par_table,
    PartitionMethod *method, vector<unsigned int> &key_pos,
    vector<ExecuteNode *> *nodes, const char *schema_name,
    const char *table_name, bool is_duplicated) {
  MySQLPartitionInsertSelectNode *node = new MySQLPartitionInsertSelectNode(
      this, modify_sql, nodes, par_table, method, key_pos, schema_name,
      table_name, is_duplicated);
  return node;
}

ExecuteNode *MySQLExecutePlan::do_get_load_local_node(DataSpace *dataspace,
                                                      const char *sql) {
  return new MySQLLoadLocalNode(this, dataspace, sql);
}

ExecuteNode *MySQLExecutePlan::do_get_load_local_external_node(
    DataSpace *dataspace, DataServer *dataserver, const char *sql) {
  return new MySQLLoadLocalExternal(this, dataspace, dataserver, sql);
}

ExecuteNode *MySQLExecutePlan::do_get_load_data_infile_external_node(
    DataSpace *dataspace, DataServer *dataserver) {
  return new MySQLLoadDataInfileExternal(this, dataspace, dataserver);
}

ExecuteNode *MySQLExecutePlan::do_get_kill_node(int cluster_id, uint32_t kid) {
  return new MySQLKillNode(this, cluster_id, kid);
}

ExecuteNode *MySQLExecutePlan::do_get_load_local_part_table_node(
    PartitionedTable *dataspace, const char *sql, const char *schema_name,
    const char *table_name) {
  return new MySQLLoadLocalPartTableNode(this, dataspace, sql, schema_name,
                                         table_name);
}
ExecuteNode *MySQLExecutePlan::do_get_load_data_infile_part_table_node(
    PartitionedTable *dataspace, const char *sql, const char *schema_name,
    const char *table_name) {
  return new MySQLLoadDataInfilePartTableNode(this, dataspace, sql, schema_name,
                                              table_name);
}

ExecuteNode *MySQLExecutePlan::do_get_load_data_infile_node(
    DataSpace *dataspace, const char *sql) {
  return new MySQLLoadDataInfileNode(this, dataspace, sql);
}

ExecuteNode *MySQLExecutePlan::do_get_load_select_node(const char *schema_name,
                                                       const char *table_name) {
  return new MySQLLoadSelectNode(this, schema_name, table_name);
}

ExecuteNode *MySQLExecutePlan::do_get_load_select_partition_node(
    const char *schema_name, const char *table_name) {
  return new MySQLLoadSelectPartitionNode(this, schema_name, table_name);
}

ExecuteNode *MySQLExecutePlan::do_get_distinct_node(list<int> column_indexes) {
  return new MySQLDistinctNode(this, column_indexes);
}
ExecuteNode *MySQLExecutePlan::do_get_rows_node(
    list<list<string> *> *row_list) {
  return new MySQLRowsNode(this, row_list);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_cluster_shutdown_node(
    int cluster_id) {
  return new MySQLDBScaleClusterShutdownNode(this, cluster_id);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_request_cluster_id_node() {
  return new MySQLDBScaleRequestClusterIdNode(this);
}
ExecuteNode *
MySQLExecutePlan::do_get_dbscale_request_all_cluster_inc_info_node() {
  return new MySQLDBScaleRequestAllClusterIncInfoNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_request_cluster_info_node() {
  return new MySQLDBScaleRequestClusterInfoNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_request_cluster_user_status_node(
    const char *id, bool only_show_running, bool show_status_count) {
  return new MySQLDBScaleRequestUserStatusNode(this, id, only_show_running,
                                               show_status_count);
}

ExecuteNode *MySQLExecutePlan::do_get_dbscale_clean_temp_table_cache_node() {
  return new MySQLDBScaleCleanTempTableCacheNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_dbscale_request_node_info_node() {
  return new MySQLDBScaleRequestNodeInfoNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_request_cluster_inc_info_node(
    PartitionedTable *space, const char *schema_name, const char *table_name) {
  return new MySQLDBScaleRequestClusterIncInfoNode(this, space, schema_name,
                                                   table_name);
}
ExecuteNode *MySQLExecutePlan::do_get_regex_filter_node(
    enum FilterFlag filter_way, int column_index, list<const char *> *pattern) {
  return new MySQLRegexFilterNode(this, filter_way, column_index, pattern);
}
ExecuteNode *MySQLExecutePlan::do_get_expr_filter_node(Expression *expr) {
  return new MySQLExprFilterNode(this, expr);
}
ExecuteNode *MySQLExecutePlan::do_get_expr_calculate_node(
    list<SelectExprDesc> &select_expr_list) {
  return new MySQLExprCalculateNode(this, select_expr_list);
}
ExecuteNode *MySQLExecutePlan::do_get_avg_show_table_status_node() {
  return new MySQLAvgShowTableStatusNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_show_data_source_node(
    list<const char *> &names, bool need_show_weight) {
  return new MySQLDBScaleShowDataSourceNode(this, names, need_show_weight);
}
ExecuteNode *MySQLExecutePlan::do_get_dynamic_configuration_node() {
  return new MySQLDynamicConfigurationNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_rep_strategy_node() {
  return new MySQLRepStrategyNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_set_server_weight_node() {
  return new MySQLDynamicSetServerWeightNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_show_option_node() {
  return new MySQLDBScaleShowOptionNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_show_dynamic_option_node() {
  return new MySQLDBScaleShowDynamicOptionNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_change_startup_config_node() {
  return new MySQLChangeStartupConfigNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_show_changed_startup_config_node() {
  return new MySQLShowChangedStartupConfigNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dynamic_add_data_server_node(
    dynamic_add_data_server_op_node *dynamic_add_data_server_oper) {
  return new MySQLDynamicAddDataServerNode(this, dynamic_add_data_server_oper);
}
ExecuteNode *MySQLExecutePlan::do_get_dynamic_add_data_source_node(
    dynamic_add_data_source_op_node *dynamic_add_data_source_oper,
    DataSourceType type) {
  return new MySQLDynamicAddDataSourceNode(this, dynamic_add_data_source_oper,
                                           type);
}
ExecuteNode *MySQLExecutePlan::do_get_dynamic_add_data_space_node(
    dynamic_add_data_space_op_node *dynamic_add_data_space_oper) {
  return new MySQLDynamicAddDataSpaceNode(this, dynamic_add_data_space_oper);
}
ExecuteNode *MySQLExecutePlan::do_get_set_schema_acl_node(
    set_schema_acl_op_node *set_schema_acl_oper) {
  return new MySQLSetSchemaACLNode(this, set_schema_acl_oper);
}
ExecuteNode *MySQLExecutePlan::do_get_set_user_not_allow_operation_time_node(
    set_user_not_allow_operation_time_node *oper) {
  return new MySQLSetUserAllowOperationTimeNode(this, oper);
}
ExecuteNode *MySQLExecutePlan::do_get_reload_user_not_allow_operation_time_node(
    string user_name) {
  return new MySQLReloadUserAllowOperationTimeNode(this, user_name);
}
ExecuteNode *MySQLExecutePlan::do_get_show_user_not_allow_operation_time_node(
    string user_name) {
  return new MySQLShowUserNotAllowOperationTimeNode(this, user_name);
}
ExecuteNode *MySQLExecutePlan::do_get_set_table_acl_node(
    set_table_acl_op_node *set_table_acl_oper) {
  return new MySQLSetTableACLNode(this, set_table_acl_oper);
}
ExecuteNode *MySQLExecutePlan::do_get_dynamic_add_slave_node(
    dynamic_add_slave_op_node *dynamic_add_slave_oper) {
  return new MySQLDynamicAddSlaveNode(this, dynamic_add_slave_oper);
}
ExecuteNode *MySQLExecutePlan::do_get_reset_tmp_table_node() {
  return new MySQLResetTmpTableNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_reload_function_type_node() {
  return new MySQLLoadFuncTypeNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_shutdown_node() {
  return new MySQLShutDownNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dynamic_change_master_node(
    dynamic_change_master_op_node *dynamic_change_master_oper) {
  return new MySQLDynamicChangeMasterNode(this, dynamic_change_master_oper);
}
ExecuteNode *
MySQLExecutePlan::do_get_dynamic_change_multiple_master_active_node(
    dynamic_change_multiple_master_active_op_node
        *dynamic_change_multiple_master_active_oper) {
  return new MySQLDynamicChangeMultipleMasterActiveNode(
      this, dynamic_change_multiple_master_active_oper);
}
ExecuteNode *MySQLExecutePlan::do_get_dynamic_change_dataserver_ssh_node(
    const char *server_name, const char *username, const char *pwd, int port) {
  return new MySQLDynamicChangeDataServerSShNode(this, server_name, username,
                                                 pwd, port);
}
ExecuteNode *MySQLExecutePlan::do_get_dynamic_remove_slave_node(
    dynamic_remove_slave_op_node *dynamic_remove_slave_oper) {
  return new MySQLDynamicRemoveSlaveNode(this, dynamic_remove_slave_oper);
}
ExecuteNode *MySQLExecutePlan::do_get_dynamic_remove_schema_node(
    const char *schema_name, bool is_force) {
  return new MySQLDynamicRemoveSchemaNode(this, schema_name, is_force);
}
ExecuteNode *MySQLExecutePlan::do_get_dynamic_remove_table_node(
    const char *table_name, bool is_force) {
  return new MySQLDynamicRemoveTableNode(this, table_name, is_force);
}
ExecuteNode *MySQLExecutePlan::do_get_dynamic_change_table_scheme_node(
    const char *table_name, const char *scheme_name) {
  return new MySQLDynamicChangeTableSchemeNode(this, table_name, scheme_name);
}
ExecuteNode *MySQLExecutePlan::do_get_dynamic_remove_node(const char *name) {
  return new MySQLDynamicRemoveOPNode(this, name);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_help_node(const char *name) {
  return new MySQLDBScaleHelpNode(this, name);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_audit_user_list_node() {
  return new MySQLDBScaleShowAuditUserListNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_slow_sql_top_n_node() {
  return new MySQLDBScaleShowSlowSqlTopNNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_request_slow_sql_top_n_node() {
  return new MySQLDBScaleRequestSlowSqlTopNNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_join_node(const char *name) {
  return new MySQLDBScaleShowJoinNode(this, name);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_shard_partition_node(
    const char *name) {
  return new MySQLDBScaleShowShardPartitionNode(this, name);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_rebalance_work_load_node(
    const char *name, list<string> sources, const char *schema_name,
    int is_remove) {
  return new MySQLDBScaleShowRebalanceWorkLoadNode(this, name, sources,
                                                   schema_name, is_remove);
}
ExecuteNode *MySQLExecutePlan::do_get_show_user_memory_status_node() {
  return new MySQLDBScaleShowUserMemoryStatusNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_show_user_status_node(
    const char *user_id, const char *user_name, bool only_show_running,
    bool instance, bool show_status_count) {
  return new MySQLDBScaleShowUserStatusNode(
      this, user_id, user_name, only_show_running, instance, show_status_count);
}
ExecuteNode *MySQLExecutePlan::do_get_show_user_processlist_node(
    const char *cluster_id, const char *user_id, int local) {
  return new MySQLDBScaleShowUserProcesslistNode(this, cluster_id, user_id,
                                                 local);
}
ExecuteNode *MySQLExecutePlan::do_get_show_session_id_node(
    const char *server_name, int connection_id) {
  return new MySQLDBScaleShowSessionIdNode(this, server_name, connection_id);
}
ExecuteNode *MySQLExecutePlan::do_get_backend_server_execute_node(
    const char *stmt_sql) {
  return new MySQLDBScaleBackendServerExecuteNode(this, stmt_sql);
}
ExecuteNode *MySQLExecutePlan::do_get_execute_on_all_masterserver_execute_node(
    const char *stmt_sql) {
  return new MySQLDBScaleExecuteOnAllMasterserverExecuteNode(this, stmt_sql);
}
ExecuteNode *MySQLExecutePlan::do_get_show_table_location_node(
    const char *schema_name, const char *table_name) {
  return new MySQLDBScaleShowTableLocationNode(this, schema_name, table_name);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_erase_auth_info_node(
    name_item *username_list, bool all_dbscale_node) {
  return new MySQLDBScaleEraseAuthInfoNode(this, username_list,
                                           all_dbscale_node);
}
ExecuteNode *MySQLExecutePlan::do_get_dynamic_update_white_node(
    bool is_add, const char *ip, const ssl_option_struct &ssl_option_value,
    const char *comment, const char *user_name) {
  return new MySQLDBScaleDynamicUpdateWhiteNode(
      this, is_add, ip, ssl_option_value, comment, user_name);
}
ExecuteNode *MySQLExecutePlan::do_get_show_monitor_point_status_node() {
  return new MySQLShowMonitorPointStatusNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_show_global_monitor_point_status_node() {
  return new MySQLShowGlobalMonitorPointStatusNode(this);
}
ExecuteNode *
MySQLExecutePlan::do_get_show_histogram_monitor_point_status_node() {
  return new MySQLShowHistogramMonitorPointStatusNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_show_outline_monitor_info_node() {
  return new MySQLShowOutlineMonitorInfoNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_create_outline_hint_node(
    dbscale_operate_outline_hint_node *oper) {
  return new MySQLDBScaleCreateOutlineHintNode(this, oper);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_flush_outline_hint_node(
    dbscale_operate_outline_hint_node *oper) {
  return new MySQLDBScaleFlushOutlineHintNode(this, oper);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_outline_hint_node(
    dbscale_operate_outline_hint_node *oper) {
  return new MySQLDBScaleShowOutlineHintNode(this, oper);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_delete_outline_hint_node(
    dbscale_operate_outline_hint_node *oper) {
  return new MySQLDBScaleDeleteOutlineHintNode(this, oper);
}
ExecuteNode *MySQLExecutePlan::do_get_com_query_prepare_node(
    DataSpace *dataspace, const char *query_sql, const char *name,
    const char *sql) {
  return new MySQLComQueryPrepareNode(this, dataspace, query_sql, name, sql);
}
ExecuteNode *MySQLExecutePlan::do_get_com_query_exec_prepare_node(
    const char *name, var_item *var_item_list) {
  return new MySQLComQueryExecPrepareNode(this, name, var_item_list);
}
ExecuteNode *MySQLExecutePlan::do_get_com_query_drop_prepare_node(
    DataSpace *dataspace, const char *name, const char *prepare_sql) {
  return new MySQLComQueryDropPrepareNode(this, dataspace, name, prepare_sql);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_flush_config_to_file_node(
    const char *file_name, bool flush_all) {
  return new MySQLDBScaleFlushConfigToFileNode(this, file_name, flush_all);
}

ExecuteNode *MySQLExecutePlan::do_get_dbscale_flush_table_info_node(
    const char *schema_name, const char *table_name) {
  return new MySQLDBScaleFlushTableInfoNode(this, schema_name, table_name);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_flush_acl_node() {
  return new MySQLDBScaleFlushACLNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_global_consistence_point_node() {
  return new MySQLDBScaleGlobalConsistencePointNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_show_pool_info_node() {
  return new MySQLDBScaleShowPoolInfoNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_show_pool_version_node() {
  return new MySQLDBScaleShowPoolVersionNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_set_pool_info_node(
    pool_node *pool_info) {
  return new MySQLResetPoolInfoNode(this, pool_info);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_reset_info_plan(
    dbscale_reset_info_op_node *oper) {
  return new MySQLResetInfoPlanNode(this, oper);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_block_node() {
  return new MySQLBlockNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_disable_server_node() {
  return new MySQLDisableServer(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_check_table_node() {
  return new MySQLCheckTableNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_check_disk_io_node() {
  return new MySQLCheckDiskIONode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_check_metadata() {
  return new MySQLCheckMetaDataNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_flush_pool_info_node(
    pool_node *pool_info) {
  return new MySQLFlushPoolInfoNode(this, pool_info);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_force_flashback_online_node(
    const char *name) {
  return new MySQLForceFlashbackOnlineNode(this, name);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_xa_recover_slave_dbscale_node(
    const char *xa_source, const char *top_source, const char *ka_update_v) {
  return new MySQLXARecoverSlaveDBScaleNode(this, xa_source, top_source,
                                            ka_update_v);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_purge_connection_pool_node(
    pool_node *pool_info) {
  return new MySQLPurgePoolInfoNode(this, pool_info);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_flush_node(
    dbscale_flush_type type) {
  return new MySQLFlushNode(this, type);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_flush_weak_pwd_node() {
  return new MySQLFlushWeekPwdFileNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_reload_config_node(
    const char *sql) {
  return new MySQLReloadConfigNode(this, sql);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_set_priority_info_node(
    string user_name, int tmp_priority_value) {
  return new MySQLSetPriorityNode(this, user_name, tmp_priority_value);
}
ExecuteNode *MySQLExecutePlan::do_get_show_engine_lock_waiting_node(
    int engine_type) {
  return new MySQLShowEngineLockWaitingNode(this, engine_type);
}
ExecuteNode *MySQLExecutePlan::do_get_show_dataserver_node() {
  return new MySQLDBScaleShowDataServerNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_show_backend_threads_node() {
  return new MySQLDBScaleShowBackendThreadsNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_show_partition_scheme_node() {
  return new MySQLDBScaleShowPartitionSchemeNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_show_schema_node(const char *schema) {
  return new MySQLDBScaleShowSchemaNode(this, schema);
}
ExecuteNode *MySQLExecutePlan::do_get_show_table_node(const char *schema,
                                                      const char *table,
                                                      bool use_like) {
  return new MySQLDBScaleShowTableNode(this, schema, table, use_like);
}
ExecuteNode *MySQLExecutePlan::do_get_migrate_clean_node(
    const char *migrate_id) {
  return new MySQLDBScaleMigrateCleanNode(this, migrate_id);
}
ExecuteNode *MySQLExecutePlan::do_get_show_lock_usage_node() {
  return new MySQLDBScaleShowLockUsageNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_show_execution_profile_node() {
  return new MySQLDBScaleShowExecutionProfileNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_select_plain_value_node(
    const char *str_value, const char *alias_name) {
  return new MySQLSelectPlainValueNode(this, str_value, alias_name);
}

ExecuteNode *MySQLExecutePlan::do_get_estimate_select_node(
    DataSpace *dataspace, const char *sql, Statement *statement) {
  return new MySQLEstimateSelectNode(this, dataspace, sql, statement);
}

ExecuteNode *MySQLExecutePlan::do_get_estimate_select_partition_node(
    vector<unsigned int> *par_ids, PartitionedTable *par_table, const char *sql,
    Statement *statement) {
  return new MySQLEstimateSelectNode(this, par_ids, par_table, sql, statement);
}

ExecuteNode *MySQLExecutePlan::do_get_rename_table_node(
    PartitionedTable *old_table, PartitionedTable *new_table,
    const char *old_schema_name, const char *old_table_name,
    const char *new_schema_name, const char *new_table_name) {
  return new MySQLRenameTableNode(this, old_table, new_table, old_schema_name,
                                  old_table_name, new_schema_name,
                                  new_table_name);
}
ExecuteNode *MySQLExecutePlan::do_get_mul_modify_node(
    map<DataSpace *, const char *> &spaces_map,
    map<DataSpace *, int> *sql_count_map, bool need_handle_more_result) {
  return new MySQLMulModifyNode(this, spaces_map, sql_count_map,
                                need_handle_more_result);
}

ExecuteNode *MySQLExecutePlan::do_get_show_partition_node() {
  return new MySQLDBScaleShowPartitionNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_show_user_sql_count_node(
    const char *user_id) {
  return new MySQLDBScaleShowUserSqlCountNode(this, user_id);
}

ExecuteNode *MySQLExecutePlan::do_get_show_status_node() {
  return new MySQLDBScaleShowStatusNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_show_catchup_node(
    const char *source_name) {
  return new MySQLShowCatchupNode(this, source_name);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_skip_wait_catchup_node(
    const char *source_name) {
  return new MySQLSkipWaitCatchupNode(this, source_name);
}
ExecuteNode *MySQLExecutePlan::do_get_show_async_task_node() {
  return new MySQLDBScaleShowAsyncTaskNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_clean_temp_table_cache_node() {
  return new MySQLDBScaleCleanTempTableCacheNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_explain_node(
    bool is_server_explain_stmt_always_extended) {
  return new MySQLDBScaleExplainNode(this,
                                     is_server_explain_stmt_always_extended);
}

ExecuteNode *MySQLExecutePlan::do_get_fetch_explain_result_node(
    list<list<string> > *result, bool is_server_explain_stmt_always_extended) {
  return new MySQLFetchExplainResultNode(
      this, result, is_server_explain_stmt_always_extended);
}

ExecuteNode *MySQLExecutePlan::do_get_show_virtual_map_node(
    PartitionedTable *tab) {
  return new MySQLDBScaleShowVirtualMapNode(this, tab);
}

ExecuteNode *MySQLExecutePlan::do_get_show_shard_map_node(
    PartitionedTable *tab) {
  return new MySQLDBScaleShowShardMapNode(this, tab);
}

ExecuteNode *MySQLExecutePlan::do_get_show_auto_increment_info_node(
    PartitionedTable *tab) {
  return new MySQLDBScaleShowAutoIncInfoNode(this, tab);
}

ExecuteNode *MySQLExecutePlan::do_get_set_auto_increment_offset_node(
    PartitionedTable *tab) {
  return new MySQLDBScaleSetAutoIncrementOffsetNode(this, tab);
}

ExecuteNode *MySQLExecutePlan::do_get_show_transaction_sqls_node() {
  return new MySQLDBScaleShowTransactionSqlsNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_set_node(DataSpace *dataspace,
                                               const char *sql) {
  return new MySQLSetNode(this, dataspace, sql);
}

ExecuteNode *MySQLExecutePlan::do_get_create_oracle_sequence_node(
    create_oracle_seq_op_node *oper) {
  return new MySQLCreateOracleSequenceNode(this, oper);
}

ExecuteNode *MySQLExecutePlan::do_get_union_all_node(
    vector<ExecuteNode *> &nodes) {
  return new MySQLUnionAllNode(this, nodes);
}

ExecuteNode *MySQLExecutePlan::do_get_union_group_node(
    vector<list<ExecuteNode *> > &nodes) {
  return new MySQLUnionGroupNode(this, nodes);
}

ExecuteNode *MySQLExecutePlan::do_get_keepmaster_node() {
  return new MySQLKeepmasterNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_dbscale_test_node() {
  return new MySQLDBScaleTestNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_async_task_control_node(
    unsigned long long id) {
  return new MySQLAsyncTaskControlNode(this, id);
}

ExecuteNode *MySQLExecutePlan::do_get_load_dataspace_file_node(
    const char *filename) {
  return new MySQLLoadDataSpaceFileNode(this, filename);
}

ExecuteNode *MySQLExecutePlan::do_get_call_sp_return_ok_node() {
  return new MySQLCallSPReturnOKNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_dbscale_purge_monitor_point_node() {
  return new MySQLDBScalePurgeMonitorPointNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_dbscale_clean_monitor_point_node() {
  return new MySQLDBScaleCleanMonitorPointNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_send_dbscale_one_column_node(
    bool only_one_row) {
  return new MySQLSendOneColumnToDBScaleNode(this, only_one_row);
}
ExecuteNode *MySQLExecutePlan::do_get_send_dbscale_mul_column_node() {
  return new MySQLSendMulColumnToDBScaleNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_send_dbscale_one_column_aggr_node(
    bool get_min) {
  return new MySQLSendOneColumnAggrToDBScaleNode(this, get_min);
}

ExecuteNode *MySQLExecutePlan::do_get_send_dbscale_exists_node() {
  return new MySQLSendExistsToDBScaleNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_default_session_var_node() {
  return new MySQLDBScaleShowDefaultSessionVarNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_version_node() {
  return new MySQLDBScaleShowVersionNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_show_version_node() {
  return new MySQLShowVersionNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_show_components_version_node() {
  return new ShowComponentsVersionNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_hostname_node() {
  return new MySQLDBScaleShowHostnameNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_all_fail_transaction_node() {
  return new MySQLDBScaleShowAllFailTransactionNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_detail_fail_transaction_node(
    const char *xid) {
  return new MySQLDBScaleShowDetailFailTransactionNode(this, xid);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_partition_table_status_node(
    const char *table_name) {
  return new MySQLDBScaleShowPartitionTableStatusNode(this, table_name);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_schema_acl_info_node(
    bool is_show_all) {
  return new MySQLDBScaleShowSchemaACLInfoNode(this, is_show_all);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_table_acl_info_node(
    bool is_show_all) {
  return new MySQLDBScaleShowTableACLInfoNode(this, is_show_all);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_path_info() {
  return new MySQLDBScaleShowPathInfoNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_warnings_node() {
  return new MySQLDBScaleShowWarningsNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_critical_errors_node() {
  return new MySQLDBScaleShowCriticalErrorNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_clean_fail_transaction_node(
    const char *xid) {
  return new MySQLDBScaleCleanFailTransactionNode(this, xid);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_base_status_node() {
  return new MySQLDBScaleShowBaseStatusNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_dbscale_mul_sync_node(
    const char *sync_topic, const char *sync_state, const char *sync_param,
    const char *sync_cond, unsigned long version_id) {
  return new MySQLDBScaleMulSyncNode(this, sync_topic, sync_state, sync_param,
                                     sync_cond, version_id);
}

ExecuteNode *MySQLExecutePlan::do_get_dbscale_add_default_session_var_node(
    DataSpace *dataspace) {
  return new MySQLDBScaleAddDefaultSessionVarNode(this, dataspace);
}
ExecuteNode *
MySQLExecutePlan::do_get_dbscale_remove_default_session_var_node() {
  return new MySQLDBScaleRemoveDefaultSessionVarNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_federated_table_node() {
  return new MySQLFederatedTableNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_empty_set_node(int field_num) {
  return new MySQLEmptySetNode(this, field_num);
}
ExecuteNode *MySQLExecutePlan::do_get_cursor_direct_node(DataSpace *dataspace,
                                                         const char *sql) {
  return new MySQLCursorDirectNode(this, dataspace, sql);
}
ExecuteNode *MySQLExecutePlan::do_get_cursor_send_node() {
  return new MySQLCursorSendNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_return_ok_node() {
  return new MySQLReturnOKNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_slave_dbscale_error_node() {
  return new MySQLSlaveDBScaleErrorNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_reset_zoo_info_node() {
  return new MySQLResetZooInfoNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_set_info_schema_mirror_tb_status_node(
    info_mirror_tb_status_node *tb_status) {
  return new MySQLSetInfoSchemaMirrorTbStatusNode(this, tb_status);
}

ExecuteNode *MySQLExecutePlan::do_get_forward_master_role_node(
    const char *sql, bool is_slow_query) {
  return new MySQLMultipleMasterForwardNode(this, sql, is_slow_query);
}
#ifndef CLOSE_ZEROMQ
ExecuteNode *MySQLExecutePlan::do_get_message_service_node() {
  return new MySQLMessageServiceNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_binlog_task_node() {
  return new MySQLBinlogTaskNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_task_node() {
  return new MySQLTaskNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_binlog_task_add_filter_node() {
  return new MySQLBinlogTaskAddFilterNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_drop_task_filter_node() {
  return new MySQLDropTaskFilterNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_drop_task_node() {
  return new MySQLDropTaskNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_show_client_task_status_node(
    const char *task_name, const char *server_task_name) {
  return new MySQLShowClientTaskStatusNode(this, task_name, server_task_name);
}
ExecuteNode *MySQLExecutePlan::do_get_show_server_task_status_node(
    const char *task_name) {
  return new MySQLShowServerTaskStatusNode(this, task_name);
}
#endif

ExecuteNode *MySQLExecutePlan::do_get_dbscale_update_audit_user_node(
    const char *username, bool is_add) {
  return new MySQLDBScaleUpdateAuditUserNode(this, username, is_add);
}

ExecuteNode *MySQLExecutePlan::do_get_connect_by_node(
    ConnectByDesc connect_by_desc) {
  return new MySQLConnectByNode(this, connect_by_desc);
}

ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_create_oracle_seq_node(
    const char *seq_schema, const char *seq_name) {
  return new MySQLDBScaleShowCreateOracleSeqPlan(this, seq_schema, seq_name);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_seq_status_node(
    const char *seq_schema, const char *seq_name) {
  return new MySQLDBScaleShowSeqStatusPlan(this, seq_schema, seq_name);
}
ExecuteNode *
MySQLExecutePlan::do_get_dbscale_show_fetchnode_buffer_usage_node() {
  return new MySQLDBScaleShowFetchNodeBufferNode(this);
}
ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_trx_block_info_node(
    bool is_local) {
  return new MySQLDBScaleShowTrxBlockInfoNode(this, is_local);
}
ExecuteNode *MySQLExecutePlan::do_get_restore_recycle_table_precheck_node(
    const char *from_schema, const char *from_table, const char *to_table,
    const char *recycle_type, DataSpace *dspace) {
  return new MySQLRestoreRecycleTablePrecheckNode(
      this, from_schema, from_table, to_table, recycle_type, dspace);
}

ExecuteNode *MySQLExecutePlan::do_get_dbscale_internal_set_node() {
  return new MySQLDBScaleInternalSetNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_empty_node() {
  return new MySQLEmptyNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_dbscale_show_prepare_cache_node() {
  return new MySQLDBScaleShowPrepareCacheNode(this);
}

ExecuteNode *MySQLExecutePlan::do_get_dbscale_flush_prepare_cache_hit_node() {
  return new MySQLDBScaleFlushPrepareCacheHitNode(this);
}

bool MySQLExecutePlan::has_duplicate_entry_in_error_packet(Packet *packet) {
  if (packet) {
    MySQLErrorResponse error(packet);
    error.unpack();
    if (error.get_error_code() == 1062) return true;
  }
  return false;
}

bool MySQLExecutePlan::check_select_error_not_rollback_in_packet(
    Packet *packet) {
  bool should_rollback = false;
  // get_migrate_tool not null, means part table select error.
  if (get_migrate_tool() != NULL) {
    should_rollback = get_migrate_tool()->check_select_error_not_rollback();
    return should_rollback;
  }
  // directexecutenode select error
  if (packet) {
    MySQLErrorResponse error(packet);
    error.unpack();
    if (error.get_error_code() == ER_LOCK_WAIT_TIMEOUT) {
      LOG_DEBUG(
          "check_select_error_not_rollback_in_packet get error code "
          "ER_LOCK_WAIT_TIMEOUT.\n");
      InnodbRollbackOnTimeout single_server_innodb_rollback_on_timeout =
          get_single_server_innodb_rollback_on_timeout();
#ifndef DBSCALE_TEST_DISABLE
      Backend *bk = Backend::instance();
      dbscale_test_info *test_info = bk->get_dbscale_test_info();
      if (!strcasecmp(test_info->test_case_name.c_str(), "rollback_error") &&
          !strcasecmp(test_info->test_case_operation.c_str(),
                      "innodb_rollback_on_timeout_on")) {
        LOG_DEBUG("Test transaction with innodb_rollback_on_timeout on.\n");
        should_rollback = true;
      } else {
        should_rollback = single_server_innodb_rollback_on_timeout ==
                          INNODB_ROLLBACK_ON_TIMEOUT_ON;
      }
#else
      should_rollback = single_server_innodb_rollback_on_timeout ==
                        INNODB_ROLLBACK_ON_TIMEOUT_ON;
#endif
    } else if (error.get_error_code() == ER_LOCK_DEADLOCK ||
               error.is_shutdown()) {
      LOG_DEBUG(
          "check_select_error_not_rollback_in_packet find dead lock or lost "
          "connection.\n");
      should_rollback = true;  // deadlock and lost connection must rollback
    }
    if (!should_rollback) {
      LOG_DEBUG(
          "check_select_error_not_rollback_in_packet check should_rollback is "
          "false.\n");
      string error_message = string(error.get_error_message());
      error_message.append(
          ". due to innodb_rollback_on_timeout_on is off. this transaction not "
          "restart.");
      error.set_error_message(error_message.c_str());
      error.pack(packet);
    }
  }
  return should_rollback;
}

void MySQLExecutePlan::do_send_error_packet() {
  if (is_dispatch_federated()) {
    ((MySQLDispatchPacketNode *)start_node)->dispatch_error_packet();
  } else {
    Packet *packet;
    packet = ((MySQLExecuteNode *)start_node)->get_error_packet();
    if (packet) {
      MySQLErrorResponse error(packet);
      error.unpack();
      LOG_INFO(
          "get an error packet, %d (%s) %s, and try to send it to client.\n",
          error.get_error_code(), error.get_sqlstate(),
          error.get_error_message());
      session->acquire_has_send_client_error_packet_mutex();
      if (session->get_has_send_client_error_packet()) {
        session->release_has_send_client_error_packet_mutex();
        LOG_DEBUG(
            "Session has send the error packet to client, so ignore this error "
            "packet.\n");
        return;
      }
      session->set_has_send_client_error_packet();
      session->release_has_send_client_error_packet_mutex();
      session->set_cur_stmt_error_code(error.get_error_code());
      if ((error.get_error_code() == ERROR_AUTH_DENIED_CODE) &&
          !strcmp(error.get_sqlstate(), "42000")) {
        string msg = error.get_error_message();
        if ((msg.find("Access denied for user") != string::npos) &&
            (msg.find("to database 'information_schema'") != string::npos) &&
            !lower_case_compare(session->get_schema(), default_login_schema)) {
          msg.append(". do you forget to use database?");
          error.set_error_message(msg.c_str());
          Packet pkt;
          error.pack(&pkt);
          handler->send_to_client(&pkt);
          return;
        }
      }
      handler->set_mul_stmt_send_error_packet(true);
      handler->send_to_client(packet);
    } else {
      if (is_send_cluster_xa_transaction_ddl_error_packet()) {
        string err_message =
            "XAER_RMFAIL: The command cannot be executed when global "
            "transaction is in the  ";
        err_message.append(session
                               ->cluster_xa_transaction_to_string(
                                   session->get_cluster_xa_transaction_state())
                               .c_str());
        err_message.append(" state");
        Packet error_packet;
        MySQLErrorResponse error(ERROR_XAER_RMFAIL_CODE, err_message.c_str(),
                                 "XAE07");
        error.pack(&error_packet);
        session->acquire_has_send_client_error_packet_mutex();
        if (session->get_has_send_client_error_packet()) {
          session->release_has_send_client_error_packet_mutex();
          LOG_DEBUG(
              "Session has send the error packet to client, so ignore this "
              "error "
              "packet.\n");
          return;
        }
        session->set_has_send_client_error_packet();
        session->release_has_send_client_error_packet_mutex();
        session->set_cur_stmt_error_code(error.get_error_code());
        handler->send_to_client(&error_packet);
        set_send_cluster_xa_transaction_ddl_error_packet(false);
        return;
      }
      LOG_ERROR(
          "MySQLExecutePlan::do_send_error_packet fail to find an error "
          "packet.\n");
      throw Error("Get an error packet.");
    }
  }
}

void MySQLExecutePlan::do_fullfil_error_info(unsigned int *error_code,
                                             string *sql_state) {
  Packet *packet;
  packet = ((MySQLExecuteNode *)start_node)->get_error_packet();
  if (packet) {
    MySQLErrorResponse response(packet);
    response.unpack();
    *error_code = (unsigned int)response.get_error_code();
    sql_state->clear();
    sql_state->append(response.get_sqlstate());
  }
}

/* class MySQLExecuteNode */

MySQLExecuteNode::MySQLExecuteNode(ExecutePlan *plan, DataSpace *dataspace)
    : ExecuteNode(plan, dataspace) {
  this->driver = (MySQLDriver *)plan->driver;
  this->session = (MySQLSession *)plan->session;
  this->handler = (MySQLHandler *)plan->handler;
  this->name = "MySQLExecuteNode";
  this->profile_id = -1;
  status = EXECUTE_STATUS_START;
  ready_rows = new AllocList<Packet *, Session *, StaticAllocator<Packet *> >();
  ready_rows->set_session(plan->session);
  by_add_pre_disaster_master = false;
}

/* class MySQLInnerNode */

MySQLInnerNode::MySQLInnerNode(ExecutePlan *plan, DataSpace *dataspace)
    : MySQLExecuteNode(plan, dataspace) {
  this->name = "MySQLInnerNode";
  all_children_finished = false;
#ifndef DBSCALE_TEST_DISABLE
  loop_count = 0;
  need_record_loop_count = false;
#endif
  node_can_swap = false;
  one_child_got_error = false;
  thread_started = false;
  ready_rows_buffer_size = 0;
}

Packet *MySQLInnerNode::get_error_packet() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if ((*it)->get_error_packet()) {
      return (*it)->get_error_packet();
    }
  }

  return NULL;
}

void MySQLInnerNode::children_execute() {
#ifdef DEBUG
  LOG_DEBUG("Node %@ children start to execute.\n", this);
#endif
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    (*it)->execute();
    if ((*it)->is_got_error()) {
      break;
    }
    node_can_swap = node_can_swap & (*it)->can_swap();
  }
  if (it != children.end()) {
#ifdef DEBUG
    LOG_DEBUG(
        "Node %@ children execute finished, but some child do not execute "
        "because previous child got error.\n",
        this);
#endif
    handle_error_all_children();
  } else {
#ifdef DEBUG
    LOG_DEBUG("Node %@ children execute finished.\n", this);
#endif
  }
}

bool MySQLInnerNode::notify_parent() {
  Packet *row;
  if (ready_rows->empty()) {
    return false;
  }

  while (!ready_rows->empty()) {
    row = ready_rows->front();
    ready_rows->pop_front();
    parent->add_row_packet(this, row);
  }
  ready_rows_buffer_size = 0;
  return true;
}

void MySQLInnerNode::set_children_error() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    (*it)->set_node_error();
  }
}

void MySQLInnerNode::set_children_thread_status_start() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    (*it)->set_thread_status_started();
  }
}

void MySQLInnerNode::handle_error_all_children() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    try {
      (*it)->handle_error_throw();
    } catch (...) {
      it++;
      for (; it != children.end(); it++) {
        try {
          (*it)->handle_error_throw();
        } catch (...) {
        }
      }
      throw;
    }
  }
}

void MySQLInnerNode::wait_children() {
#ifdef DEBUG
  LOG_DEBUG("Node %@ %s start to wait.\n", this, this->name);
#endif
  if (all_children_finished) {
    handle_error_all_children();
    status = EXECUTE_STATUS_BEFORE_COMPLETE;
  } else {
    list<MySQLExecuteNode *>::iterator it;
    for (it = children.begin(); it != children.end(); ++it) {
      (*it)->start_thread();
    }
    set_children_thread_status_start();
    plan->start_all_bthread();

    unsigned int finished_child = 0;

    for (it = children.begin(); it != children.end(); ++it) {
      if (!(*it)->notify_parent() && (*it)->is_finished()) {
        finished_child++;
      }
      if ((*it)->is_got_error()) {
        one_child_got_error = true;
      }
#ifndef DBSCALE_TEST_DISABLE
      if (need_record_loop_count && loop_count++ == 10) {
        LOG_DEBUG(
            "throw ExecuteNodeError to test for option "
            "'max-fetchnode-ready-rows-size'\n");
        ACE_Time_Value sleep_tv(2, 0);
        ACE_OS::sleep(sleep_tv);
        throw ExecuteNodeError(
            "test for option 'max-fetchnode-ready-rows-size'");
      }
#endif
    }

    if (finished_child == children.size()) {
      handle_error_all_children();
      all_children_finished = true;
      status = EXECUTE_STATUS_BEFORE_COMPLETE;
    } else {
      status = EXECUTE_STATUS_HANDLE;
    }
  }
#ifdef DEBUG
  LOG_DEBUG("Node %@ %s wait finished.\n", this, this->name);
#endif
}

void MySQLInnerNode::init_row_map() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    row_map[*it] =
        new AllocList<Packet *, Session *, StaticAllocator<Packet *> >();
    row_map[*it]->set_session(plan->session);
    row_map_size[*it] = 0;
    buffer_row_map_size[*it] = 0;
  }
}

void MySQLInnerNode::clean() {
  Packet *packet;
  MySQLExecuteNode *free_node;

  status = EXECUTE_STATUS_COMPLETE;

  // clean ready packets
  while (!ready_rows->empty()) {
    packet = ready_rows->front();
    ready_rows->pop_front();
    delete packet;
  }

  while (!children.empty()) {
    free_node = children.front();
    free_node->clean();

    // clean remaining rows
    if (row_map[free_node]) {
      while (!row_map[free_node]->empty()) {
        packet = row_map[free_node]->front();
        row_map[free_node]->pop_front();
        delete packet;
      }

      delete row_map[free_node];
      row_map[free_node] = NULL;
    }

    // delete child node
    children.pop_front();
    delete free_node;
  }

  do_clean();
}

void MySQLInnerNode::handle_swap_connections() {
  if (!plan->get_connection_swaped_out()) {
    plan->set_connection_swaped_out(true);
    list<MySQLExecuteNode *>::iterator it;
    int conn_num = 0;
    for (it = children.begin(); it != children.end(); ++it) {
      Connection *conn = (*it)->get_connection_from_node();
      if (conn) {
        conn->set_session(session);
        struct event *conn_event = conn->get_conn_socket_event();
        evutil_socket_t conn_socket = conn->get_conn_socket();
        SocketEventBase *conn_base = conn->get_socket_base();
#ifdef DEBUG
        ACE_ASSERT(conn_base);
#endif
        session->add_server_base(conn_socket, conn_base, conn_event);
        LOG_DEBUG(
            "Finish to prepare the conn socket session swap for conn %@ and "
            "session %@.\n",
            conn, session);
        conn_num++;
      }
    }
    session->set_mul_connection_num(conn_num);
  }
}

/* class MySQLClusterXATransactionNode */
MySQLClusterXATransactionNode::MySQLClusterXATransactionNode(ExecutePlan *plan)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLClusterXATransactionNode";
  error_packet = NULL;
  packet = NULL;
  xa_conn = NULL;
  dataspace = NULL;
  got_error = false;
  st_type = plan->statement->get_stmt_node()->type;
  xa_sql = plan->statement->get_sql();
}
void MySQLClusterXATransactionNode::handle_send_client_packet(
    Packet *packet, bool is_row_packet) {
  handler->send_mysql_packet_to_client_by_buffer(packet, is_row_packet);
}
void MySQLClusterXATransactionNode::handle_error_packet(Packet *packet) {
  status = EXECUTE_STATUS_COMPLETE;
  error_packet = packet;
  MySQLErrorResponse response(error_packet);
  response.unpack();
  LOG_DEBUG(
      "MySQLClusterXATransactionNode get an error packet when execute xa_sql "
      "[%s]: %d (%s) "
      "%s\n",
      xa_sql, response.get_error_code(), response.get_sqlstate(),
      response.get_error_message());
  if (response.is_shutdown()) {
    if (xa_conn) {
      handler->clean_dead_conn(&xa_conn, dataspace);
    }
  }
  throw ErrorPacketException();
}

void MySQLClusterXATransactionNode::handle_more_result(Packet *packet) {
  try {
    handler->receive_from_server(xa_conn, packet);
    while (!driver->is_eof_packet(packet)) {
      if (driver->is_error_packet(packet)) {
        handle_error_packet(packet);
      }
      handle_send_client_packet(packet, false);

      handler->receive_from_server(xa_conn, packet);
    }
    handle_send_client_packet(packet, false);
    MySQLEOFResponse eof(packet);
    eof.unpack();
    // receive rows and last eof
    handler->receive_from_server(xa_conn, packet);
#ifndef DBSCALE_TEST_DISABLE
    Backend *bk = Backend::instance();
    dbscale_test_info *test_info = bk->get_dbscale_test_info();
    if (!strcasecmp(test_info->test_case_name.c_str(), "client_exception") &&
        !strcasecmp(test_info->test_case_operation.c_str(),
                    "client_exception")) {
      delete packet;
      packet = Backend::instance()->get_new_packet(DEFAULT_PACKET_SIZE,
                                                   session->get_packet_alloc());
      LOG_DEBUG("test after delete packet, then ClientBroken\n");
      throw ClientBroken();
    }
#endif
    while ((!driver->is_eof_packet(packet)) &&
           (!driver->is_error_packet(packet))) {
      handle_send_client_packet(packet, true);
      handler->receive_from_server(xa_conn, packet);
    }
    if (driver->is_error_packet(packet)) handle_error_packet(packet);
    if (driver->is_eof_packet(packet)) {
      if (support_show_warning)
        handle_warnings_OK_and_eof_packet(plan, packet, handler, dataspace,
                                          xa_conn);
    }
#ifndef DBSCALE_TEST_DISABLE
    handler->handler_last_send.start_timing();
#endif
    handle_send_client_packet(packet, false);
    handler->flush_net_buffer();
#ifndef DBSCALE_TEST_DISABLE
    handler->handler_last_send.end_timing_and_report();
#endif
  } catch (...) {
    this->packet = packet;
    throw;
  }

  this->packet = packet;
}

void MySQLClusterXATransactionNode::execute() {
  Backend *backend = Backend::instance();
  string xid_str = "";
  size_t start_pos = 0;
  size_t end_pos = 0;
  dataspace = backend->get_catalog();

  if (session->get_session_state() == SESSION_STATE_WORKING) {
#ifdef DEBUG
    node_start_timing();
#endif
    LOG_DEBUG("MySQLClusterXATransactionNode sql : %s .\n", xa_sql);

    packet = backend->get_new_packet(DEFAULT_PACKET_SIZE, NULL);
    Packet exec_packet(DEFAULT_PACKET_SIZE, NULL);

    try {
      Packet *real_exec_packet = NULL;
      MySQLQueryRequest query(xa_sql);
      query.set_sql_replace_char(
          plan->session->get_query_sql_replace_null_char());
      query.pack(&exec_packet);
      real_exec_packet = &exec_packet;

#ifndef DBSCALE_TEST_DISABLE
      handler->send_op.start_timing();
#endif

      xa_conn = handler->send_to_server_retry(dataspace, real_exec_packet,
                                              session->get_schema(), false);

      if (!xa_conn) {
        LOG_ERROR("fail to get connections to execute xa command.\n");
        throw Error("fail to get connections to execute xa command");
      } else {
        xa_conn->set_cluster_xa_session(session);
      }
#ifndef DBSCALE_TEST_DISABLE
      handler->send_op.end_timing_and_report();
#endif
    } catch (ExecuteNodeError &e) {
      got_error = true;
      if (xa_conn) {
        handler->clean_dead_conn(&xa_conn, dataspace);
      }
      status = EXECUTE_STATUS_COMPLETE;
      throw e;
    } catch (exception &e) {
      status = EXECUTE_STATUS_COMPLETE;
      LOG_ERROR("MySQLClusterXATransactionNode fail due to exception [%s].\n",
                e.what());
      string error_message(
          "MySQLClusterXATransactionNode fail due to exception:");
      error_message.append(e.what());
      got_error = true;
      if (xa_conn) {
        handler->clean_dead_conn(&xa_conn, dataspace);
      }
      throw ExecuteNodeError(error_message.c_str());
    }
  }
  /*receive packet*/
  try {
    bool has_more_result = false;

    do {
      handler->receive_from_server(xa_conn, packet);
      if (driver->is_error_packet(packet)) {
        LOG_DEBUG("MySQLClusterXATransactionNode : receive error packet.\n");
        has_more_result = false;
        handle_error_packet(packet);
      } else if (driver->is_ok_packet(packet)) {
        MySQLOKResponse ok(packet);
        ok.unpack();
        has_more_result = ok.has_more_result();
        LOG_DEBUG("MySQLClusterXATransactionNode : receive ok packet.\n");
        handler->send_to_client(packet);
        if (support_show_warning)
          handle_warnings_OK_and_eof_packet(plan, packet, handler, dataspace,
                                            xa_conn);
      } else if (driver->is_result_set_header_packet(packet)) {
        LOG_DEBUG(
            "MySQLClusterXATransactionNode : receive result_set packet.\n");
        handle_send_client_packet(packet, false);
        handle_more_result(packet);
        has_more_result = false;
      } else {
#ifdef DEBUG
        ACE_ASSERT(0);  // should not be here
#endif
        handle_send_client_packet(packet, false);
        has_more_result = false;
      }
#ifdef DEBUG
      LOG_DEBUG("Has more result %d.\n", has_more_result ? 1 : 0);
#endif
    } while (has_more_result);
    switch (st_type) {
      case STMT_XA_START_TRANSACTION:
        start_pos =
            plan->statement->get_stmt_node()->xid_item->xid_start_pos - 1;
        end_pos = plan->statement->get_stmt_node()->xid_item->xid_end_pos;
        xid_str = string(xa_sql, start_pos, end_pos - start_pos);
        LOG_DEBUG(
            "start_pos is %u, end_pos is %u, xa start transaction with xid "
            "%s.\n",
            start_pos, end_pos, xid_str.c_str());
        session->set_session_cluster_xid(xid_str);
        session->set_cluster_xa_transaction_conn(xa_conn);
        session->set_cluster_xa_transaction_state(SESSION_XA_TRX_XA_ACTIVE);
        /* after receive STMT_XA_START_TRANSACTION,
         * we should set xa session.*/
        xa_conn->set_cluster_xa_session(session);
        break;
      case STMT_XA_END_TRANSACTION:
        session->set_cluster_xa_transaction_state(SESSION_XA_TRX_XA_IDLE);
        break;
      case STMT_XA_PREPARE_TRANSACTION:
        session->set_cluster_xa_transaction_state(SESSION_XA_TRX_XA_PREPARED);
        break;
      case STMT_XA_COMMIT_TRANSACTION:
      case STMT_XA_COMMIT_ONE_PHASE_TRANSACTION:
      case STMT_XA_ROLLBACK_TRANSACTION:
        session->reset_cluster_xa_transaction();
        handler->put_back_connection(dataspace, xa_conn);
        xa_conn = NULL;
        break;
      default:
        break;
    }
  } catch (ErrorPacketException &e) {
    LOG_DEBUG(
        "MySQLClusterXATransactionNode get an error packet when execute xa_sql "
        "[%s].\n",
        xa_sql);
    status = EXECUTE_STATUS_COMPLETE;
    if (xa_conn && error_packet) {
      handler->put_back_connection(dataspace, xa_conn);
      xa_conn = NULL;
    }
    throw e;
  } catch (ExecuteNodeError &e) {
    got_error = true;
    if (xa_conn) {
      handler->clean_dead_conn(&xa_conn, dataspace);
    }
    status = EXECUTE_STATUS_COMPLETE;
    throw e;
  } catch (exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    LOG_ERROR("MySQLClusterXATransactionNode fail due to exception [%s].\n",
              e.what());
    string error_message(
        "MySQLClusterXATransactionNode fail due to exception:");
    error_message.append(e.what());
    got_error = true;
    if (xa_conn) {
      handler->clean_dead_conn(&xa_conn, dataspace);
    }
    throw ExecuteNodeError(error_message.c_str());
  }

#ifdef DEBUG
  node_end_timing();
  LOG_DEBUG("MySQLClusterXATransactionNode %@ cost %d ms\n", this,
            node_cost_time);
#endif
  status = EXECUTE_STATUS_COMPLETE;
  return;
}
void MySQLClusterXATransactionNode::clean() {
  if (xa_conn) {
    if (!got_error || error_packet) {
      handler->put_back_connection(dataspace, xa_conn);
      xa_conn = NULL;
    } else {
      handler->clean_dead_conn(&xa_conn, dataspace);
    }
  }
  if (error_packet && error_packet != packet) {
    delete error_packet;
    error_packet = NULL;
  }
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
}

/* class MySQLLockNode */
MySQLLockNode::MySQLLockNode(
    ExecutePlan *plan,
    map<DataSpace *, list<lock_table_item *> *> *lock_table_list)
    : MySQLExecuteNode(plan), conn(NULL) {
  this->lock_table_list = lock_table_list;
  this->name = "MySQLLockNode";
  got_error = false;
  error_packet = NULL;
}
void MySQLLockNode::clean() {
  if (lock_table_list) {
    map<DataSpace *, list<lock_table_item *> *>::iterator it;
    while (!lock_table_list->empty()) {
      it = lock_table_list->begin();
      list<lock_table_item *> *table_list = it->second;
      lock_table_list->erase(it);
      table_list->clear();
      delete table_list;
    }
    delete lock_table_list;
    lock_table_list = NULL;
  }
  if (conn) {
    if (!got_error || error_packet) {
      handler->put_back_connection(dataspace, conn);
    } else {
      handler->clean_dead_conn(&conn, dataspace);
    }
    conn = NULL;
  }
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
}

string MySQLLockNode::adjust_table_name(string ori_name) {
  string ret;
  vector<string> split_name;
  boost::split(split_name, ori_name, boost::is_any_of("."));
  if (split_name.size() == 1) {
    ret.assign("`");
    ret.append(split_name.at(0));
    ret.append("`");
  } else if (split_name.size() == 2) {
    ret.assign("`");
    ret.append(split_name.at(0));
    ret.append("`.`");
    ret.append(split_name.at(1));
    ret.append("`");
  }
  return ret;
}

string MySQLLockNode::build_sql(DataSpace *space) {
  list<lock_table_item *> *table_list = (*lock_table_list)[space];
  list<lock_table_item *>::iterator lock_it;
  string str = "lock tables ";
  for (lock_it = table_list->begin(); lock_it != table_list->end(); lock_it++) {
    if (lock_it != table_list->begin()) {
      str += ",";
    }
    str += adjust_table_name((*lock_it)->name);
    str += " ";
    if ((*lock_it)->type == LOCK_TYPE_READ) {
      str += "read";
    } else {
      str += "write";
    }
  }
  LOG_DEBUG("lock sql [%s] for space %@\n", str.c_str(), space);
  lock_table_list->erase(space);
  table_list->clear();
  delete table_list;
  return str;
}

void MySQLLockNode::execute() {
#ifdef DEBUG
  node_start_timing();
#endif
  packet = Backend::instance()->get_new_packet();
  Packet exec_packet;
  MySQLQueryRequest query;
  query.set_sql_replace_char(plan->session->get_query_sql_replace_null_char());
  map<DataSpace *, list<lock_table_item *> *>::iterator it;
  DataSpace *space = NULL;
  while (!lock_table_list->empty()) {
    it = lock_table_list->begin();
    space = it->first;
    string str = build_sql(space);
    const char *sql = (const char *)str.c_str();
    query.set_query(sql);
    query.pack(&exec_packet);

    try {
      conn = handler->send_to_server_retry(
          space, &exec_packet, session->get_schema(), session->is_read_only());
      LOG_DEBUG("Receiving lock result from server\n");
      handler->receive_from_server(conn, packet);
      if (driver->is_error_packet(packet)) {
        status = EXECUTE_STATUS_COMPLETE;
        error_packet = packet;
        MySQLErrorResponse response(error_packet);
        if (response.is_shutdown()) {
          if (conn) {
            handler->clean_dead_conn(&conn, space);
          }
          session->set_server_shutdown(true);
          dataspace = space;
        }
        throw ErrorPacketException();
      }
      if (lock_table_list->size() == 0) {
        handler->record_affected_rows(packet);
        handler->send_to_client(packet);
      }

      if (conn) {
        handler->put_back_connection(space, conn);
        conn = NULL;
      }
    } catch (ErrorPacketException &e) {
      LOG_DEBUG("MySQLLockNode get an error packet when execute sql [%s].\n",
                sql);
      throw e;
    } catch (ExecuteNodeError &e) {
      if (conn) {
        handler->clean_dead_conn(&conn, space);
      }
      got_error = true;
      status = EXECUTE_STATUS_COMPLETE;
      throw e;
    } catch (exception &e) {
      got_error = true;
      status = EXECUTE_STATUS_COMPLETE;
      LOG_DEBUG("MySQLLockNode fail due to exception [%s].\n", e.what());
      string error_message("MySQLLockNode fail due to exception:");
      error_message.append(e.what());
      if (conn) {
        handler->clean_dead_conn(&conn, space);
      }
      throw ExecuteNodeError(error_message.c_str());
      ;
    }
  }
  status = EXECUTE_STATUS_COMPLETE;
  delete lock_table_list;
  lock_table_list = NULL;
#ifdef DEBUG
  node_end_timing();
#endif
#ifdef DEBUG
  LOG_DEBUG("MySQLLockNode %@ cost %d ms\n", this, node_cost_time);
#endif
}

/* class  MySQLTransactionUNLockNode */

MySQLTransactionUNLockNode::MySQLTransactionUNLockNode(ExecutePlan *plan,
                                                       const char *sql,
                                                       bool is_send_to_client)
    : MySQLExecuteNode(plan), conn(NULL) {
  this->name = "MySQLTransactionUNLockNode";
  this->sql = sql;
  this->is_send_to_client = is_send_to_client;
  got_error = false;
  packet = NULL;
  error_packet = NULL;
}

MySQLTransactionUNLockNode::~MySQLTransactionUNLockNode() {}

void MySQLTransactionUNLockNode::clean() {
  session->clear_running_xa_map();
  if (conn) {
    if (!got_error || error_packet) {
      handler->put_back_connection(dataspace, conn);
    } else {
      handler->clean_dead_conn(&conn, dataspace);
    }
    conn = NULL;
  }
  if (!handler->is_keep_conn()) {
    session->add_back_kept_cross_join_tmp_table();
    map<DataSpace *, Connection *> kept_connections =
        session->get_kept_connections();
    if (!kept_connections.empty()) {
      map<DataSpace *, Connection *>::iterator it;
      for (it = kept_connections.begin(); it != kept_connections.end(); it++) {
        handler->put_back_connection(it->first, it->second);
      }
    }
  }
  if (error_packet && error_packet != packet) {
    delete error_packet;
    error_packet = NULL;
  }
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
  if (optimize_xa_non_modified_sql) session->clean_xa_modified_conn_map();
  // session->clear_auto_increment_delete_values();
  LOG_DEBUG("Finish clean\n");
}

void MySQLTransactionUNLockNode::deal_savepoint() {
  // savepoint is not support for xa transaction
  // savepoint is not support for cluster xa transaction
  if ((!enable_xa_transaction ||
       session->get_session_option("close_session_xa").int_val != 0) &&
      session->check_for_transaction() &&
      !session->is_in_cluster_xa_transaction() &&
      plan->statement->get_stmt_node()->type == STMT_SAVEPOINT) {
    if (is_send_to_client) {
      LOG_DEBUG("deal_savepoint in TransactionUNLockNode %@\n", this);
      session->add_savepoint(
          plan->statement->get_stmt_node()->sql->savepoint_oper->point);
    }
  }
}

void MySQLTransactionUNLockNode::handle_if_no_kept_conn() {
  status = EXECUTE_STATUS_COMPLETE;
  deal_savepoint();
  if (!is_send_to_client || session->is_call_store_procedure()) return;

  bool has_dead_conn = false;
  vector<conn_result>::iterator it = commit_conn_result.begin();
  for (; it != commit_conn_result.end(); it++) {
    if (it->is_dead_conn) {
      has_dead_conn = true;
      break;
    }
  }

  if (has_dead_conn)
    throw ExecuteNodeError(
        "Get exception for commit with dead server connection.");

  send_ok_packet_to_client(handler, 0, 0);
  return;
}

void MySQLTransactionUNLockNode::handle_if_not_in_transaction() {
  status = EXECUTE_STATUS_COMPLETE;
  if (!is_send_to_client || session->is_call_store_procedure()) return;

  send_ok_packet_to_client(handler, 0, 0);
  return;
}

void MySQLTransactionUNLockNode::report_trans_end_for_consistence_point() {
  if (session->get_need_wait_for_consistence_point()) {
    session->end_commit_consistence_transaction();
  }
}
bool MySQLTransactionUNLockNode::execute_send_part() {
  map<DataSpace *, Connection *> kept_connections =
      session->get_kept_connections();
  if (kept_connections.empty()) {
    handle_if_no_kept_conn();
    return true;
  }
  if (!session->is_in_transaction() &&
      plan->statement->get_stmt_node()->type != STMT_UNLOCK_TB) {
    handle_if_not_in_transaction();
    return true;
  }

  stmt_type st_type = plan->statement->get_stmt_node()->type;
  if (st_type == STMT_COMMIT) {
    if (session->get_need_wait_for_consistence_point()) {
      session->start_commit_consistence_transaction();
    }
  }

  MySQLQueryRequest query(sql);
  query.set_sql_replace_char(plan->session->get_query_sql_replace_null_char());
  Packet exec_packet;
  query.pack(&exec_packet);

  map<DataSpace *, Connection *>::iterator it2;
  for (it2 = kept_connections.begin(); it2 != kept_connections.end();) {
    dataspace = it2->first;
    try {
#ifndef DBSCALE_TEST_DISABLE
      dbscale_test_info *test_info = plan->session->get_dbscale_test_info();
      if (!strcasecmp(test_info->test_case_name.c_str(),
                      "send_to_server_retry") &&
          !strcasecmp(test_info->test_case_operation.c_str(),
                      "throw_errors_commit") &&
          !strcasecmp(dataspace->get_name(), "tbl_bob")) {
        throw ExecuteNodeError("dbscale test execute node error.");
      }
#endif
      conn = handler->send_to_server_retry(dataspace, &exec_packet, "", false);
      it2++;
    } catch (...) {
      LOG_ERROR("TransactionUNLockNode fail send due to exception.\n");
      LOG_ERROR("Error dataspace name is [%s]\n", dataspace->get_name());
      list<string> *transaction_sqls = session->get_transaction_sqls();
      if (transaction_sqls->size()) {
        string error_massage("All SQLs related to this transaction are:\n");
        list<string>::iterator it_sql = transaction_sqls->begin();
        for (; it_sql != transaction_sqls->end(); it_sql++) {
          error_massage.append("SQL: ");
          error_massage.append(*it_sql);
          error_massage.append("\n");
        }
        LOG_ERROR("%s\n", error_massage.c_str());
      }
      conn_result con_ret;
      con_ret.is_dead_conn = true;
      con_ret.conn = NULL;
      con_ret.space = it2->first;
      con_ret.packet = NULL;
      commit_conn_result.push_back(con_ret);
      conn = it2->second;
      handler->clean_dead_conn(&conn, it2->first);
      kept_connections.erase(it2++);
    }

    conn = NULL;
  }

  /* Check if we can swap, the function is used in DirectExecutionNode.  If it
   * can swap, the state changes from SESSION_STATE_WORKING =>
   * SESSION_STATE_WAITING_SERVER. Otherwise, session state cahnge from
   * SESSION_STATE_WORKING => SESSION_STATE_HANDLING_RESULT */
  if (session->is_may_backend_exec_swap_able()) {
    map<DataSpace *, Connection *>::iterator it3;
    for (it3 = kept_connections.begin(); it3 != kept_connections.end(); it3++) {
      conn = it3->second;
      if (conn) {
        conn->set_session(session);
        struct event *conn_event = conn->get_conn_socket_event();
        evutil_socket_t conn_socket = conn->get_conn_socket();
        SocketEventBase *conn_base = conn->get_socket_base();
#ifdef DEBUG
        ACE_ASSERT(conn_base);
#endif
        session->add_server_base(conn_socket, conn_base, conn_event);
#ifdef DEBUG
        LOG_DEBUG(
            "Finish to prepare the conn socket session swap for conn %@ and "
            "session %@.\n",
            conn, session);
#endif
      }
    }
    conn = NULL;
    session->set_mul_connection_num(kept_connections.size());
    return true;
  }
  return false;
}
void MySQLTransactionUNLockNode::execute() {
#ifdef DEBUG
  node_start_timing();
#endif
  if (enable_cluster_xa_transaction &&
      session->is_in_cluster_xa_transaction()) {
    LOG_ERROR("is in xa transaction, cann't execute transaction sqls.\n");
    string err_message =
        "XAER_RMFAIL: The command cannot be executed when global transaction "
        "is in the  ";
    err_message.append(session
                           ->cluster_xa_transaction_to_string(
                               session->get_cluster_xa_transaction_state())
                           .c_str());
    err_message.append(" state");
    error_packet = Backend::instance()->get_new_packet();
    MySQLErrorResponse error(ERROR_XAER_RMFAIL_CODE, err_message.c_str(),
                             "XAE07");
    error.pack(error_packet);
    throw ErrorPacketException();
  }

  if (close_cross_node_transaction && session->session_modify_mul_server()) {
    LOG_ERROR(
        "Refuse to execute cross node transaction when "
        "close_cross_node_transaction != 0.\n");
    throw Error(
        "Refuse to execute cross node transaction when "
        "close_cross_node_transaction != 0.");
  }
  if (enable_xa_transaction &&
      plan->session->get_session_option("close_session_xa").int_val != 0 &&
      session->session_modify_mul_server()) {
    LOG_ERROR(
        "enable-xa-transaction =1 and current session close-session-xa !=0, so "
        "current session should not execute cross node transaction.\n");
    throw Error(
        "enable-xa-transaction =1 and current session close-session-xa !=0, so "
        "current session should not execute cross node transaction.");
  }

  if (session->get_session_state() == SESSION_STATE_WORKING) {
    if (execute_send_part()) return;
  }
  execute_receive_part();
}

bool MySQLTransactionUNLockNode::can_swap() {
  return session->is_may_backend_exec_swap_able();
}
void MySQLTransactionUNLockNode::execute_receive_part() {
  map<DataSpace *, Connection *> kept_connections =
      session->get_kept_connections();
  stmt_type st_type = plan->statement->get_stmt_node()->type;
  packet = Backend::instance()->get_new_packet();
  try {
    map<DataSpace *, Connection *>::iterator it;
    for (it = kept_connections.begin(); it != kept_connections.end(); it++) {
      try {
        conn = it->second;
        handler->receive_from_server(conn, packet);
        if (driver->is_error_packet(packet)) {
          conn_result con_ret;
          con_ret.conn = conn;
          con_ret.space = it->first;
          con_ret.is_dead_conn = false;

          MySQLErrorResponse response(packet);
          if (response.is_shutdown()) {
            if (conn) {
              con_ret.is_dead_conn = true;
            }
            session->set_server_shutdown(true);
            if (session->defined_lock_need_kept_conn(it->first))
              session->remove_defined_lock_kept_conn(it->first);
          }
          Packet *tmp_packet = Backend::instance()->get_new_packet();
          con_ret.packet = packet;
          packet = tmp_packet;
          commit_conn_result.push_back(con_ret);
          conn = NULL;
          continue;
        }
        conn->reset();
        handler->put_back_connection(it->first, it->second);
        conn = NULL;

      } catch (...) {
        LOG_DEBUG("TransactionUNLockNode fail receive due to exception.\n");
        conn_result con_ret;
        con_ret.is_dead_conn = true;
        con_ret.conn = NULL;
        con_ret.space = it->first;
        con_ret.packet = NULL;
        commit_conn_result.push_back(con_ret);
        if (session->defined_lock_need_kept_conn(it->first))
          session->remove_defined_lock_kept_conn(it->first);
        handler->clean_dead_conn(&conn, it->first);
      }
    }
    conn = NULL;

    if (commit_conn_result.empty()) {
      if (is_send_to_client) {
        handler->deal_autocommit_with_ok_eof_packet(packet);
        handler->record_affected_rows(packet);
        if (!session->is_call_store_procedure()) {
#ifdef DEBUG
          LOG_DEBUG("Sending transaction or unlock result to client\n");
#endif
          handler->send_to_client(packet);
        }
        deal_savepoint();
        is_send_to_client = false;
      }
    } else {
      /* If there is dead conn, we ignore all error packet, and throw
       * ExecuteNodeError. Otherwise, we store and use the first
       * error_packet.*/
      bool has_dead_conn = false;
      error_packet = NULL;
      vector<conn_result>::iterator it = commit_conn_result.begin();
      for (; it != commit_conn_result.end(); it++) {
        if (it->is_dead_conn) {
          has_dead_conn = true;
          if (it->conn) handler->clean_dead_conn(&(it->conn), it->space);
          continue;
        }
        if (it->packet) {
          if (error_packet)
            delete it->packet;
          else
            error_packet = it->packet;
          if (it->conn) handler->put_back_connection(it->space, it->conn);
        }
      }

      if (has_dead_conn)
        throw ExecuteNodeError(
            "Get exception for commit with dead server connection.");
      if (error_packet) throw ErrorPacketException();
      throw ExecuteNodeError("Unexpect behavior.");
    }
  } catch (ErrorPacketException &e) {
    LOG_DEBUG(
        "TransactionUNLockNode get an error packet when execute sql [%s].\n",
        sql);
    if (st_type == STMT_COMMIT) report_trans_end_for_consistence_point();
    status = EXECUTE_STATUS_COMPLETE;
    throw e;
  } catch (ExecuteNodeError &e) {
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace);
      conn = NULL;
    }
    got_error = true;
    if (st_type == STMT_COMMIT) report_trans_end_for_consistence_point();
    status = EXECUTE_STATUS_COMPLETE;
    throw e;
  } catch (exception &e) {
    got_error = true;
    if (st_type == STMT_COMMIT) report_trans_end_for_consistence_point();

    status = EXECUTE_STATUS_COMPLETE;
    LOG_DEBUG("TransactionUNLockNode fail due to exception [%s].\n", e.what());
    string error_message("TransactionUNLockNode fail due to exception:");
    error_message.append(e.what());
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace);
      /* Set the conn = NULL in case clean this conn again in deconstruct
       * method. */
      conn = NULL;
    }
    throw ExecuteNodeError(error_message.c_str());
    ;
  }
#ifdef DEBUG
  node_end_timing();
#endif
#ifdef DEBUG
  LOG_DEBUG("MySQLTransactionUNLockNode %@ cost %d ms\n", this, node_cost_time);
#endif
  if (st_type == STMT_COMMIT) report_trans_end_for_consistence_point();
  status = EXECUTE_STATUS_COMPLETE;
}

/* class MySQLXATransactionNode */
MySQLXATransactionNode::MySQLXATransactionNode(ExecutePlan *plan,
                                               const char *sql,
                                               bool is_send_to_client)
    : MySQLTransactionUNLockNode(plan, sql, is_send_to_client),
      has_optimized_readonly_conn(false) {
  this->name = "MySQLXATransactionNode";
  st_type = plan->statement->get_stmt_node()->type;
  is_error = false;
  warnings = 0;
  xa_command_type = XA_ROLLBACK;
  fail_num = 0;
  can_swap = false;
  has_swap = false;
  xa_state = XA_INIT_STATE;
  all_kept_conn_optimized = false;
  commit_fail = false;
}

void handle_conn_and_session_for_error(Connection *conn, Session *s,
                                       DataSpace *space) {
  s->get_handler()->clean_dead_conn(&conn, space);
}

int MySQLXATransactionNode::exec_non_xa_for_all_kept_conn(
    map<DataSpace *, Connection *> *kept_connections, DataSpace *dataspace,
    XACommand type, bool is_send_only) {
  try {
    if (is_send_only) {
      map<DataSpace *, Connection *>::iterator it;
      unsigned long xid = session->get_xa_id();
      char tmp[256];
      // TODO: This read-only connection will clean by backend thread in the
      // future #1427
      for (it = kept_connections->begin(); it != kept_connections->end();) {
        if (it->second->get_resource_status() == RESOURCE_STATUS_DEAD) {
          LOG_ERROR("Kept connection has been killed!\n");
          throw HandlerError("Connection has been killed!");
        }

        int len = sprintf(
            tmp, "%lu-%u-%d-%d-%u", xid, it->first->get_virtual_machine_id(),
            it->second->get_group_id(), Backend::instance()->get_cluster_id(),
            it->second->get_thread_id());
        tmp[len] = '\0';
        string sql = "XA END '";
        sql += tmp;
        sql += "'; XA ROLLBACK '";
        sql += tmp;
        sql += "';";
        if (it->first != dataspace) {
          try {
            if (it->second) {
              it->second->execute(sql.c_str(), 2);
              it->second->set_start_xa_conn(false);
            }
            it++;
          } catch (...) {
            if (session->defined_lock_need_kept_conn(it->first))
              session->remove_defined_lock_kept_conn(it->first);
            session->remove_kept_connection(it->first);
            session->remove_using_conn(it->second);
            it->second->get_pool()->add_back_to_dead(it->second);
            it->second = NULL;
            kept_connections->erase(it++);
          }
        } else {
          it++;
        }
      }
      if (dataspace && kept_connections->count(dataspace)) {
        if (type == XA_PREPARE) {
          int len = sprintf(
              tmp, "%lu-%u-%d-%d-%u", xid, dataspace->get_virtual_machine_id(),
              dataspace->get_group_id(), Backend::instance()->get_cluster_id(),
              ((*kept_connections)[dataspace])->get_thread_id());
          tmp[len] = '\0';
          string sql = "XA END '";
          sql += tmp;
          sql += "';XA COMMIT '";
          sql += tmp;
          sql += "' ONE PHASE";
          ((*kept_connections)[dataspace])->execute(sql.c_str(), 2);
          ((*kept_connections)[dataspace])->set_start_xa_conn(false);
        } else if (type == XA_ROLLBACK) {
          string sql = "XA END '";
          sql += tmp;
          sql += "'; XA ROLLBACK '";
          sql += tmp;
          sql += "';";
          ((*kept_connections)[dataspace])->execute(sql.c_str(), 2);
          ((*kept_connections)[dataspace])->set_start_xa_conn(false);
        } else {
          LOG_ERROR(
              "XACommand is not XA_PREPARE or XA_ROLLBACK in "
              "MySQLXATransactionNode\n");
          throw HandlerError("XACommand is not XA_PREPARE or XA_ROLLBACK");
        }
      }

      if (can_swap) {
        xa_swap_conns(kept_connections);
        session->set_mul_connection_num(kept_connections->size());
      }
      return 0;
    } else {
      if (dataspace && kept_connections->count(dataspace)) {
        if (((*kept_connections)[dataspace])->get_resource_status() ==
            RESOURCE_STATUS_DEAD) {
          LOG_ERROR("Kept connection has been killed!\n");
          throw HandlerError("Connection has been killed!");
        }

        ((*kept_connections)[dataspace])->handle_client_modify_return();
      }
    }
  } catch (...) {
    if (dataspace && kept_connections->count(dataspace)) {
      if (session->defined_lock_need_kept_conn(dataspace))
        session->remove_defined_lock_kept_conn(dataspace);
      session->remove_kept_connection(dataspace);
      session->remove_using_conn((*kept_connections)[dataspace]);
      ((*kept_connections)[dataspace])
          ->get_pool()
          ->add_back_to_dead(((*kept_connections)[dataspace]));
      ((*kept_connections)[dataspace]) = NULL;
      is_error = true;
    }
    receive_for_non_modify_connection(kept_connections, dataspace);
    return 1;
  }
  receive_for_non_modify_connection(kept_connections, dataspace);
  return 0;
}

void MySQLXATransactionNode::receive_for_non_modify_connection(
    map<DataSpace *, Connection *> *kept_connections, DataSpace *dataspace) {
  map<DataSpace *, Connection *>::iterator it;
  for (it = kept_connections->begin(); it != kept_connections->end(); it++) {
    if (it->first != dataspace) {
      try {
        if (it->second) {
          it->second->handle_client_modify_return();
        }
      } catch (...) {
        if (session->defined_lock_need_kept_conn(it->first))
          session->remove_defined_lock_kept_conn(it->first);
        session->remove_kept_connection(it->first);
        session->remove_using_conn(it->second);
        it->second->get_pool()->add_back_to_dead(it->second);
        it->second = NULL;
      }
    }
  }
}

list<DataSpace *> MySQLXATransactionNode::get_transaction_modified_spaces(
    Session *s) {
  map<DataSpace *, list<string> > *space_map = s->get_redo_logs_map();
  list<DataSpace *> dataspace_list;
  if (space_map == 0) {
    LOG_ERROR("redo log map is empty.\n");
    throw Error("redo log map is empty.");
  }
  map<DataSpace *, list<string> >::iterator it = space_map->begin();
  list<string>::iterator list_it;
  for (; it != space_map->end(); it++) {
    for (list_it = (it->second).begin(); list_it != (it->second).end();
         list_it++) {
      if (is_modify_sql(*list_it)) {
        dataspace_list.push_back(it->first);
        break;
      }
    }
  }
  return dataspace_list;
}

void MySQLXATransactionNode::xa_swap_conns(
    map<DataSpace *, Connection *> *kept_connections) {
  Connection *conn = NULL;
  map<DataSpace *, Connection *>::iterator it3;
  for (it3 = kept_connections->begin(); it3 != kept_connections->end(); it3++) {
    conn = it3->second;
    if (conn) {
      conn->set_session(session);
      struct event *conn_event = conn->get_conn_socket_event();
      evutil_socket_t conn_socket = conn->get_conn_socket();
      SocketEventBase *conn_base = conn->get_socket_base();
#ifdef DEBUG
      ACE_ASSERT(conn_base);
#endif
      session->add_server_base(conn_socket, conn_base, conn_event);
#ifdef DEBUG
      LOG_DEBUG(
          "Finish to prepare the conn socket session swap for conn %@ and "
          "session %@.\n",
          conn, session);
#endif
      // has_swap = true should only when session has add_server_base
      has_swap = true;
    }
  }
}
void MySQLXATransactionNode::xa_swap_conns(Connection *conn) {
  conn->set_session(session);
  struct event *conn_event = conn->get_conn_socket_event();
  evutil_socket_t conn_socket = conn->get_conn_socket();
  SocketEventBase *conn_base = conn->get_socket_base();
#ifdef DEBUG
  ACE_ASSERT(conn_base);
#endif
  session->add_server_base(conn_socket, conn_base, conn_event);
#ifdef DEBUG
  LOG_DEBUG(
      "Finish to prepare the conn socket session swap for conn %@ and session "
      "%@.\n",
      conn, session);
#endif
  session->set_mul_connection_num(1);
  has_swap = true;
}

void MySQLXATransactionNode::xa_swap_conns(map<Connection *, bool> *tx_rrlkc) {
  map<Connection *, bool>::iterator tx_it;
  Connection *conn = NULL;
  for (tx_it = tx_rrlkc->begin(); tx_it != tx_rrlkc->end(); tx_it++) {
    conn = tx_it->first;
    if (conn) {
      conn->set_session(session);
      struct event *conn_event = conn->get_conn_socket_event();
      evutil_socket_t conn_socket = conn->get_conn_socket();
      SocketEventBase *conn_base = conn->get_socket_base();
#ifdef DEBUG
      ACE_ASSERT(conn_base);
#endif
      session->add_server_base(conn_socket, conn_base, conn_event);
#ifdef DEBUG
      LOG_DEBUG(
          "Finish to prepare the conn socket session swap for conn %@ and "
          "session %@.\n",
          conn, session);
#endif
      has_swap = true;
    }
  }
}
int MySQLXATransactionNode::exec_xa_for_all_kept_conn(
    map<DataSpace *, Connection *> *kept_connections, XACommand type,
    bool is_send) {
  int ret = 0, tmp = 0;
  map<DataSpace *, Connection *>::iterator it;
  Driver *driver = Driver::get_driver();
  XA_helper *xa_helper = driver->get_xa_helper();
  unsigned long xid = session->get_xa_id();
  if (type == XA_PREPARE) {
    if (is_send) {
      for (it = kept_connections->begin(); it != kept_connections->end();) {
        tmp = xa_helper->exec_xa_commit_first_phase(session, it->first,
                                                    it->second);
        if (tmp) {
          ret++;
          int r = 1;
          if (!ACE_Reactor::instance()->reactor_event_loop_done()) {
            r = xa_helper->handle_xa_if_prepared_server_alived(
                session, it->first, it->second, true);
          }
          if (tmp == ERR_XA_CONN || r) {
            if (session->defined_lock_need_kept_conn(it->first))
              session->remove_defined_lock_kept_conn(it->first);
            handle_conn_and_session_for_error(it->second, session, it->first);
            char tmp_xid[256];
            int len = sprintf(tmp_xid, "%lu-%u-%d-%d-%u", xid,
                              it->first->get_virtual_machine_id(),
                              it->second->get_group_id(),
                              Backend::instance()->get_cluster_id(),
                              it->second->get_thread_id());
            tmp_xid[len] = '\0';
            Backend::instance()->add_source_need_rollback_xid_machine(
                it->first->get_data_source(), tmp_xid);
          }
          kept_connections->erase(it++);
        } else
          it++;
      }
      if (can_swap) {
        map<Connection *, bool> *tx_rrlkc =
            session->get_tx_record_redo_log_kept_connections();
        xa_swap_conns(tx_rrlkc);
        xa_swap_conns(kept_connections);
        session->set_mul_connection_num(kept_connections->size() +
                                        tx_rrlkc->size());
      }
      return ret;
    } else {
      map<Connection *, bool> *tx_rrlkc =
          session->get_tx_record_redo_log_kept_connections();
      map<Connection *, bool>::iterator tx_it;
      for (tx_it = tx_rrlkc->begin(); tx_it != tx_rrlkc->end(); tx_it++) {
        tmp = xa_helper->handle_after_xa_prepare(session, tx_it->first);
        if (tmp) {
          ret++;
        }
        if (tmp == ERR_XA_CONN) {
          session->set_tx_record_redo_log_connection_to_dead(tx_it->first);
        }
      }
      session->clean_all_tx_record_redo_log_kept_connections();
      for (it = kept_connections->begin(); it != kept_connections->end();) {
        tmp = xa_helper->handle_after_xa_prepare(session, it->second);
        (it->second)->reset();
        if (tmp) {
          ret++;
          int r = 1;
          if (!ACE_Reactor::instance()->reactor_event_loop_done()) {
            r = xa_helper->handle_xa_if_prepared_server_alived(
                session, it->first, it->second, true);
          }
          if (tmp == ERR_XA_CONN || r) {
            if (session->defined_lock_need_kept_conn(it->first))
              session->remove_defined_lock_kept_conn(it->first);
            handle_conn_and_session_for_error(it->second, session, it->first);
            char tmp_xid[256];
            int len = sprintf(tmp_xid, "%lu-%u-%d-%d-%u", xid,
                              it->first->get_virtual_machine_id(),
                              it->second->get_group_id(),
                              Backend::instance()->get_cluster_id(),
                              it->second->get_thread_id());
            tmp_xid[len] = '\0';
            Backend::instance()->add_source_need_rollback_xid_machine(
                it->first->get_data_source(), tmp_xid);
          }
          kept_connections->erase(it++);
        } else
          it++;
      }
      return ret;
    }
  } else if (type == XA_ROLLBACK) {
    if (is_send) {
      for (it = kept_connections->begin(); it != kept_connections->end();) {
        tmp = xa_helper->exec_xa_rollback(session, it->first, it->second);
        if (tmp) {
          // For rollback, we assume that the only possible reason for execution
          // fail is server unavaliable.
          ret++;
          int r = 1;
          if (!ACE_Reactor::instance()->reactor_event_loop_done()) {
            r = xa_helper->handle_xa_if_prepared_server_alived(
                session, it->first, it->second, true);
          }
          if (tmp == ERR_XA_CONN || r) {
            if (session->defined_lock_need_kept_conn(it->first))
              session->remove_defined_lock_kept_conn(it->first);
            handle_conn_and_session_for_error(it->second, session, it->first);
            char tmp_xid[256];
            int len = sprintf(tmp_xid, "%lu-%u-%d-%d-%u", xid,
                              it->first->get_virtual_machine_id(),
                              it->second->get_group_id(),
                              Backend::instance()->get_cluster_id(),
                              it->second->get_thread_id());
            tmp_xid[len] = '\0';
            Backend::instance()->add_source_need_rollback_xid_machine(
                it->first->get_data_source(), tmp_xid);
          }

          kept_connections->erase(it++);
        } else
          it++;
      }
    }
    return ret;
  } else if (type == XA_ROLLBACK_AFTER_PREPARE) {
    if (is_send) {
      for (it = kept_connections->begin(); it != kept_connections->end();) {
        tmp = xa_helper->exec_xa_rollback_after_prepare(session, it->first,
                                                        it->second);
        if (tmp) {
          int r = 1;
          if (!ACE_Reactor::instance()->reactor_event_loop_done()) {
            r = xa_helper->handle_xa_if_prepared_server_alived(
                session, it->first, it->second, true);
          }
          if (r) {
            char tmp[256];
            int len = sprintf(tmp, "%lu-%u-%d-%d-%u", xid,
                              it->first->get_virtual_machine_id(),
                              it->second->get_group_id(),
                              Backend::instance()->get_cluster_id(),
                              it->second->get_thread_id());
            tmp[len] = '\0';
            Backend::instance()->add_source_need_rollback_xid_machine(
                it->first->get_data_source(), tmp);
          }
          // For rollback, we assume that the only possible reason for execution
          // fail is server unavaliable.
          ret++;
          if (session->defined_lock_need_kept_conn(it->first))
            session->remove_defined_lock_kept_conn(it->first);
          handle_conn_and_session_for_error(it->second, session, it->first);
          kept_connections->erase(it++);
        } else
          it++;
      }
    }
    return ret;
  } else if (type == XA_COMMIT) {
    if (is_send) {
#ifndef DBSCALE_TEST_DISABLE
      /*just for test federated_max_rows*/
      Backend *bk = Backend::instance();
      dbscale_test_info *test_info = bk->get_dbscale_test_info();
      if (!strcasecmp(test_info->test_case_name.c_str(), "xa_running_test") &&
          !strcasecmp(test_info->test_case_operation.c_str(), "commit_block")) {
        LOG_DEBUG(
            "Do dbscale test operation 'commit_block' for case "
            "'xa_running_test'.\n");
        while (strcasecmp(test_info->test_case_operation.c_str(), "")) {
          ACE_OS::sleep(1);
        }
        LOG_DEBUG(
            "Finish test operation 'commit_block' for case "
            "'xa_running_test'.\n");
      }
#endif

      for (it = kept_connections->begin(); it != kept_connections->end();) {
        tmp = xa_helper->exec_xa_commit_second_phase(session, it->first,
                                                     it->second);
        if (tmp) {
          // For commit, we assume that the only possible reason for execution
          // fail is server unavaliable.
          ret++;
          int r = 1;
          if (!ACE_Reactor::instance()->reactor_event_loop_done()) {
            r = xa_helper->handle_xa_if_prepared_server_alived(
                session, it->first, it->second, false);
          }
          if (r) {
            // the xid machine need commit success, add to backend and wait for
            // recover
            char tmp[256];
            int len = sprintf(tmp, "%lu-%u-%d-%d-%u", xid,
                              it->first->get_virtual_machine_id(),
                              it->second->get_group_id(),
                              Backend::instance()->get_cluster_id(),
                              it->second->get_thread_id());
            tmp[len] = '\0';
            Backend::instance()->add_source_need_recover_xid_machine(
                it->first->get_data_source(), tmp);
            commit_fail = true;
          } else
            handle_xa_success_spaces.push_back(it->first);
          if (session->defined_lock_need_kept_conn(it->first))
            session->remove_defined_lock_kept_conn(it->first);
          handle_conn_and_session_for_error(it->second, session, it->first);
          kept_connections->erase(it++);
        } else
          it++;
      }
      if (can_swap) {
        xa_swap_conns(kept_connections);
        session->set_mul_connection_num(kept_connections->size());
      }
      return ret;
    } else {
      for (it = kept_connections->begin(); it != kept_connections->end();
           it++) {
        tmp = xa_helper->handle_after_xa_commit(session, it->first, it->second);
        if (tmp) {
          // For commit, we assume that the only possible reason for execution
          // fail is server unavaliable.
          ret++;
          int r = 1;
          if (!ACE_Reactor::instance()->reactor_event_loop_done()) {
            r = xa_helper->handle_xa_if_prepared_server_alived(
                session, it->first, it->second, false);
          }
          if (r) {
            // the xid machine need commit success, add to backend and wait
            // for recover
            unsigned long xid = session->get_xa_id();
            char tmp[256];
            int len = sprintf(tmp, "%lu-%u-%d-%d-%u", xid,
                              it->first->get_virtual_machine_id(),
                              it->second->get_group_id(),
                              Backend::instance()->get_cluster_id(),
                              it->second->get_thread_id());
            tmp[len] = '\0';
            Backend::instance()->add_source_need_recover_xid_machine(
                it->first->get_data_source(), tmp);
            commit_fail = true;
          } else
            handle_xa_success_spaces.push_back(it->first);
          if (session->defined_lock_need_kept_conn(it->first))
            session->remove_defined_lock_kept_conn(it->first);
          handle_conn_and_session_for_error(it->second, session, it->first);
        }
      }
      if (!commit_fail) {
        unsigned long xid = session->get_xa_id();
        char tmp_xid[256];
        int len = sprintf(tmp_xid, "%d-%lu",
                          Backend::instance()->get_cluster_id(), xid);
        tmp_xid[len] = '\0';

        // only all connection commit success, then do clean
        for (it = kept_connections->begin(); it != kept_connections->end();
             it++) {
          MySQLXA_purge_thread::instance()->add_xid_to_purge(
              it->first->get_data_source(), tmp_xid);
        }
        vector<DataSpace *>::iterator it2 = handle_xa_success_spaces.begin();
        for (; it2 != handle_xa_success_spaces.end(); it2++) {
          MySQLXA_purge_thread::instance()->add_xid_to_purge(
              (*it2)->get_data_source(), tmp_xid);
        }
      }
      return ret;
    }
  }
  return ret;
}
void MySQLXATransactionNode::change_xa_state(XASwapState state) {
  LOG_DEBUG("xa state change from [%d] to [%d] \n", xa_state, state);
  xa_state = state;
}

bool MySQLXATransactionNode::xa_init() {
  kept_connections = session->get_kept_connections();
  map<DataSpace *, Connection *>::iterator it = kept_connections.begin();
  for (; it != kept_connections.end(); ++it) {
    if (it->first == Backend::instance()->get_auth_data_space()) {
      DataSpace *auth_space = it->first;
      Connection *auth_conn = it->second;
      handle_auth_space_for_xa(auth_space, auth_conn);
      kept_connections.erase(it);
      break;
    }
  }
  if (kept_connections.empty()) {
    handle_if_no_kept_conn();
    return false;
  }
  if (!session->is_in_transaction()) {
    handle_if_not_in_transaction();
    return false;
  }
  if (!session->get_xa_id()) {
    handle_if_not_in_transaction();
    return false;
  }

  if (st_type == STMT_COMMIT) {
    if (session->get_xa_need_wait_for_consistence_point()) {
      session->start_commit_consistence_transaction();
    }
    if (!session->get_has_global_flush_read_lock()) {
      /*In flush tables with read lock mode, one server may has more than one
       * kept conn to execute flush command and hold the global read lock.  IN
       * this case, do xa commit will deadlock. So in this case, we should do
       * xa rollback, which is the default value for xa_command_type.*/
      xa_command_type = XA_PREPARE;
    }
  }

  if (!optimize_xa_non_modified_sql)
    dataspace_list = get_transaction_modified_spaces(session);
  else
    handle_optimize_xa_non_modified_sql();
  can_swap = session->is_may_backend_exec_swap_able();
  change_xa_state(XA_FIRST_PHASE_SEND_STATE);
  return true;
}

void MySQLXATransactionNode::handle_auth_space_for_xa(DataSpace *auth_space,
                                                      Connection *auth_conn) {
  unsigned long xid = session->get_xa_id();
  char tmp[256];
  int len =
      sprintf(tmp, "%lu-%u-%d-%d-%u", xid, auth_space->get_virtual_machine_id(),
              auth_conn->get_group_id(), Backend::instance()->get_cluster_id(),
              auth_conn->get_thread_id());
  tmp[len] = '\0';
  string sql = "XA END '";
  sql += tmp;
  sql += "'; XA ROLLBACK '";
  sql += tmp;
  sql += "';";
  try {
    auth_conn->execute(sql.c_str(), 2);
    auth_conn->handle_client_modify_return();
    auth_conn->set_start_xa_conn(false);
  } catch (...) {
    session->remove_kept_connection(auth_space);
    session->remove_using_conn(auth_conn);
    auth_conn->get_pool()->add_back_to_dead(auth_conn);
  }
}

void MySQLXATransactionNode::handle_optimize_xa_non_modified_sql() {
  if (optimize_xa_non_modified_sql && !has_optimized_readonly_conn &&
      !plan->session->is_in_lock() && plan->session->is_in_transaction()) {
    has_optimized_readonly_conn = true;
    bool contain_readonly_conn = false;
    map<DataSpace *, Connection *>::iterator it;
    for (it = kept_connections.begin(); it != kept_connections.end();) {
      char tmp[256];
      int len = sprintf(
          tmp, "%lu-%u-%d-%d-%u", session->get_xa_id(),
          it->first->get_virtual_machine_id(), it->second->get_group_id(),
          Backend::instance()->get_cluster_id(), it->second->get_thread_id());
      tmp[len] = '\0';
      if (readonly_conn_can_optimize(it->first, it->second, tmp)) {
        kept_connections.erase(it++);
        contain_readonly_conn = true;
        continue;
      }
      it++;
    }
    if (contain_readonly_conn) {
      session->get_status()->item_inc(TIMES_XA_OPTIMIZE_READ_ONLY_STMT);
      for (unsigned int i = 0; i < optimize_xa_non_modified_sql_thread_num;
           ++i) {
        Backend::instance()->get_xa_readonly_conn_handler(i)->signal_cond();
      }
      if (kept_connections.empty()) {
        all_kept_conn_optimized = true;
      }
    }

    dataspace_list.clear();
    map<DataSpace *, Connection *>::iterator map_it;
    for (map_it = kept_connections.begin(); map_it != kept_connections.end();
         map_it++) {
      dataspace_list.push_back(map_it->first);
    }
  }
}

bool MySQLXATransactionNode::readonly_conn_can_optimize(DataSpace *dataspace,
                                                        Connection *conn,
                                                        string xid) {
  // should not put back the read only conn in the lock mode.
  if (!session->get_has_global_flush_read_lock() &&
      !plan->session->is_xa_modified_conn(conn)) {
    // should not put back the read only conn in get_lock.
    if (session->defined_lock_need_kept_conn(dataspace)) return false;
    if (session->remove_kept_connection(dataspace)) {
      session->remove_using_conn(conn);
      // should check whether remove kept_connections success or not
      // Otherwise there are two threads share one connection
      Backend::instance()
          ->get_xa_readonly_conn_handler(
              atoll(xid.c_str()) % optimize_xa_non_modified_sql_thread_num)
          ->add_conn_to_map(conn, xid);
      LOG_DEBUG(
          "in xa transaction, add readonly connection [%@] to backend thread, "
          "this backend thread will send 'XA ROLLBACK' to server\n",
          conn);
      return true;
    }
  }
  return false;
}
void MySQLXATransactionNode::report_xa_trans_end_for_consistence_point() {
  if (session->get_xa_need_wait_for_consistence_point()) {
    session->end_commit_consistence_transaction();
  }
}
void MySQLXATransactionNode::execute() {
  if (close_cross_node_transaction && session->session_modify_mul_server()) {
    LOG_ERROR(
        "Refuse to execute cross node transaction when "
        "close_cross_node_transaction != 0.\n");
    throw Error(
        "Refuse to execute cross node transaction when "
        "close_cross_node_transaction != 0.");
  }

  if (all_kept_conn_optimized) {
    status = EXECUTE_STATUS_COMPLETE;
    change_xa_state(XA_END_STATE);
  }
  try {
    if (xa_state == XA_INIT_STATE) {
      if (!xa_init()) return;
    }

    if (!all_kept_conn_optimized &&
        (dataspace_list.size() == 1 || dataspace_list.size() == 0)) {
      DataSpace *space = NULL;
      if (dataspace_list.size() == 1) {
        space = dataspace_list.front();
        if (xa_state == XA_FIRST_PHASE_SEND_STATE)
          session->get_status()->item_inc(
              TIMES_XA_OPTIMIZE_ONLY_AFFECT_ONE_SERVER);
      }
      switch (xa_state) {
        case XA_FIRST_PHASE_SEND_STATE: {
          fail_num = exec_non_xa_for_all_kept_conn(&kept_connections, space,
                                                   xa_command_type, true);
          // if get error, do not swap
          if (fail_num) {
            change_xa_state(XA_END_STATE);
            break;
          }
          change_xa_state(XA_FIRST_PHASE_RECV_STATE);
          if (has_swap) {
            has_swap = false;
            return;
          }
        }
        case XA_FIRST_PHASE_RECV_STATE: {
          fail_num = exec_non_xa_for_all_kept_conn(&kept_connections, space,
                                                   xa_command_type, false);
          change_xa_state(XA_END_STATE);
          break;
        }
        default: {
          LOG_ERROR("XATransactionNode get unknown xa_state [%d]\n", xa_state);
#ifdef DEBUG
          ACE_ASSERT(0);
#endif
        }
      }
    } else if (!all_kept_conn_optimized) {
      switch (xa_state) {
        case XA_FIRST_PHASE_SEND_STATE: {
          unsigned long xid = session->get_xa_id();
          session->add_running_xa_map(xid,
                                      xa_command_type == XA_PREPARE
                                          ? RUNNING_XA_PREPARE
                                          : RUNNING_XA_ROLLBACK,
                                      &kept_connections);

          session->record_relate_source_redo_log(&kept_connections);
          fail_num = exec_xa_for_all_kept_conn(&kept_connections,
                                               xa_command_type, true);
          change_xa_state(XA_FIRST_PHASE_RECV_STATE);
          if (has_swap) {
            has_swap = false;
            return;
          }
        }
        case XA_FIRST_PHASE_RECV_STATE: {
          fail_num += exec_xa_for_all_kept_conn(&kept_connections,
                                                xa_command_type, false);
          kept_connections.clear();
          kept_connections = session->get_kept_connections();

          if (xa_command_type == XA_PREPARE) {
            if (fail_num == 0) {
              session->acquire_using_conn_mutex();
              // check be killed before go to the last commit, if this check
              // passed, kill will not terminate this handler during the second
              // phase commit.
              if (handler->be_killed()) {
                session->release_using_conn_mutex();
                throw ThreadIsKilled();
              }
              session->set_xa_prepared();
              session->release_using_conn_mutex();
              xa_command_type = XA_COMMIT;
            } else {
              is_error = true;
              xa_command_type = XA_ROLLBACK_AFTER_PREPARE;
            }
          }
          change_xa_state(XA_SECOND_PHASE_SEND_STATE);
        }
        case XA_SECOND_PHASE_SEND_STATE: {
          unsigned long xid = session->get_xa_id();
          if (session->add_running_xa_map(xid, xa_command_type == XA_COMMIT
                                                   ? RUNNING_XA_COMMIT
                                                   : RUNNING_XA_ROLLBACK)) {
            // this running xa transaction contains fail source conn, so abort
            // finial commit.
            if (xa_command_type == XA_COMMIT)
              xa_command_type = XA_ROLLBACK_AFTER_PREPARE;
            LOG_INFO(
                "Set xa transaction %d to XA_ROLLBACK_AFTER_PREPARE for "
                "session %@.\n",
                xid, session);
          }

          if (xa_command_type == XA_COMMIT ||
              xa_command_type == XA_ROLLBACK_AFTER_PREPARE) {
            if (exec_xa_for_all_kept_conn(&kept_connections, xa_command_type,
                                          true)) {
              if (Backend::instance()->get_backend_server_version() ==
                      MYSQL_VERSION_57 ||
                  Backend::instance()->get_backend_server_version() ==
                      MYSQL_VERSION_8) {
                warnings++;
                WarnInfo *w = new WarnInfo(
                    WARNING_COMMIT_DELAY_SUCCESS_CODE,
                    dbscale_err_msg[WARNING_COMMIT_DELAY_SUCCESS_CODE]);
                session->add_warning_info(w);
              } else {
                is_error = true;
              }
            }
          }
          change_xa_state(XA_SECOND_PHASE_RECV_STATE);
          if (has_swap) {
            has_swap = false;
            return;
          }
        }
        case XA_SECOND_PHASE_RECV_STATE: {
          if (xa_command_type == XA_COMMIT ||
              xa_command_type == XA_ROLLBACK_AFTER_PREPARE) {
            if (exec_xa_for_all_kept_conn(&kept_connections, xa_command_type,
                                          false)) {
              if (Backend::instance()->get_backend_server_version() ==
                      MYSQL_VERSION_57 ||
                  Backend::instance()->get_backend_server_version() ==
                      MYSQL_VERSION_8) {
                warnings++;
                WarnInfo *w = new WarnInfo(
                    WARNING_COMMIT_DELAY_SUCCESS_CODE,
                    dbscale_err_msg[WARNING_COMMIT_DELAY_SUCCESS_CODE]);
                session->add_warning_info(w);
              } else {
                is_error = true;
              }
            }
          }
          change_xa_state(XA_END_STATE);
          break;
        }
        default: {
          LOG_ERROR("XATransactionNode get unknown xa_state [%d]\n", xa_state);
#ifdef DEBUG
          ACE_ASSERT(0);
#endif
        }
      }
    }
    if (!session->get_has_global_flush_read_lock()) {
      // In global read lock mode, should not add conn back to free
      session->remove_all_connection_to_free();
    }

    if (is_send_to_client) {
      try {
        // TODO: if commit failed, block the failed source.
        if (is_error) {
          packet = NULL;
          if (xa_command_type == XA_COMMIT) {
            packet = handler->get_error_packet(
                ERROR_XA_EXECUTE_FAIL_CODE,
                "Get error during final transaction commit phase.", NULL);
          } else {
            packet = handler->get_error_packet(
                ERROR_XA_EXECUTE_FAIL_CODE, "Fail to end the XA transaction!",
                NULL);
          }
          if (!session->is_call_store_procedure()) {
            handler->send_to_client(packet);
            delete packet;
            packet = NULL;
            session->set_has_send_client_error_packet();
          }
          error_packet = packet;
          if (session->is_call_store_procedure())
            throw ErrorPacketException();
          else
            throw ExecuteNodeError(
                "MySQLXATransactionNode::execute get error.");
        } else {
          if (!session->is_call_store_procedure()) {
            send_ok_packet_to_client(handler, 0, warnings);
          }
        }
      } catch (...) {
        status = EXECUTE_STATUS_COMPLETE;
        throw;
      }

      is_send_to_client = false;
    }

    status = EXECUTE_STATUS_COMPLETE;
    if (st_type == STMT_COMMIT) report_xa_trans_end_for_consistence_point();

  } catch (Exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    if (st_type == STMT_COMMIT) report_xa_trans_end_for_consistence_point();

    LOG_ERROR("XATransactionNode fail due to Exception [%s].\n", e.what());
    throw;
  } catch (exception &e) {
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    if (st_type == STMT_COMMIT) report_xa_trans_end_for_consistence_point();

    LOG_ERROR("XATransactionNode fail due to exception [%s].\n", e.what());
    string error_message("XATransactionNode fail due to exception:");
    error_message.append(e.what());
    throw ExecuteNodeError(error_message.c_str());
    ;
  }
}

/* class  MySQLDirectExecuteNode */

MySQLDirectExecuteNode::MySQLDirectExecuteNode(ExecutePlan *plan,
                                               DataSpace *dataspace,
                                               const char *sql)
    : MySQLExecuteNode(plan, dataspace),
      sql(sql),
      conn(NULL),
      got_error(false),
      packet(NULL),
      error_packet(NULL) {
  this->name = "MySQLDirectExecuteNode";
  stmt_insert_like = is_stmt_insert_like(plan);
  select_uservar_flag = statement->get_select_uservar_flag();
  select_field_num = statement->get_select_field_num();
  if (select_uservar_flag) {
    uservar_vec = *(statement->get_select_uservar_vec());
  }
  field_num = 0;
  federated_max_rows = (uint64_t)(
      session->get_session_option("max_federated_cross_join_rows").ulong_val);
  cross_join_max_rows = (uint64_t)(
      session->get_session_option("max_cross_join_moved_rows").ulong_val);
  direct_prepare_exec_packet = false;
  has_change_user = false;
  is_show_full_fields = false;
  is_show_full_fields_via_mirror_tb = false;
  is_direct_node = true;
  kept_lock_new_connection = false;
  defined_lock_field_num = 0;

#ifndef DBSCALE_TEST_DISABLE
  /*just for test federated_max_rows*/
  Backend *bk = Backend::instance();
  dbscale_test_info *test_info = bk->get_dbscale_test_info();
  if (!strcasecmp(test_info->test_case_name.c_str(), "federated_max_rows") &&
      !strcasecmp(test_info->test_case_operation.c_str(), "abort")) {
    federated_max_rows = 1;
  }
#endif
}

bool MySQLDirectExecuteNode::is_stmt_insert_like(ExecutePlan *plan) {
  return plan->statement->get_stmt_node()->table_list_head &&
         (plan->statement->get_stmt_node()->type == STMT_INSERT_SELECT ||
          plan->statement->get_stmt_node()->type == STMT_INSERT ||
          plan->statement->get_stmt_node()->type == STMT_REPLACE);
}

void MySQLDirectExecuteNode::clean() {
  if (conn && !(plan->get_is_parse_transparent() &&
                (session->is_in_transaction() ||
                 session->is_in_cluster_xa_transaction()))) {
    Backend *backend = Backend::instance();
    stmt_type type = plan->statement->get_stmt_node()->type;
    if (plan->get_is_parse_transparent() ||
        (has_change_user &&
         (type == STMT_GRANT || type == STMT_SET_PASSWORD ||
          type == STMT_DROP_USER || type == STMT_CREATE_USER ||
          type == STMT_SHOW_DATABASES || type == STMT_SHOW_TABLES ||
          type == STMT_SHOW_GRANTS || type == STMT_REVOKE ||
          type == STMT_ALTER_USER ||
          (type == STMT_SELECT &&
           dataspace == backend->get_metadata_data_space() &&
           datasource_in_one)))) {
      bool add_back_to_dead = true;
      if (!got_error) {
        try {
          conn->set_session(session);
          const char *schema_name = NULL;
          if (session->is_drop_current_schema())
            schema_name = default_login_schema;
          else
            schema_name = session->get_schema();
          if (conn->change_user(admin_user, admin_password, NULL,
                                schema_name)) {
            conn->set_changed_user_conn(false);
            map<string, string> *session_var_map =
                session->get_session_var_map();
            conn->set_session_var(session_var_map, false, has_change_user);
            add_back_to_dead = false;
          }
          session->remove_using_conn(conn);
        } catch (exception &e) {
          session->remove_using_conn(conn);
          LOG_ERROR("When change user back to admin user with error:%s\n",
                    e.what());
        }
      }
      if (add_back_to_dead) {
        handler->clean_dead_conn(&conn, dataspace, false);
      } else {
        handler->put_back_connection(dataspace, conn);
      }
    } else {
      if (!got_error || error_packet) {
        handler->put_back_connection(dataspace, conn);
      } else {
        handler->clean_dead_conn(&conn, dataspace);
      }
    }
    conn = NULL;
  }
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
  if (session->get_work_session())
    session->get_work_session()->increase_finished_federated_num();

  if (session->is_drop_current_schema()) {
    session->set_schema(NULL);
  }
  if (plan->get_need_destroy_dataspace_after_exec()) {
    LOG_DEBUG("destroy dataspace named [%s] in DirectExecutionNode clean.\n",
              dataspace->get_name());
    delete dataspace;
    dataspace = NULL;
  }
}
void MySQLDirectExecuteNode::update_column_display_name(Packet *packet,
                                                        string field_str) {
  MySQLColumnResponse col_resp(packet);
  col_resp.unpack();
  col_resp.set_column(field_str.c_str());
  col_resp.pack(packet);
}

void MySQLDirectExecuteNode::prepare_work(bool &need_check_last_insert_id,
                                          int64_t &before_insert_id) {
  int enable_last_insert_id_session =
      session->get_session_option("enable_last_insert_id").int_val;
  stmt_node *st = plan->statement->get_stmt_node();
  if (enable_last_insert_id_session && stmt_insert_like) {
    // prepare the auto_incrment info and old last insert id value
    AutoIncStatus inc_status = plan->statement->get_auto_inc_status();
    need_check_last_insert_id = inc_status != NO_AUTO_INC_FIELD;
    if ((dataspace && dataspace->get_dataspace_type() == PARTITION_TYPE) ||
        ((dataspace && dataspace->get_dataspace_type() == TABLE_TYPE &&
          dataspace->is_partitioned()))) {
      need_check_last_insert_id = false;
    } else if (st->last_insert_id_num) {
      need_check_last_insert_id = true;
    }
    stmt_type cur_type = plan->statement->get_stmt_node()->type;
    if (plan->get_is_parse_transparent() && is_stmt_type_of_dml(cur_type)) {
      need_check_last_insert_id = true;
    }
    if (need_check_last_insert_id) {
      before_insert_id = handler->get_session()->get_last_insert_id();
      handler->get_last_insert_id_from_server(dataspace, &conn,
                                              before_insert_id, true);
    }
  } else if (enable_last_insert_id_session) {
    if (st->last_insert_id_num) {
      if (dataspace && dataspace->get_dataspace_type() == PARTITION_TYPE) {
        LOG_ERROR(
            "Not support last_insert_id for non insert partition table.\n");
        throw Error(
            "Not support last insert id for non insert partition table.");
      }
      need_check_last_insert_id = true;
    }
    if (need_check_last_insert_id) {
      before_insert_id = handler->get_session()->get_last_insert_id();
      handler->get_last_insert_id_from_server(dataspace, &conn,
                                              before_insert_id, true);
    }
  }
}

void MySQLDirectExecuteNode::handle_set_last_insert_id(
    bool need_check_last_insert_id, int64_t &before_insert_id) {
  int enable_last_insert_id_session =
      session->get_session_option("enable_last_insert_id").int_val;
  if (enable_last_insert_id_session && need_check_last_insert_id && conn) {
    int64_t after_insert_id = 0;
    try {
      after_insert_id =
          handler->get_last_insert_id_from_server(dataspace, &conn);
      int64_t session_before_insert_id =
          handler->get_session()->get_last_insert_id();
      LOG_DEBUG("session before:%B, before:%B, after:%B.\n",
                session_before_insert_id, before_insert_id, after_insert_id);
      if ((session_before_insert_id != after_insert_id)) {
        handler->get_session()->set_last_insert_id(after_insert_id);
      }
    } catch (exception &e) {
      LOG_DEBUG("Update session last insert id with error:%s", e.what());
    }
  }
}
bool MySQLDirectExecuteNode::handle_ok_packet(Packet *packet,
                                              bool need_check_last_insert_id,
                                              int64_t &before_insert_id,
                                              bool &is_non_modified_conn) {
  bool has_more_result = false;
  MySQLOKResponse ok(packet);
  ok.unpack();
  uint64_t affect_rows = ok.get_affected_rows();
  if (affect_rows) is_non_modified_conn = false;
  has_more_result = ok.has_more_result();
#ifdef DEBUG
  LOG_DEBUG("affected rows =%d, has_more_result = %d\n", affect_rows,
            has_more_result ? 1 : 0);
#endif
  if (!has_more_result) {
    // we only record the affected rows info if the ok packet is the last
    // packet
    session->set_affected_rows(affect_rows);
  }

  int enable_last_insert_id_session =
      session->get_session_option("enable_last_insert_id").int_val;
  if (enable_last_insert_id_session && need_check_last_insert_id) {
    int64_t after_insert_id = 0;
    after_insert_id = handler->get_last_insert_id_from_server(dataspace, &conn);
    int64_t session_before_insert_id =
        handler->get_session()->get_last_insert_id();
    LOG_DEBUG("session before:%d, before:%d, after:%d.\n",
              session_before_insert_id, before_insert_id, after_insert_id);
    if ((session_before_insert_id != after_insert_id)) {
      handler->get_session()->set_last_insert_id(after_insert_id);
      // if has more result set, we may get a new last insert id later
      if (has_more_result) before_insert_id = after_insert_id;
    }
  }

  stmt_type type = plan->statement->get_stmt_node()->type;
  if (type == STMT_CHANGE_DB) {
    const char *schema_name =
        plan->statement->get_stmt_node()->sql->change_db_oper->dbname->name;
    conn->set_schema(schema_name);
    session->set_schema(schema_name);
  }
  if (type == STMT_CREATE_VIEW || type == STMT_DROP_VIEW)
    plan->statement->handle_create_or_drop_view(plan);
  if (type == STMT_CREATE_EVENT || type == STMT_DROP_EVENT) {
    if (enable_event) plan->statement->handle_create_or_drop_event();
  }
  if (type == STMT_GRANT || type == STMT_DROP_USER ||
      type == STMT_CREATE_USER || type == STMT_SHOW_DATABASES ||
      type == STMT_SHOW_TABLES || type == STMT_SET_PASSWORD ||
      type == STMT_REVOKE || type == STMT_ALTER_USER)
    execute_grant_sqls();
  if ((type == STMT_CREATE_TB || type == STMT_CREATE_SELECT ||
       type == STMT_CREATE_LIKE) &&
      plan->statement->get_stmt_node()->sql->create_tb_oper->op_tmp == 1) {
    if (!session->is_dataspace_keeping_connection(dataspace) &&
        session->get_space_from_kept_space_map(dataspace) == NULL) {
      session->set_kept_connection(dataspace, conn);
    }
    join_node *table =
        plan->statement->get_stmt_node()->sql->create_tb_oper->table;
    const char *schema_name =
        table->schema_name ? table->schema_name : plan->statement->get_schema();
    const char *table_name = table->table_name;
    conn->inc_tmp_tb_num(schema_name, table_name);
    plan->session->add_temp_table_set(schema_name, table_name, conn);
    plan->statement->get_stmt_node()->is_create_or_drop_temp_table = true;
  } else if (type == STMT_DROP_TB) {
    table_link *table = plan->statement->get_stmt_node()->table_list_head;
    while (table) {
      const char *schema_name = table->join->schema_name
                                    ? table->join->schema_name
                                    : plan->statement->get_schema();
      const char *table_name = table->join->table_name;
      if (conn->dec_tmp_tb_num(schema_name, table_name)) {
        plan->session->remove_temp_table_set(schema_name, table_name);
        plan->statement->get_stmt_node()->is_create_or_drop_temp_table = true;
      }
      table = table->next;
    }
  } else if (type == STMT_ALTER_TABLE) {
    table_link *table = plan->statement->get_stmt_node()->table_list_head;
    const char *schema_name = table->join->schema_name
                                  ? table->join->schema_name
                                  : plan->statement->get_schema();
    const char *table_name = table->join->table_name;
    if (plan->session->is_temp_table(schema_name, table_name))
      plan->statement->get_stmt_node()->is_create_or_drop_temp_table = true;
  } else if (type == STMT_DROP_USER) {
    // drop plainpwd from backend map
    string tmp_username = plan->statement->get_stmt_node()
                              ->sql->drop_user_oper->user_clause->user_name;
    Backend::instance()->drop_sha2_plainpwd(tmp_username);
  }
  return has_more_result;
}

void MySQLDirectExecuteNode::handle_send_client_packet(Packet *packet,
                                                       bool is_row_packet) {
  handler->send_mysql_packet_to_client_by_buffer(packet, is_row_packet);
}

void MySQLDirectExecuteNode::handle_send_field_packet(Packet *packet) {
  handle_send_client_packet(packet, false);
}

void MySQLDirectExecuteNode::handle_send_last_eof_packet(Packet *packet) {
  handle_send_client_packet(packet, false);
}

bool MySQLDirectExecuteNode::handle_result_set(Packet *packet) {
  bool has_more_result = false;
  try {
    uint64_t row_num = 0;

    handler->receive_from_server(conn, packet);

    vector<MySQLColumnType> v_ctype;
    if (session->is_binary_resultset() && !direct_prepare_exec_packet &&
        is_direct_node) {
      if (session->get_prepare_item(session->get_execute_stmt_id())) {
        list<MySQLColumnType> *column_type_list =
            session->get_prepare_item(session->get_execute_stmt_id())
                ->get_column_type_list();
        list<MySQLColumnType>::iterator it_c = column_type_list->begin();
        for (; it_c != column_type_list->end(); it_c++) {
          v_ctype.push_back(*it_c);
        }
      }
    }

    size_t pos = 0;
    while (!driver->is_eof_packet(packet)) {
      if (driver->is_error_packet(packet)) {
        handle_error_packet(packet);
      }

      if (session->is_binary_resultset() && !direct_prepare_exec_packet &&
          is_direct_node) {
        if (!v_ctype.empty() && v_ctype.size() > pos) {
          MySQLColumnType prepare_type = v_ctype[pos];
          MySQLColumnResponse col_resp(packet);
          col_resp.unpack();
          MySQLColumnType col_type = col_resp.get_column_type();
          if (col_type != prepare_type) {
            col_resp.set_column_type(prepare_type);
            col_resp.pack(packet);
          }
          LOG_DEBUG(
              "Adjust the column %s from type %d to type %d for binary result "
              "convert.\n",
              col_resp.get_column(), int(col_type), int(prepare_type));
        }
      }

      if (plan->statement->has_sql_schema() &&
          dataspace->get_dataspace_type() == PARTITION_TYPE &&
          dataspace->get_virtual_machine_id() != 0) {
        MySQLColumnResponse col_resp(packet);
        col_resp.unpack();
        LOG_DEBUG("reset field packet schema name from [%s] to [%s]\n",
                  col_resp.get_schema(), plan->statement->get_sql_schema());
        col_resp.set_schema(plan->statement->get_sql_schema());
        col_resp.pack(packet);
      }
      if (get_kept_defined_lock_conn()) {
        MySQLFieldListColumnResponse response(packet);
        response.unpack();
        if (!strcasecmp(defined_lock_name.c_str(), response.get_column()))
          defined_lock_field_num = field_num;
        response.pack(packet);
      }
      handle_send_field_packet(packet);
      field_num++;

      handler->receive_from_server(conn, packet);
      pos++;
    }
    handler->deal_autocommit_with_ok_eof_packet(packet);
    handle_send_client_packet(packet, false);
    MySQLEOFResponse eof(packet);
    eof.unpack();
    // receive rows and last eof
    handler->receive_from_server(conn, packet);
#ifndef DBSCALE_TEST_DISABLE
    Backend *bk = Backend::instance();
    dbscale_test_info *test_info = bk->get_dbscale_test_info();
    if (!strcasecmp(test_info->test_case_name.c_str(), "client_exception") &&
        !strcasecmp(test_info->test_case_operation.c_str(),
                    "client_exception")) {
      delete packet;
      packet = Backend::instance()->get_new_packet(DEFAULT_PACKET_SIZE,
                                                   session->get_packet_alloc());
      LOG_DEBUG("test after delete packet, then ClientBroken\n");
      throw ClientBroken();
    }
#endif
    if (is_show_full_fields_via_mirror_tb && driver->is_eof_packet(packet)) {
      LOG_DEBUG(
          "via mirror table to show full fields, got empty set, reset it as "
          "error packet\n");
      uint8_t num = driver->get_packet_number(packet);
      delete packet;
      string err_msg = "Table '";
      err_msg.append(show_full_fields_schema_name);
      err_msg.append(".");
      err_msg.append(show_full_fields_table_name);
      err_msg.append("' doesn't exist (via mirror table)");
      MySQLErrorResponse err_resp(1146, err_msg.c_str(), "42S02", num);
      bool can_swap = session->is_may_backend_exec_swap_able();
      packet = Backend::instance()->get_new_packet(
          DEFAULT_PACKET_SIZE, can_swap ? session->get_packet_alloc() : NULL);
      err_resp.pack(packet);
#ifndef DBSCALE_TEST_DISABLE
      handler->handler_last_send.start_timing();
#endif
      handle_send_client_packet(packet, false);
      handler->flush_net_buffer();
#ifndef DBSCALE_TEST_DISABLE
      handler->handler_last_send.end_timing_and_report();
#endif
      has_more_result = false;
    } else {
      // need_refactor_show_variables_packet only use for sgrdb
      // when execute sql `show variables like "%validate%"`
      stmt_type type = plan->statement->get_stmt_node()->type;
      bool need_refactor_show_variables_packet = false;
#ifdef IS_GRID_VERSION
      bool need_refactor_select_version_comment = false;
#endif
      if (type == STMT_SHOW_VARIABLES && need_validate_password) {
        string show_sql(plan->statement->get_sql());
        boost::to_upper(show_sql);
        if (show_sql.find("LIKE") != string::npos &&
            show_sql.find("VALIDATE") != string::npos) {
          need_refactor_show_variables_packet = true;
        }
      }
#ifdef IS_GRID_VERSION
      else if (Backend::instance()->is_grid() && type == STMT_SELECT) {
        /*
          when login grid
          will send LOGIN_SELECT_VERSION_COMMENT sql result to client
          we will stop this result to client
          and send custom version comment to client
        */
        const char *sql = plan->statement->get_sql();
        size_t len = strlen(sql);
        if (len == LOGIN_SELECT_VERSION_COMMENT_LEN) {
          if (strcmp(sql, LOGIN_SELECT_VERSION_COMMENT) == 0) {
            need_refactor_select_version_comment = true;
          }
        }
      }
#endif
      while ((!driver->is_eof_packet(packet)) &&
             (!driver->is_error_packet(packet))) {
        row_num++;
        if (plan->is_federated() && row_num > federated_max_rows) {
          status = EXECUTE_STATUS_COMPLETE;
          got_error = true;
          LOG_ERROR(
              "Reach the max rows of [%u] for federated middle result set.\n",
              row_num);
          throw ExecuteNodeError(
              "Reach the max row number of federated middle result set.");
        }
        if (plan->statement->is_cross_node_join() &&
            row_num > cross_join_max_rows) {
          status = EXECUTE_STATUS_COMPLETE;
          got_error = true;
          LOG_ERROR(
              "Reach the max rows of [%u] for cross node join max moved "
              "rows.\n",
              row_num);
          throw ExecuteNodeError(
              "Reach the max row number of cross node join max moved rows.");
        }
        if (session->is_binary_resultset() && !direct_prepare_exec_packet &&
            is_direct_node) {
          MySQLRowResponse row_resp(packet);
          if (session->get_prepare_item(session->get_execute_stmt_id())) {
            list<MySQLColumnType> *column_type_list =
                session->get_prepare_item(session->get_execute_stmt_id())
                    ->get_column_type_list();
            row_resp.convert_to_binary(
                column_type_list,
                handler->get_convert_result_to_binary_stringstream(), &packet);
          } else {
            LOG_ERROR("Failed to get prepare item for execute #%d.\n",
                      session->get_execute_stmt_id());
            throw ExecuteCommandFail("Cannot get prepare item.");
          }
        }
#ifdef IS_GRID_VERSION
        if (need_refactor_select_version_comment) {
          handler->receive_from_server(conn, packet);
          continue;
        }
#endif
        if (need_refactor_show_variables_packet) {
          MySQLRowResponse row_res(packet);
          uint64_t str_len = 0;
          const char *column_str = row_res.get_str(0, &str_len);
          string var_name(column_str, str_len);
          if (var_name.find("validate_password") != string::npos) {
            handler->receive_from_server(conn, packet);
            continue;
          }
        }
        if (plan->statement->get_is_first_column_dbscale_row_id()) {
          rebuild_packet_first_column_dbscale_row_id(packet, row_num,
                                                     field_num);
        }
        if (get_kept_defined_lock_conn()) {
          defined_lock_type dl_type =
              plan->statement->get_stmt_node()->defined_lock;
          if (DEFINED_RELEASE_ALL_LOCKS == dl_type) {
            session->reset_defined_lock_depth();
          } else {
            MySQLRowResponse response(packet);
            response.unpack();
            if (!response.field_is_null(defined_lock_field_num)) {
              uint64_t field_len = 0;
              string column_str =
                  response.get_str(defined_lock_field_num, &field_len);
              column_str = column_str.substr(0, field_len);
              if (!strcasecmp(column_str.c_str(), "1")) {
                if (DEFINED_GET_LOCK == dl_type) {
                  session->defined_lock_depth_add();
                } else if (DEFINED_RELEASE_LOCK == dl_type) {
                  session->defined_lock_depth_sub();
                }
              }
            }
          }
        }
        handle_send_client_packet(packet, true);
        handler->receive_from_server(conn, packet);
      }

      if (plan->session->is_call_store_procedure() ||
          plan->session->get_has_more_result()) {
        rebuild_eof_with_has_more_flag(packet, driver);
        LOG_DEBUG(
            "For the call store procedure or multiple stmt, the last eof "
            "should be"
            " with flag has_more_result in direct node.\n");
      }

      if (driver->is_error_packet(packet)) {
        has_more_result = false;
        handle_error_packet(packet);
      }
      if (driver->is_eof_packet(packet)) {
        if (need_refactor_show_variables_packet) {
          Packet new_packet;
          list<const char *> row_data;
          char value[64];

          row_data.push_back("validate_password");
          sprintf(value, "%d", need_validate_password);
          row_data.push_back(value);
          MySQLRowResponse need_validate_row(row_data);
          need_validate_row.pack(&new_packet);
          handle_send_client_packet(&new_packet, true);

          row_data.clear();
          new_packet.rewind();
          row_data.push_back("validate_password_length");
          sprintf(value, "%d", validate_password_length);
          row_data.push_back(value);
          MySQLRowResponse validate_len_row(row_data);
          validate_len_row.pack(&new_packet);
          handle_send_client_packet(&new_packet, true);

          row_data.clear();
          new_packet.rewind();
          row_data.push_back("validate_password_mixed_case_count");
          sprintf(value, "%d", validate_password_mixed_case_count);
          row_data.push_back(value);
          MySQLRowResponse validate_case_row(row_data);
          validate_case_row.pack(&new_packet);
          handle_send_client_packet(&new_packet, true);

          row_data.clear();
          new_packet.rewind();
          row_data.push_back("validate_password_number_count");
          sprintf(value, "%d", validate_password_number_count);
          row_data.push_back(value);
          MySQLRowResponse validate_num_row(row_data);
          validate_num_row.pack(&new_packet);
          handle_send_client_packet(&new_packet, true);

          row_data.clear();
          new_packet.rewind();
          row_data.push_back("validate_password_special_char_count");
          sprintf(value, "%d", validate_password_special_char_count);
          row_data.push_back(value);
          MySQLRowResponse validate_special_row(row_data);
          validate_special_row.pack(&new_packet);
          handle_send_client_packet(&new_packet, true);

          row_data.clear();
          new_packet.rewind();
          row_data.push_back("validate_password_policy");
          sprintf(value, "%s", validate_password_policy.c_str());
          row_data.push_back(value);
          MySQLRowResponse validate_policy_row(row_data);
          validate_policy_row.pack(&new_packet);
          handle_send_client_packet(&new_packet, true);

          row_data.clear();
          new_packet.rewind();
          row_data.push_back("validate_password_dictionary_file");
          sprintf(value, "%s", validate_password_dictionary_file.c_str());
          row_data.push_back(value);
          MySQLRowResponse validate_weak_dict_row(row_data);
          validate_weak_dict_row.pack(&new_packet);
          handle_send_client_packet(&new_packet, true);
        }
#ifdef IS_GRID_VERSION
        else if (need_refactor_select_version_comment) {
          Packet new_packet;
          list<const char *> row_data;
          string everdb_version = "";
          Backend::instance()->get_everdb_powerby_version_with_prefix(
              everdb_version, EVERDB_VERSION_LOGIN_VERSION_PREFIX);

          row_data.push_back(everdb_version.c_str());
          MySQLRowResponse need_validate_row(row_data);
          need_validate_row.pack(&new_packet);
          handle_send_client_packet(&new_packet, true);
        }
#endif

        if (support_show_warning)
          handle_warnings_OK_and_eof_packet(plan, packet, handler, dataspace,
                                            conn);
      }
#ifndef DBSCALE_TEST_DISABLE
      handler->handler_last_send.start_timing();
#endif
      handler->deal_autocommit_with_ok_eof_packet(packet);
      handle_send_last_eof_packet(packet);
      handler->flush_net_buffer();
#ifndef DBSCALE_TEST_DISABLE
      handler->handler_last_send.end_timing_and_report();
#endif
      if (plan->session->get_has_more_result()) {
        has_more_result = false;
      } else if (!plan->session->is_call_store_procedure() ||
                 plan->statement->is_call_store_procedure_directly()) {
        eof.set_packet(packet);
        eof.unpack();
        has_more_result = eof.has_more_result();
      } else
        has_more_result = false;
      if (plan->statement->get_stmt_node()->type == STMT_SELECT) {
        if (conn) {
          if (has_sql_calc_found_rows()) {
            session->set_found_rows(0);
            found_rows(handler, driver, conn, session);
            Packet *packet_tmp = session->get_error_packet();
            if (packet_tmp) {
              handle_error_packet(packet_tmp);
            }
          } else {
            session->set_found_rows(row_num);
          }
        }
        session->set_real_fetched_rows(row_num);
      }
    }
  } catch (...) {
    this->packet = packet;
    throw;
  }
  /* Make direct node's packet point to the new packet */
  this->packet = packet;
  return has_more_result;
}

void MySQLDirectExecuteNode::handle_error_packet(Packet *packet) {
  status = EXECUTE_STATUS_COMPLETE;
  error_packet = packet;
  MySQLErrorResponse response(error_packet);
  response.unpack();
  LOG_DEBUG(
      "DirectExecuteNode get an error packet when execute sql [%s]: %d (%s) "
      "%s\n",
      sql, response.get_error_code(), response.get_sqlstate(),
      response.get_error_message());
  if (response.is_shutdown()) {
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace);
    }
    session->set_server_shutdown(true);
  }
  if (handle_tokudb_lock_timeout(conn, packet)) {
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace);
    }
  }
  throw ErrorPacketException();
}

bool check_server_pool_dead_conn_too_frequency(DataSpace *space) {
  if (space) return false;
  DataSource *source = space->get_data_source();
  if (!source) return false;
  DataServer *server = source->get_master_server();
  if (!server) return false;
  unsigned int dead_count_one_min = server->get_pool_dead_count_min();
  if (dead_count_one_min > MAX_DEAD_CONN_PER_MIN) {
    LOG_INFO(
        "Find dead_count_one_min to be %d, larger than MAX_DEAD_CONN_PER_MIN "
        "%d for server %s.\n",
        dead_count_one_min, MAX_DEAD_CONN_PER_MIN, server->get_name());
    return true;
  }
  return false;
}

Connection *MySQLDirectExecuteNode::connect_with_cur_user(stmt_type type) {
  string user_host = handler->get_user_host();
  unsigned int pos = user_host.find("@");
  user = user_host.substr(0, pos);

  if ((!strcmp(user.c_str(), supreme_admin_user) ||
       !strcmp(user.c_str(), normal_admin_user) ||
       !strcmp(user.c_str(), dbscale_internal_user))) {
    LOG_DEBUG(
        "This is a root/dbscale/dbscale_internal user, no need to change user "
        "any more.\n");
    return NULL;
  }
  if (type == STMT_SELECT && Backend::instance()->is_mirror_user(user)) {
    LOG_DEBUG("This is a mirror user, skip change user.\n");
    return NULL;
  }

  if (check_server_pool_dead_conn_too_frequency(dataspace)) {
    LOG_ERROR(
        "Too frequence vist auth source, skip change user, plz do 'insert into "
        "dbscale.mirror_user values('%s'); dbscale reload mirror user;'",
        user.c_str());
    return NULL;
  }
  host = user_host.substr(pos + 1, user_host.length() - pos - 1);
  try {
    conn = dataspace->get_connection(session);
    if (!conn) {
      got_error = true;
      return NULL;
    }
    if (session->get_cur_auth_plugin() == CACHING_SHA2_PASSWORD) {
      password = "";
    } else {
      conn->get_password(password, user.c_str(), host.c_str(),
                         Backend::instance()->get_backend_server_version());
    }
    conn->set_session(session);
    if (conn->change_user(user.c_str(), NULL, password.c_str(),
                          session->get_schema())) {
      session->insert_using_conn(conn);
      conn->set_changed_user_conn(true);
    } else {
      conn->get_pool()->add_back_to_dead(conn);
      conn = NULL;
    }
  } catch (HandlerError &e_handler) {
    got_error = true;
    throw;
  } catch (exception &e) {
    LOG_ERROR("change user to %s with error:%s\n", user_host.c_str(), e.what());
    if (conn) {
      conn->get_pool()->add_back_to_dead(conn);
    }
    conn = NULL;
  }
  return conn;
}
bool MySQLDirectExecuteNode::deal_with_com_prepare(
    bool read_only, MySQLExecuteRequest &exec_req,
    MySQLPrepareItem *prepare_item) {
  Packet tmp_packet;
  handler->receive_from_server(conn, &tmp_packet);
  if (driver->is_error_packet(&tmp_packet)) {
    MySQLErrorResponse error(&tmp_packet);
    error.unpack();
    LOG_ERROR("command prepare execute failed: %d (%s) %s\n",
              error.get_error_code(), error.get_sqlstate(),
              error.get_error_message());
    LOG_ERROR("Got error packet when execute MYSQL_COM_EXECUTE.\n");
    // if success return true
    return false;
  } else {
    MySQLPrepareResponse prep_resp(&tmp_packet);
    prep_resp.unpack();
    if (read_only) {
      prepare_item->set_r_stmt_id(prep_resp.statement_id);
    } else {
      prepare_item->set_w_stmt_id(prep_resp.statement_id);
    }
    if (!conn->inc_prepare_num(prep_resp.statement_id)) {
      LOG_ERROR(
          "There is no prepare statement in connection [%@] when execute drop "
          "prepare statement.\n",
          conn);
      throw HandlerError("prepare_num has overflowed.");
    }
    if (prep_resp.num_params > 0) {
      handler->receive_from_server(conn, &tmp_packet);
      while (!driver->is_eof_packet(&tmp_packet)) {
        handler->receive_from_server(conn, &tmp_packet);
      }
    }
    if (prep_resp.num_columns > 0) {
      handler->receive_from_server(conn, &tmp_packet);
      while (!driver->is_eof_packet(&tmp_packet)) {
        handler->receive_from_server(conn, &tmp_packet);
      }
    }
    if (read_only) {
      session->set_prepare_read_connection(exec_req.stmt_id, conn);
    } else {
      if (!session->is_dataspace_keeping_connection(dataspace) &&
          session->get_space_from_kept_space_map(dataspace) == NULL) {
        session->set_kept_connection(dataspace, conn);
      }
    }
  }
  return true;
}
void MySQLDirectExecuteNode::re_init_com_prepare_conn(
    Packet **real_exec_packet) {
  *real_exec_packet = session->get_binary_packet();
  MySQLExecuteRequest exec_req(*real_exec_packet);
  exec_req.unpack();
  MySQLPrepareItem *prepare_item = session->get_prepare_item(exec_req.stmt_id);
  bool read_only = prepare_item->is_read_only();
  if (read_only && !session->is_keeping_connection()) {
    if (conn == NULL) {
      conn = session->get_prepare_read_connection(exec_req.stmt_id);
    }
    if (conn == NULL) {
      session->set_executing_prepare(true);
      conn = handler->send_to_server_retry(dataspace,
                                           prepare_item->get_prepare_packet(),
                                           session->get_schema(), read_only);
      if (!deal_with_com_prepare(true, exec_req, prepare_item)) {
        return;
      }
    }
    exec_req.stmt_id = prepare_item->get_r_stmt_id();
    exec_req.pack(*real_exec_packet);
  } else {
    if (prepare_item->get_w_stmt_id() == 0) {
      conn = handler->send_to_server_retry(dataspace,
                                           prepare_item->get_prepare_packet(),
                                           session->get_schema(), false, false);
      if (!deal_with_com_prepare(false, exec_req, prepare_item)) {
        return;
      }
    }
    exec_req.stmt_id = prepare_item->get_w_stmt_id();
    exec_req.pack(*real_exec_packet);
  }

  if (*real_exec_packet == NULL) {
    throw ExecuteNodeError("Get NULL packet for execute_command.\n");
  }
}

bool MySQLDirectExecuteNode::can_swap() {
  return session->is_may_backend_exec_swap_able();
}

inline void MySQLDirectExecuteNode::handle_call_sp_directly() {
  if (plan->statement->is_call_store_procedure_directly()) {
    LOG_DEBUG(
        "in MySQLDirectExecuteNode, call sp directly, current call_sp_nest=%d, "
        "decrease one.\n",
        session->get_call_sp_nest());
    session->decrease_call_sp_nest();
    session->pop_cur_sp_worker_one_recurse();
  }
}

inline void get_schema_and_table_name_capitalize(string table_name_or_full,
                                                 string &schema_name,
                                                 string &table_name) {
  boost::to_upper(table_name_or_full);
  size_t dot_pos = table_name_or_full.find(".");
  if (dot_pos == string::npos) {
    schema_name = "";
  } else {
    schema_name = string(table_name_or_full, 0, dot_pos);
    boost::trim(schema_name);
    boost::trim_if(schema_name, boost::is_any_of("`"));
    boost::trim_if(schema_name, boost::is_any_of("\""));
  }
  table_name =
      string(table_name_or_full, dot_pos == string::npos ? 0 : dot_pos + 1);
  boost::trim(table_name);
  boost::trim_if(table_name, boost::is_any_of("`"));
  boost::trim_if(table_name, boost::is_any_of("\""));
}

void MySQLDirectExecuteNode::execute() {
  if (session->get_session_state() == SESSION_STATE_WORKING) {
    if (plan->statement->get_stmt_node()->type ==
        STMT_DBSCALE_SHOW_MIGRATE_CLEAN_TABLES)
      sql = "SELECT * FROM dbscale.MIGRATE_CLEAN_SHARD;";
#ifdef DEBUG
    node_start_timing();
#endif
    LOG_DEBUG("MySQLDirectExecuteNode sql : %s .\n", sql);
    execute_profile = session->get_profile_handler();
    profile_id =
        execute_profile->start_serial_monitor(get_executenode_name(), "", sql);
    before_insert_id = 0;
    need_check_last_insert_id = false;

    prepare_work(need_check_last_insert_id, before_insert_id);

    bool can_swap = session->is_may_backend_exec_swap_able();

    packet = Backend::instance()->get_new_packet(
        DEFAULT_PACKET_SIZE, can_swap ? session->get_packet_alloc() : NULL);
    Packet exec_packet(DEFAULT_PACKET_SIZE,
                       can_swap ? session->get_packet_alloc() : NULL);

    Backend *backend = Backend::instance();

    try {
      stmt_type type = plan->statement->get_stmt_node()->type;
      Packet *real_exec_packet = NULL;
      string sql_tmp;
      if (session->is_binary_resultset() &&
          dataspace == backend->get_catalog() &&
          !plan->statement->is_cross_node_join() &&
          !backend->get_metadata_data_space()) {
        re_init_com_prepare_conn(&real_exec_packet);
        direct_prepare_exec_packet = true;
      } else {
        bool inforschema_mirror_table_available =
            (enable_info_schema_mirror_table > 0) && datasource_in_one &&
            (!plan->session->is_call_store_procedure()) &&
            (dataspace == backend->get_auth_data_space() ||
             dataspace == backend->get_metadata_data_space()) &&
            (!strcmp(session->get_username(), supreme_admin_user) ||
             !strcmp(session->get_username(), normal_admin_user) ||
             !strcmp(session->get_username(), dbscale_internal_user) ||
             Backend::instance()->is_mirror_user(
                 string(session->get_username())));

        if (inforschema_mirror_table_available &&
            (type == STMT_SELECT || type == STMT_SHOW_FIELDS ||
             type == STMT_SHOW_TABLES || type == STMT_SHOW_INDEX)) {
          string origin_sql(sql);
          try {
            set<string> infor_tb_replaced;
            string default_schema = plan->session->get_schema();
            boost::to_upper(default_schema);
            string only_used_mirror_tb_name;
            string only_used_schema_name;
            string only_used_table_name;
            switch (type) {
              case STMT_SHOW_FIELDS: {
                show_fields_op_node *oper =
                    plan->statement->get_stmt_node()->sql->show_fields_oper;
                if (oper->is_show_full && !(plan->statement->get_stmt_node()
                                                ->cur_rec_scan->condition)) {
                  table_link *table =
                      plan->statement->get_stmt_node()->table_list_head;
                  const char *schema_name = table->join->schema_name
                                                ? table->join->schema_name
                                                : default_schema.c_str();
                  const char *table_name = table->join->table_name;
                  sql_tmp =
                      "select COLUMN_NAME as `Field`, COLUMN_TYPE as `Type`, "
                      "COLLATION_NAME as `Collation`, IS_NULLABLE as `Null`, "
                      "COLUMN_KEY as `Key`, COLUMN_DEFAULT as `Default`, EXTRA "
                      "as `Extra`, PRIVILEGES as `Privileges`, COLUMN_COMMENT "
                      "as `Comment` from dbscale.COLUMNS where TABLE_SCHEMA='";
                  sql_tmp.append(schema_name);
                  sql_tmp.append("' and TABLE_NAME='");
                  sql_tmp.append(table_name);
                  sql_tmp.append("'");
                  if (oper->is_show_like_or_where) {
                    sql_tmp.append("and COLUMN_NAME like '");
                    sql_tmp.append(oper->is_show_like_or_where);
                    sql_tmp.append("'");
                  }
                  infor_tb_replaced.insert("COLUMNS");
                  is_show_full_fields = true;
                  show_full_fields_schema_name = schema_name;
                  show_full_fields_table_name = table_name;
                  only_used_mirror_tb_name = "COLUMNS";
                  only_used_schema_name = schema_name;
                  only_used_table_name = table_name;
                } else {
                  sql_tmp = origin_sql;
                }
                break;
              }
              case STMT_SHOW_TABLES: {
                show_tables_op_node *show_tables_oper =
                    plan->statement->get_stmt_node()->sql->show_tables_oper;
                if (!(plan->statement->get_stmt_node()
                          ->cur_rec_scan->condition)) {
                  string schema_name = "Tables_in_";
                  string tmp;
                  if (show_tables_oper->db_name) {
                    tmp = show_tables_oper->db_name;
                  } else {
                    tmp = default_schema;
                  }
                  boost::to_lower(tmp);
                  schema_name.append(tmp);
                  if (show_tables_oper->is_show_like_or_where) {
                    schema_name.append(" (");
                    schema_name.append(show_tables_oper->is_show_like_or_where);
                    schema_name.append(")");
                  }
                  sql_tmp = "select TABLE_NAME as `";
                  sql_tmp.append(schema_name);
                  if (show_tables_oper->is_show_full) {
                    sql_tmp.append("`, TABLE_TYPE as `Table_type");
                  }
                  sql_tmp.append("` from dbscale.TABLES where TABLE_SCHEMA='");
                  if (show_tables_oper->db_name) {
                    sql_tmp.append(show_tables_oper->db_name);
                  } else {
                    sql_tmp.append(default_schema);
                  }
                  if (show_tables_oper->is_show_like_or_where) {
                    sql_tmp.append("' and TABLE_NAME like '");
                    sql_tmp.append(show_tables_oper->is_show_like_or_where);
                  }
                  sql_tmp.append("'");
                  infor_tb_replaced.insert("TABLES");
                  only_used_mirror_tb_name = "TABLES";
                  only_used_schema_name = tmp;
                  only_used_table_name = DBSCALE_RESERVED_STR;
                } else {
                  sql_tmp = origin_sql;
                }
                break;
              }
              case STMT_SHOW_INDEX: {
                if (!(plan->statement->get_stmt_node()
                          ->cur_rec_scan->condition)) {
                  table_link *table =
                      plan->statement->get_stmt_node()->table_list_head;
                  const char *schema_name = table->join->schema_name
                                                ? table->join->schema_name
                                                : default_schema.c_str();
                  const char *table_name = table->join->table_name;
                  sql_tmp =
                      "select TABLE_NAME as `Table`, NON_UNIQUE as "
                      "`Non_unique`, INDEX_NAME as `Key_name`, SEQ_IN_INDEX as "
                      "`Seq_in_index`, COLUMN_NAME as `Column_name`, COLLATION "
                      "as `Collation`, CARDINALITY as `Cardinality`, SUB_PART "
                      "as `Sub_part`, PACKED as `Packed`, NULLABLE as `Null`, "
                      "INDEX_TYPE as `Index_type`, COMMENT as `Comment`, "
                      "INDEX_COMMENT as `Index_comment` from "
                      "dbscale.STATISTICS where TABLE_SCHEMA='";
                  sql_tmp.append(schema_name);
                  sql_tmp.append("' and TABLE_NAME='");
                  sql_tmp.append(table_name);
                  sql_tmp.append("'");
                  infor_tb_replaced.insert("STATISTICS");
                  only_used_mirror_tb_name = "STATISTICS";
                  only_used_schema_name = schema_name;
                  only_used_table_name = table_name;
                } else {
                  sql_tmp = origin_sql;
                }
                break;
              }
              case STMT_SELECT: {
                sql_tmp = sql;
                bool default_schema_is_information_schema =
                    (default_schema == "INFORMATION_SCHEMA");
                table_link *table =
                    plan->statement->get_stmt_node()->table_list_head;
                vector<pair<table_link *, string> > table_replace_pos;
                while (table) {
                  string schema_name;
                  string table_name;
                  string table_name_or_full =
                      string(sql_tmp, table->start_pos - 1,
                             table->end_pos - table->start_pos + 1);
                  get_schema_and_table_name_capitalize(table_name_or_full,
                                                       schema_name, table_name);
                  if ((!schema_name.empty() &&
                       schema_name != "INFORMATION_SCHEMA") ||
                      (schema_name.empty() &&
                       !default_schema_is_information_schema) ||
                      (table_name != "COLUMNS" && table_name != "TABLES" &&
                       table_name != "KEY_COLUMN_USAGE" &&
                       table_name != "TABLE_CONSTRAINTS" &&
                       table_name != "STATISTICS" &&
                       (table_name != "PARTITIONS" ||
                        (table_name == "PARTITIONS" &&
                         Backend::instance()
                             ->get_info_mirror_tb_exclude_partitions())) &&
                       table_name != "ROUTINES")) {
                    table = table->next;
                    continue;
                  }
                  table_replace_pos.push_back(make_pair(table, table_name));
                  table = table->next;
                }
                if (!table_replace_pos.empty()) {
                  vector<pair<table_link *, string> >::iterator it =
                      table_replace_pos.begin();
                  int alter_pos_val = 0;
                  for (; it != table_replace_pos.end(); it++) {
                    string replace_val = string("dbscale.");
                    replace_val.append(it->second);
                    infor_tb_replaced.insert(it->second);
                    sql_tmp.replace(
                        size_t((int)(it->first->start_pos - 1) + alter_pos_val),
                        it->first->end_pos - it->first->start_pos + 1,
                        replace_val);
                    alter_pos_val +=
                        replace_val.length() -
                        int(it->first->end_pos - it->first->start_pos + 1);
                  }
                }
              }
              default:
                break;
            }

            bool can_use_informationschema_mirror_table = false;
            if (!only_used_schema_name.empty()) {
              can_use_informationschema_mirror_table =
                  Backend::instance()->is_informationschema_mirror_tb_status_ok(
                      only_used_mirror_tb_name, only_used_schema_name,
                      only_used_table_name);
            } else if (!infor_tb_replaced.empty()) {
              can_use_informationschema_mirror_table =
                  Backend::instance()
                      ->is_all_informationschema_mirror_tb_status_ok(
                          infor_tb_replaced);
            }
            if (can_use_informationschema_mirror_table) {
              sql = sql_tmp.c_str();
              LOG_DEBUG(
                  "informationschema_mirror_tb_status ok, DirectExecutionNode "
                  "sql to auth/metadata sql is changed to [%s]\n",
                  sql);
              if (is_show_full_fields) is_show_full_fields_via_mirror_tb = true;
            }
          } catch (exception &e) {
            sql = origin_sql.c_str();
            LOG_WARN(
                "mirror table get exception [%s] for sql [%s], skip the mirror "
                "replace operation.\n",
                e.what(), sql);
          }
        }
        MySQLQueryRequest query(sql);
        query.set_sql_replace_char(
            plan->session->get_query_sql_replace_null_char());
        query.pack(&exec_packet);
        real_exec_packet = &exec_packet;
      }

      if (!plan->get_is_parse_transparent() &&
          (type == STMT_GRANT || type == STMT_DROP_USER ||
           type == STMT_CREATE_USER || type == STMT_SHOW_DATABASES ||
           type == STMT_SHOW_TABLES || type == STMT_SET_PASSWORD ||
           type == STMT_REVOKE || type == STMT_SHOW_GRANTS ||
           type == STMT_ALTER_USER || type == STMT_NON ||
           (type == STMT_SELECT &&
            dataspace == backend->get_metadata_data_space() &&
            datasource_in_one))) {
        /*If add new type here, should also modify the
         * MySQLDirectExecuteNode::clean to ensure this conn will be added
         * back to dead.*/
        if (connect_with_cur_user(type)) {
          has_change_user = true;
        }
      }
      defined_lock_type dl_type =
          plan->statement->get_stmt_node()->defined_lock;
      if (dl_type != DEFINED_LOCK_NON) {
        if (enable_cluster_xa_transaction &&
            session->is_in_cluster_xa_transaction()) {
          if (session->check_cluster_conn_is_xa_conn(conn)) {
            LOG_DEBUG(
                "conn %@ is cluster xa conn, we won't handle it executenode "
                "error.\n",
                conn);
            conn = NULL;
          }
          throw ExecuteNodeError(
              "Can't execute get_lock() or release_lock() in cluster xa "
              "transaction");
        }
        defined_lock_name = session->get_defined_lock_field_name();
        conn = session->get_defined_lock_kept_conn(dataspace);
        if (conn) {
          session->set_lock_kept_conn(true);
          set_kept_defined_lock_conn(true);
          session->check_kept_connection_for_same_server(dataspace);
        } else {
          if (dl_type == DEFINED_GET_LOCK) {
            session->set_lock_kept_conn(true);
            set_kept_defined_lock_conn(true);
          }
        }
      }

#ifndef DBSCALE_TEST_DISABLE
      handler->send_op.start_timing();
#endif

      if (conn) {
        if (conn->get_session_var_map_md5() !=
                session->get_session_var_map_md5() ||
            has_change_user) {
          map<string, string> *session_var_map = session->get_session_var_map();
          conn->set_session_var(session_var_map, true, has_change_user);
        }
        const char *sch = session->get_schema();
        try {
          if (sch || sch[0]) conn->change_schema(sch);
        } catch (HandlerError &e) {
          if (enable_acl) {
            string msg = e.what();
            if (msg.find("1044 (42000) Access denied for user") !=
                string::npos) {
              throw ExecuteNodeError(e.what());
            }
          }
        } catch (...) {
          throw;
        }
        conn->reset();
        handler->send_to_server(conn, real_exec_packet);
      } else {
        if (plan->statement->is_cross_node_join()) {
          conn = handler->send_to_server_retry(dataspace, real_exec_packet,
                                               session->get_schema(), false);
        } else {
          if (direct_prepare_exec_packet) {
            // TODO: mark as can not swap
            conn = handler->send_to_server_retry(
                dataspace, real_exec_packet, session->get_schema(),
                session->is_read_only(), false, false);

          } else {
            conn = handler->send_to_server_retry(
                dataspace, real_exec_packet, session->get_schema(),
                session->is_read_only(), true, false);
          }
        }
      }
      if (conn) {
        plan->set_single_server_innodb_rollback_on_timeout(
            conn->get_server()->get_innodb_rollback_on_timeout());
      }
#ifndef DBSCALE_TEST_DISABLE
      handler->send_op.end_timing_and_report();
#endif
    } catch (ExecuteNodeError &e) {
      session->set_lock_kept_conn(false);
      if (conn) {
        handler->clean_dead_conn(&conn, dataspace);
      }
      got_error = true;
      status = EXECUTE_STATUS_COMPLETE;
      throw e;
    } catch (exception &e) {
      session->set_lock_kept_conn(false);
      got_error = true;
      status = EXECUTE_STATUS_COMPLETE;
      LOG_ERROR("DirectExecuteNode fail due to exception [%s].\n", e.what());
      string error_message("DirectExecuteNode fail due to exception:");
      error_message.append(e.what());
      if (conn) {
        handler->clean_dead_conn(&conn, dataspace);
      }
      throw ExecuteNodeError(error_message.c_str());
      ;
    }

    if (!conn->is_odbconn() && session->is_may_backend_exec_swap_able()) {
      conn->set_session(session);
      struct event *conn_event = conn->get_conn_socket_event();
      evutil_socket_t conn_socket = conn->get_conn_socket();
      SocketEventBase *conn_base = conn->get_socket_base();
#ifdef DEBUG
      ACE_ASSERT(conn_base);
#endif
      session->add_server_base(conn_socket, conn_base, conn_event);
#ifdef DEBUG
      LOG_DEBUG(
          "Finish to prepare the conn socket session swap for conn %@ and "
          "session %@.\n",
          conn, session);
#endif

      return;
    }
  }

  /*session state is SESSION_STATE_HANDLING_RESULT for swap mode.*/
  const char *server_name = NULL;
  bool is_readonly_conn = true;

  /*The handler of ExecuteNode should be reset for
   * session_state_handling_result, cause the handler may has been
   * swapped.*/
  try {
    bool has_more_result = false;
    server_name = conn->get_server()->get_name();
    conn->handle_merge_exec_begin_or_xa_start(session);

    do {
      handler->receive_from_server(conn, packet);
      if (driver->is_error_packet(packet)) {
        has_more_result = false;
        is_readonly_conn = false;
        handle_set_last_insert_id(need_check_last_insert_id, before_insert_id);
        handle_error_packet(packet);
      } else if (driver->is_ok_packet(packet)) {
        handler->deal_with_metadata_execute(
            plan->statement->get_stmt_node()->type, plan->statement->get_sql(),
            session->get_schema(), plan->statement->get_stmt_node());
        if (support_show_warning)
          handle_warnings_OK_and_eof_packet(plan, packet, handler, dataspace,
                                            conn);
        has_more_result = handle_ok_packet(packet, need_check_last_insert_id,
                                           before_insert_id, is_readonly_conn);
        if (!plan->session->is_call_store_procedure() &&
            !plan->statement->is_cross_node_join() &&
            !plan->statement->is_union_table_sub() &&
            !plan->session->get_is_silence_ok_stmt()) {
          if (plan->session->get_has_more_result()) {
            rebuild_ok_with_has_more_flag(packet, driver);
            LOG_DEBUG(
                "For multiple stmt, the ok packet of middle stmt should be with"
                "flag has_more_result.\n");
          }
          handler->deal_autocommit_with_ok_eof_packet(packet);
          handler->send_to_client(packet);
        } else
          LOG_DEBUG(
              "In store procedure call or cross node join, so skip sending"
              " of ok packet in direct node.\n");
      } else if (driver->is_result_set_header_packet(packet)) {
        handle_send_client_packet(packet, false);
        has_more_result = handle_result_set(packet);
        if (!has_more_result) {
          handle_set_last_insert_id(need_check_last_insert_id,
                                    before_insert_id);
        }
      } else {
#ifdef DEBUG
        ACE_ASSERT(0);  // should not be here
#endif
        handle_send_client_packet(packet, false);
        has_more_result = false;
      }
#ifdef DEBUG
      LOG_DEBUG("Has more result %d.\n", has_more_result ? 1 : 0);
#endif
    } while (has_more_result);

    if (get_kept_defined_lock_conn() &&
        session->get_defined_lock_depth() <= 0) {
      session->clean_defined_lock_conn();
      session->set_lock_kept_conn(false);
    }

    if (select_uservar_flag) {
      deal_with_var(conn, plan, driver);
    }
  } catch (ErrorPacketException &e) {
    LOG_DEBUG("DirectExecuteNode get an error packet when execute sql [%s].\n",
              sql);
    status = EXECUTE_STATUS_COMPLETE;
    if (conn && error_packet && plan->session->is_call_store_procedure()) {
      handler->put_back_connection(dataspace, conn);
      conn = NULL;
    }

    handle_call_sp_directly();
    throw e;
  } catch (ExecuteNodeError &e) {
    if (get_kept_defined_lock_conn()) {
      session->clean_defined_lock_conn();
      session->set_lock_kept_conn(false);
    }
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace);
    }
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    handle_call_sp_directly();
    throw e;
  } catch (exception &e) {
    if (get_kept_defined_lock_conn()) {
      session->clean_defined_lock_conn();
      session->set_lock_kept_conn(false);
    }
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    LOG_ERROR("DirectExecuteNode fail due to exception [%s].\n", e.what());
    string error_message("DirectExecuteNode fail due to exception:");
    error_message.append(e.what());
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace);
    }
    handle_call_sp_directly();
    throw ExecuteNodeError(error_message.c_str());
    ;
  }

#ifdef DEBUG
  node_end_timing();
#endif
  execute_profile->end_execute_monitor(profile_id);
  handle_call_sp_directly();
#ifdef DEBUG
  LOG_DEBUG("MySQLDirectNode %@ cost %d ms\n", this, node_cost_time);
#endif
  status = EXECUTE_STATUS_COMPLETE;

  if (plan->statement->get_stmt_node()->has_unknown_func)
    is_readonly_conn = false;
  if (!is_readonly_conn && conn) {
    session->record_xa_modified_conn(conn);
  }
  record_xa_modify_sql(plan, session, dataspace, sql, is_readonly_conn);
  record_modify_server(plan, session, server_name,
                       dataspace->get_virtual_machine_id(), is_readonly_conn);
}  // end of MySQLDirectExecuteNode::execute()

void MySQLDirectExecuteNode::execute_grant_sqls() {
  list<string> sqls = plan->statement->get_grant_sqls();
  try {
    list<string>::iterator it = sqls.begin();
    it++;
    for (; it != sqls.end(); it++) {
      LOG_DEBUG("Execute grant sql:[%s].\n", it->c_str());
      conn->execute_one_modify_sql_with_sqlerror(it->c_str());
    }
  } catch (dbscale::sql::SQLError &e) {
    string tmp("Got error packet when execute grant sqls, due to ");
    tmp += e.what();
    LOG_ERROR("%s.\n", tmp.c_str());
    throw ExecuteNodeError(tmp.c_str());
  }
}

/* class MySQLSetNode */
MySQLSetNode::MySQLSetNode(ExecutePlan *plan, DataSpace *dataspace,
                           const char *sql)
    : MySQLExecuteNode(plan), conn(NULL) {
  this->sql = sql;
  this->dataspace = dataspace;
  this->name = "MySQLSetNode";
  got_error = false;
  packet = NULL;
  error_packet = NULL;
}
bool MySQLSetNode::is_need_trans(string value) {
  size_t length = value.length();
  size_t pos = 0;
  pos = value.find("'");
  while (pos != string::npos && pos < length) {
    if (pos + 1 < length && value[pos + 1] == '\'') {
      pos++;
    } else {
      int i = pos - 1;
      size_t count = 0;
      while (i >= 0) {
        if (value[i] == '\\') {
          count++;
          i--;
        } else {
          break;
        }
      }
      if (!(count % 2)) {
        return true;
      }
    }
    if (pos + 1 < length) {
      pos = value.find("'", pos + 1);
    } else {
      pos++;
    }
  }
  return false;
}

bool MySQLSetNode::can_swap() {
  return session->is_may_backend_exec_swap_able();
}

void MySQLSetNode::replace_query_sql(string &query_sql) {
  if (plan->statement->get_stmt_node()->row_count_num) {
    plan->statement->replace_sql_function_field(
        FUNCTION_TYPE_ROW_COUNT, query_sql,
        plan->statement->get_stmt_node()->scanner, plan);
  }
  if (plan->statement->get_stmt_node()->last_insert_id_num) {
    plan->statement->replace_sql_function_field(
        FUNCTION_TYPE_LAST_INSERT_ID, query_sql,
        plan->statement->get_stmt_node()->scanner, plan);
  }
}

void MySQLSetNode::exe_state_working() {
#ifndef DBSCALE_TEST_DISABLE
  dbscale_test_info *test_info = plan->session->get_dbscale_test_info();
  if (!strcasecmp(test_info->test_case_name.c_str(),
                  "set_session_error_condition") &&
      !strcasecmp(test_info->test_case_operation.c_str(), "server_exception")) {
    sql = "bad sql";
  }
#endif
  Packet exec_packet;
  string query_sql(sql);
  replace_query_sql(query_sql);
  MySQLQueryRequest query(query_sql.c_str());
  query.set_sql_replace_char(plan->session->get_query_sql_replace_null_char());
  query.pack(&exec_packet);
  real_sql += query_sql.c_str();

  map<DataSpace *, Connection *> kept_connections =
      session->get_kept_connections();
  try {
    if (!kept_connections.empty()) {
      map<DataSpace *, Connection *>::iterator it = kept_connections.begin();
      for (; it != kept_connections.end();) {
        conn = it->second;
        if (conn) {
          dataspace = it->first;
          conn->reset();
          try {
            handler->set_user_var(&conn, dataspace);
            handler->send_to_server(conn, &exec_packet);
            local_connections[dataspace] = conn;
          } catch (...) {
            LOG_ERROR("MySQLSetNode fail send due to exception.\n");
            LOG_ERROR("Error dataspace name is [%s]\n", dataspace->get_name());
            conn_result con_ret;
            con_ret.is_dead_conn = true;
            con_ret.conn = conn;
            con_ret.space = dataspace;
            con_ret.packet = NULL;
            commit_conn_result.push_back(con_ret);
            if (session->defined_lock_need_kept_conn(it->first))
              session->remove_defined_lock_kept_conn(it->first);
            session->remove_kept_connection(dataspace);
            session->remove_using_conn(conn);
            if (conn) {
              conn->get_pool()->add_back_to_dead(conn);
              kept_connections.erase(it++);
              conn = NULL;
            }
            if (local_connections.empty()) throw;
            break;
          }
        }
        it++;
      }
    } else {
      conn = handler->send_to_server_retry(
          dataspace, &exec_packet, session->get_schema(),
          session->is_read_only(), true, false);
    }
  } catch (ExecuteNodeError &e) {
    if (session->defined_lock_need_kept_conn(dataspace))
      session->remove_defined_lock_kept_conn(dataspace);
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace);
    }
    conn_set_session_var();
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  } catch (exception &e) {
    if (session->defined_lock_need_kept_conn(dataspace))
      session->remove_defined_lock_kept_conn(dataspace);
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace);
    }
    conn_set_session_var();
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    LOG_DEBUG("SetNode fail due to exception [%s].\n", e.what());
    string error_message("SetNode fail due to exception:");
    error_message.append(e.what());
    throw ExecuteNodeError(error_message.c_str());
  }
}

void MySQLSetNode::exe_state_handling_result() {
  try {
#ifdef DEBUG
    LOG_DEBUG("Receiving result from server\n");
#endif
    if (!local_connections.empty()) {
      map<DataSpace *, Connection *>::iterator it = local_connections.begin();
      for (; it != local_connections.end(); it++) {
        conn = it->second;
        try {
          conn->handle_merge_exec_begin_or_xa_start(session);

          handler->receive_from_server(conn, packet);
          if (driver->is_error_packet(packet)) {
            conn_result con_ret;
            con_ret.conn = conn;
            con_ret.space = it->first;
            con_ret.is_dead_conn = false;

            MySQLErrorResponse response(packet);
            if (response.is_shutdown()) {
              if (conn) {
                con_ret.is_dead_conn = true;
              }
              session->set_server_shutdown(true);
            }
            Packet *tmp_packet = Backend::instance()->get_new_packet();
            con_ret.packet = packet;
            packet = tmp_packet;
            commit_conn_result.push_back(con_ret);
            conn = NULL;
            continue;
          }
        } catch (ErrorPacketException &e) {
          LOG_DEBUG("SetNode get an error packet when execute sql [%s].\n",
                    sql);
          conn_result con_ret;
          con_ret.is_dead_conn = false;
          con_ret.conn = conn;
          con_ret.space = it->first;
          con_ret.packet = packet;
          packet = NULL;
          commit_conn_result.push_back(con_ret);
        } catch (...) {
          LOG_ERROR("MySQLSetNode fail send due to exception.\n");
          conn_result con_ret;
          con_ret.is_dead_conn = true;
          con_ret.conn = conn;
          con_ret.space = it->first;
          con_ret.packet = NULL;
          commit_conn_result.push_back(con_ret);
          session->remove_kept_connection(it->first);
          session->remove_using_conn(it->second);
          it->second->get_pool()->add_back_to_dead(it->second);
        }
      }
    } else {
      conn->handle_merge_exec_begin_or_xa_start(session);

      handler->receive_from_server(conn, packet);
    }

    if (!commit_conn_result.empty()) {
      bool has_dead_conn = false;
      error_packet = NULL;
      vector<conn_result>::iterator it = commit_conn_result.begin();
      for (; it != commit_conn_result.end(); it++) {
        if (it->is_dead_conn) {
          has_dead_conn = true;
          if (it->conn) handler->clean_dead_conn(&(it->conn), it->space);
          continue;
        }
        if (it->packet) {
          if (error_packet)
            delete it->packet;
          else
            error_packet = it->packet;
          if (it->conn) handler->put_back_connection(it->space, it->conn);
        }
      }

      if (has_dead_conn)
        throw ExecuteNodeError(
            "Get exception for SetNode with dead server connection.");
      if (error_packet) throw ErrorPacketException();
      throw ExecuteNodeError("Unexpect behavior.");
    }
    if (driver->is_error_packet(packet)) {
      status = EXECUTE_STATUS_COMPLETE;
      error_packet = packet;
      packet = NULL;
      MySQLErrorResponse response(error_packet);
      if (response.is_shutdown()) {
        if (conn) {
          handler->clean_dead_conn(&conn, dataspace);
        }
        session->set_server_shutdown(true);
      }
      throw ErrorPacketException();
    }
    if (driver->is_ok_packet(packet)) {
      deal_with_var(conn, plan, driver);
    }
    status = EXECUTE_STATUS_COMPLETE;
    if (!session->is_call_store_procedure()) {
      handler->deal_autocommit_with_ok_eof_packet(packet);
      handler->record_affected_rows(packet);
#ifndef DBSCALE_TEST_DISABLE
      dbscale_test_info *test_info = plan->session->get_dbscale_test_info();
      if (!strcasecmp(test_info->test_case_name.c_str(),
                      "set_session_error_condition") &&
          !strcasecmp(test_info->test_case_operation.c_str(),
                      "client_exception")) {
        throw ClientBroken();
      }
#endif
      handler->send_to_client(packet);
    }
    record_xa_modify_sql(plan, session, dataspace, real_sql.c_str(), false);
  } catch (ErrorPacketException &e) {
    conn_set_session_var();
    LOG_DEBUG("SetNode get an error packet when execute sql [%s].\n", sql);
    throw;
  } catch (exception &e) {
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace);
    }
    conn_set_session_var();
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    LOG_DEBUG("SetNode fail due to exception [%s].\n", e.what());
    string error_message("SetNode fail due to exception:");
    error_message.append(e.what());
    throw ExecuteNodeError(error_message.c_str());
  }
}

bool MySQLSetNode::handle_swap() {
  /*  Check if we can swap, the function is used in Swapable Node.  If it can
   *  swap, the state changes from SESSION_STATE_WORKING =>
   *  SESSION_STATE_WAITING_SERVER. Otherwise, session state cahnge from
   *  SESSION_STATE_WORKING => SESSION_STATE_HANDLING_RESULT */
  if (session->is_may_backend_exec_swap_able()) {
    if (local_connections.size()) {
      map<DataSpace *, Connection *>::iterator it3;
      for (it3 = local_connections.begin(); it3 != local_connections.end();
           it3++) {
        conn = it3->second;
        if (conn) {
          conn->set_session(session);
          struct event *conn_event = conn->get_conn_socket_event();
          evutil_socket_t conn_socket = conn->get_conn_socket();
          SocketEventBase *conn_base = conn->get_socket_base();
#ifdef DEBUG
          ACE_ASSERT(conn_base);
#endif
          session->add_server_base(conn_socket, conn_base, conn_event);
#ifdef DEBUG
          LOG_DEBUG(
              "Finish to prepare the conn socket session swap for conn %@ and "
              "session %@.\n",
              conn, session);
#endif
        }
      }
      conn = NULL;
    } else {
      if (conn) {
        conn->set_session(session);
        struct event *conn_event = conn->get_conn_socket_event();
        evutil_socket_t conn_socket = conn->get_conn_socket();
        SocketEventBase *conn_base = conn->get_socket_base();
#ifdef DEBUG
        ACE_ASSERT(conn_base);
#endif
        session->add_server_base(conn_socket, conn_base, conn_event);
#ifdef DEBUG
        LOG_DEBUG(
            "Finish to prepare the conn socket session swap for conn %@ and "
            "session %@.\n",
            conn, session);
#endif
      }
    }
    session->set_mul_connection_num(local_connections.size());
    return true;
  }
  return false;
}

void MySQLSetNode::execute() {
  if (session->get_session_state() == SESSION_STATE_WORKING) {
    if (!packet) packet = Backend::instance()->get_new_packet();

    exe_state_working();
    if (handle_swap()) return;
  }

  exe_state_handling_result();

}  // end of MySQLSetNode::execute()

void MySQLSetNode::conn_set_session_var() {
  map<DataSpace *, Connection *> kept_connections =
      session->get_kept_connections();
  bool deal_with_exception = false;
  if (kept_connections.size()) {
    map<DataSpace *, Connection *>::iterator it = kept_connections.begin();
    for (; it != kept_connections.end(); it++) {
      Connection *kept_conn = it->second;
      if (kept_conn) {
        try {
          if (session->get_session_var_map_md5() !=
              kept_conn->get_session_var_map_md5())
            kept_conn->set_session_var(session->get_session_var_map());
        } catch (HandlerError &e) {
          kept_conn->clean_session_var_map();
          continue;
        } catch (exception &e) {
          deal_with_exception = true;
          if (kept_conn) {
            handler->clean_dead_conn(&kept_conn, dataspace);
          }
          continue;
        }
      }
    }
  } else if (conn) {
    try {
      if (session->get_session_var_map_md5() != conn->get_session_var_map_md5())
        conn->set_session_var(session->get_session_var_map());
    } catch (HandlerError &e) {
      conn->clean_session_var_map();
    } catch (exception &e) {
      if (conn) {
        handler->clean_dead_conn(&conn, dataspace);
      }
      deal_with_exception = true;
    }
  }
  if (deal_with_exception) {
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    string error_message("SetNode fail due to exception:");
    throw ExecuteNodeError(error_message.c_str());
  }
}

void MySQLSetNode::clean() {
  if (conn) {
    if (!got_error || error_packet) {
      handler->put_back_connection(dataspace, conn);
    } else {
      handler->clean_dead_conn(&conn, dataspace);
    }
    conn = NULL;
  }
  if (error_packet) delete error_packet;
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
}

#ifndef DBSCALE_DISABLE_SPARK
/* class MySQLSparkNode */
MySQLSparkNode::MySQLSparkNode(ExecutePlan *plan, SparkConfigParameter config,
                               bool is_insert)
    : MySQLExecuteNode(plan), cond(mutex) {
  this->config = config;
  this->name = "MySQLSparkNode";
  read_rows = new list<vector<string> >();
  kept_rows = new list<vector<string> >();
  column_received = false;
  service_finished = false;
  finished = false;
  got_error = false;
  head_packet = NULL;
  eof_packet = NULL;
  spark_result.success = false;
  fetch_result.success = true;
  kept_rows_size = 0;
  bthread = NULL;
  thread_status = THREAD_STOP;
  max_spark_buffer_rows = 0;
  if (max_fetchnode_ready_rows_size > 18446744073709551615UL / 1024UL) {
    max_spark_buffer_rows_size = 18446744073709551615UL;
  } else {
    max_spark_buffer_rows_size = max_fetchnode_ready_rows_size * 1024;
  }
  this->is_insert_select = is_insert;
  has_started = false;
}

void MySQLSparkNode::clean() {
  if (thread_status == THREAD_STARTED) {
    LOG_DEBUG("Fetch node %@ cleaning wait.\n", this);
    this->wait_for_cond();
    LOG_DEBUG("Fetch node %@ cleaning wait finish.\n", this);
  } else if (thread_status == THREAD_CREATED) {
    bthread->re_init_backend_thread();
    bthread->get_pool()->add_back_to_free(bthread);
  }
  if (head_packet) {
    delete head_packet;
    head_packet = NULL;
  }
  if (eof_packet) {
    delete eof_packet;
    eof_packet = NULL;
  }
  if (!column_list.empty()) {
    list<Packet *>::iterator it = column_list.begin();
    for (; it != column_list.end(); ++it) {
      delete *it;
    }
    column_list.clear();
  }
  delete kept_rows;
  delete read_rows;
}

void MySQLSparkNode::stop_spark_service() {
  SparkKillParameter spark_kill_param;
  spark_kill_param.job_id = config.job_id;
  SparkReturnValue return_value = kill_spark_service(spark_kill_param);
  if (!return_value.success) {
    LOG_ERROR("Fail to stop spark service:%s\n",
              return_value.error_string.c_str());
  }
}

bool MySQLSparkNode::notify_parent() {
  int ret = false;
  ACE_Guard<ACE_Thread_Mutex> guard(mutex);
  // check fetch node finished or get error
  if (is_finished() || got_error) {
    if (got_error) {
      LOG_ERROR("Spark node got exception [%s].\n", error_msg.c_str());
      throw Error(error_msg.c_str());
    } else {
#ifdef DEBUG
      LOG_DEBUG("SparkNode %@ is finished.\n", this);
#endif
      ret = false;
    }
  } else {
    if (ready_rows->empty()) cond.wait();

    if (got_error) {
      LOG_ERROR("Spark node got exception [%s].\n", error_msg.c_str());
      throw Error(error_msg.c_str());
    } else if (ready_rows->empty()) {  // the result set is empty
      while (ready_rows->empty()) {
        if (is_finished()) {
          LOG_DEBUG("FetchNode %@ is finished after wait.\n", this);
          ret = false;
          break;
        }
        cond.wait();
      }
    } else {
      if (kept_rows_size > 0) {
        parent->swap_ready_list_with_row_map_list(this, &ready_rows,
                                                  kept_rows_size, 0);
        kept_rows_size = 0;
      }
      ret = true;
    }
  }

  return ret;
}

void *start_spark_service(void *arg) {
  MySQLSparkNode *node = (MySQLSparkNode *)arg;
  SparkConfigParameter config = node->config;

  node->plan->session->start_executing_spark();
  SparkReturnValue return_value = run_spark_service(config);
  node->plan->session->stop_executing_spark();

  node->set_spark_return_value(return_value);
  return NULL;
}

void MySQLSparkNode::execute() {
  if (has_started) return;
  has_started = true;

  bool thread_created = false;
  try {
    start_spark_thread();
    thread_status = THREAD_STARTED;
    thread_created = true;
    plan->start_all_bthread();

    mutex.acquire();
    if (!column_received && thread_status != THREAD_STOP) {
      cond.wait();
    }
    mutex.release();

    if (service_finished && !spark_result.success) {
      LOG_ERROR("SparkNode got error [%s].\n",
                spark_result.error_string.c_str());
      throw Error(spark_result.error_string.c_str());
    }
    if (got_error) {
      LOG_ERROR("SparkNode got error [%s].\n", error_msg.c_str());
      throw Error(error_msg.c_str());
    }

    head_packet = Backend::instance()->get_new_packet();
    MySQLResultSetHeaderResponse result_set_header(columns.size());
    result_set_header.pack(head_packet);
    if (!is_insert_select)
      handler->send_mysql_packet_to_client_by_buffer(head_packet);

    set_column_packet();

    if (!is_insert_select) {
      list<Packet *>::iterator it = column_list.begin();
      for (; it != column_list.end(); ++it) {
        handler->send_mysql_packet_to_client_by_buffer(*it);
      }
    }

    eof_packet = Backend::instance()->get_new_packet();
    MySQLEOFResponse eof;
    eof.pack(eof_packet);
    handler->deal_autocommit_with_ok_eof_packet(eof_packet);

    if (is_insert_select) {
      LOG_DEBUG("Spark insert select, execute just return.\n");
      return;
    }

    handler->send_mysql_packet_to_client_by_buffer(eof_packet);

    send_row_packet();

    if (service_finished && !spark_result.success) {
      LOG_ERROR("SparkNode got error [%s].\n",
                spark_result.error_string.c_str());
      throw Error(spark_result.error_string.c_str());
    }
    if (got_error) {
      LOG_ERROR("SparkNode got error [%s].\n", error_msg.c_str());
      throw Error(error_msg.c_str());
    }

    MySQLEOFResponse end;
    end.pack(eof_packet);
    handler->deal_autocommit_with_ok_eof_packet(eof_packet);
    handler->send_mysql_packet_to_client_by_buffer(eof_packet);

    handler->flush_net_buffer();
  } catch (...) {
    if (thread_created) ACE_Thread::join(l_handle);
    handler->flush_net_buffer();
    status = EXECUTE_STATUS_COMPLETE;
    LOG_ERROR("Got exception when execute spark node.\n");
    throw;
  }

  ACE_Thread::join(l_handle);

  if (service_finished && !spark_result.success) {
    LOG_ERROR("SparkNode got error [%s].\n", spark_result.error_string.c_str());
    throw Error(spark_result.error_string.c_str());
  }
  if (got_error) {
    LOG_ERROR("SparkNode got error [%s].\n", error_msg.c_str());
    throw Error(error_msg.c_str());
  }

#ifdef DEBUG
  LOG_DEBUG("MySQLSparkNode %@ cost %d ms.\n", this, node_cost_time);
#endif
  status = EXECUTE_STATUS_COMPLETE;
}

void MySQLSparkNode::set_column_packet() {
  const char *catalog = "def";
  const char *schema = session->get_schema();
  const char *table = "tables";
  const char *org_table = "tables";

  vector<string>::iterator it = columns.begin();
  for (; it != columns.end(); ++it) {
    Packet *packet = Backend::instance()->get_new_packet();
    MySQLColumnResponse field(catalog, schema, table, org_table, it->c_str(),
                              it->c_str(), 8, NAME_LEN + 1,
                              MYSQL_TYPE_VAR_STRING, 0, 0, 0);
    field.pack(packet);
    column_list.push_back(packet);
  }
}

void MySQLSparkNode::send_row_packet() {
  Packet packet(row_packet_size);
  while (1) {
    if (session->get_is_killed()) {
      LOG_INFO("Session is killed when SparkNode send row packet.\n");
      throw Error("Session is killed.");
    }
    if (ACE_Reactor::instance()->reactor_event_loop_done()) {
      LOG_INFO("DBScale is exiting when SparkNode send row packet.\n");
      throw Error("DBScale is exiting.");
    }
    mutex.acquire();
    if (kept_rows_size == 0) {
      if (finished) {
        mutex.release();
        return;
      } else {
        cond.wait();
      }
    }
    if (kept_rows_size > 0) {
      list<vector<string> > *tmp_rows;
      tmp_rows = read_rows;
      read_rows = kept_rows;
      kept_rows = tmp_rows;
      kept_rows_size = 0;
    }
    mutex.release();

    while (!read_rows->empty()) {
      try {
        pack_row_data(&packet, read_rows->front());
        handler->send_mysql_packet_to_client_by_buffer(&packet);
      } catch (...) {
        read_rows->pop_front();
        LOG_ERROR("MySQLSparkNode failed to send rows to client.\n");
        throw;
      }
      read_rows->pop_front();
    }
  }
}

void MySQLSparkNode::add_to_ready_rows(vector<vector<string> > &datas) {
  try {
    ACE_Guard<ACE_Thread_Mutex> guard(mutex);
    if (!datas.empty()) {
      Packet *packet = NULL;
      kept_rows_size += datas.size();
      if ((kept_rows_size & 0x03FF) == 0 || max_spark_buffer_rows == 0) {
        vector<string>::iterator it = datas[0].begin();
        int row_size = 0;
        for (; it != datas[0].end(); ++it) {
          row_size += it->length();
        }
        row_size = row_size > row_packet_size ? row_size : row_packet_size;
        max_spark_buffer_rows = max_spark_buffer_rows_size / row_size;
        if (!max_spark_buffer_rows) max_spark_buffer_rows = 1000;
      }
      vector<vector<string> >::iterator it = datas.begin();
      for (; it != datas.end(); ++it) {
        packet = Backend::instance()->get_new_packet(row_packet_size);
        pack_row_data(packet, *it);
        ready_rows->push_back(packet);
      }
      cond.signal();
    }
  } catch (...) {
    cond.signal();
    throw;
  }
}

void MySQLSparkNode::add_to_kept_rows(vector<vector<string> > &datas) {
  try {
    ACE_Guard<ACE_Thread_Mutex> guard(mutex);
    if (!datas.empty()) {
      kept_rows->insert(kept_rows->end(), datas.begin(), datas.end());
      cond.signal();
      kept_rows_size += datas.size();
      if ((kept_rows_size & 0x03FF) == 0 || max_spark_buffer_rows == 0) {
        vector<string>::iterator it = datas[0].begin();
        int row_size = 0;
        for (; it != datas[0].end(); ++it) {
          row_size += it->length();
        }
        row_size = row_size > row_packet_size ? row_size : row_packet_size;
        max_spark_buffer_rows = max_spark_buffer_rows_size / row_size;
        if (!max_spark_buffer_rows) max_spark_buffer_rows = 1000;
      }
    }
  } catch (...) {
    cond.signal();
    throw;
  }
}

void MySQLSparkNode::pack_row_data(Packet *packet, vector<string> &row_data) {
  list<const char *> tmp_row_data;
  vector<string>::iterator it = row_data.begin();
  for (; it != row_data.end(); ++it) {
    tmp_row_data.push_back(it->c_str());
  }
  MySQLRowResponse row(tmp_row_data);
  row.pack(packet);
}

int MySQLSparkNode::svc() {
  SparkFetchParameter param;
  param.job_id = config.job_id;
  if (!column_received) {
    columns = get_columns_from_spark(param, fetch_result).columns;
    if (!fetch_result.success) {
      LOG_ERROR("SparkNode got error [%s].\n",
                fetch_result.error_string.c_str());
      got_error = true;
      set_error_msg(fetch_result.error_string);
      mutex.acquire();
      column_received = true;
      cond.signal();
      mutex.release();
      return FINISHED;
    }
    if (columns.empty()) {
      if (service_finished) {
        columns = get_columns_from_spark(param, fetch_result).columns;
        if (!fetch_result.success) {
          LOG_ERROR("SparkNode got error [%s].\n",
                    fetch_result.error_string.c_str());
          got_error = true;
          set_error_msg(fetch_result.error_string);
          mutex.acquire();
          column_received = true;
          cond.signal();
          mutex.release();
          return FINISHED;
        }
        if (columns.empty()) {
          LOG_ERROR("Fail to get columns from spark.\n");
          got_error = true;
          set_error_msg("Fail to get columns from spark.");
          mutex.acquire();
          column_received = true;
          cond.signal();
          mutex.release();
          return FINISHED;
        }
      }
      return NEED_SLEEP;
    }
    mutex.acquire();
    column_received = true;
    cond.signal();
    mutex.release();
  }
  vector<vector<string> > fetch_datas;
  while (1) {
    if (session->get_is_killed()) {
      LOG_INFO("Session is killed when SparkNode fetch data.\n");
      stop_spark_service();
      throw Error("Session is killed.");
    }
    if (ACE_Reactor::instance()->reactor_event_loop_done()) {
      LOG_INFO("DBScale is exiting when SparkNode fetch data.\n");
      stop_spark_service();
      throw Error("DBScale is exiting.");
    }
    // string Copy-On-Write
    fetch_datas = fetch_data_from_spark(param, fetch_result).data_list;

#ifndef DBSCALE_TEST_DISABLE
    Backend *bk = Backend::instance();
    dbscale_test_info *test_info = bk->get_dbscale_test_info();
    if (!strcasecmp(test_info->test_case_name.c_str(),
                    "dbscale_spark_insert") &&
        !strcasecmp(test_info->test_case_operation.c_str(), "exception")) {
      got_error = true;
      set_error_msg("Got exception when fetch data.");
      stop_spark_service();
      break;
    }
#endif

    if (!fetch_result.success) {
      LOG_ERROR("SparkNode got error [%s].\n",
                fetch_result.error_string.c_str());
      got_error = true;
      set_error_msg(fetch_result.error_string);
      break;
    }
    if (fetch_datas.empty()) {
      if (service_finished) {
        fetch_datas = fetch_data_from_spark(param, fetch_result).data_list;
        if (!fetch_result.success) {
          LOG_ERROR("SparkNode got error [%s].\n",
                    fetch_result.error_string.c_str());
          got_error = true;
          set_error_msg(fetch_result.error_string);
          break;
        }
        if (fetch_datas.empty()) {
          break;
        } else {
          if (is_insert_select)
            add_to_ready_rows(fetch_datas);
          else
            add_to_kept_rows(fetch_datas);
          continue;
        }
      }
      return NEED_SLEEP;
    }

    if (is_insert_select)
      add_to_ready_rows(fetch_datas);
    else
      add_to_kept_rows(fetch_datas);

    if ((unsigned long)kept_rows_size > max_spark_buffer_rows) {
      return NEED_SLEEP;
    }
  }
  if (is_insert_select) {
    ACE_Thread::join(l_handle);
    status = EXECUTE_STATUS_COMPLETE;
  }
  mutex.acquire();
  finished = true;
  cond.signal();
  mutex.release();
  return FINISHED;
}

void MySQLSparkNode::start_spark_thread() {
  int ret =
      ACE_Thread::spawn((ACE_THR_FUNC)start_spark_service, this,
                        THR_JOINABLE | THR_SCHED_DEFAULT, &l_id, &l_handle);
  if (ret == -1) {
    LOG_ERROR("Got error when create a new thread to run spark service.\n");
    throw Error("Got error when create a new thread to run spark service.");
  }

  bthread = plan->get_one_bthread();
  if (!bthread) {
    got_error = true;
    LOG_ERROR(
        "Fail to get a backend thread from pool, so the spark node stop.\n");
    throw Error(
        "Fail to get a backend thread from pool, so the spark node stop.");
  }
  thread_status = THREAD_CREATED;
  bthread->set_task(this);
}
#endif

/* class MySQLFetchNode */

MySQLFetchNode::MySQLFetchNode(ExecutePlan *plan, DataSpace *dataspace,
                               const char *sql)
    : MySQLExecuteNode(plan, dataspace), conn(NULL), cond(mutex) {
  this->sql.append(sql);
  this->name = "MySQLFetchNode";
  has_get_partition_key_pos = false;
  thread_status = THREAD_STOP;
  bthread = NULL;
  sql_sent = false;
  got_error = false;
  this->eof_packet = NULL;
  this->end_packet = NULL;
  is_get_end_packet = false;
  this->error_packet = NULL;
  ready_rows_size = 0;

  ready_buffer_rows_size = 0;
  if (max_fetchnode_ready_rows_size > 18446744073709551615UL / 1024UL) {
    max_fetchnode_buffer_rows_size = 18446744073709551615UL;
  } else {
    max_fetchnode_buffer_rows_size = max_fetchnode_ready_rows_size * 1024;
  }
  migrate_partition_key_pos_vec = NULL;
  header_received = false;
  pkt_list.clear();
  if (use_packet_pool) {
    id_in_plan = session->get_next_id_in_plan() %
                 ((size_t)session->get_packet_pool_count());
  } else {
    id_in_plan = 0;
  }
  kept_ready_packets = 0;
  local_fetch_signal_batch = fetch_signal_batch;
  force_use_non_trx_conn = false;
  ready_rows->set_session(plan->session);
  plan->session->set_has_fetch_node(true);
  LOG_DEBUG("Plan %@ FetchNode %@ id %Q\n", plan, this, id_in_plan);
}

void MySQLFetchNode::execute() {
  if (plan->session->check_for_transaction() &&
      session->get_session_option("cursor_use_free_conn").int_val) {
    force_use_non_trx_conn = plan->is_cursor;
  }
  if (!sql_sent) {
    session->set_skip_mutex_in_operation(true);
    try {
#ifndef DBSCALE_TEST_DISABLE
      Backend *bk = Backend::instance();
      dbscale_test_info *test_info = bk->get_dbscale_test_info();
      if (!strcasecmp(test_info->test_case_name.c_str(), "fetchnode") &&
          !strcasecmp(test_info->test_case_operation.c_str(), "unkown_error")) {
        throw Error("unknown error.");
      }
#endif
      Packet exec_packet;
      MySQLQueryRequest query(sql.c_str());
      query.set_sql_replace_char(
          plan->session->get_query_sql_replace_null_char());
      query.pack(&exec_packet);
      if (plan->get_migrate_tool()) {
        conn = plan->get_migrate_tool()->get_migrate_read_conn();
        handler->send_to_server(conn, &exec_packet);
      } else {
        if (plan->statement->is_cross_node_join()) {
          conn = handler->send_to_server_retry(
              dataspace, &exec_packet, session->get_schema(), false, true,
              false, force_use_non_trx_conn);
        } else {
          conn = handler->send_to_server_retry(
              dataspace, &exec_packet, session->get_schema(),
              session->is_read_only(), true, false, force_use_non_trx_conn);
        }
      }
      LOG_DEBUG("Get connection %@ for node %@.\n", conn, this);
      sql_sent = true;
      session->set_skip_mutex_in_operation(false);
    } catch (Exception &e) {
      mutex.acquire();
      got_error = true;
      mutex.release();
      status = EXECUTE_STATUS_COMPLETE;
      if (conn) {
        if (!plan->get_migrate_tool())
          handler->clean_dead_conn(&conn, dataspace, !force_use_non_trx_conn);
        else {
          conn->set_status(CONNECTION_STATUS_TO_DEAD);
          conn = NULL;
        }
      }
      LOG_ERROR("Send packet error : %s.\n", e.what());
      error_message.append(e.what());
      record_migrate_error_message(plan, error_message);
      return;
    } catch (exception &e) {
      mutex.acquire();
      got_error = true;
      status = EXECUTE_STATUS_COMPLETE;
      mutex.release();
      if (conn) {
        if (!plan->get_migrate_tool())
          handler->clean_dead_conn(&conn, dataspace, !force_use_non_trx_conn);
        else {
          conn->set_status(CONNECTION_STATUS_TO_DEAD);
          conn = NULL;
        }
      }
      error_message.append("Execute Node failed: ");
      error_message.append(e.what());
      LOG_ERROR("[%s]\n", error_message.c_str());
      record_migrate_error_message(plan, error_message);
      return;
    }
  }
}

bool MySQLFetchNode::can_swap() {
  return session->is_may_backend_exec_swap_able() & (!got_error);
}

void MySQLFetchNode::clean() {
  /* check if the fetch thread finished */
  if (thread_status == THREAD_STARTED) {
    LOG_DEBUG("Fetch node %@ cleaning wait.\n", this);
    this->wait_for_cond();
    LOG_DEBUG("Fetch node %@ cleaning wait finish.\n", this);
  } else if (thread_status == THREAD_CREATED) {
    bthread->re_init_backend_thread();
    bthread->get_pool()->add_back_to_free(bthread);
  } else {
    got_error = true;
  }

  Packet *packet;
  if (eof_packet) {
    delete eof_packet;
    eof_packet = NULL;
  }

  while (!field_packets.empty()) {
    packet = field_packets.front();
    field_packets.pop_front();
    delete packet;
  }
  field_packets.clear();
  packet = NULL;

  while (!ready_rows->empty()) {
    packet = ready_rows->front();
    ready_rows->pop_front();
    delete packet;
  }

  while (!ready_rows_kept.empty()) {
    packet = ready_rows_kept.front();
    ready_rows_kept.pop_front();
    delete packet;
  }
  kept_ready_packets = 0;

  ready_rows->clear();
  packet = NULL;

  if (use_packet_pool && !pkt_list.empty()) {
    int s = (int)pkt_list.size();
    session->put_free_packet_back_to_pool(id_in_plan, s, pkt_list);
    pkt_list.clear();
  }

  if (end_packet) {
    delete end_packet;
    end_packet = NULL;
  }

  if (conn) {
    if (!got_error || error_packet) {
      if (!plan->get_migrate_tool())
        handler->put_back_connection(dataspace, conn, force_use_non_trx_conn);
    } else {
      if (!plan->get_migrate_tool())
        handler->clean_dead_conn(&conn, dataspace, !force_use_non_trx_conn);
      else
        conn->set_status(CONNECTION_STATUS_TO_DEAD);
    }
    conn = NULL;
  }

  if (error_packet) {
    if (error_packet != &header_packet) {
      delete error_packet;
    }
    error_packet = NULL;
  }
  if (migrate_partition_key_pos_vec) {
    migrate_partition_key_pos_vec->clear();
    delete migrate_partition_key_pos_vec;
    migrate_partition_key_pos_vec = NULL;
  }
}

void MySQLFetchNode::add_kept_list_to_ready_and_signal() {
  try {
    ACE_Guard<ACE_Thread_Mutex> guard(mutex);
    if (!ready_rows_kept.empty()) {
      while (!ready_rows_kept.empty()) {
        ready_rows->push_back(ready_rows_kept.front());

        ready_buffer_rows_size += ready_rows_kept.front()->total_capacity();
        ready_rows_size++;
        ready_rows_kept.pop_front();
      }
      cond.signal();
      kept_ready_packets = 0;
    }
  } catch (ListOutOfMemError &e) {
    throw;
  } catch (...) {
    cond.signal();
    throw;
  }
}

void MySQLFetchNode::get_partition_key_pos() {
  if (plan->get_migrate_tool() && !has_get_partition_key_pos) {
    has_get_partition_key_pos = true;
    vector<const char *> *key_names_vec =
        plan->get_migrate_tool()->get_partition_key_names();
    if (key_names_vec == NULL)
      migrate_partition_key_pos_vec = NULL;
    else {
      migrate_partition_key_pos_vec = new vector<unsigned int>();
      vector<const char *>::iterator it = key_names_vec->begin();
      list<Packet *>::iterator it_field;
      unsigned int index = 0;
      // record the pos of partition key
      for (it_field = field_packets.begin(); it_field != field_packets.end();
           it_field++, index++) {
        MySQLColumnResponse col_resp(*it_field);
        col_resp.unpack();

        for (it = key_names_vec->begin(); it != key_names_vec->end(); it++) {
          if (strcmp(*it, col_resp.get_column()) == 0) {
            migrate_partition_key_pos_vec->push_back(index);
            break;
          }
        }
      }
    }
  }
}
bool MySQLFetchNode::migrate_filter(Packet *row) {
  if (migrate_partition_key_pos_vec == NULL) return true;
  MySQLRowResponse row_res(row);
  vector<string> partition_key_value;
  vector<unsigned int>::iterator it = migrate_partition_key_pos_vec->begin();
  for (; it != migrate_partition_key_pos_vec->end(); it++) {
    unsigned int pos = *it;
    // check whether the value of partition key is NULL
    if (!row_res.field_is_null(pos)) {
      uint64_t str_len = 0;
      const char *column_str = row_res.get_str(pos, &str_len);
      string tmp;
      tmp.append(column_str, str_len);
      partition_key_value.push_back(tmp);
    } else {
      LOG_ERROR("Get partition key value[NULL] when do migrate.\n");
#ifdef DEBUG
      ACE_ASSERT(0);
#endif
      throw Error("Get partition key value[NULL] when do migrate.");
    }
  }
  vector<const char *> values;
  for (unsigned i = 0; i < partition_key_value.size(); i++) {
    values.push_back(partition_key_value.at(i).c_str());
  }
  return plan->get_migrate_tool()->filter(&values);
}
/* start the FetchNode thread using this method. */
void MySQLFetchNode::start_thread() {
  if (thread_status == THREAD_STOP && !got_error &&
      !plan->get_fetch_node_no_thread()) {
    // start the new thread
    bthread = plan->get_one_bthread();
    if (!bthread) {
      got_error = true;
      LOG_ERROR(
          "Fail to get a backend thread from pool, so the fetch node stop.\n");
      throw Error(
          "Fail to get a backend thread from pool, so the fetch node stop");
    }
    bthread->set_task(this);
    thread_status = THREAD_CREATED;
  }
}

int MySQLFetchNode::fetch_row() {
  Packet *packet;
  if (is_get_end_packet || got_error) {
    status = EXECUTE_STATUS_COMPLETE;
    return 0;
  }
  if (!header_received && receive_header_packets()) {
    return 1;
  }
  ExecuteProfileHandler *execute_profile = session->get_profile_handler();
  unsigned int tmp_id = execute_profile->start_parallel_monitor(
      get_executenode_name(), conn->get_server()->get_name(), sql.c_str());

  try {
    ready_rows_size = 0;
    packet = Backend::instance()->get_new_packet(
        row_packet_size, session->get_fetch_node_packet_alloc());
    handler->receive_from_server(conn, packet);

    unsigned int count_index = -1;
    handle_federated_empty_resultset(packet, count_index);
    // Receiving row packets
    while (!driver->is_eof_packet(packet)) {
      if (driver->is_error_packet(packet)) {
        LOG_ERROR("FetchNode %@ get an error packet.\n", this);
        Packet *tmp_packet = NULL;
        // Receiving error packet
        // packet is located on session->get_fetch_node_packet_alloc()
        // session need refresh session->get_fetch_node_packet_alloc()
        // so mv packet memory from session->get_fetch_node_packet_alloc() to
        // new memory
        transfer_packet(&packet, &tmp_packet);
        handle_error_packet(tmp_packet);
        return 1;
      }
      if (count_index != (unsigned int)-1) {
        uint64_t count_num = 1;
        MySQLRowResponse count_row(packet);
        count_num = count_row.get_uint(count_index);
        if (count_num == 0) {
          wakeup_federated_follower();
        }
        count_index = -1;
      }
#ifdef DEBUG
      LOG_DEBUG("FetchNode %@ get a row packet.\n", this);
#endif
      parent->add_row_packet(this, packet);
      ready_rows_size++;
      if (max_fetchnode_ready_rows_size &&
          ready_buffer_rows_size > max_fetchnode_buffer_rows_size) {
#ifndef DBSCALE_TEST_DISABLE
        LOG_INFO("FetchNode Full\n");
#endif
        return 1;
      }
      bool row_map_size_overtop =
          parent->get_need_restrict_row_map_size() &&
          ((parent->get_buffer_rowmap_size(this) >=
            max_fetchnode_buffer_rows_size) ||
           (parent->get_rowmap_size(this) >= rowmap_size_config));
      if (row_map_size_overtop) return 1;
      packet = Backend::instance()->get_new_packet(
          row_packet_size, session->get_fetch_node_packet_alloc());
      handler->receive_from_server(conn, packet);
    }
    if (driver->is_eof_packet(packet)) {
      if (support_show_warning)
        handle_warnings_OK_and_eof_packet(plan, packet, handler, dataspace,
                                          conn);
    }
    LOG_DEBUG("FetchNode %@ got a eof packet.\n", this);
    is_get_end_packet = true;
    delete packet;
  } catch (...) {
    LOG_ERROR("FetchNode %@ exit cause self error: %s.\n", this, e.what());
    handle_error_packet(NULL);
    error_message.append("Execute Node failed: ");
    error_message.append(e.what());
    if (conn)
      handler->clean_dead_conn(&conn, dataspace, !force_use_non_trx_conn);
    conn = NULL;
  }
  execute_profile->end_execute_monitor(tmp_id);
  return 1;
}

void MySQLFetchNode::handle_error_throw() {
  if (got_error) throw_error_packet();
}

/** Return ture if send row packet to parent, else return false.
 * Return ture if send row packet to parent, else return false.
 */
bool MySQLFetchNode::notify_parent() {
  Packet *packet;
  int ret = false;
  if (plan->get_fetch_node_no_thread()) {
    session->set_skip_mutex_in_operation(true);
    ret = fetch_row();
    session->set_skip_mutex_in_operation(false);
    if (is_finished() || got_error) {
      if (got_error) {
        ;  // we do nothing here, we would throw exception later.
      } else {
#ifdef DEBUG
        LOG_DEBUG("FetchNode %@ is finished.\n", this);
#endif
      }
      ret = false;
      status = EXECUTE_STATUS_COMPLETE;
    }
    return ret;
  }

  ACE_Guard<ACE_Thread_Mutex> guard(mutex);
  // check fetch node finished or get error
  if (is_finished() || got_error) {
    if (got_error) {
      throw_error_packet();
    } else {
#ifdef DEBUG
      LOG_DEBUG("FetchNode %@ is finished.\n", this);
#endif
      ret = false;
    }
  } else {
    if (ready_rows->empty()) cond.wait();

    if (got_error) {
      throw_error_packet();
      ret = false;
    } else if (ready_rows->empty()) {  // the result set is empty
      ACE_ASSERT(is_finished());
      LOG_DEBUG("FetchNode %@ is finished after wait.\n", this);
      ret = false;
    } else {
      bool is_migrate =
          plan->get_migrate_tool() && migrate_partition_key_pos_vec;
      bool row_map_size_overtop =
          parent->get_need_restrict_row_map_size() &&
          ((parent->get_buffer_rowmap_size(this) >=
            max_fetchnode_buffer_rows_size) ||
           (parent->get_rowmap_size(this) >= rowmap_size_config));
#ifdef DEBUG
      if (parent->get_need_restrict_row_map_size())
        LOG_DEBUG("the parent of fetchnode %s is %d,parent buffer size is %u\n",
                  parent->get_executenode_name(), row_map_size_overtop,
                  parent->get_buffer_rowmap_size(this));
#endif

      // filter packet for partition table
      if (is_migrate) {
        if (!row_map_size_overtop) {
          while (!ready_rows->empty()) {
            packet = ready_rows->front();
            ready_rows->pop_front();
            ready_rows_size--;
            if (migrate_filter(packet))
              parent->add_row_packet(this, packet);
            else
              delete packet;
          }
        }
      } else if (!row_map_size_overtop) {
        if (ready_rows_size > 0) {
          parent->swap_ready_list_with_row_map_list(
              this, &ready_rows, ready_rows_size, ready_buffer_rows_size);
          ready_rows_size = 0;
          ready_buffer_rows_size = 0;
        }
      }
      ret = true;
    }
  }

  return ret;
}

/* If the fetch node get an empty result set, check the if
 * it contains federated table, if is and there's only one last
 * federated thread has not registed to the rule, then the leader
 * federated thread has not be created, wakeup a waiting thread.
 */
void MySQLFetchNode::wakeup_federated_follower() {
  set<string> name_vec;
  plan->statement->get_fe_tmp_table_name(sql, name_vec);
  set<string>::iterator it = name_vec.begin();
  string tmp_table_name;
  Backend *backend = Backend::instance();
  for (; it != name_vec.end(); it++) {
    tmp_table_name.assign(*it);
    LOG_DEBUG("Get federated cross node join table name %s from sql %s.\n",
              tmp_table_name.c_str(), sql.c_str());
    DataSpace *send_space = backend->get_data_space_for_table(
        TMP_TABLE_SCHEMA, tmp_table_name.c_str());
    DataSource *send_source = send_space->get_data_source();
    CrossNodeJoinManager *cross_join_manager =
        backend->get_cross_node_join_manager();
    Session *work_session =
        cross_join_manager->get_work_session(tmp_table_name);
#ifdef DEBUG
    ACE_ASSERT(work_session);
#endif
    if (!send_source) tmp_table_name.append("_par0");
    TransferRuleManager *rule_manager =
        work_session->get_transfer_rule_manager();
    TransferRule *rule =
        rule_manager->get_transfer_rule_by_remote_table_name(tmp_table_name);
    if (rule->is_leader_thread()) {
      rule->wakeup_one_follower();
    }
  }
}

void MySQLFetchNode::handle_federated_empty_resultset(
    Packet *packet, unsigned int &count_index) {
  // TODO:Change the way of checking the sql if is a federated tmp table join
  // sql.
  if (session->get_session_option("cross_node_join_method").int_val ==
          DATA_MOVE_READ &&
      sql.find(TMP_TABLE_NAME) != string::npos) {
    if (driver->is_eof_packet(packet)) {
      wakeup_federated_follower();
    } else {
      // Check the query if contains 'COUNT()'.
      list<AggregateDesc> aggr_list;
      get_aggregate_functions(plan->statement->get_stmt_node()->scanner,
                              aggr_list);
      list<AggregateDesc>::iterator it_agg = aggr_list.begin();
      for (; it_agg != aggr_list.end(); it_agg++) {
        if (it_agg->type == AGGREGATE_TYPE_COUNT) {
          count_index = it_agg->column_index;
          break;
        }
      }
    }
  }
}

void MySQLFetchNode::handle_dead_xa_conn(Connection *conn, DataSpace *space) {
  if (conn->is_start_xa_conn()) {
    unsigned long xid = session->get_xa_id();
    char tmp[256];
    int len =
        sprintf(tmp, "%lu-%u-%d-%d-%u", xid, space->get_virtual_machine_id(),
                conn->get_group_id(), Backend::instance()->get_cluster_id(),
                conn->get_thread_id());
    tmp[len] = '\0';
    string sql = "XA END '";
    sql += tmp;
    sql += "'; XA ROLLBACK '";
    sql += tmp;
    sql += "';";
    Connection *exe_conn = NULL;
    exe_conn = space->get_connection(session);
    if (exe_conn) {
      try {
        exe_conn->execute(sql.c_str(), 2);
        exe_conn->handle_client_modify_return();
        conn->set_start_xa_conn(false);
        exe_conn->get_pool()->add_back_to_free(conn);
        exe_conn = NULL;
      } catch (...) {
        LOG_DEBUG("Fail to rollback %s on conn %@ before add to dead\n",
                  sql.c_str(), conn);
        if (exe_conn) {
          exe_conn->get_pool()->add_back_to_dead(exe_conn);
        }
      }
    }
  }
}

int MySQLFetchNode::svc() {
  Packet *packet = NULL;
  if (is_finished()) return FINISHED;
#ifdef DEBUG
  node_start_timing();
#endif

  if (!header_received) LOG_DEBUG("FetchNode %@ SQL : %s\n", this, sql.c_str());
  if (!header_received && receive_header_packets()) {
    return FINISHED;
  }
  ExecuteProfileHandler *execute_profile = session->get_profile_handler();
  unsigned int tmp_id = execute_profile->start_parallel_monitor(
      get_executenode_name(), conn->get_server()->get_name(), sql.c_str());
  int get_row_packet_num = 0;
#ifndef DBSCALE_TEST_DISABLE
  int loop_count = 1;
#endif
  try {
    if (use_packet_pool && pkt_list.empty()) {
      session->get_free_packet_from_pool(
          id_in_plan, packet_pool_packet_bundle_local, &pkt_list);
    }
    if (!use_packet_pool || pkt_list.empty()) {
      packet = Backend::instance()->get_new_packet(row_packet_size);
    } else {
      packet = pkt_list.front();
      pkt_list.pop_front();
    }
    handler->receive_from_server(conn, packet);
    get_partition_key_pos();  // get partition pos for migrate
    unsigned int count_index = -1;
    handle_federated_empty_resultset(packet, count_index);
    // Receiving row packets
    while (!driver->is_eof_packet(packet)) {
      if (driver->is_error_packet(packet)) {
        MySQLErrorResponse error(packet);
        error.unpack();
        LOG_ERROR("FetchNode node %@ get an error packet %@, %d (%s) %s.\n",
                  this, packet, error.get_error_code(), error.get_sqlstate(),
                  error.get_error_message());

        handle_error_packet(packet);
        packet = NULL;
        return FINISHED;
      }

      if (parent->is_finished()) {
        if (!session->is_keeping_connection()) {
          LOG_INFO(
              "FetchNode %@ is not keeping connection, exit cause parent "
              "finished.\n",
              this);
          if (conn) {
            if (!plan->get_migrate_tool()) {
              handle_dead_xa_conn(conn, dataspace);
              handler->clean_dead_conn(&conn, dataspace,
                                       !force_use_non_trx_conn);
            } else
              conn->set_status(CONNECTION_STATUS_TO_DEAD);
            conn = NULL;
          }
          handle_error_packet(NULL);
          delete packet;
          packet = NULL;
          return FINISHED;
        } else {
          while (!driver->is_eof_packet(packet)) {
            handler->receive_from_server(conn, packet);
            if (driver->is_error_packet(packet)) {
              LOG_ERROR("FetchNode %@ get an error packet.\n", this);
              handle_error_packet(packet);
              packet = NULL;
              return FINISHED;
            }
          }
          break;
        }
      }

      if (count_index != (unsigned int)-1) {
        uint64_t count_num = 1;
        MySQLRowResponse count_row(packet);
        count_num = count_row.get_uint(count_index);
        if (count_num == 0) {
          wakeup_federated_follower();
        }
        count_index = -1;
      }
#ifdef DEBUG
      LOG_DEBUG("FetchNode %@ get a row packet.\n", this);
#endif
#ifndef DBSCALE_TEST_DISABLE
      loop_count++;
      if (loop_count == 5) {
        Backend *bk = Backend::instance();
        dbscale_test_info *test_info = bk->get_dbscale_test_info();
        if (test_info->test_case_name.length() &&
            !strcasecmp(test_info->test_case_name.c_str(), "load_select") &&
            !strcasecmp(test_info->test_case_operation.c_str(),
                        "load_select_fetch_fail")) {
          ACE_OS::sleep(2);
          throw Exception("dbscale test fail.");
        }
      }
#endif

      get_row_packet_num++;
      kept_ready_packets++;
      ready_rows_kept.push_back(packet);

      packet = NULL;
      if (kept_ready_packets >= local_fetch_signal_batch) {
        add_kept_list_to_ready_and_signal();
      }

      if (get_row_packet_num >= MAX_FETCH_NODE_RECEIVE) {
        if (kept_ready_packets > 0) {
          add_kept_list_to_ready_and_signal();
        }
        return WORKING;
      }
      if (max_fetchnode_ready_rows_size) {
        bool need_break = false;
        while (max_fetchnode_ready_rows_size &&
               ready_buffer_rows_size >= max_fetchnode_buffer_rows_size) {
          LOG_DEBUG("Fetchnode full,ready_buffer_rows_size in node is %u \n",
                    ready_buffer_rows_size);
          if (parent->is_finished() ||
              ACE_Reactor::instance()->reactor_event_loop_done()) {
            LOG_INFO(
                "fetchnode sleeping cancelled because parent finished or ACE "
                "reactor event loop done.\n");

            if ((parent->is_finished() && !session->is_keeping_connection()) ||
                ACE_Reactor::instance()->reactor_event_loop_done()) {
              LOG_INFO(
                  "FetchNode %@ is not keeping connection, exit cause parent "
                  "finished.\n",
                  this);
              if (conn) {
                if (!plan->get_migrate_tool()) {
                  handle_dead_xa_conn(conn, dataspace);
                  handler->clean_dead_conn(&conn, dataspace,
                                           !force_use_non_trx_conn);
                } else
                  conn->set_status(CONNECTION_STATUS_TO_DEAD);
                conn = NULL;
              }
              handle_error_packet(NULL);
              delete packet;
              packet = NULL;
              return FINISHED;
            } else {
              while (!driver->is_eof_packet(packet)) {
                handler->receive_from_server(conn, packet);
                if (driver->is_error_packet(packet)) {
                  LOG_ERROR("FetchNode %@ get an error packet.\n", this);
                  handle_error_packet(packet);
                  packet = NULL;
                  return FINISHED;
                }
              }
              need_break = true;
              break;
            }
          }
          if (kept_ready_packets > 0) {
            add_kept_list_to_ready_and_signal();
          }
          return NEED_SLEEP;
        }
        if (need_break) break;
      }
      if (use_packet_pool && pkt_list.empty()) {
        session->get_free_packet_from_pool(
            id_in_plan, packet_pool_packet_bundle_local, &pkt_list);
      }
      if (!use_packet_pool || pkt_list.empty()) {
        packet = Backend::instance()->get_new_packet(row_packet_size);
      } else {
        packet = pkt_list.front();
        pkt_list.pop_front();
      }
      handler->receive_from_server(conn, packet);
    }
    if (kept_ready_packets > 0) {
      add_kept_list_to_ready_and_signal();
    }

    if (driver->is_eof_packet(packet)) {
      if (support_show_warning)
        handle_warnings_OK_and_eof_packet(plan, packet, handler, dataspace,
                                          conn);
    }
  } catch (ListOutOfMemError &e) {
    LOG_ERROR("FetchNode %@ exit cause self error: %s\n", this, e.what());
    string msg = "FetchNode exit cause self error:";
    msg += e.what();
    record_migrate_error_message(plan, msg);
    error_message.append("There are no enough memory for the sql.");
    handle_error_packet(NULL);
    if (packet) {
      delete packet;
      packet = NULL;
    }
    cond.signal();
    return FINISHED;
  } catch (exception &e) {
    LOG_ERROR("FetchNode %@ exit cause self error: %s.\n", this, e.what());
    string msg = "FetchNode exit cause self error:";
    msg += e.what();
    record_migrate_error_message(plan, msg);
    handle_error_packet(NULL);
    error_message.append("Execute Node failed: ");
    error_message.append(e.what());
    if (packet) {
      delete packet;
      packet = NULL;
    }
    cond.signal();
    return FINISHED;
  }

  if (conn) {
    if (has_sql_calc_found_rows()) {
      found_rows(handler, driver, conn, session);
      Packet *packet_tmp = session->get_error_packet();
      if (packet_tmp) {
        LOG_ERROR("FetchNode %@ get an error packet.\n", this);
        handle_error_packet(packet_tmp);
        cond.signal();
        return FINISHED;
      }
    }
  }

  LOG_DEBUG("FetchNode %@ got a eof packet.\n", this);
  // Receiving end packet
  mutex.acquire();
  end_packet = packet;
  packet = NULL;
  status = EXECUTE_STATUS_COMPLETE;
  cond.signal();
  mutex.release();
  execute_profile->end_execute_monitor(tmp_id);

  if (plan->statement->get_stmt_node()->has_unknown_func && conn)
    session->record_xa_modified_conn(conn);

  // release the conn assp
  if (conn) {
    if (!plan->get_migrate_tool())
      handler->put_back_connection(dataspace, conn, force_use_non_trx_conn);
    conn = NULL;
  }
#ifdef DEBUG
  node_end_timing();
#endif
#ifdef DEBUG
  LOG_DEBUG("MySQLFetchNode %@ cost %d ms\n", this, node_cost_time);
#endif

  return FINISHED;
}

int MySQLFetchNode::receive_header_packets() {
  Packet *tmp_packet;
  header_received = true;
  try {
    conn->handle_merge_exec_begin_or_xa_start(session);
    handler->receive_from_server(conn, &header_packet);
    if (driver->is_ok_packet(&header_packet)) {
      LOG_DEBUG("fetch node get an ok packet, sql[%s].\n", sql.c_str());
      throw HandlerError("Unexpected error, fetch node get an ok packet.");
    }

    if (driver->is_error_packet(&header_packet)) {
      MySQLErrorResponse error(&header_packet);
      error.unpack();
      LOG_ERROR("FetchNode node %@ get an error packet %@, %d (%s) %s.\n", this,
                &header_packet, error.get_error_code(), error.get_sqlstate(),
                error.get_error_message());

      handle_error_packet(&header_packet);
      return 1;
    }

    tmp_packet = Backend::instance()->get_new_packet(row_packet_size);
    // Receiving column packets.
    handler->receive_from_server(conn, tmp_packet);
    while (!driver->is_eof_packet(tmp_packet)) {
      if (driver->is_error_packet(tmp_packet)) {
        handle_error_packet(tmp_packet);
        return 1;
      }
      field_packets.push_back(tmp_packet);
      tmp_packet = Backend::instance()->get_new_packet(row_packet_size);
      handler->receive_from_server(conn, tmp_packet);
    }
  } catch (Exception &e) {
    mutex.acquire();
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    if (conn) {
      if (!plan->get_migrate_tool())
        handler->clean_dead_conn(&conn, dataspace, !force_use_non_trx_conn);
      else {
        conn->set_status(CONNECTION_STATUS_TO_DEAD);
        conn = NULL;
      }
    }
    LOG_ERROR("Receive header error : %s.\n", e.what());
    error_message.append(e.what());
    cond.signal();
    mutex.release();
    return 1;
  } catch (exception &e) {
    mutex.acquire();
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    if (conn) {
      if (!plan->get_migrate_tool())
        handler->clean_dead_conn(&conn, dataspace, !force_use_non_trx_conn);
      else {
        conn->set_status(CONNECTION_STATUS_TO_DEAD);
        conn = NULL;
      }
    }
    LOG_ERROR("Receive header error : unknown error.\n");
    error_message.append("Execute Node failed: ");
    error_message.append(e.what());
    cond.signal();
    mutex.release();
    return 1;
  }

  LOG_DEBUG("Fetch Node %@ receive header finished.\n", this);
  // Set eof packet
  eof_packet = tmp_packet;
  return 0;
}

/* class MySQLSendNode */

MySQLSendNode::MySQLSendNode(ExecutePlan *plan) : MySQLInnerNode(plan) {
  send_header_flag = true;
  this->name = "MySQLSendNode";
  this->send_packet_profile_id = -1;
  this->wait_child_profile_id = -1;
  select_uservar_flag = statement->get_select_uservar_flag();
  select_field_num = statement->get_select_field_num();
  if (select_uservar_flag) {
    uservar_vec = *(statement->get_select_uservar_vec());
  }
  this->federated_max_rows = (uint64_t)(
      session->get_session_option("max_federated_cross_join_rows").ulong_val);
  this->cross_join_max_rows = (uint64_t)(
      session->get_session_option("max_cross_join_moved_rows").ulong_val);
#ifndef DBSCALE_TEST_DISABLE
  /*just for test federated_max_rows*/
  Backend *bk = Backend::instance();
  dbscale_test_info *test_info = bk->get_dbscale_test_info();
  if (!strcasecmp(test_info->test_case_name.c_str(), "federated_max_rows") &&
      !strcasecmp(test_info->test_case_operation.c_str(), "abort")) {
    this->federated_max_rows = 1;
  }
#endif
  row_num = 0;
  tmp_id = 0;
  node_can_swap = session->is_may_backend_exec_swap_able();
  pkt_list.clear();
  pkt_list_size = 0;
  field_num = 0;
  ready_rows->set_session(plan->session);
}

void MySQLSendNode::send_header() {
  LOG_DEBUG("Start to Send header.\n");
  Packet *header_packet = get_header_packet();
  handler->send_mysql_packet_to_client_by_buffer(header_packet);
  list<Packet *> *field_packets = get_field_packets();

  vector<MySQLColumnType> v_ctype;
  if (session->is_binary_resultset()) {
    if (session->get_prepare_item(session->get_execute_stmt_id())) {
      list<MySQLColumnType> *column_type_list =
          session->get_prepare_item(session->get_execute_stmt_id())
              ->get_column_type_list();
      list<MySQLColumnType>::iterator it_c = column_type_list->begin();
      for (; it_c != column_type_list->end(); it_c++) {
        v_ctype.push_back(*it_c);
      }
    }
  }

  size_t pos = 0;
  for (std::list<Packet *>::iterator it = field_packets->begin();
       it != field_packets->end(); it++) {
    if (select_uservar_flag) {
      MySQLColumnResponse column_response(*it);
      column_response.unpack();
      bool is_number = column_response.is_number();
      field_is_number_vec.push_back(is_number);
    }

    if (session->is_binary_resultset()) {
      if (!v_ctype.empty() && v_ctype.size() > pos) {
        MySQLColumnType prepare_type = v_ctype[pos];
        MySQLColumnResponse col_resp(*it);
        col_resp.unpack();
        MySQLColumnType col_type = col_resp.get_column_type();
        if (col_type != prepare_type) {
          col_resp.set_column_type(prepare_type);
          col_resp.pack(*it);
        }
        LOG_DEBUG(
            "Adjust the column %s from type %d to type %d for binary result "
            "convert.\n",
            col_resp.get_column(), int(col_type), int(prepare_type));
      }
    }

    handler->send_mysql_packet_to_client_by_buffer(*it);
    ++field_num;
    pos++;
  }
  Packet *eof_packet = get_eof_packet();
  handler->deal_autocommit_with_ok_eof_packet(eof_packet);
  handler->send_mysql_packet_to_client_by_buffer(eof_packet);
}

void MySQLExecuteNode::rebuild_packet_first_column_dbscale_row_id(
    Packet *row, uint64_t row_num, unsigned int field_num) {
  MySQLRowResponse row_resp(row);
  vector<string> vec;
  row_resp.fill_columns_to_vector(&vec, field_num);
  char tmp[21];
  sprintf(tmp, "%lu", row_num);
  vec[0] = tmp;
  list<const char *> row_data;
  for (unsigned int i = 0; i < field_num; ++i) {
    row_data.push_back(vec[i].c_str());
  }
  MySQLRowResponse row_resp2(row_data);
  row_resp2.pack(row);
}

void MySQLSendNode::send_row(MySQLExecuteNode *ready_child) {
  send_packet_profile_id = session->get_profile_handler()->start_serial_monitor(
      get_executenode_name(), "SendNode send packet", "",
      send_packet_profile_id);
  Packet *end_row = NULL;
  if (!row_map[ready_child]->empty()) end_row = row_map[ready_child]->back();
  if (statement->get_select_uservar_flag() && end_row != NULL) {
    MySQLRowResponse row_response(end_row);
    set_select_uservar_by_result(end_row, field_is_number_vec, uservar_vec,
                                 select_field_num, session);
  }
  Packet *row = NULL;
  if (!plan->is_federated() && !plan->statement->is_cross_node_join() &&
      !session->is_binary_resultset()) {
    while (!row_map[ready_child]->empty()) {
      row_num++;
      try {
        row = row_map[ready_child]->front();
        if (is_first_column_dbscale_row_id) {
          rebuild_packet_first_column_dbscale_row_id(row, row_num, field_num);
        }
        handler->send_mysql_packet_to_client_by_buffer(row);
      } catch (...) {
        row_map[ready_child]->pop_front();
        delete row;
        throw;
      }
      row_map[ready_child]->pop_front();
      if (use_packet_pool) {
        pkt_list.push_back(row);
        pkt_list_size++;
        // if ((size_t)pkt_list_size >= packet_pool_packet_bundle_local) {
        //  session->put_free_packet_back_to_pool(ready_child->get_id_in_plan(),
        //  pkt_list_size, pkt_list);
        //}
      } else {
        delete row;
      }
    }
  } else {
    while (!row_map[ready_child]->empty()) {
      if (plan->is_federated() && row_num >= federated_max_rows) {
        status = EXECUTE_STATUS_COMPLETE;
        LOG_ERROR(
            "Reach the max rows of [%u] for federated middle result set.\n",
            row_num);
        throw ExecuteNodeError(
            "Reach the max row number of federated middle result set.");
      }
      if (plan->statement->is_cross_node_join() &&
          row_num > cross_join_max_rows) {
        status = EXECUTE_STATUS_COMPLETE;
        LOG_ERROR(
            "Reach the max rows of [%u] for cross node join max moved rows.\n",
            row_num);
        throw ExecuteNodeError(
            "Reach the max row number of cross node join max moved rows.");
      }
      row = row_map[ready_child]->front();
      row_num++;
      if (is_first_column_dbscale_row_id) {
        rebuild_packet_first_column_dbscale_row_id(row, row_num, field_num);
      }
      if (session->is_binary_resultset()) {
        MySQLRowResponse row_resp(row);
        if (session->get_prepare_item(session->get_execute_stmt_id())) {
          list<MySQLColumnType> *column_type_list =
              session->get_prepare_item(session->get_execute_stmt_id())
                  ->get_column_type_list();
          row_resp.convert_to_binary(
              column_type_list,
              handler->get_convert_result_to_binary_stringstream(), &row);
        } else {
          LOG_ERROR("Failed to get prepare item for execute #%d.\n",
                    session->get_execute_stmt_id());
          throw ExecuteCommandFail("Cannot get prepare item.");
        }
      }
      try {
        handler->send_mysql_packet_to_client_by_buffer(row);
      } catch (...) {
        row_map[ready_child]->pop_front();
        delete row;
        throw;
      }
      row_map[ready_child]->pop_front();
      if (use_packet_pool) {
        pkt_list.push_back(row);
        pkt_list_size++;
        // if ((size_t)pkt_list_size >= packet_pool_packet_bundle_local) {
        //  session->put_free_packet_back_to_pool(ready_child->get_id_in_plan(),
        //  pkt_list_size, pkt_list);
        //}
      } else {
        delete row;
      }
    }
  }
  if (use_packet_pool && !pkt_list.empty()) {
    session->put_free_packet_back_to_pool(ready_child->get_id_in_plan(),
                                          pkt_list_size, pkt_list);
  }
  session->get_profile_handler()->end_execute_monitor(send_packet_profile_id);
}

void MySQLSendNode::clear_row_map() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    Packet *row;
    while (!row_map[*it]->empty()) {
      row = row_map[*it]->front();
      row_map[*it]->pop_front();
      delete row;
    }
  }
}

void MySQLSendNode::send_eof() {
  LOG_DEBUG("Start to send eof.\n");
  Packet *end_packet = get_end_packet();
  if (!end_packet) {
    build_eof_packet();
    end_packet = &generated_eof;
  }
  if (plan->session->is_call_store_procedure() ||
      plan->session->get_has_more_result()) {
    rebuild_eof_with_has_more_flag(end_packet, driver);
    LOG_DEBUG(
        "For the call store procedure or multiple stmt, the last eof"
        " should be with flag has_more_result in send node.\n");
  }
  LOG_DEBUG("end_packet = %@\n", end_packet);
  handler->deal_autocommit_with_ok_eof_packet(end_packet);
  handler->send_mysql_packet_to_client_by_buffer(end_packet);
}

void MySQLSendNode::handle_children() {
  bool has_handle_child = false;
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if (!row_map[*it]->empty()) {
      handle_child(*it);
      has_handle_child = true;
    }
  }
  if (has_handle_child && plan->get_fetch_node_no_thread()) {
    handle_discarded_packets();
    session->reset_fetch_node_packet_alloc();
  }
}

void MySQLSendNode::execute() {
  ExecuteProfileHandler *execute_profile = session->get_profile_handler();
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START:
        tmp_id = execute_profile->start_serial_monitor(get_executenode_name(),
                                                       "SendNode all", "");
        init_row_map();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        if (node_can_swap && plan->plan_can_swap()) {
          handle_swap_connections();
          return;
        }
        status = EXECUTE_STATUS_WAIT;

      case EXECUTE_STATUS_WAIT:
        try {
          wait_child_profile_id =
              session->get_profile_handler()->start_serial_monitor(
                  get_executenode_name(), "SendNode wait", "",
                  wait_child_profile_id);

          wait_children();
          if (one_child_got_error && !all_children_finished) {
            status = EXECUTE_STATUS_FETCH_DATA;
            clear_row_map();
          }
          session->get_profile_handler()->end_execute_monitor(
              wait_child_profile_id);
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;

      case EXECUTE_STATUS_HANDLE:
        try {
          handle_children();
        } catch (ClientBroken &e) {
          if (plan->get_fetch_node_no_thread()) set_children_error();
          throw;
        }
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_BEFORE_COMPLETE:
        if (send_header_flag) {
          send_header();
          send_header_flag = false;
        }
        send_eof();
        flush_net_buffer();
        status = EXECUTE_STATUS_COMPLETE;

      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        node_end_timing();
#endif
        execute_profile->end_execute_monitor(tmp_id);
#ifdef DEBUG
        LOG_DEBUG("MySQLSendNode %@ cost %d ms\n", this, node_cost_time);
#endif
        break;

      default:
        break;
    }
  }
  if (!has_sql_calc_found_rows()) {
    session->set_found_rows(row_num);
  }
  session->set_real_fetched_rows(row_num);
}

void append_column_value_to_replace_sql(string &replace_sql,
                                        const char *column_str,
                                        uint64_t column_len, ResultType type,
                                        CharsetType ctype) {
  if (type == RESULT_TYPE_STRING) {
    string tmp;
    tmp.append(column_str, column_len);
    deal_with_str_column_value(&tmp, ctype);
    replace_sql.append(tmp);
  } else
    replace_sql.append(column_str, column_len);
}
/* class MySQLQueryForMulColumnNode*/
MySQLQueryForMulColumnNode::MySQLQueryForMulColumnNode(ExecutePlan *plan,
                                                       DataSpace *dataspace,
                                                       const char *sql)
    : MySQLDirectExecuteNode(plan, dataspace, sql), has_get_one_row(false) {
  this->name = "MySQLQueryForMulColumnNode";
#ifdef DEBUG
  ACE_ASSERT(plan->get_sub_query_result_node());
#endif
  res_node = plan->get_sub_query_result_node();
  res_node->replace_sql = string("select 1 from dual where 1=0");
  col_nums = 0;
  is_direct_node = false;
}

void MySQLQueryForMulColumnNode::handle_send_client_packet(Packet *packet,
                                                           bool is_row_packet) {
  if (is_row_packet) {
    if (has_get_one_row) {
      throw dbscale::sql::SQLError(dbscale_err_msg[ERROR_ONE_ROW_CODE], "21000",
                                   ERROR_ONE_ROW_CODE);
    }
    if (!has_get_one_row) {
      res_node->replace_sql.clear();
      res_node->row_columns.clear();
    }
    handle_one_row(packet);
    has_get_one_row = true;
  }
}

void MySQLQueryForMulColumnNode::handle_send_field_packet(Packet *packet) {
  MySQLColumnResponse col_resp(packet);
  col_resp.unpack();
  MySQLColumnType col_type = col_resp.get_column_type();
  col_types.push_back(col_type);
  col_nums++;
}

void MySQLQueryForMulColumnNode::handle_one_row(Packet *packet) {
  MySQLRowResponse row(packet);

  unsigned int i = 0;
  for (; i < col_nums; i++) {
    ResultType res = get_result_type_from_column_type(col_types[i]);
    string one_column;
    if (res == RESULT_TYPE_STRING) one_column.append("'");
    if (!row.field_is_null(i)) {
      uint64_t str_len = 0;
      const char *column_tmp = row.get_str(i, &str_len);
      CharsetType ctype = session->get_client_charset_type();
      append_column_value_to_replace_sql(one_column, column_tmp, str_len, res,
                                         ctype);
    } else
      one_column.append("NULL");
    if (res == RESULT_TYPE_STRING) one_column.append("'");
    res_node->row_columns.push_back(one_column);
  }
}

/* class MySQLQueryForOneColumnNode */
MySQLQueryForOneColumnNode::MySQLQueryForOneColumnNode(ExecutePlan *plan,
                                                       DataSpace *dataspace,
                                                       const char *sql,
                                                       bool only_one_row)
    : MySQLDirectExecuteNode(plan, dataspace, sql),
      has_get_one_row(false),
      col_type(MYSQL_TYPE_END),
      only_one_row(only_one_row) {
  this->name = "MySQLQueryForOneColumnNode";
#ifdef DEBUG
  ACE_ASSERT(plan->get_sub_query_result_node());
#endif
  res_node = plan->get_sub_query_result_node();
  if (replace_empty_result_null)
    res_node->replace_sql = string("NULL");
  else
    res_node->replace_sql = string("select 1 from dual where 1=0");
  row_num = 0;
  cross_join_max_rows = (uint64_t)(
      session->get_session_option("max_cross_join_moved_rows").ulong_val);
  is_direct_node = false;
}
void MySQLQueryForOneColumnNode::handle_send_client_packet(Packet *packet,
                                                           bool is_row_packet) {
  if (is_row_packet) {
    if (only_one_row && has_get_one_row) {
      throw dbscale::sql::SQLError(dbscale_err_msg[ERROR_ONE_ROW_CODE], "21000",
                                   ERROR_ONE_ROW_CODE);
    }
    if (!has_get_one_row) res_node->replace_sql.clear();
    handle_one_row(packet);
    has_get_one_row = true;
  }
}

void MySQLQueryForOneColumnNode::handle_send_field_packet(Packet *packet) {
  if (col_type == MYSQL_TYPE_END) {
    MySQLColumnResponse col_resp(packet);
    col_resp.unpack();
    col_type = col_resp.get_column_type();
  } else {
    throw dbscale::sql::SQLError(dbscale_err_msg[ERROR_ONE_COLUMN_CODE],
                                 "21000", ERROR_ONE_COLUMN_CODE);
  }
}

void MySQLQueryForOneColumnNode::handle_one_row(Packet *packet) {
  row_num++;
  if (row_num > cross_join_max_rows) {
    LOG_ERROR(
        "Reach the max rows of [%u] for cross node join max moved rows.\n",
        row_num);
    throw ExecuteNodeError(
        "Reach the max row number of cross node join max moved rows.");
  }

  MySQLRowResponse row(packet);

  ResultType res = get_result_type_from_column_type(col_type);
  string &replace_sql = res_node->replace_sql;

  if (has_get_one_row) replace_sql.append(",");

  if (res == RESULT_TYPE_STRING) replace_sql.append("'");

  if (!row.field_is_null(0)) {
    uint64_t str_len = 0;
    const char *column_tmp = row.get_str(0, &str_len);
    CharsetType ctype = session->get_client_charset_type();
    append_column_value_to_replace_sql(replace_sql, column_tmp, str_len, res,
                                       ctype);
  } else
    replace_sql.append("NULL");

  if (res == RESULT_TYPE_STRING) replace_sql.append("'");
}

/* class MySQLQueryForOneColumnAggrNode */
MySQLQueryForOneColumnAggrNode::MySQLQueryForOneColumnAggrNode(
    ExecutePlan *plan, DataSpace *dataspace, const char *sql, bool get_min)
    : MySQLDirectExecuteNode(plan, dataspace, sql),
      col_type(MYSQL_TYPE_END),
      has_get_one_row(false),
      get_min(get_min ? 1 : -1) {
  this->name = "MySQLQueryForOneColumnAggrNode";
#ifdef DEBUG
  ACE_ASSERT(plan->get_sub_query_result_node());
#endif
  res_node = plan->get_sub_query_result_node();
  res_node->replace_sql = string("select 1 from dual where 1=0");
  is_direct_node = false;
}

void MySQLQueryForOneColumnAggrNode::handle_send_client_packet(
    Packet *packet, bool is_row_packet) {
  if (is_row_packet) {
    if (!has_get_one_row) res_node->replace_sql.clear();
    handle_one_row(packet);
    has_get_one_row = true;
  }
}

void MySQLQueryForOneColumnAggrNode::handle_send_last_eof_packet(
    Packet *packet) {
  if (!has_get_one_row) return;
  ACE_UNUSED_ARG(packet);
  string &replace_sql = res_node->replace_sql;
  replace_sql.append("select ");

  ResultType res = get_result_type_from_column_type(col_type);

  if (res == RESULT_TYPE_STRING) replace_sql.append("'");

  MySQLRowResponse row(&aggr_packet);
  if (!row.field_is_null(0)) {
    uint64_t str_len = 0;
    const char *column_tmp = row.get_str(0, &str_len);
    CharsetType ctype = session->get_client_charset_type();
    append_column_value_to_replace_sql(replace_sql, column_tmp, str_len, res,
                                       ctype);
  } else
    replace_sql.append("NULL");

  if (res == RESULT_TYPE_STRING) replace_sql.append("'");
}

void MySQLQueryForOneColumnAggrNode::handle_send_field_packet(Packet *packet) {
  if (col_type == MYSQL_TYPE_END) {
    MySQLColumnResponse col_resp(packet);
    col_resp.unpack();
    col_type = col_resp.get_column_type();
  } else {
    throw dbscale::sql::SQLError(dbscale_err_msg[ERROR_ONE_COLUMN_CODE],
                                 "21000", ERROR_ONE_COLUMN_CODE);
  }
}

void MySQLQueryForOneColumnAggrNode::handle_one_row(Packet *row) {
  if (!has_get_one_row) {
    MySQLRowResponse row_p(row);
    uint64_t row_len = row->length();
    reset_packet_size(&aggr_packet, row_len + 9);
    row_p.pack(&aggr_packet);
  } else {
    MySQLRowResponse row1(&aggr_packet);
    MySQLRowResponse row2(row);
    int ret = MySQLColumnCompare::compare(&row1, &row2, 0, col_type,
                                          CHARSET_TYPE_OTHER, true);
    if (ret * get_min == 1) {
      uint64_t row_len = row->length();
      reset_packet_size(&aggr_packet, row_len + 9);
      row2.pack(&aggr_packet);
    }
  }
}

/* class MySQLQueryExistsNode */
MySQLQueryExistsNode::MySQLQueryExistsNode(ExecutePlan *plan,
                                           DataSpace *dataspace,
                                           const char *sql)
    : MySQLDirectExecuteNode(plan, dataspace, sql), has_get_one_row(false) {
  this->name = "MySQLQueryExistsNode";
#ifdef DEBUG
  ACE_ASSERT(plan->get_sub_query_result_node());
#endif
  res_node = plan->get_sub_query_result_node();
  res_node->replace_sql = string("select 1 from dual where 1=0");
  is_direct_node = false;
}
void MySQLQueryExistsNode::handle_send_client_packet(Packet *packet,
                                                     bool is_row_packet) {
  ACE_UNUSED_ARG(packet);
  if (is_row_packet) {
    if (!has_get_one_row) {
      res_node->replace_sql = string("select 1 from dual where 1=1");
      has_get_one_row = true;
    }
  }
}

/* class MySQLSendToDBScaleNode */
MySQLSendToDBScaleNode::MySQLSendToDBScaleNode(ExecutePlan *plan)
    : MySQLSendNode(plan) {
  node_can_swap = false;
  this->name = "MySQLSendToDBScaleNode";
}

void MySQLSendToDBScaleNode::send_row(MySQLExecuteNode *ready_child) {
  ACE_UNUSED_ARG(ready_child);
  throw NotImplementedError();
}

/* class MySQLSendMulColumnToDBScaleNode*/
MySQLSendMulColumnToDBScaleNode::MySQLSendMulColumnToDBScaleNode(
    ExecutePlan *plan)
    : MySQLSendToDBScaleNode(plan) {
  this->name = "MySQLSendMulColumnToDBScaleNode";
  col_nums = 0;
  res_node = plan->get_sub_query_result_node();
  res_node->replace_sql = string("select 1 from dual where 1=0");
  has_get_one_row = false;
}

void MySQLSendMulColumnToDBScaleNode::send_row(MySQLExecuteNode *ready_child) {
  Packet *row;
  if (row_map[ready_child]->size() > 1 || row_num != 0)
    throw dbscale::sql::SQLError(dbscale_err_msg[ERROR_ONE_ROW_CODE], "21000",
                                 ERROR_ONE_ROW_CODE);
  if (!has_get_one_row && row_map[ready_child]->size() > 0) {
    res_node->replace_sql.clear();
    res_node->row_columns.clear();
    has_get_one_row = true;
  }
  while (!row_map[ready_child]->empty()) {
    row = row_map[ready_child]->front();
    handle_one_row(row);
    row_map[ready_child]->pop_front();
    delete row;
    row_num++;
  }
}

void MySQLSendMulColumnToDBScaleNode::send_header() {
  list<Packet *> *field_packets = get_field_packets();

  list<Packet *>::iterator it = field_packets->begin();
  for (; it != field_packets->end(); it++) {
    MySQLColumnResponse col_resp(*it);
    col_resp.unpack();
    MySQLColumnType col_type = col_resp.get_column_type();
    col_types.push_back(col_type);
  }
  col_nums = field_packets->size();
}

void MySQLSendMulColumnToDBScaleNode::handle_one_row(Packet *packet) {
  MySQLRowResponse row(packet);

  unsigned int i = 0;
  for (; i < col_nums; i++) {
    ResultType res = get_result_type_from_column_type(col_types[i]);
    string one_column;
    if (res == RESULT_TYPE_STRING) one_column.append("'");
    if (!row.field_is_null(i)) {
      uint64_t str_len = 0;
      const char *column_tmp = row.get_str(i, &str_len);
      CharsetType ctype = session->get_client_charset_type();
      append_column_value_to_replace_sql(one_column, column_tmp, str_len, res,
                                         ctype);
    } else
      one_column.append("NULL");
    if (res == RESULT_TYPE_STRING) one_column.append("'");
    res_node->row_columns.push_back(one_column);
  }
}

/* MySQLSendOneColumnToDBScaleNode */
MySQLSendOneColumnToDBScaleNode::MySQLSendOneColumnToDBScaleNode(
    ExecutePlan *plan, bool only_one_row)
    : MySQLSendToDBScaleNode(plan),
      col_type(MYSQL_TYPE_END),
      only_one_row(only_one_row),
      has_get_one_row(false) {
  this->name = "MySQLSendOneColumnToDBScaleNode";
#ifdef DEBUG
  ACE_ASSERT(plan->get_sub_query_result_node());
#endif
  res_node = plan->get_sub_query_result_node();
  if (replace_empty_result_null)
    res_node->replace_sql = string("NULL");
  else
    res_node->replace_sql = string("select 1 from dual where 1=0");
}

void MySQLSendOneColumnToDBScaleNode::send_header() {
  list<Packet *> *field_packets = get_field_packets();
  if (field_packets->size() != 1) {
    throw dbscale::sql::SQLError(dbscale_err_msg[ERROR_ONE_COLUMN_CODE],
                                 "21000", ERROR_ONE_COLUMN_CODE);
  }
  MySQLColumnResponse col_resp(field_packets->front());
  col_resp.unpack();
  col_type = col_resp.get_column_type();
}

void MySQLSendOneColumnToDBScaleNode::send_row(MySQLExecuteNode *ready_child) {
  Packet *row;
  if (only_one_row && (row_map[ready_child]->size() > 1 || row_num != 0))
    throw dbscale::sql::SQLError(dbscale_err_msg[ERROR_ONE_ROW_CODE], "21000",
                                 ERROR_ONE_ROW_CODE);
  if (!has_get_one_row && row_map[ready_child]->size() > 0) {
    res_node->replace_sql.clear();
    has_get_one_row = true;
  }
  while (!row_map[ready_child]->empty()) {
    row = row_map[ready_child]->front();
    handle_one_row(row);
    row_map[ready_child]->pop_front();
    delete row;
    row_num++;
  }
}

void MySQLSendOneColumnToDBScaleNode::handle_one_row(Packet *packet) {
  MySQLRowResponse row(packet);

  ResultType res = get_result_type_from_column_type(col_type);
  string &replace_sql = res_node->replace_sql;

  if (row_num) replace_sql.append(",");

  if (res == RESULT_TYPE_STRING) replace_sql.append("'");

  if (!row.field_is_null(0)) {
    uint64_t str_len = 0;
    const char *column_tmp = row.get_str(0, &str_len);
    CharsetType ctype = session->get_client_charset_type();
    append_column_value_to_replace_sql(replace_sql, column_tmp, str_len, res,
                                       ctype);
  } else
    replace_sql.append("NULL");

  if (res == RESULT_TYPE_STRING) replace_sql.append("'");
}

/* class MySQLSendOneColumnAggrToDBScaleNode */
MySQLSendOneColumnAggrToDBScaleNode::MySQLSendOneColumnAggrToDBScaleNode(
    ExecutePlan *plan, bool get_min)
    : MySQLSendToDBScaleNode(plan),
      col_type(MYSQL_TYPE_END),
      get_min(get_min ? 1 : -1),
      has_get_one_row(false) {
  this->name = "MySQLSendOneColumnAggrToDBScaleNode";
#ifdef DEBUG
  ACE_ASSERT(plan->get_sub_query_result_node());
#endif
  res_node = plan->get_sub_query_result_node();
  res_node->replace_sql = string("select 1 from dual where 1=0");
}

void MySQLSendOneColumnAggrToDBScaleNode::send_eof() {
  if (!has_get_one_row) return;
  string &replace_sql = res_node->replace_sql;
  replace_sql.append("select ");

  ResultType res = get_result_type_from_column_type(col_type);

  if (res == RESULT_TYPE_STRING) replace_sql.append("'");

  MySQLRowResponse row(&aggr_packet);
  if (!row.field_is_null(0)) {
    uint64_t str_len = 0;
    const char *column_tmp = row.get_str(0, &str_len);
    CharsetType ctype = session->get_client_charset_type();
    append_column_value_to_replace_sql(replace_sql, column_tmp, str_len, res,
                                       ctype);
  } else
    replace_sql.append("NULL");

  if (res == RESULT_TYPE_STRING) replace_sql.append("'");
}

void MySQLSendOneColumnAggrToDBScaleNode::send_header() {
  list<Packet *> *field_packets = get_field_packets();
  if (field_packets->size() != 1) {
    throw dbscale::sql::SQLError(dbscale_err_msg[ERROR_ONE_COLUMN_CODE],
                                 "21000", ERROR_ONE_COLUMN_CODE);
  }
  MySQLColumnResponse col_resp(field_packets->front());
  col_resp.unpack();
  col_type = col_resp.get_column_type();
}

void MySQLSendOneColumnAggrToDBScaleNode::send_row(
    MySQLExecuteNode *ready_child) {
  if (!has_get_one_row && row_map[ready_child]->size() > 0) {
    res_node->replace_sql.clear();
    has_get_one_row = true;
  }
  Packet *row;
  if (!row_map[ready_child]->empty()) {
    row = row_map[ready_child]->front();
    handle_one_row(row);
    row_map[ready_child]->pop_front();
    delete row;
    row_num++;
  }
}

void MySQLSendOneColumnAggrToDBScaleNode::handle_one_row(Packet *row) {
  if (!row_num) {
    MySQLRowResponse row_p(row);
    uint64_t row_len = row->length();
    reset_packet_size(&aggr_packet, row_len + 9);
    row_p.pack(&aggr_packet);
  } else {
    MySQLRowResponse row1(&aggr_packet);
    MySQLRowResponse row2(row);
    int ret = MySQLColumnCompare::compare(&row1, &row2, 0, col_type,
                                          CHARSET_TYPE_OTHER, true);
    if (ret * get_min == 1) {
      uint64_t row_len = row->length();
      reset_packet_size(&aggr_packet, row_len + 9);
      row2.pack(&aggr_packet);
    }
  }
}

/* MySQLSendExistsToDBScaleNode */
MySQLSendExistsToDBScaleNode::MySQLSendExistsToDBScaleNode(ExecutePlan *plan)
    : MySQLSendToDBScaleNode(plan) {
  this->name = "MySQLSendExistsToDBScaleNode";
#ifdef DEBUG
  ACE_ASSERT(plan->get_sub_query_result_node());
#endif
  res_node = plan->get_sub_query_result_node();
  res_node->replace_sql = string("select 1 from dual where 1=0");
  set_res_node = false;
}

void MySQLSendExistsToDBScaleNode::send_row(MySQLExecuteNode *ready_child) {
  Packet *row;
  while (!row_map[ready_child]->empty()) {
    row = row_map[ready_child]->front();
    if (!set_res_node) {
      res_node->replace_sql = string("select 1 from dual where 1=1");
      set_res_node = true;
    }
    row_map[ready_child]->pop_front();
    delete row;
    row_num++;
  }
}

/* class MySQLIntoOutfileNode */
MySQLIntoOutfileNode::MySQLIntoOutfileNode(ExecutePlan *plan)
    : MySQLSendNode(plan) {
  this->name = "MySQLIntoOutfileNode";
  affect_rows = 0;
  into_outfile_item *into_outfile = plan->statement->get_into_outfile();
  is_local = into_outfile->is_local;
  filename = into_outfile->filename;

  node_can_swap = false;
  row_buffer = new char[DEFAULT_BUFFER_SIZE];
}

void MySQLIntoOutfileNode::prepare_fifo_or_load() {
  if (plan->statement->insert_select_via_fifo()) {
    LOG_DEBUG("external INSERT SELECT via fifo start to open fifo\n");
    open_fifo(plan);
    LOG_DEBUG("external INSERT SELECT via fifo end to open fifo\n");
  } else if (plan->statement->is_load_insert_select()) {
    load_insert_select();
  } else {
    param.set_got_error(false);
    param.set_external_load_flag(UNDEFINE);
    pipe_fd = 0;
    if (!is_local) {
      file.open(filename);
    }
  }
}

MySQLIntoOutfileNode::~MySQLIntoOutfileNode() { delete[] row_buffer; }

void *monitor_worker_via_fifo(void *arg) {
  spawn_param *param = (spawn_param *)arg;
  if (param->fd) {
    ExternalLoadResultHandler exp_handler;
    try {
      exp_handler.handler_expect_result(param->fd, param);
      const char *inserted_rows = exp_handler.get_inserted_rows();
      param->insert_rows = atoll(inserted_rows);
    } catch (...) {
      param->set_got_error(true);
    }
    if (param->fd) {
      pclose(param->fd);
    }
  }
  return NULL;
}

void *load_insert_select_exec(void *arg) {
  MySQLIntoOutfileNode *node = (MySQLIntoOutfileNode *)arg;
  char fields_term =
      node->plan->session->get_session_option("load_insert_select_fields_term")
          .char_val[0];
  char lines_term =
      node->plan->session->get_session_option("load_insert_select_lines_term")
          .char_val[0];
  string load_sql("LOAD DATA INFILE '");
  load_sql.append(node->get_filename());
  load_sql.append("' INTO TABLE ");
  load_sql.append(node->plan->statement->get_insert_select_modify_schema());
  load_sql.append(".");
  load_sql.append(node->plan->statement->get_insert_select_modify_table());
  load_sql.append(" FIELDS TERMINATED BY '");
  load_sql.append(1, fields_term);
  load_sql.append("' ENCLOSED BY '\"' LINES TERMINATED BY '");
  load_sql.append(1, lines_term);
  load_sql.append("'");

  LOG_DEBUG("LOAD DATA for INSERT SELECT sql [%s]\n", load_sql.c_str());

  Statement *new_stmt = NULL;
  ExecutePlan *new_plan = NULL;
  try {
    Parser *parser = MySQLParser::instance();
    new_stmt = parser->parse(
        load_sql.c_str(),
        allow_dot_in_ident);  // use global value of allow_dot_in_ident
    new_stmt->set_default_schema(node->plan->statement->get_schema());
    new_stmt->set_load_insert_select(true);
    new_plan = (ExecutePlan *)node->handler->get_execute_plan(new_stmt);
    new_stmt->generate_execution_plan(new_plan);
    new_plan->execute();
  } catch (...) {
    LOG_ERROR("Load insert select error in load execution.\n");
    node->set_got_error();
  }
  if (new_stmt) {
    new_stmt->free_resource();
    delete new_stmt;
    new_stmt = NULL;
  }
  if (new_plan) {
    delete new_plan;
    new_plan = NULL;
  }
  return NULL;
}

void MySQLIntoOutfileNode::load_insert_select() {
  param.set_got_error(false);
  param.set_external_load_flag(UNDEFINE);
  pipe_fd = 0;
  int ret =
      ACE_Thread::spawn((ACE_THR_FUNC)load_insert_select_exec, this,
                        THR_JOINABLE | THR_SCHED_DEFAULT, &l_id, &l_handle);
  if (ret == -1) {
    LOG_ERROR("Error when execute load.\n");
    throw HandlerError("Error when execute load.");
  }
  file.open(filename);
}

void MySQLIntoOutfileNode::open_fifo(ExecutePlan *plan) {
  string insert_select_via_fifo_cmd;
  string script = plan->statement->get_local_load_script();
  if (script[0] != '/') {
    insert_select_via_fifo_cmd.append("./");
  }
  insert_select_via_fifo_cmd.append(script);
  insert_select_via_fifo_cmd.append(" ");
  insert_select_via_fifo_cmd.append(
      plan->statement->get_insert_select_modify_schema());
  insert_select_via_fifo_cmd.append(" ");
  insert_select_via_fifo_cmd.append(
      plan->statement->get_insert_select_modify_table());
  insert_select_via_fifo_cmd.append(" , \\\" ");
  insert_select_via_fifo_cmd.append(filename);
  LOG_DEBUG("full command [%s].\n", insert_select_via_fifo_cmd.c_str());
  fd = popen(insert_select_via_fifo_cmd.c_str(), "r");
  if (fd == NULL) {
    fprintf(stderr, "execute command failed");
    LOG_ERROR("Fail to execute command %s.\n",
              insert_select_via_fifo_cmd.c_str());
    throw HandlerError("Fail to execute command.");
  }

  param.set_got_error(false);
  param.insert_rows = 0;
  param.fd = fd;
  param.set_external_load_flag(UNDEFINE);
  int ret =
      ACE_Thread::spawn((ACE_THR_FUNC)monitor_worker_via_fifo, &param,
                        THR_JOINABLE | THR_SCHED_DEFAULT, &t_id, &t_handle);
  if (ret == -1) {
    LOG_ERROR("Error when execute command %s.\n",
              insert_select_via_fifo_cmd.c_str());
    throw HandlerError("Error when execute command.");
  }
  file.open(filename);
  while (param.get_external_load_flag() == UNDEFINE) {
    LOG_DEBUG("Waiting for external load script report load file status.\n");
    ACE_OS::sleep(1);
  }
  if (param.get_external_load_flag() == FILE_NOT_OPEN) {
    file.close();
    ACE_Thread::join(t_handle);
    throw ExecuteNodeError(
        "Failed to execute statement, fail to open file, check log for more "
        "information.");
  }
}

void MySQLIntoOutfileNode::send_header() {
  string catalog_name;
  string schema_name;
  string table_name;
  uint32_t column_len = 0;
  list<Packet *> *field_packets = get_field_packets();
  for (std::list<Packet *>::iterator it = field_packets->begin();
       it != field_packets->end(); it++) {
    MySQLColumnResponse column_response(*it);
    column_response.unpack();
    is_column_number.push_back(column_response.is_number());
    column_len += column_response.get_column_length();
    if (it == field_packets->begin()) {
      catalog_name = column_response.get_catalog();
      schema_name = column_response.get_schema();
      table_name = column_response.get_table();
    }
  }
  // rebuild a column, and send to client
  if (is_local) {
    Packet res_header_packet;
    MySQLResultSetHeaderResponse result_set_header(1, 0);
    result_set_header.pack(&res_header_packet);
    handler->send_mysql_packet_to_client_by_buffer(&res_header_packet);
    MySQLColumnResponse my_col(catalog_name.c_str(), schema_name.c_str(),
                               table_name.c_str(), table_name.c_str(),
                               "into-outfile-col", "into-outfile-col", 8,
                               column_len + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
    Packet col_packet;
    my_col.pack(&col_packet);
    handler->send_mysql_packet_to_client_by_buffer(&col_packet);
    Packet *eof_packet = get_eof_packet();
    handler->deal_autocommit_with_ok_eof_packet(eof_packet);
    handler->send_mysql_packet_to_client_by_buffer(eof_packet);
    handler->flush_net_buffer();
  }
}

void MySQLIntoOutfileNode::send_row(MySQLExecuteNode *ready_child) {
  into_outfile_item *into_outfile = plan->statement->get_into_outfile();
  check_lines_term_null(into_outfile);
  Packet *row;
  while (!row_map[ready_child]->empty()) {
    row = row_map[ready_child]->front();
    write_into_outfile(row, into_outfile, file, &param);
    affect_rows++;
    if (is_local) {
      handler->send_to_client(row);
    } else if (param.has_got_error() ||
               param.get_external_load_flag() == FINISH) {
      status = EXECUTE_STATUS_COMPLETE;
      file.close();
      if (plan->statement->insert_select_via_fifo()) {
        ACE_Thread::join(t_handle);
      }
      throw ExecuteNodeError(
          "Got error when execute statement, check log for more information.");
    }

    row_map[ready_child]->pop_front();
    delete row;
  }
}

void MySQLIntoOutfileNode::execute() {
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START:
        prepare_fifo_or_load();
        init_row_map();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        status = EXECUTE_STATUS_WAIT;

      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;

      case EXECUTE_STATUS_HANDLE:
#ifdef DEBUG
        node_start_timing();
#endif
        handle_children();
#ifdef DEBUG
        node_end_timing();
#endif
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_BEFORE_COMPLETE:
        if (send_header_flag) {
          send_header();
          send_header_flag = false;
        }
        status = EXECUTE_STATUS_COMPLETE;

      case EXECUTE_STATUS_COMPLETE: {
        if (!is_local) {
          file.close();
        }
        if (plan->statement->insert_select_via_fifo()) {
          LOG_DEBUG(
              "external INSERT SELECT via fifo start wait monitor thread\n");
          ACE_Thread::join(t_handle);
          LOG_DEBUG(
              "external INSERT SELECT via fifo end wait monitor thread\n");

          if (param.has_got_error()) {
            LOG_ERROR(
                "Got error when execute INSERT SELECT statement, %Q rows "
                "inserted.\n",
                param.insert_rows);
            throw ExecuteNodeError(
                "Got error when execute INSERT SELECT statement, "
                "please check log for more information.");
          } else {
            if (!session->is_call_store_procedure())
              send_ok_packet_to_client(handler, param.insert_rows, 0);
#ifdef DEBUG
            LOG_DEBUG(
                "MySQLIntoOutfileNode %@ for external INSERT SELECT cost %d "
                "ms\n",
                this, node_cost_time);
#endif
          }
        } else if (plan->statement->is_load_insert_select()) {
          LOG_DEBUG("Wait for load thread.\n");
          ACE_Thread::join(l_handle);
          LOG_DEBUG("End wait for load thread.\n");
          Packet *result_packet = session->get_result_packet();
          if (param.has_got_error() || !result_packet) {
            LOG_ERROR("Got error when execute INSERT SELECT statement.\n",
                      param.insert_rows);
            throw ExecuteNodeError(
                "Got error when execute INSERT SELECT statement, "
                "please check log for more information.");
          }
          handler->send_to_client(result_packet);
          delete result_packet;
          session->set_result_packet(NULL);
        } else if (!is_local) {
          if (!session->is_call_store_procedure())
            send_ok_packet_to_client(handler, affect_rows, 0);
#ifdef DEBUG
          LOG_DEBUG("MySQLIntoOutfileNode %@ cost %d ms\n", this,
                    node_cost_time);
#endif
        } else {
          send_eof();
          flush_net_buffer();
        }
        break;
      }
      default:
        break;
    }
  }
}

/* class MySQLSelectIntoNode */
MySQLSelectIntoNode::MySQLSelectIntoNode(ExecutePlan *plan)
    : MySQLSendNode(plan) {
  this->name = "MySQLSelectIntoNode";
  affect_rows = 0;
  select_into_item *select_into = plan->statement->get_select_into();
  generate_select_into_vec(select_into);
  node_can_swap = false;
}

void MySQLSelectIntoNode::generate_select_into_vec(
    select_into_item *select_into) {
  name_item *into_list = select_into->into_list;
  name_item *head = into_list;
  do {
    bool is_uservar = into_list->is_uservar;
    string name = into_list->name;
    if (is_uservar) {
      boost::to_upper(name);
    }
    pair<bool, string> is_uservar_pair(is_uservar, name);
    select_into_vec.push_back(is_uservar_pair);
    into_list = into_list->next;
  } while (into_list != head);
}

void MySQLSelectIntoNode::write_into_name_list(Packet *row) {
  Session *session = plan->session;
  MySQLRowResponse row_response(row);
  unsigned int fields_num = field_is_number_vec.size();

  for (unsigned int i = 0; i != fields_num; i++) {
    if (!select_into_vec[i].first) {
      continue;
    }

    string uservar_name = select_into_vec[i].second;
    if (row_response.field_is_null(i)) {
      write_result_to_uservar(session, uservar_name, NULL);
      continue;
    }

    uint64_t length;
    const char *str = row_response.get_str(i, &length);
    write_result_to_uservar(session, uservar_name, str, length,
                            field_is_number_vec[i]);
  }
}

void MySQLSelectIntoNode::send_header() {
  list<Packet *> *field_packets = get_field_packets();
  for (std::list<Packet *>::iterator it = field_packets->begin();
       it != field_packets->end(); it++) {
    MySQLColumnResponse column_response(*it);
    column_response.unpack();
    field_is_number_vec.push_back(column_response.is_number());
  }
}

void MySQLSelectIntoNode::send_row(MySQLExecuteNode *ready_child) {
  Packet *row;
  while (!row_map[ready_child]->empty()) {
    row = row_map[ready_child]->front();
    affect_rows++;
    row_map[ready_child]->pop_front();
    try {
      if (affect_rows > 1) {
        throw dbscale::sql::SQLError(
            dbscale_err_msg[ERROR_SELECT_INTO_ONE_ROW_CODE], "42000",
            ERROR_SELECT_INTO_ONE_ROW_CODE);
      } else if (field_is_number_vec.size() != select_into_vec.size()) {
        throw dbscale::sql::SQLError(
            dbscale_err_msg[ERROR_SELECT_INTO_DIFF_COLUMNS_CODE], "21000",
            ERROR_SELECT_INTO_DIFF_COLUMNS_CODE);
      } else {
        write_into_name_list(row);
      }
      if (statement->get_select_uservar_flag()) {
        set_select_uservar_by_result(
            row, field_is_number_vec,
            *(plan->statement->get_select_uservar_vec()),
            plan->statement->get_select_field_num(), plan->session);
      }
      delete row;
      row = NULL;
    } catch (dbscale::sql::SQLError &e) {
      delete row;
      row = NULL;
      throw e;
    }
  }
}

void MySQLSelectIntoNode::execute() {
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START:
        init_row_map();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        status = EXECUTE_STATUS_WAIT;

      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;

      case EXECUTE_STATUS_HANDLE:
#ifdef DEBUG
        node_start_timing();
#endif
        try {
          handle_children();
        } catch (...) {
          status = EXECUTE_STATUS_COMPLETE;
          throw;
        }
#ifdef DEBUG
        node_end_timing();
#endif
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_BEFORE_COMPLETE:
        if (send_header_flag) {
          send_header();
          send_header_flag = false;
        }
        status = EXECUTE_STATUS_COMPLETE;

      case EXECUTE_STATUS_COMPLETE:
        if (!session->is_call_store_procedure())
          send_ok_packet_to_client(handler, affect_rows, 0);
#ifdef DEBUG
        LOG_DEBUG("MySQLIntoOutfileNode %@ cost %d ms\n", this, node_cost_time);
#endif
        break;

      default:
        break;
    }
  }
}

void MySQLIntoOutfileNode::clean() {
  MySQLSendNode::clean();
  if (plan->statement->insert_select_via_fifo() ||
      plan->statement->is_load_insert_select()) {
    for (int i = 0; i < 3; i++) {  // delete fifo, try 3 times.
      if (!remove(filename)) {     // return 0, delete fifo successful.
        break;
      }
      if (i == 2) {  // try 3 times but failed.
        LOG_ERROR("Error deleting fifo file.\n");
      }
    }
  }
}

/* class MySQLSortNode */

MySQLSortNode::MySQLSortNode(ExecutePlan *plan, list<SortDesc> *sort_desc_para)
    : MySQLInnerNode(plan) {
  need_restrict_row_map_size = true;
  if (sort_desc_para) {
    list<SortDesc>::iterator it = sort_desc_para->begin();
    for (; it != sort_desc_para->end(); it++) sort_desc.push_back(*it);
  }
  this->name = "MySQLSortNode";
  column_inited_flag = false;
  status = EXECUTE_STATUS_START;
  adjust_column_flag = false;
  check_column_type = false;
  sort_node_size = 0;
  sort_ready_nodes = ready_rows;
  node_can_swap = session->is_may_backend_exec_swap_able();
  can_swap_ready_rows = false;

#ifndef DBSCALE_TEST_DISABLE
  dbscale_test_info *test_info = plan->session->get_dbscale_test_info();
  if (!strcasecmp(test_info->test_case_name.c_str(), "fetchnode") &&
      !strcasecmp(test_info->test_case_operation.c_str(),
                  "max_fetchnode_ready_rows_size")) {
    LOG_DEBUG(
        "Do dbscale test operation 'max_fetch_node_ready_rows_size' for case "
        "'fetchnode'\n");
    need_record_loop_count = true;
  }
#endif
}

/* Here we use Binary Inserting Sort*/
void MySQLSortNode::insert_row(MySQLExecuteNode *child) {
  Packet *packet = row_map[child]->front();

  int low = 0, high = sort_node_size - 1;
  int m = 0;
  while (low <= high) {
    m = (low + high) / 2;
    if (MySQLColumnCompare::compare(merge_sort_vec[m], packet, &sort_desc,
                                    &column_types) < 0) {
      low = m + 1;
    } else {
      high = m - 1;
    }
  }

  for (int j = sort_node_size; j > low; j--) {
    merge_sort_vec[j] = merge_sort_vec[j - 1];
    sort_pos[j] = sort_pos[j - 1];
  }

  merge_sort_vec[low] = packet;
  sort_pos[low] = child;
  sort_node_size++;
}

void MySQLSortNode::pop_row() {
  MySQLExecuteNode *node = sort_pos[sort_node_size - 1];
  Packet *packet = merge_sort_vec[sort_node_size - 1];
  sort_ready_nodes->push_back(packet);
  ready_rows_buffer_size += packet->total_capacity();
  ACE_ASSERT(packet != NULL);
#ifdef DEBUG
  LOG_DEBUG("packet = %@ in pop_row\n", packet);
#endif
  buffer_row_map_size[node] -= row_map[node]->front()->total_capacity();
  row_map[node]->pop_front();
  row_map_size[node]--;
  sort_node_size--;
}

unsigned int MySQLSortNode::get_last_rows() {
  unsigned int rows = 0;
  list<MySQLExecuteNode *>::iterator it_child;
  for (it_child = children.begin(); it_child != children.end(); ++it_child) {
    rows += row_map[*it_child]->size();
  }

  return rows;
}

void MySQLSortNode::add_last_one_list() {
  MySQLExecuteNode *node = NULL;
  list<Packet *, StaticAllocator<Packet *> > *row_list = NULL;
  Packet *packet = NULL;
  node = sort_pos[0];
  row_list = row_map[node];
  while (!row_list->empty()) {
    packet = row_list->front();
    sort_ready_nodes->push_back(packet);
    ready_rows_buffer_size += packet->total_capacity();
    buffer_row_map_size[node] -= packet->total_capacity();
    row_list->pop_front();
    row_map_size[node]--;
  }
}

void MySQLSortNode::sort() {
  sort_node_size = 0;
  list<MySQLExecuteNode *>::iterator it_child;
  for (it_child = children.begin(); it_child != children.end(); ++it_child) {
    if (!row_map[*it_child]->empty()) {
      insert_row(*it_child);
    }
  }
  pop_row();

  map<MySQLExecuteNode *, list<Packet *>::iterator>::iterator it;
  while (true) {
    if (!row_map[sort_pos[sort_node_size]]->empty()) {
      insert_row(sort_pos[sort_node_size]);
      pop_row();
    } else {
      return;
    }
  }
}

void MySQLSortNode::last_sort() {
  if (get_last_rows() == 0) {
    return;
  }

  sort_node_size = 0;
  list<MySQLExecuteNode *>::iterator it_child;
  for (it_child = children.begin(); it_child != children.end(); ++it_child) {
    if (!row_map[*it_child]->empty()) {
      insert_row(*it_child);
    }
  }
  pop_row();

  if (sort_node_size == 0) {
    add_last_one_list();
    return;
  }

  map<MySQLExecuteNode *, list<Packet *>::iterator>::iterator it;
  while (true) {
    if (sort_node_size == 1 && row_map[sort_pos[sort_node_size]]->empty()) {
      add_last_one_list();
      return;
    } else if (!row_map[sort_pos[sort_node_size]]->empty()) {
      insert_row(sort_pos[sort_node_size]);
      pop_row();
    } else {
      pop_row();
    }
  }
}

void MySQLSortNode::init_merge_sort_variables() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    merge_sort_vec.push_back(NULL);
    sort_pos.push_back(NULL);
  }
}

void MySQLSortNode::execute() {
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START:
        init_row_map();
        init_merge_sort_variables();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        if (node_can_swap && plan->plan_can_swap()) {
          handle_swap_connections();
          return;
        }
        status = EXECUTE_STATUS_WAIT;

      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;

      case EXECUTE_STATUS_HANDLE: {
#ifdef DEBUG
        node_start_timing();
#endif
        init_column_types(get_field_packets(), column_types, column_num,
                          &column_inited_flag);
        adjust_column_index();
        check_column_valid();
        sort();
#ifdef DEBUG
        node_end_timing();
#endif
        status = EXECUTE_STATUS_FETCH_DATA;

        LOG_DEBUG("sortnode sorted ready buffer size is %Q,count is %u.\n",
                  ready_rows_buffer_size, sort_ready_nodes->size());
        unsigned long size_of_packets =
            sort_ready_nodes->size() * (sizeof(Packet));
        unsigned long size_of_ace_data_blocks =
            sort_ready_nodes->size() * (sizeof(ACE_Data_Block));
        unsigned long size_of_packet_pointers =
            sort_ready_nodes->size() * (sizeof(Packet *));
        unsigned long total_list_use_mem =
            (size_of_packets + size_of_ace_data_blocks +
             size_of_packet_pointers) /
            1024;
        unsigned long left_max_sorted_rows_buffer = 0;
        if (sort_rows_size > total_list_use_mem) {
          left_max_sorted_rows_buffer = sort_rows_size - total_list_use_mem;
        }

        if ((ready_rows_buffer_size / 1024) < left_max_sorted_rows_buffer) {
          LOG_DEBUG(
              "ready_rows_buffer_size is %Q ,left_max_sorted_rows_buffer is "
              "%Q.\n",
              ready_rows_buffer_size, left_max_sorted_rows_buffer);
          break;
        } else
          return;
      }
      case EXECUTE_STATUS_BEFORE_COMPLETE:
#ifdef DEBUG
        node_start_timing();
#endif
        last_sort();
#ifdef DEBUG
        node_end_timing();
#endif
        status = EXECUTE_STATUS_COMPLETE;
        break;

      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        LOG_DEBUG("MySQLSortNode %@ cost %d ms\n", this, node_cost_time);
#endif
        break;

      default:
        ACE_ASSERT(0);
        break;
    }
  }
}

void MySQLSortNode::adjust_column_index() {
  if (!adjust_column_flag) {
    list<SortDesc>::iterator it;
    for (it = sort_desc.begin(); it != sort_desc.end(); it++) {
      if (it->column_index < 0) {
        it->column_index += column_num;
      }
    }
    adjust_column_flag = true;
  }
}

void MySQLSortNode::check_column_valid() {
  if (!check_column_type) {
    list<SortDesc>::iterator it;
    for (it = sort_desc.begin(); it != sort_desc.end(); it++) {
      if (!field_is_valid_for_compare(column_types[it->column_index])) {
        LOG_ERROR("Unsupport column type %d for compare.\n",
                  column_types[it->column_index]);
        throw ExecuteNodeError(
            "Unsupport column type for compare,\
                               please check the log for detail.");
      }
    }
    check_column_type = true;
  }
}

/* class MySQLProjectNode */

MySQLProjectNode::MySQLProjectNode(ExecutePlan *plan, unsigned skip_columns)
    : MySQLInnerNode(plan) {
  this->skip_columns = skip_columns;
  this->name = "MySQLProjectNode";
  field_packets_inited = false;
  header_packet_inited = false;
  columns_num = 0;
  node_can_swap = session->is_may_backend_exec_swap_able();
}

void MySQLProjectNode::init_header_packet() {
  first_child = children.front();
  MySQLResultSetHeaderResponse header(first_child->get_header_packet());
  header.unpack();
  uint64_t columns = header.get_columns();
  columns_num = columns - skip_columns;
  header.set_columns(columns_num);
  header.pack(&header_packet);
  header_packet_inited = true;
}

void MySQLProjectNode::init_field_packet() {
  unsigned int i = 0;
  field_packets.clear();
  first_child = children.front();
  list<Packet *> *field_list = first_child->get_field_packets();
  list<Packet *>::iterator it = field_list->begin();

  columns_num = field_list->size() - skip_columns;
  while (i++ < columns_num) {
    field_packets.push_back(*it);
    it++;
  }
  field_packets_inited = true;
}

void MySQLProjectNode::handle_child(MySQLExecuteNode *child) {
  Packet *row_packet;
  Packet *new_row;
  while (!row_map[child]->empty()) {
    row_packet = row_map[child]->front();
    row_map[child]->pop_front();

    MySQLRowResponse row(row_packet);
    row.set_current(columns_num - 1);
    row.move_next();
    char *start_pos = row.get_row_data();
    char *end_pos = row.get_current_data();
    uint64_t row_length = end_pos - start_pos;
    row.set_row_data(start_pos);
    row.set_row_length(row_length);
    new_row = Backend::instance()->get_new_packet(row_packet_size);
    row.pack(new_row);
    ready_rows->push_back(new_row);
    delete row_packet;
  }
}

void MySQLProjectNode::handle_children() {
  LOG_DEBUG("Node %@ start handle children.\n", this);
  init_columns_num();
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if (!row_map[*it]->empty()) handle_child(*it);
  }
  LOG_DEBUG("Node %@ handle children complete.\n", this);
}

void MySQLProjectNode::execute() {
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START:
        init_row_map();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        if (node_can_swap && plan->plan_can_swap()) {
          handle_swap_connections();
          return;
        }
        status = EXECUTE_STATUS_WAIT;

      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;

      case EXECUTE_STATUS_HANDLE:
#ifdef DEBUG
        node_start_timing();
#endif
        handle_children();
#ifdef DEBUG
        node_end_timing();
#endif
        status = EXECUTE_STATUS_FETCH_DATA;
        return;
        break;

      case EXECUTE_STATUS_BEFORE_COMPLETE:
        status = EXECUTE_STATUS_COMPLETE;
      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        LOG_DEBUG("MySQLProjectNode %@ cost %d ms\n", this, node_cost_time);
#endif
        break;

      default:
        ACE_ASSERT(0);
        break;
    }
  }
}

/* class MySQLLimitNode */

MySQLLimitNode::MySQLLimitNode(ExecutePlan *plan, long offset, long num)
    : MySQLInnerNode(plan), offset(offset), num(num), row_count(0) {
  this->name = "MySQLLimitNode";
  node_can_swap = session->is_may_backend_exec_swap_able();
}

int MySQLLimitNode::handle_child(MySQLExecuteNode *child) {
  Packet *packet = NULL;
  while (!row_map[child]->empty()) {
    packet = row_map[child]->front();
    row_map[child]->pop_front();
    if (row_count < offset) {
      delete packet;
      row_count++;
      continue;
    }
    if (row_count >= offset + num) {
      delete packet;
      return 1;
    }
    ready_rows->push_back(packet);
    row_count++;
  }
  return 0;
}

void MySQLLimitNode::handle_children() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if (!row_map[*it]->empty())
      if (handle_child(*it)) break;
  }
  LOG_DEBUG(
      "Limit node %@, row_count %d, ready_rows %d"
      "after handle_children.\n",
      this, row_count, ready_rows->size());
}

void MySQLLimitNode::execute() {
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START:
        init_row_map();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        if (node_can_swap && plan->plan_can_swap()) {
          handle_swap_connections();
          return;
        }
        status = EXECUTE_STATUS_WAIT;

      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;

      case EXECUTE_STATUS_HANDLE:
#ifdef DEBUG
        node_start_timing();
#endif
        if (row_count < offset + num) handle_children();
        if (ready_rows->empty() &&
            (all_children_finished || row_count >= offset + num)) {
          status = EXECUTE_STATUS_COMPLETE;
        } else if (row_count >= offset + num) {
          status = EXECUTE_STATUS_HANDLE;
        } else {
          status = EXECUTE_STATUS_FETCH_DATA;
        }
#ifdef DEBUG
        node_end_timing();
#endif
        if (!ready_rows->empty()) return;
        break;
      case EXECUTE_STATUS_BEFORE_COMPLETE:
        status = EXECUTE_STATUS_COMPLETE;
      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        LOG_DEBUG("MySQLLimitNode %@ cost %d ms\n", this, node_cost_time);
#endif
        break;

      default:
        ACE_ASSERT(0);
        break;
    }
  }
}

/* class MySQLKillNode */

MySQLKillNode::MySQLKillNode(ExecutePlan *plan, int cluster_id, uint32_t kid)
    : MySQLExecuteNode(plan), kid(kid), cluster_id(cluster_id) {
  this->name = "MySQLKillNode";
}

void MySQLKillNode::execute() {
#ifdef DEBUG
  ACE_ASSERT(kid != handler->get_session()->get_thread_id());
#endif
  Session *h = NULL;
#ifndef CLOSE_MULTIPLE
  Backend *backend = Backend::instance();
  if (multiple_mode && cluster_id != ERROR_CLUSTER_ID &&
      cluster_id != backend->get_cluster_id()) {
    MultipleManager *mul = MultipleManager::instance();
    if (mul->get_is_cluster_master()) {
      string kill_sql("KILL ");
      char s_cluster_id[20];
      sprintf(s_cluster_id, "%d", cluster_id);
      kill_sql.append(s_cluster_id);
      kill_sql.append(" ");
      char s_kid[20];
      sprintf(s_kid, "%d", kid);
      kill_sql.append(s_kid);
      mul->execute_master_query(kill_sql.c_str(), cluster_id);
    }
    Packet ok_packet;
    MySQLOKResponse ok(0, 0);
    ok.pack(&ok_packet);
    handler->deal_autocommit_with_ok_eof_packet(&ok_packet);
    handler->record_affected_rows(&ok_packet);
    if (!plan->session->is_call_store_procedure()) {
      handler->send_to_client(&ok_packet);
    }

    status = EXECUTE_STATUS_COMPLETE;
    return;
  }
#endif
  try {
    h = driver->get_session_by_thread_id_for_kill(kid);
    if (!h) {
      throw FindKillThreadFail();
    }
    if (h->is_executing_spark()) {
#ifndef DBSCALE_DISABLE_SPARK
      string job_id = h->get_cur_spark_job_id();
      SparkKillParameter spark_kill_param;
      spark_kill_param.job_id = job_id;
      SparkReturnValue return_value = kill_spark_service(spark_kill_param);
      if (!return_value.success) {
        throw Error(return_value.error_string.c_str());
      }
#endif
    } else {
      h->enable_prepare_kill();
      if (h->get_transfer_rule_manager()) {
        handle_federated_situation(h);
      }
      driver->prepare_session_kill(h);
      h->killall_using_conns();
      driver->finish_session_kill(h);
    }
  } catch (FindKillThreadFail &e) {
    LOG_WARN("Fail to find the kill thread %d.\n", kid);
    status = EXECUTE_STATUS_COMPLETE;
    throw ExecuteNodeError("Fail to find kill thread id.");
  } catch (ThreadKillFailed &e) {
    LOG_ERROR("Fail to kill thread %d, due to %s.\n", kid, e.what());
    status = EXECUTE_STATUS_COMPLETE;
    string message("Fail to kill thread due to ");
    message += e.what();
    throw ExecuteNodeError(message.c_str());
  } catch (Exception &e) {
    LOG_ERROR("Got unexpect expection for kill node.\n");
    status = EXECUTE_STATUS_COMPLETE;
    throw ExecuteNodeError(e.what());
  }

  Packet ok_packet;
  MySQLOKResponse ok(0, 0);
  ok.pack(&ok_packet);
  handler->deal_autocommit_with_ok_eof_packet(&ok_packet);
  handler->record_affected_rows(&ok_packet);
  if (!plan->session->is_call_store_procedure()) {
    handler->send_to_client(&ok_packet);
  }

  status = EXECUTE_STATUS_COMPLETE;
}

bool MySQLKillNode::handle_federated_situation(Session *h) {
  bool ret = false;
  while (!ret) {
    LOG_DEBUG("Waiting for federated session finished.\n");
    bool session_available = driver->check_session_available_aquire(h);
    if (session_available) {
      if (h->federated_session_finished())
        ret = true;
      else {
        int killed_session_count = 0;
        try {
          killed_session_count =
              h->get_transfer_rule_manager()->killall_transfer_rule_sessions();
        } catch (...) {
          driver->release_session_mutex();
          throw;
        }
        h->set_federated_num(killed_session_count);
      }
      driver->release_session_mutex();
    } else {
      ret = true;
    }
    if (h->federated_session_finished()) break;
    ACE_OS::sleep(2);
  }
  return ret;
}

/* class MySQLOKNode */

MySQLOKNode::MySQLOKNode(ExecutePlan *plan) : MySQLInnerNode(plan) {
  this->name = "MySQLOKNode";
}

void MySQLOKNode::execute() {
  init_row_map();
  MySQLExecuteNode *node = children.front();
  node->execute();
  node->notify_parent();
  handler->deal_with_metadata_execute(
      plan->statement->get_stmt_node()->type, plan->statement->get_sql(),
      session->get_schema(), plan->statement->get_stmt_node());

  if (!plan->session->is_call_store_procedure() &&
      !plan->statement->is_cross_node_join() &&
      !plan->statement->is_union_table_sub() &&
      !plan->session->get_is_silence_ok_stmt()) {
    Packet *packet = row_map[node]->front();
    handler->deal_autocommit_with_ok_eof_packet(packet);
    handler->record_affected_rows(packet);
    if (!plan->get_migrate_tool()) {
      if (plan->session->get_has_more_result()) {
        rebuild_ok_with_has_more_flag(packet, driver);
        LOG_DEBUG(
            "For multiple stmt, the ok packet of middle stmt should be with"
            "flag has_more_result.\n");
      }
      handler->send_to_client(packet);
    }
    LOG_DEBUG("OK Node send packet %@ to client.\n", packet);
  } else
    LOG_DEBUG(
        "In store procedure call or cross node join or union table subquery, "
        "so skip the sending"
        " of ok packet in ok node with call [%d] cross join [%d] union [%d] "
        "sliense [%d].\n",
        plan->session->is_call_store_procedure() ? 1 : 0,
        plan->statement->is_cross_node_join() ? 1 : 0,
        plan->statement->is_union_table_sub() ? 1 : 0,
        plan->session->get_is_silence_ok_stmt() ? 1 : 0);
  status = EXECUTE_STATUS_COMPLETE;
}

/* class MySQLSlaveDBScaleErrorNode */
void MySQLSlaveDBScaleErrorNode::execute() {
  try {
    Backend *backend = Backend::instance();
    if (backend->get_need_send_stop_slave_flag()) {
      LOG_DEBUG("slave dbscale server need stop slave\n");
      DataSpace *catalog = backend->get_catalog();
      catalog->execute_one_modify_sql("stop slave");
      backend->set_need_send_stop_slave_flag(false);
    }

    LOG_DEBUG(
        "this is a dbscale server and 'slave-dbscale-mode' is off,so it is "
        "denied to be a slave\n");
    Packet error_packet;
    MySQLErrorResponse error(
        9001, "'slave-dbscale-mode' is off, dbscale is denied to be a slave",
        "", 0);
    error.pack(&error_packet);
    handler->send_to_client(&error_packet);
  } catch (exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    LOG_ERROR("Execute Node fail in MySQLSlaveDBScaleErrorNode due to [%s].\n",
              e.what());
    throw;
  }
  status = EXECUTE_STATUS_COMPLETE;
}

/* class MySQLExprCalculateNode */
MySQLExprCalculateNode::MySQLExprCalculateNode(
    ExecutePlan *plan, list<SelectExprDesc> &select_expr_list)
    : MySQLInnerPipeNode(plan) {
  this->name = "MySQLExprCalculateNode";
  this->expr_list = select_expr_list;
  inited_expr_list = false;
  column_num = 0;
}

void MySQLExprCalculateNode::handle_child(MySQLExecuteNode *child) {
  Packet *row;
  init_expr_list();
  while (!row_map[child]->empty()) {
    row = row_map[child]->front();
    row_map[child]->pop_front();
    row = reset_packet_value(row);
    ready_rows->push_back(row);
  }
}
void MySQLExprCalculateNode::calculate_expr(SelectExprDesc *desc) {
  desc->is_null = false;
  desc->column_data.clear();
  ExpressionValue expr_value;
  desc->expr->get_expression_value(&expr_value);
  switch (expr_value.value_type) {
    case RESULT_TYPE_NULL: {
      desc->is_null = true;
    } break;
    case RESULT_TYPE_NUM: {
      char column[GMP_N_DIGITS + 5];
      char formator[10];
      sprintf(formator, "%%.%dFf", expr_value.gmp_value.deci_num);
      int len = gmp_sprintf(column, formator, expr_value.gmp_value.value);
      column[len] = '\0';
      desc->column_data.append(column);
    } break;
    case RESULT_TYPE_BOOL: {
      if (expr_value.bool_value)
        desc->column_data.append("1");
      else
        desc->column_data.append("0");
    } break;
    case RESULT_TYPE_STRING: {
      desc->column_data.append(expr_value.str_value);
    } break;
    default:
      LOG_ERROR("Unsupport result type for MySQLExprCalculateNode.\n");
      throw ExecuteNodeError(
          "Unsupport result type for MySQLExprCalculateNode.");
  }
}
Packet *MySQLExprCalculateNode::reset_packet_value(Packet *row) {
  MySQLRowResponse row_res(row);
  int column_len = 0;
  char *column_data = NULL;
  Packet *new_row = Backend::instance()->get_new_packet(row_packet_size);
  new_row->wr_ptr(new_row->base() + PACKET_HEADER_SIZE);

  list<SelectExprDesc>::iterator it;
  list<FieldExpression *>::iterator fe_it;
  for (fe_it = field_expr_list.begin(); fe_it != field_expr_list.end();
       fe_it++) {
    (*fe_it)->set_row(row);
  }

  unsigned int need_len = 1;  // for terminate '\0'
  it = expr_list.begin();
  fe_it = field_expr_list.begin();
  for (unsigned int i = 0; i < column_num; i++) {
    row_res.set_current(i);
    if (it != expr_list.end() && it->index == i) {
      need_len = need_len + 9 + it->column_data.size();
      it++;
    } else {
      need_len = need_len + 9 + row_res.get_current_length();
    }
    if (fe_it != field_expr_list.end() &&
        (unsigned int)((*fe_it)->column_index) == i) {
      (*fe_it)->is_null = row_res.field_is_null();
    }
    row_res.move_next();
  }
  for (it = expr_list.begin(); it != expr_list.end(); it++) {
    calculate_expr(&(*it));
  }
  unsigned int remain_len =
      new_row->size() - (new_row->wr_ptr() - new_row->base());
  if (remain_len < need_len) {
    uint64_t resize_len = need_len + new_row->wr_ptr() - new_row->base();
    LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
              new_row->size(), resize_len);
    new_row->size(resize_len);
  }
  it = expr_list.begin();
  for (unsigned int i = 0; i < column_num; i++) {
    row_res.set_current(i);
    if (it != expr_list.end() && it->index == i) {
      if (it->is_null) {
        new_row->packchar(
            '\373');  //'\373' represents that the field value is NULL
      } else {
        new_row->pack_lenenc_int(it->column_data.size());
        new_row->packdata(it->column_data.c_str(), it->column_data.size());
      }
      it++;
    } else {
      if (row_res.field_is_null())
        new_row->packchar(
            '\373');  //'\373' represents that the field value is NULL
      else {
        column_len = row_res.get_current_length();
        column_data = row_res.get_current_data();
        new_row->pack_lenenc_int(column_len);
        new_row->packdata(column_data, column_len);
      }
    }
    row_res.move_next();
  }
  size_t load_length = new_row->length() - PACKET_HEADER_SIZE;
  pack_header(new_row, load_length);
  delete row;
  return new_row;
}
void get_expression_type(list<Packet *> *field_packets,
                         FieldExpression *expression, ResultType &result_type);

void init_expr_low(Expression *expr, STMT_EXPR_TYPE stmt_expr_type,
                   list<Packet *> *field_packets,
                   list<FieldExpression *> &field_expression_list) {
  expr_type expr_type = expr->type;
  switch (expr_type) {
    case EXPR_EQ:
    case EXPR_GR:
    case EXPR_LESS:
    case EXPR_GE:
    case EXPR_LESSE:
    case EXPR_NE:
    case EXPR_EQ_N:
    case EXPR_AND:
    case EXPR_OR:
    case EXPR_XOR:
    case EXPR_ELSE: {
      BoolBinaryExpression *bi_expr = (BoolBinaryExpression *)expr;
      init_expr_low(bi_expr->left, stmt_expr_type, field_packets,
                    field_expression_list);
      init_expr_low(bi_expr->right, stmt_expr_type, field_packets,
                    field_expression_list);
    } break;

    case EXPR_IF_COND:
    case EXPR_WHEN_THEN: {
      ConditionExpression *cond_expr = (ConditionExpression *)expr;
      init_expr_low(cond_expr->left, stmt_expr_type, field_packets,
                    field_expression_list);
      init_expr_low(cond_expr->right, stmt_expr_type, field_packets,
                    field_expression_list);
    } break;

    case EXPR_MINUS: {
      ArithmeticUnaryExpression *un_expr = (ArithmeticUnaryExpression *)expr;
      init_expr_low(un_expr->right, stmt_expr_type, field_packets,
                    field_expression_list);
    } break;

    case EXPR_ADD:
    case EXPR_SUB:
    case EXPR_MUL:
    case EXPR_DIV: {
      ArithmeticBinaryExpression *bi_expr = (ArithmeticBinaryExpression *)expr;
      init_expr_low(bi_expr->left, stmt_expr_type, field_packets,
                    field_expression_list);
      init_expr_low(bi_expr->right, stmt_expr_type, field_packets,
                    field_expression_list);
    } break;
    case EXPR_IS:
    case EXPR_IS_NOT: {
      TruthExpression *t_expr = (TruthExpression *)expr;
      init_expr_low(t_expr->left, stmt_expr_type, field_packets,
                    field_expression_list);
      init_expr_low(t_expr->right, stmt_expr_type, field_packets,
                    field_expression_list);
    } break;

    case EXPR_IF:
    case EXPR_IFNULL:
    case EXPR_CASE: {
      if (stmt_expr_type == FIELD_EXPR) {
        TerCondExpression *ter_cond_expr = (TerCondExpression *)expr;
        expr_list_item *item = ter_cond_expr->list_expr->expr_list_head;
        do {
          init_expr_low(item->expr, stmt_expr_type, field_packets,
                        field_expression_list);
          item = item->next;
        } while (item != ter_cond_expr->list_expr->expr_list_head);
        if (ter_cond_expr->else_expr)
          init_expr_low(ter_cond_expr->else_expr, stmt_expr_type, field_packets,
                        field_expression_list);
      } else {
        throw NotImplementedError("Filter expression is too complex.");
      }
    } break;

    case EXPR_FIELD: {
      ResultType result_type;
      FieldExpression *field_expr = (FieldExpression *)expr;
      if (field_expr->column_index < 0) {
        field_expr->column_index += field_packets->size();
      }
      get_expression_type(field_packets, field_expr, result_type);
      field_expr->result_type = result_type;
      field_expression_list.push_back(field_expr);
    } break;

    case EXPR_INT:
    case EXPR_STRING:
    case EXPR_FLOAT:
    case EXPR_NULL:
      break;

    default:
      throw NotImplementedError("expression is too complex");
  }
}

void MySQLExprCalculateNode::init_expr_list() {
  if (!inited_expr_list) {
    list<SelectExprDesc>::iterator it;
    for (it = expr_list.begin(); it != expr_list.end(); it++) {
      init_expr_low(it->expr, FIELD_EXPR, get_field_packets(), field_expr_list);
    }
    column_num = get_field_packets()->size();
    inited_expr_list = true;
  }
}

/* class MySQLConnectByNode */
MySQLConnectByNode::MySQLConnectByNode(ExecutePlan *plan,
                                       ConnectByDesc connect_by_desc)
    : MySQLInnerPipeNode(plan) {
  this->name = "MySQLConnectByNode";
  inited_column_index = false;
  where_index = connect_by_desc.where_index;
  start_index = connect_by_desc.start_index;
  prior_index = connect_by_desc.prior_index;
  recur_index = connect_by_desc.recur_index;
  row_id = 0;
  max_result_num =
      session->get_session_option("max_connect_by_result_num").uint_val;
  loop_flag = new bitset<MAX_CONNECT_BY_CACHED_ROWS>();
  ignore_flag = new bitset<MAX_CONNECT_BY_CACHED_ROWS>();
}

void MySQLConnectByNode::do_clean() {
  unsigned int i = 0;
  // the clean may cost a long time,
  // witch may delay the recieving of next request packet from client.
  for (; i < row_id; ++i) {
    if (!loop_flag->test(i)) delete id_prior_packets[i].second;
  }
  if (loop_flag) delete loop_flag;
  if (ignore_flag) delete ignore_flag;
}

void MySQLConnectByNode::handle_discarded_packets() {
  if (loop_flag->all()) return;
  unsigned int i = 0;
  for (; i < row_id; ++i) {
    if (!loop_flag->test(i)) {
      loop_flag->set(i);
      delete id_prior_packets[i].second;
    }
  }
}

void MySQLConnectByNode::init_column_index() {
  list<Packet *> *field_packets = get_field_packets();
  unsigned int column_num = field_packets->size();

  if (where_index) where_index += column_num;
  start_index += column_num;
  prior_index += column_num;
  recur_index += column_num;

  if (start_index < 0 || prior_index < 0 || recur_index < 0 ||
      where_index < 0) {
    LOG_ERROR("Fail to init connect by column index.\n");
    throw Error("Fail to init connect by column index.");
  }

  inited_column_index = true;
}

void MySQLConnectByNode::handle_child(MySQLExecuteNode *child) {
  if (!inited_column_index) init_column_index();

  Packet *row = NULL;
  while (!row_map[child]->empty()) {
    if (row_id >= max_result_num || row_id >= MAX_CONNECT_BY_CACHED_ROWS) {
      LOG_ERROR("Reach the max connect by result cache number.\n");
      throw Error("Reach the max connect by result cache number.");
    }
    row = row_map[child]->front();
    row_map[child]->pop_front();
    MySQLRowResponse row_res(row);
    uint64_t row_len;
    bool is_null;

    int where_result = 1;
    if (where_index) {
      is_null = row_res.field_is_null(where_index);
      if (!is_null) where_result = row_res.get_int((unsigned int)where_index);
    }

    int start_flag = 0;
    is_null = row_res.field_is_null(start_index);
    if (!is_null) start_flag = row_res.get_int((unsigned int)start_index);

    string prior_value;
    is_null = row_res.field_is_null(prior_index);
    if (!is_null) {
      const char *prior_data =
          row_res.get_str((unsigned int)prior_index, &row_len);
      prior_value.assign(prior_data, row_len);
    }

    string recur_value;
    is_null = row_res.field_is_null(recur_index);
    if (!is_null) {
      const char *recur_data =
          row_res.get_str((unsigned int)recur_index, &row_len);
      recur_value.assign(recur_data, row_len);
    }

    if (!where_result) ignore_flag->set(row_id);
    if (start_flag) start_ids.push_back(row_id);
    id_prior_packets[row_id] = make_pair(prior_value, row);
    if (!recur_value.empty()) recur_ids[recur_value].push_back(row_id);
    ++row_id;
  }
}

/* How to implement connect by:
 * 1. Erase the connect by part from original query sql, and append start, where
 * conditions, append prior and recursion related columns to the select fiedld
 * list. For example: SELECT * FROM t1 where t1.c3<100 window connect by t1.c1 =
 * prior t1.c2 start with t1.c1 = 1; will be changed to: SELECT *, t1.c1, t1.c2,
 * (t1.c1 = 1), (t1.c3<100) FROM t1;
 * 2. Store all results data in id_prior_packets, the key is row id, the value
 * is prior column value and row packet.
 * 3. Check the (t1.c1 = 1) column，if 1, means the row is the start row, put
 * into start_ids.
 * 4. Check the (t1.c3<100) column, if 0, the row will be filtered, put into
 * ignore_flag.
 * 5. Store each row in recur_ids, the key is recur column vlaue, value is all
 * the row ids has the same recur value.
 * 6. Use loop_flag to check if a row has already been put into ready_rows,
 * check each row before put into ready_rows, if 1, means there is a connect by
 * data loop, do not support.
 * 7. Use start_ids to start the recursive function:
 *    a. Put the row to ready_rows.
 *    b. Get the prior column value of the row.
 *    c. Check recur_ids, if there are rows has recur column value equal to the
 * prior column vlaue d  if yes goto a if no return
 * */
void MySQLConnectByNode::handle_before_complete() {
  list<unsigned int>::iterator it = start_ids.begin();
  for (; it != start_ids.end(); ++it) {
    if (loop_flag->test(*it)) {
      LOG_ERROR("Not support CONNECT BY loop for query data.\n");
      throw Error("Not support CONNECT BY loop for query data.");
    }
    if (!ignore_flag->test(*it)) {
      ready_rows->push_back(id_prior_packets[*it].second);
      loop_flag->set(*it);
    }
    find_recur_row_result(*it);
  }
}

void MySQLConnectByNode::find_recur_row_result(unsigned int id) {
  string prior_value = id_prior_packets[id].first;
  if (recur_ids.count(prior_value)) {
    list<unsigned int> id_list = recur_ids[prior_value];
    list<unsigned int>::iterator it = id_list.begin();
    for (; it != id_list.end(); ++it) {
      if (loop_flag->test(*it)) {
        LOG_ERROR("Not support CONNECT BY loop for query data.\n");
        throw Error("Not support CONNECT BY loop for query data.");
      }
      if (!ignore_flag->test(*it)) {
        ready_rows->push_back(id_prior_packets[*it].second);
        loop_flag->set(*it);
      }
      find_recur_row_result(*it);
    }
  }
}

/* class MySQLAvgNode */
MySQLAvgNode::MySQLAvgNode(ExecutePlan *plan, list<AvgDesc> &avg_list)
    : MySQLInnerPipeNode(plan) {
  for (auto &desc : avg_list) {
    AvgDesc desc_clone;
    desc_clone.clone(desc);
    this->avg_list.push_back(desc_clone);
  }
  inited_column_index = false;
  this->name = "MySQLAvgNode";
}
void MySQLAvgNode::clean() {
  MySQLInnerNode::clean();
  for (auto &desc : avg_list) {
    if (desc.mpf_inited) mpf_clear(desc.value);
  }
}

void MySQLAvgNode::init_column_index() {
  if (!inited_column_index) {
    list<Packet *> *field_packets = get_field_packets();
    unsigned int column_num = field_packets->size();

    /* adjust column index */
    list<AvgDesc>::iterator it;
    for (it = avg_list.begin(); it != avg_list.end(); it++) {
      it->sum_index += column_num;
      it->count_index += column_num;
      if (it->avg_index < 0) it->avg_index += column_num;
    }

    /* adjust the avg result decimal */
    list<Packet *>::iterator it_field;
    unsigned int index = 0;
    for (it_field = field_packets->begin(); it_field != field_packets->end();
         it_field++, index++) {
      MySQLColumnResponse col_resp(*it_field);
      col_resp.unpack();
      for (it = avg_list.begin(); it != avg_list.end(); it++) {
        if (it->avg_index == (int)index) {
          avg_column_type[it->avg_index] = col_resp.get_column_type();
          it->decimal = col_resp.get_decimals();
          LOG_DEBUG("decimal=%d, index=%d\n", it->decimal, index);
          break;
        }
      }
    }
    inited_column_index = true;
  }
}

void MySQLAvgNode::handle_child(MySQLExecuteNode *child) {
  Packet *row;
  mpf_t sum;
  uint64_t count = 0;
  list<AvgDesc>::iterator it;
  char avg_column[GMP_N_DIGITS + 5];

  init_column_index();

  unsigned int column_num = get_field_packets()->size();
  mpf_init2(sum, DECIMAL_STORE_BIT);
  while (!row_map[child]->empty()) {
    row = row_map[child]->front();
    row_map[child]->pop_front();
    MySQLRowResponse row_res(row);
    Packet *new_row = Backend::instance()->get_new_packet(row_packet_size);
    vector<int> column_len(column_num, 0);
    vector<string> column_data(column_num, "");
    vector<bool> reset_value(column_num, false);
    unsigned int needed_length = 0;

    new_row->wr_ptr(new_row->base() + PACKET_HEADER_SIZE);
    for (it = avg_list.begin(); it != avg_list.end(); it++) {
      it->reset_value = true;
      if (!row_res.field_is_null(it->sum_index) &&
          !row_res.field_is_null(it->count_index)) {
        row_res.get_mpf(it->sum_index, sum);
        count = row_res.get_uint(it->count_index);
        if (count == 0) {
          it->reset_value = false;
          continue;
        }
      } else {
        it->reset_value = false;
        continue;
      }
      mpf_div_ui(it->value, sum, count);
    }

    for (unsigned int i = 0; i < column_num; i++) {
      row_res.set_current(i);
      for (it = avg_list.begin(); it != avg_list.end(); it++) {
        if (it->avg_index == (int)i && it->reset_value) {
          if (it->decimal == 0)
            column_len[i] = gmp_sprintf(avg_column, "%.4Ff", it->value);
          else if (it->decimal == 31 &&
                   avg_column_type[i] == MYSQL_TYPE_DOUBLE) {
            column_len[i] = gmp_sprintf(avg_column, "%.16Ff", it->value);
            for (int j = column_len[i] - 1; j > 0; j--) {
              if (avg_column[j] == '0') {
                column_len[i]--;
              } else if (avg_column[j] == '.') {
                column_len[i]--;
                break;
              } else {
                break;
              }
            }
          } else {
            char formator[10];
            sprintf(formator, "%%.%dFf", it->decimal);
            column_len[i] = gmp_sprintf(avg_column, formator, it->value);
          }
          column_data[i] = avg_column;
          reset_value[i] = true;
          break;
        }
      }

      if (!row_res.field_is_null() && !reset_value[i]) {
        column_len[i] = row_res.get_current_length();
      }
      needed_length += column_len[i];
      row_res.move_next();
    }

    needed_length += 1 + 9 * column_num;
    /**
     *   max(1,9)    // '\373' or max required length of pack_lenenc_int()
     * + column_len  // packed data length
     * + 1           // for terminate '\0'
     */
    if (new_row->size() - (new_row->wr_ptr() - new_row->base()) <
        needed_length) {
      uint64_t resize_len = needed_length + new_row->wr_ptr() - new_row->base();
      LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
                new_row->size(), resize_len);
      new_row->size(resize_len);
    }

    for (unsigned int i = 0; i < column_num; i++) {
      row_res.set_current(i);
      if (row_res.field_is_null() && !reset_value[i])
        new_row->packchar(
            '\373');  //'\373' represents that the field value is NULL
      else {
        if (!reset_value[i]) {
          column_data[i] = row_res.get_current_data();
        }
        new_row->pack_lenenc_int(column_len[i]);
        new_row->packdata(column_data[i].c_str(), column_len[i]);
      }
      row_res.move_next();
    }
    size_t load_length = new_row->length() - PACKET_HEADER_SIZE;
    pack_header(new_row, load_length);
    ready_rows->push_back(new_row);
    delete row;
  }
  mpf_clear(sum);
}

/* class MySQLUnionGroupNode */

MySQLUnionGroupNode::MySQLUnionGroupNode(ExecutePlan *plan,
                                         vector<list<ExecuteNode *> > &nodes)
    : MySQLExecuteNode(plan, NULL) {
  this->name = "MySQLUnionGroupNode";
  this->nodes = nodes;
  finished_group_num = 0;
  column_size = 0;
  got_error = false;
}

void MySQLUnionGroupNode::check_union_columns() {
  if (column_size == 0) {
    MySQLResultSetHeaderResponse header(
        ((MySQLExecuteNode *)(nodes[0].front()))->get_header_packet());
    header.unpack();
    column_size = header.get_columns();
  }
  list<ExecuteNode *>::iterator it = nodes[finished_group_num].begin();
  for (; it != nodes[finished_group_num].end(); it++) {
    MySQLResultSetHeaderResponse header(
        ((MySQLExecuteNode *)(*it))->get_header_packet());
    header.unpack();
    if (column_size != header.get_columns()) {
      got_error = true;
      status = EXECUTE_STATUS_COMPLETE;
      MySQLErrorResponse error(
          1222, "The used SELECT statements have a different number of columns",
          "21000");
      error.pack(&error_packet);
      throw ErrorPacketException();
    }
  }
}

void MySQLUnionGroupNode::handle_children() {
  list<ExecuteNode *>::iterator it;
  for (it = nodes[finished_group_num].begin();
       it != nodes[finished_group_num].end(); ++it) {
    if (!row_map[*it]->empty()) handle_child((MySQLExecuteNode *)(*it));
  }
}

void MySQLUnionGroupNode::handle_child(MySQLExecuteNode *child) {
  Packet *row;
  while (!row_map[child]->empty()) {
    row = row_map[child]->front();
    ready_rows->push_back(row);
    row_map[child]->pop_front();
  }
}

void MySQLUnionGroupNode::execute() {
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START: {
#ifdef DEBUG
        node_start_timing();
#endif
        init_row_map();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;
      }
      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        status = EXECUTE_STATUS_WAIT;
      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;
      case EXECUTE_STATUS_HANDLE:
        try {
          handle_children();
        } catch (...) {
          status = EXECUTE_STATUS_COMPLETE;
          throw;
        }
        status = EXECUTE_STATUS_FETCH_DATA;
        break;
      case EXECUTE_STATUS_BEFORE_COMPLETE:
        status = EXECUTE_STATUS_COMPLETE;
      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        node_end_timing();
#endif
#ifdef DEBUG
        LOG_DEBUG("MySQLUnionGroupNode %@ cost %d ms\n", this, node_cost_time);
#endif
        break;

      default:
        break;
    }
  }
}

Packet *MySQLUnionGroupNode::get_error_packet() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if ((*it)->get_error_packet()) {
      return (*it)->get_error_packet();
    }
  }
  if (got_error) {
    return &error_packet;
  }
  return NULL;
}

void MySQLUnionGroupNode::children_execute() {
  LOG_DEBUG("Node %@ children start to execute.\n", this);
  if (finished_group_num < nodes.size()) {
    list<ExecuteNode *> execute_nodes = nodes[finished_group_num];
    list<ExecuteNode *>::iterator it = execute_nodes.begin();
    for (; it != execute_nodes.end(); ++it) {
      (*it)->execute();
    }
  }
  LOG_DEBUG("Node %@ children execute finished.\n", this);
}

bool MySQLUnionGroupNode::notify_parent() {
  Packet *row;
  if (ready_rows->empty()) {
    return false;
  }

  while (!ready_rows->empty()) {
    row = ready_rows->front();
    ready_rows->pop_front();
    parent->add_row_packet(this, row);
  }

  return true;
}

void MySQLUnionGroupNode::set_children_thread_status_start(
    unsigned int group_num) {
  list<ExecuteNode *>::iterator it = nodes[group_num].begin();
  for (; it != nodes[group_num].end(); ++it) {
    ((MySQLExecuteNode *)(*it))->set_thread_status_started();
  }
}

void MySQLUnionGroupNode::handle_error_all_children() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    try {
      (*it)->handle_error_throw();
    } catch (...) {
      it++;
      for (; it != children.end(); it++) {
        try {
          (*it)->handle_error_throw();
        } catch (...) {
        }
      }
      throw;
    }
  }
}

void MySQLUnionGroupNode::wait_children() {
  LOG_DEBUG("Node %@ start to wait.\n", this);
  list<ExecuteNode *> node_list = nodes[finished_group_num];
  list<ExecuteNode *>::iterator it = node_list.begin();
  for (; it != node_list.end(); it++) {
    ((MySQLExecuteNode *)(*it))->start_thread();
  }
  set_children_thread_status_start(finished_group_num);
  plan->start_all_bthread();

  unsigned int finished_child = 0;
  it = node_list.begin();
  for (; it != node_list.end(); it++) {
    if (!((MySQLExecuteNode *)(*it))->notify_parent() && (*it)->is_finished()) {
      finished_child++;
    }
  }
  // Check union columns for one group after this group nodes got header
  // packets.
  check_union_columns();
  if (finished_child == node_list.size()) {
    finished_group_num++;
  }

  if (finished_group_num == nodes.size()) {
    handle_error_all_children();
    status = EXECUTE_STATUS_BEFORE_COMPLETE;
  } else {
    status = EXECUTE_STATUS_HANDLE;
  }
  LOG_DEBUG("Node %@ wait finished.\n", this);
}

void MySQLUnionGroupNode::init_row_map() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    row_map[*it] =
        new AllocList<Packet *, Session *, StaticAllocator<Packet *> >();
    row_map[*it]->set_session(plan->session);
    row_map_size[*it] = 0;
  }
}

void MySQLUnionGroupNode::clean() {
  Packet *packet;
  MySQLExecuteNode *free_node;

  status = EXECUTE_STATUS_COMPLETE;

  // clean ready packets
  while (!ready_rows->empty()) {
    packet = ready_rows->front();
    ready_rows->pop_front();
    delete packet;
  }

  while (!children.empty()) {
    free_node = children.front();
    free_node->clean();

    // clean remaining rows
    if (row_map[free_node]) {
      while (!row_map[free_node]->empty()) {
        packet = row_map[free_node]->front();
        row_map[free_node]->pop_front();
        delete packet;
      }

      delete row_map[free_node];
      row_map[free_node] = NULL;
    }

    // delete child node
    children.pop_front();
    delete free_node;
  }
}

/* class MySQLUnionAllNode */
MySQLUnionAllNode::MySQLUnionAllNode(ExecutePlan *plan,
                                     vector<ExecuteNode *> &nodes)
    : MySQLInnerPipeNode(plan) {
  this->name = "MySQLUnionAllNode";
  column_size = 0;
  this->nodes = nodes;
  got_error = false;
}

void MySQLUnionAllNode::handle_children() {
  if (column_size == 0) check_union_columns();
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if (!row_map[*it]->empty()) handle_child(*it);
  }
  status = EXECUTE_STATUS_FETCH_DATA;
}

void MySQLUnionAllNode::check_union_columns() {
  vector<ExecuteNode *>::iterator it = nodes.begin();
  MySQLResultSetHeaderResponse header(
      ((MySQLExecuteNode *)(*it))->get_header_packet());
  header.unpack();
  column_size = header.get_columns();
  it++;
  for (; it != nodes.end(); it++) {
    MySQLResultSetHeaderResponse header(
        ((MySQLExecuteNode *)(*it))->get_header_packet());
    header.unpack();
    if (column_size != header.get_columns()) {
      got_error = true;
      status = EXECUTE_STATUS_COMPLETE;
      MySQLErrorResponse error(
          1222, "The used SELECT statements have a different number of columns",
          "21000");
      error.pack(&error_packet);
      throw ErrorPacketException();
    }
  }
}

Packet *MySQLUnionAllNode::get_error_packet() {
  Packet *error = MySQLInnerPipeNode::get_error_packet();
  if (error) {
    return error;
  } else if (got_error) {
    return &error_packet;
  }
  return NULL;
}

/* class MySQLInnerPipeNode */
MySQLInnerPipeNode::MySQLInnerPipeNode(ExecutePlan *plan)
    : MySQLInnerNode(plan) {
  this->name = "MySQLInnerPipeNode";
  node_can_swap = session->is_may_backend_exec_swap_able();
}
void MySQLInnerPipeNode::handle_children() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if (!row_map[*it]->empty()) handle_child(*it);
  }
  status = EXECUTE_STATUS_FETCH_DATA;
}

void MySQLInnerPipeNode::handle_child(MySQLExecuteNode *child) {
  Packet *row;
  while (!row_map[child]->empty()) {
    row = row_map[child]->front();
    ready_rows->push_back(row);
    row_map[child]->pop_front();
  }
}

void MySQLInnerPipeNode::execute() {
  ExecuteProfileHandler *execute_profile = session->get_profile_handler();
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START: {
        init_row_map();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;
      }
      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        if (node_can_swap && plan->plan_can_swap()) {
          handle_swap_connections();
          return;
        }
        status = EXECUTE_STATUS_WAIT;
      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;
      case EXECUTE_STATUS_HANDLE:
#ifdef DEBUG
        node_start_timing();
#endif
        profile_id = execute_profile->start_serial_monitor(
            get_executenode_name(), profile_id);
        try {
          handle_children();
        } catch (...) {
          status = EXECUTE_STATUS_COMPLETE;
          throw;
        }
#ifdef DEBUG
        node_end_timing();
#endif
        execute_profile->end_execute_monitor(profile_id);
        status = EXECUTE_STATUS_FETCH_DATA;
        return;
        break;
      case EXECUTE_STATUS_BEFORE_COMPLETE:
        handle_before_complete();
        status = EXECUTE_STATUS_COMPLETE;
      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        LOG_DEBUG("MySQLInnerPipeNode %@ cost %d ms\n", this, node_cost_time);
#endif
        break;

      default:
        break;
    }
  }
}
/* class MySQLOKMergeNode */

MySQLOKMergeNode::MySQLOKMergeNode(ExecutePlan *plan) : MySQLInnerNode(plan) {
  this->name = "MySQLOKMergeNode";
  ok_packet = NULL;
  affect_rows = 0;
  warnings = 0;
}

void MySQLOKMergeNode::handle_children() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if (!row_map[*it]->empty()) handle_child(*it);
  }

  status = EXECUTE_STATUS_FETCH_DATA;
}

void MySQLOKMergeNode::handle_child(MySQLExecuteNode *child) {
  Packet *packet = NULL;
  int first_msg = 0;
  int second_msg = 0;
  stmt_type type = plan->statement->get_stmt_node()->type;
  while (!row_map[child]->empty()) {
    packet = row_map[child]->front();
    row_map[child]->pop_front();
    MySQLOKResponse ok(packet);
    ok.unpack();
    affect_rows += ok.get_affected_rows();
    warnings += ok.get_warnings();
    if (type == STMT_UPDATE) {
      first_msg +=
          session->get_matched_or_change_message("matched:", ok.get_message());
      second_msg +=
          session->get_matched_or_change_message("Changed:", ok.get_message());
    }
    LOG_DEBUG("After handle modify node %@, affected_row[%d], warnings[%d].\n",
              child, affect_rows, warnings);
    if (!ok_packet) {
      ok_packet = packet;
      LOG_DEBUG("Keep packet %@ for ok packet rebuild.\n", packet);
    } else {
      LOG_DEBUG("Delete packet %@ after handle modify node %@.\n", packet,
                this);
      delete packet;
    }
  }
  if (type == STMT_UPDATE) {
    string ok_msg = "Rows matched: ";
    ok_msg.append(to_string(first_msg));
    ok_msg.append("  Changed: ");
    ok_msg.append(to_string(second_msg));
    ok_msg.append("  Warnings: ");
    ok_msg.append(to_string(warnings));
    update_msg = ok_msg;
  }
  LOG_DEBUG("Handle modify node %@ finish.\n", child);
}

void MySQLOKMergeNode::rebuild_ok_packet() {
  LOG_DEBUG("Rebuild ok packet with affected_rows [%d] warnings [%d].\n",
            affect_rows, warnings);
  if (ok_packet) {
    MySQLOKResponse ok(ok_packet);
    ok.set_affected_rows(affect_rows);
    ok.set_warnings(warnings);
    ok.set_message(update_msg);
    Packet *new_packet = Backend::instance()->get_new_packet(row_packet_size);
    ok.pack(new_packet);
    delete ok_packet;
    ok_packet = new_packet;
  } else {
    MySQLOKResponse ok(affect_rows, warnings);
    ok_packet = Backend::instance()->get_new_packet(row_packet_size);
    ok.pack(ok_packet);
  }
  ready_rows->push_back(ok_packet);
  ok_packet = NULL;
}

void MySQLOKMergeNode::execute() {
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START: {
        init_row_map();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;
      }

      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        status = EXECUTE_STATUS_WAIT;

      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;

      case EXECUTE_STATUS_HANDLE:
#ifdef DEBUG
        node_start_timing();
#endif
        handle_children();
#ifdef DEBUG
        node_end_timing();
#endif
        break;

      case EXECUTE_STATUS_BEFORE_COMPLETE:
        rebuild_ok_packet();
        status = EXECUTE_STATUS_COMPLETE;

      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        LOG_DEBUG("MySQLOKMergeNode %@ cost %d ms\n", this, node_cost_time);
#endif
        break;

      default:
        break;
    }
  }
}

/* classs MySQLModifyNode */

MySQLModifyNode::MySQLModifyNode(ExecutePlan *plan, DataSpace *dataspace)
    : MySQLExecuteNode(plan, dataspace),
      cond_sql(sql_mutex),
      cond_notify(mutex),
      new_stmt_for_shard(NULL) {
  this->name = "MySQLModifyNode";
  thread_status = THREAD_STOP;
  bthread = NULL;
  got_error = false;
  conn = NULL;
  status = EXECUTE_STATUS_START;
  this->error_packet = NULL;
}

void MySQLModifyNode::handle_error_packet(Packet *packet) {
  try {
    ACE_Guard<ACE_Thread_Mutex> guard(mutex);
    record_migrate_error_message(plan, packet,
                                 "Modify node get an error packet");
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    error_packet = packet;
    if (handle_tokudb_lock_timeout(conn, packet)) {
      if (conn) {
        handler->clean_dead_conn(&conn, dataspace);
      }
    }
    cond_notify.signal();
  } catch (...) {
    cond_notify.signal();
  }
}

void MySQLModifyNode::execute() {
  if (thread_status == THREAD_STOP) {
    status = EXECUTE_STATUS_FETCH_DATA;

#ifndef DBSCALE_TEST_DISABLE
    Backend *bk = Backend::instance();
    dbscale_test_info *test_info = bk->get_dbscale_test_info();
    if (!strcasecmp(test_info->test_case_name.c_str(), "bthread") &&
        !strcasecmp(test_info->test_case_operation.c_str(), "get_null")) {
      bthread = NULL;
    } else {
#endif
      Backend *backend = Backend::instance();
      bthread = backend->get_backend_thread_pool()->get_one_from_free();
#ifndef DBSCALE_TEST_DISABLE
    }
#endif
    if (!bthread) {
      got_error = true;
      LOG_ERROR("Fail to get thread for MySQLModifyNode::execute.\n");
      throw Error(
          "Fail to get a backend thread from pool, so stop execute the sql");
    }
    thread_status = THREAD_CREATED;
    LOG_DEBUG("Modify node %@ start to work.\n", this);
    bthread->set_task(this);
    thread_status = THREAD_STARTED;
    bthread->wakeup_handler_thread();
    // After the acquire of lock, the backend thread must is start to running
    // the task.
  }
}

void MySQLModifyNode::clean() {
  if (thread_status == THREAD_STARTED) {
    LOG_DEBUG("Modify node %@ cleaning wait.\n", this);
    this->wait_for_cond();
    LOG_DEBUG("Modify node %@ cleaning wait finish.\n", this);
  } else if (thread_status == THREAD_CREATED) {
    bthread->re_init_backend_thread();
    bthread->get_pool()->add_back_to_free(bthread);
  } else {
    got_error = true;
  }
  Packet *packet = NULL;
  while (!ready_rows->empty()) {
    packet = ready_rows->front();
    ready_rows->pop_front();
    delete packet;
  }

  string *s;
  while (!sql_list.empty()) {
    s = sql_list.front();
    sql_list.pop_front();
    delete s;
  }

  if (conn) {
    if (!got_error || error_packet) {
      if (plan->statement->is_cross_node_join())
        handler->put_back_connection(dataspace, conn, true);
      else if (!plan->get_migrate_tool())
        handler->put_back_connection(dataspace, conn);
    } else {
      if (plan->statement->is_cross_node_join())
        handler->clean_dead_conn(&conn, dataspace, false);
      else if (!plan->get_migrate_tool())
        handler->clean_dead_conn(&conn, dataspace);
      else {
        conn->set_status(CONNECTION_STATUS_TO_DEAD);
      }
    }
    conn = NULL;
  }

  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
  if (new_stmt_for_shard) {
    new_stmt_for_shard->free_resource();
    delete new_stmt_for_shard;
    new_stmt_for_shard = NULL;
  }

  if (session->is_drop_current_schema()) {
    session->set_schema(NULL);
  }
}

static int check_ddl_need_retry(Session *session, DataSpace *space,
                                const char *sql) {
  string query_sql =
      "SELECT PROCESSLIST_STATE FROM PERFORMANCE_SCHEMA.THREADS WHERE "
      "PROCESSLIST_INFO=\"";
  string tmp_sql = find_and_replace_quota(sql);
  query_sql.append(tmp_sql.c_str()).append("\"");
  Connection *conn = NULL;
  TimeValue tv(backend_sql_net_timeout);
  bool need_recheck = true;
  while (need_recheck) {
    need_recheck = false;
    try {
      conn = space->get_connection(session);
      if (conn) {
        bool find_meta_lock = false;
        vector<string> result;
        conn->query_for_one_column(query_sql.c_str(), 0, &result, &tv, true);
        if (result.size() == 0) {
          conn->get_pool()->add_back_to_free(conn);
          return 1;
        }
        vector<string>::iterator it = result.begin();
        for (; it != result.end(); ++it) {
          string value = *it;
          if (value.find("metadata lock") != string::npos) {
            find_meta_lock = true;
            break;
          }
        }
        conn->get_pool()->add_back_to_free(conn);
        conn = NULL;
        if (find_meta_lock) return 0;
        need_recheck = true;  // wait ddl run finished
        sleep(5);
      }
    } catch (...) {
      if (conn) {
        conn->get_pool()->add_back_to_dead(conn);
      }
    }
  }
  return -1;
}

int MySQLModifyNode::svc() {
  status = EXECUTE_STATUS_FETCH_DATA;
  string *s = NULL;
  Packet *packet = NULL;
  string real_sql;
  bool off_sql_log_bin = false;
  size_t list_size = 0;
  bool need_retry = false;
  TimeValue tv(ddl_recv_timeout, 0);
  stmt_type sql_type = plan->statement->get_stmt_node()->type;
  if (run_ddl_retry &&
      (sql_type == STMT_DROP_TB || sql_type == STMT_RENAME_TABLE)) {
    need_retry = true;
    LOG_DEBUG("run ddl statement so can retry if failed\n");
  }
  int retry_count = 0;
retry:
  try {
    while (1) {
      if (ACE_Reactor::instance()->reactor_event_loop_done()) {
        LOG_ERROR(
            "MySQLModifyNode svc failed due to reactor_event_loop_done\n");
        throw Error(
            "MySQLModifyNode svc failed due to reactor_event_loop_done");
      }
      sql_mutex.acquire();
      if (!retry_count && sql_list.empty()) {
        LOG_DEBUG("Modify node %@ start to wait_children new sql.\n", this);
        cond_sql.wait();
      }
      if (!sql_list.empty() && !retry_count) {
        s = sql_list.front();
        sql_list.pop_front();
        sql = string(*s);
      }
      if (!retry_count) list_size = sql_list.size();
      sql_mutex.release();

      if (!session->is_keeping_connection()) {
        if (parent->is_finished()) {
          LOG_INFO("ModifyNode %@ exit cause parent finished.\n", this);

          if (plan->statement->is_cross_node_join()) {
            // Do nothing
          } else if (!plan->get_migrate_tool())
            conn = session->get_kept_connection(dataspace);
          else
            conn = plan->get_migrate_tool()->get_migrate_write_conn(this);
          if (conn) {
            if (plan->statement->is_cross_node_join())
              handler->clean_dead_conn(&conn, dataspace, false);
            else if (!plan->get_migrate_tool())
              handler->clean_dead_conn(&conn, dataspace);
            else {
              conn->set_status(CONNECTION_STATUS_TO_DEAD);
              conn = NULL;
            }
          }
          delete s;
          status = EXECUTE_STATUS_COMPLETE;
          return FINISHED;
        }
      }

      LOG_DEBUG("Modify node handle sql [%s] with [%d] sql append.\n",
                sql.c_str(), list_size);
      if (!strlen(sql.c_str())) {
        LOG_DEBUG("Modify node %@ finish.\n", this);
        delete s;
        s = NULL;
        break;
      }

      if (enable_xa_transaction &&
          plan->session->get_session_option("close_session_xa").int_val == 0) {
        real_sql.clear();
        real_sql += sql;
      }

      Packet exec_packet;
      MySQLQueryRequest query(sql.c_str());
      query.set_sql_replace_char(
          plan->session->get_query_sql_replace_null_char());
      query.pack(&exec_packet);
      delete s;
      s = NULL;
#ifdef DEBUG
      if (retry_count == 0) node_start_timing();
#endif
      stmt_type st_type = plan->statement->get_stmt_node()->type;
      bool is_ddl =
          (STMT_DDL_START < st_type && st_type < STMT_DDL_END) ? true : false;
      bool skip_keep_conn = false;
      if (need_retry) {
        int check_working_time = 0;
        while (dataspace->data_source->get_work_state() !=
                   DATASOURCE_STATE_WORKING &&
               check_working_time < 60) {
          sleep(1);
          check_working_time += 1;
        }
        if (check_working_time == 60) {
          conn = NULL;
          retry_count = 5;  // no need retry
          LOG_ERROR(
              "Fail to get an usable connection, because %s is "
              "not "
              "working\n",
              dataspace->get_name());
          throw Error(
              "Fail to get an usable connection, because space is not working");
        }
      }
      if (plan->get_migrate_tool()) {
        conn = plan->get_migrate_tool()->get_migrate_write_conn(this);
        conn->reset();
        handler->send_to_server(conn, &exec_packet);
      } else {
        if (plan->statement->is_cross_node_join()) {
          if (dataspace->get_data_source() &&
              dataspace->get_data_source()->get_master_server() &&
              dataspace->get_data_source()
                  ->get_master_server()
                  ->is_mgr_server() &&
              strstr(sql.c_str(), TMP_TABLE_SCHEMA) != NULL &&
              strstr(sql.c_str(), TMP_TABLE_NAME) != NULL) {
            off_sql_log_bin = true;
          }
          skip_keep_conn = true;
          conn = handler->send_to_server_retry(
              dataspace, &exec_packet, session->get_schema(), false, true, true,
              skip_keep_conn, off_sql_log_bin);
        } else if (is_ddl) {
          skip_keep_conn = true;
          if ((st_type == STMT_DROP_DB &&
               session->is_temp_table_set_contain_db(session->get_schema())) ||
              plan->statement->get_stmt_node()->is_create_or_drop_temp_table) {
            skip_keep_conn = false;
          }
          conn = handler->send_to_server_retry(
              dataspace, &exec_packet, session->get_schema(),
              session->is_read_only(), false, false, skip_keep_conn,
              off_sql_log_bin);
        } else {
          conn = handler->send_to_server_retry(dataspace, &exec_packet,
                                               session->get_schema(),
                                               session->is_read_only());
        }
      }
      const char *server_name = conn->get_server()->get_name();
      bool non_modified_conn = true;
      LOG_DEBUG("Modify node receiving result from server\n");
      packet = Backend::instance()->get_new_packet(row_packet_size);
#ifndef DBSCALE_TEST_DISABLE
      if (on_test_stmt) {
        Backend *bk = Backend::instance();
        dbscale_test_info *test_info = bk->get_dbscale_test_info();
        if (!strcasecmp(test_info->test_case_name.c_str(),
                        "auth_source_fail") &&
            !strcasecmp(test_info->test_case_operation.c_str(),
                        "fail_for_create_drop_db")) {
          if (dataspace == bk->get_auth_data_space()) {
            bk->set_dbscale_test_info("", "", "");
            throw Error("Fail to receive from auth.");
          }
        }
      }
#endif
      if (need_retry) {
        conn->set_need_kill_timeout_conn(false);
        handler->receive_from_server(conn, packet, &tv);
        conn->set_need_kill_timeout_conn(true);
      }

      else
        handler->receive_from_server(conn, packet);
#ifdef DEBUG
      node_end_timing();
#endif
      sql = "";
      if (driver->is_error_packet(packet)) {
        non_modified_conn = false;
        MySQLErrorResponse error(packet);
        error.unpack();
        if (retry_count && need_retry &&
            Backend::instance()->check_ddl_return_retry_ok(
                plan->statement->get_stmt_node()->type,
                plan->statement->get_stmt_node(), error.get_error_code())) {
          LOG_DEBUG("after retry, ddl statement run success \n");
          delete packet;
          packet = Backend::instance()->get_new_packet();
          MySQLOKResponse ok(0, 0);
          ok.pack(packet);
        } else {
          if (!plan->get_migrate_tool()) {
            LOG_DEBUG("Modify node %@ get an error packet %@, %d (%s) %s.\n",
                      this, packet, error.get_error_code(),
                      error.get_sqlstate(), error.get_error_message());
          } else {
            LOG_ERROR("Modify node %@ get an error packet %@, %d (%s) %s.\n",
                      this, packet, error.get_error_code(),
                      error.get_sqlstate(), error.get_error_message());
          }
          handle_error_packet(packet);
          packet = NULL;
          return FINISHED;
        }
      }
      if (driver->is_ok_packet(packet)) {
        handler->deal_autocommit_with_ok_eof_packet(packet);
        if (!plan->get_migrate_tool()) {
          if (support_show_warning)
            handle_warnings_OK_and_eof_packet(
                plan, packet, handler, dataspace, conn,
                plan->statement->is_cross_node_join());

          if (plan->statement->get_stmt_node()->type == STMT_DROP_TB) {
            table_link *table =
                plan->statement->get_stmt_node()->table_list_head;
            if (table) {  // only will have one table
              const char *schema_name = table->join->schema_name
                                            ? table->join->schema_name
                                            : plan->statement->get_schema();
              const char *table_name = table->join->table_name;
              if (conn->dec_tmp_tb_num(schema_name, table_name)) {
                plan->session->remove_temp_table_set(schema_name, table_name);
                plan->statement->get_stmt_node()->is_create_or_drop_temp_table =
                    true;
              }
            }
          }

          MySQLOKResponse ok(packet);
          ok.unpack();
          if (ok.get_affected_rows() > 0) {
            non_modified_conn = false;
          }
        } else {
          MySQLOKResponse ok(packet);
          ok.unpack();
          uint16_t warnings = ok.get_warnings();
          if (warnings) {
            LOG_ERROR("Modify node %@ get OK packet with %u warnings.\n", this,
                      (unsigned int)warnings);
            delete packet;
            packet = handler->get_error_packet(
                9002, "Migrate modify node get OK packet with warnings.", NULL);
            handle_error_packet(packet);
            packet = NULL;
            print_warning_infos(conn);
            return FINISHED;
          }
        }
      }

      {
        ACE_Guard<ACE_Thread_Mutex> guard(mutex);
        ready_rows->push_back(packet);
        packet = NULL;
        cond_notify.signal();
      }

      if (conn) {
        if (!plan->get_migrate_tool()) {
          if (plan->statement->is_cross_node_join()) {
            if (off_sql_log_bin) {
              if (conn->get_session_var_map_md5() !=
                  session->get_session_var_map_md5()) {
                conn->set_session_var(session->get_session_var_map());
              }
            }
          }
          handler->put_back_connection(dataspace, conn, skip_keep_conn);
        }
      }

      if (!plan->get_migrate_tool()) {
        record_xa_modify_sql(plan, session, dataspace, real_sql.c_str(),
                             non_modified_conn);
        if (!non_modified_conn) {
          session->record_xa_modified_conn(conn);
        }
        record_modify_server(plan, session, server_name,
                             dataspace->get_virtual_machine_id(),
                             non_modified_conn);
      }
      conn = NULL;

      LOG_DEBUG("Modify node %@ finish sql.\n", this);
    }
  } catch (exception &e) {
    LOG_ERROR("Modify node thread %@ got exception %s.\n", this, e.what());
    if (need_retry && retry_count < 5) {
      retry_count++;
      int ret = check_ddl_need_retry(session, dataspace, sql.c_str());
      try {
        if (ret == 0) {
          if (conn && conn->get_need_kill_timeout_connection())
            handler->kill_timeout_connection(conn, packet, true, &tv);
        } else if (ret == 1) {
          if (conn) {
            conn->get_pool()->add_back_to_dead(conn);
            conn = NULL;
          }
          if (packet) {
            delete packet;
            packet = NULL;
          }
          sleep(monitor_interval + 1);
          goto retry;
        } else {
          if (conn) {
            if (conn->get_need_kill_timeout_connection())
              handler->kill_timeout_connection(conn, packet, true, &tv);
            conn->get_pool()->add_back_to_dead(conn);
            conn = NULL;
          }
          if (packet) {
            delete packet;
            packet = NULL;
          }
          sleep(monitor_interval + 1);
          goto retry;
        }
      } catch (exception &kill_exception) {
        LOG_ERROR("try to kill timeout connection faild \n");
      }
    }

    if (conn) {
      if (plan->statement->is_cross_node_join())
        handler->clean_dead_conn(&conn, dataspace, false);
      else if (!plan->get_migrate_tool())
        handler->clean_dead_conn(&conn, dataspace);
      else {
        conn->reset_cluster_xa_session_before_to_dead();
        conn->set_status(CONNECTION_STATUS_TO_DEAD);
        conn = NULL;
      }
    }
    if (packet) delete packet;

    handle_error_packet(NULL);
    if (s) delete s;
    s = NULL;
    cond_notify.signal();
    return FINISHED;
  }
  mutex.acquire();
  status = EXECUTE_STATUS_COMPLETE;
  cond_notify.signal();
  mutex.release();
#ifdef DEBUG
  LOG_DEBUG("MySQLModifyNode %@ cost %d ms\n", this, node_cost_time);
#endif
  return FINISHED;
}

bool MySQLModifyNode::notify_parent() {
  int ret = false;
  Packet *packet = NULL;

  ACE_Guard<ACE_Thread_Mutex> guard(mutex);
  // check fetch node finished or get error
  if (is_finished() || got_error) {
    if (got_error) {
      throw_error_packet();
    }
    ret = false;
  } else {
    if (ready_rows->empty()) cond_notify.wait();

    if (got_error) {
      throw_error_packet();
    } else if (ready_rows->empty()) {  // the result set is empty
      ACE_ASSERT(is_finished());
      ret = false;
    } else {
      while (!ready_rows->empty()) {
        packet = ready_rows->front();
        ready_rows->pop_front();
        parent->add_row_packet(this, packet);
        ret = true;
      }
    }
  }

  return ret;
}

void MySQLModifyNode::add_sql_during_exec(const char *stmt_sql, size_t len) {
  add_sql(stmt_sql, len, true);
}

void MySQLModifyNode::add_sql(const char *stmt_sql, size_t len,
                              bool need_shard_parse) {
  string *s;
  if (dataspace && dataspace->get_virtual_machine_id() > 0) {
    string tmp;
    if (len)
      tmp.assign(stmt_sql, len);
    else
      tmp.assign(stmt_sql);
    if (tmp.length() > 0) {
      if (need_shard_parse && !new_stmt_for_shard) {
        Parser *parser = MySQLParser::instance();
        new_stmt_for_shard = parser->parse(
            tmp.c_str(), plan->statement->get_allow_dot_in_ident(), true, NULL,
            NULL, NULL, handler->get_session()->get_client_charset_type());
      }
      string new_sql_tmp;
      len = 0;
      adjust_virtual_machine_schema(
          dataspace->get_virtual_machine_id(), dataspace->get_partition_id(),
          tmp.c_str(), plan->statement->get_schema(),
          need_shard_parse ? new_stmt_for_shard->get_stmt_node()
                           : plan->statement->get_latest_stmt_node(),
          plan->statement->get_record_scan_all_table_spaces_map(), new_sql_tmp);
      s = new string(new_sql_tmp.c_str());
    } else
      s = new string(stmt_sql);
  } else {
    if (len)
      s = new string(stmt_sql, len);
    else
      s = new string(stmt_sql);
  }
  LOG_DEBUG("Modify node %@ add sql [%s].\n", this, s->c_str());

  try {
    ACE_Guard<ACE_Thread_Mutex> guard(sql_mutex);
    sql_list.push_back(s);
    cond_sql.signal();
  } catch (...) {
    cond_sql.signal();
    throw;
  }
}

/* class MySQLModifyLimitNode */
MySQLModifyLimitNode::MySQLModifyLimitNode(ExecutePlan *plan, record_scan *rs,
                                           PartitionedTable *table,
                                           vector<unsigned int> &par_ids,
                                           unsigned int limit, string &sql)
    : MySQLInnerNode(plan) {
  this->name = "MySQLModifyLimitNode";
  this->sql = sql;
  this->par_ids = par_ids;
  this->rs = rs;
  this->limit = limit;
  this->table = table;
  total_affect_rows = 0;
  total_warning_rows = 0;
}

void MySQLModifyLimitNode::modify_limit_in_sql(string &sql,
                                               unsigned int limit_num) const {
  // generate the limit context
  char new_limit_num[128];
  size_t len = sprintf(new_limit_num, "%u", limit_num);

  unsigned int start_pos, end_pos;
  get_limit_start_and_end(sql, start_pos, end_pos);

  string new_sql;
  new_sql.append(sql.c_str(), STR_LEN_INDEX(0, start_pos - 1));
  new_sql.append(new_limit_num, len);
  new_sql.append(sql.c_str() + end_pos);

  sql.swap(new_sql);
}

/*
  sql:select xxx from xxx limit_100_________where ...;(‘_’ means space)
                                |  |
                            start  end
*/
void MySQLModifyLimitNode::get_limit_start_and_end(
    string &sql, unsigned int &start_pos, unsigned int &end_pos) const {
  start_pos = 0;
  end_pos = sql.length();

  unsigned int start_search_pos = rs->limit_pos + 5;
  for (unsigned int i = start_search_pos; i < sql.length(); i++) {
    if (isdigit(sql[i])) {
      start_pos = i;
      break;
    }
  }

  for (unsigned int i = start_pos; i < sql.length(); i++) {
    if (!isdigit(sql[i])) {
      end_pos = i;
      break;
    }
  }
  return;
}

void MySQLModifyLimitNode::execute() {
  LinkedHashMap modify_nodes;
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START: {
        create_modify_node(modify_nodes);
        init_row_map();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;
      }

      case EXECUTE_STATUS_FETCH_DATA: {
        generate_new_sql(modify_nodes);
        children_execute();
        if (node_can_swap && plan->plan_can_swap()) {
          handle_swap_connections();
          return;
        }
        status = EXECUTE_STATUS_WAIT;
      }

      case EXECUTE_STATUS_WAIT: {
        LOG_DEBUG("MySQLModifyLimitNode wait\n");
        try {
          wait_children();
        } catch (ErrorPacketException &e) {
          children_modify_end();
          LOG_DEBUG("MySQLModifyLimitNode wait error \n");
          status = EXECUTE_STATUS_COMPLETE;
          throw ErrorPacketException(
              "get an error packet during the modify node\n");
        }
        break;
      }

      case EXECUTE_STATUS_HANDLE: {
        LOG_DEBUG("MySQLModifyLimitNode handle\n");
        handle_children_affect_rows(modify_nodes);
        break;
      }

      case EXECUTE_STATUS_BEFORE_COMPLETE:
        rebuild_ok_packet();
        status = EXECUTE_STATUS_COMPLETE;

      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        LOG_DEBUG("MySQLModifyLimitNode ok\n");
#endif
        break;

      default:
        ACE_ASSERT(0);
        break;
    }
  }
}

void MySQLModifyLimitNode::generate_new_sql(LinkedHashMap &modify_nodes) {
  unsigned int each_limit_num = limit / modify_nodes.size();
  LinkedHashMap::iterator iter = modify_nodes.begin();
  for (size_t i = 0; iter != modify_nodes.end(); iter++, i++) {
    MySQLModifyNode *tmp = (*iter).first;
    string &exec_sql = modify_sqls[tmp->dataspace];
    if (i == modify_nodes.size() - 1) {
      unsigned int limit_num = each_limit_num + limit % modify_nodes.size();
      (*iter).second = limit_num;
      modify_limit_in_sql(exec_sql, limit_num);
    } else {
      (*iter).second = each_limit_num;
      modify_limit_in_sql(exec_sql, each_limit_num);
    }

    tmp->add_sql(exec_sql.c_str(), exec_sql.length());
  }
}

void MySQLModifyLimitNode::handle_children_affect_rows(
    LinkedHashMap &modify_nodes) {
  handle_response_packet(modify_nodes);
  if (!modify_nodes.empty()) {
    generate_new_sql(modify_nodes);
  } else
    children_modify_end();
  status = EXECUTE_STATUS_WAIT;
  return;
}

void MySQLModifyLimitNode::handle_response_packet(LinkedHashMap &modify_nodes) {
  LinkedHashMap::iterator map_iter = modify_nodes.begin();
  bool all_finish = true;
  for (; map_iter != modify_nodes.end(); map_iter++) {
    if ((*map_iter).second != 0) {
      all_finish = false;
      break;
    }
  }

  if (all_finish == true) {
    map_iter = modify_nodes.begin();
    for (; map_iter != modify_nodes.end();) {
      MySQLModifyNode *tmp = (MySQLModifyNode *)((map_iter++)->first);
      modify_nodes.erase(tmp);
      child_add_sql(tmp, "", 0);
    }
    return;
  } else {
    map<ExecuteNode *,
        AllocList<Packet *, Session *, StaticAllocator<Packet *> > *>::iterator
        iter;
    for (iter = row_map.begin(); iter != row_map.end(); iter++) {
      list<Packet *, StaticAllocator<Packet *> > *pack_list = (*iter).second;
      if (pack_list->empty()) {
        MySQLModifyNode *tmp = (MySQLModifyNode *)((*iter).first);
        child_add_sql(tmp, "", 0);
        continue;
      }
      Packet *packet = pack_list->front();
      pack_list->pop_front();
      if (driver->is_ok_packet(packet)) {
        MySQLOKResponse ok(packet);
        ok.unpack();
        uint16_t warnings = ok.get_warnings();
        if (warnings) {
          LOG_ERROR("Modify node %@ get OK packet with %u warnings.\n", this,
                    (unsigned int)warnings);
        }
        uint64_t affect_rows = ok.get_affected_rows();
        limit -= affect_rows;
        total_affect_rows += affect_rows;
        total_warning_rows += warnings;

        MySQLModifyNode *tmp = (MySQLModifyNode *)((*iter).first);
        unsigned int count;
        if (modify_nodes.find(tmp, count)) {
          if (affect_rows < (uint64_t)(count)) {
            modify_nodes.erase(tmp);
            child_add_sql(tmp, "", 0);
          }
        }
        delete packet;
      }
    }
  }
}

void MySQLModifyLimitNode::clean() {
  MySQLInnerNode::clean();
  while (!children.empty()) {
    MySQLExecuteNode *free_node = children.front();
    children.pop_front();
    free_node->clean();
    delete free_node;
  }
}

void MySQLModifyLimitNode::create_modify_node(LinkedHashMap &modify_nodes) {
  for (size_t i = 0; i < par_ids.size(); i++) {
    DataSpace *ds = table_get_par_dataspace(table, par_ids[i]);
    MySQLModifyNode *node = new MySQLModifyNode(plan, ds);
    modify_nodes.insert(node, 0);
    string adjust_sql = sql;
    table_sql_for_par(table, par_ids[i], adjust_sql);
    modify_sqls[ds] = adjust_sql;
    add_child(node);
  }
}

DataSpace *MySQLModifyLimitNode::table_get_par_dataspace(
    PartitionedTable *par_table, unsigned int id) const {
  return par_table->get_partition(id);
}

void MySQLModifyLimitNode::table_sql_for_par(PartitionedTable *par_table,
                                             unsigned int id,
                                             string &sql) const {
  ACE_UNUSED_ARG(id);
  sql = plan->statement->adjust_stmt_sql_for_shard(par_table, sql.c_str());
}

void MySQLModifyLimitNode::children_modify_end() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); it++) {
    child_add_sql((MySQLModifyNode *)(*it), "", 0);
  }
}

void MySQLModifyLimitNode::rebuild_ok_packet() {
  MySQLOKResponse ok(total_affect_rows, total_warning_rows);
  Packet *ok_packet = Backend::instance()->get_new_packet(row_packet_size);
  ok.pack(ok_packet);
  handler->deal_autocommit_with_ok_eof_packet(ok_packet);
  try {
    handler->send_to_client(ok_packet);
    delete ok_packet;
  } catch (exception &e) {
    delete ok_packet;
    throw e;
  }
}

/* class   MySQLNormalMergeNode */

MySQLNormalMergeNode::MySQLNormalMergeNode(ExecutePlan *plan,
                                           const char *modify_sql,
                                           vector<ExecuteNode *> *nodes)
    : MySQLInnerNode(plan),
      modify_sql(modify_sql),
      init_column_flag(false),
      select_complete_handled(false) {
  this->name = "MySQLNormalMergeNode";
  field_num = 0;
  if (nodes) {
    vector<ExecuteNode *>::iterator it = nodes->begin();
    for (; it != nodes->end(); it++) {
      this->add_select_child(*it);
    }
  }
  max_sql_len = MAX_PACKET_SIZE - PACKET_HEADER_SIZE - 4096;
  // this node don't need to control the RAM.
  plan->session->set_has_fetch_node(false);
  replace_set_value = false;
  generated_sql_length = insert_select_sql_size - PACKET_HEADER_SIZE;
}

void MySQLNormalMergeNode::clean() {
  if (!select_complete_handled && !session->is_in_explain())
    handle_select_complete();
  MySQLExecuteNode *free_node;
  Packet *packet = NULL;
  while (!select_node_children.empty()) {
    free_node = select_node_children.front();
    select_node_children.pop_front();

    // clean remaining rows
    if (row_map.count(free_node)) {
      while (!row_map[free_node]->empty()) {
        packet = row_map[free_node]->front();
        row_map[free_node]->pop_front();
        delete packet;
      }
      delete row_map[free_node];
      row_map[free_node] = NULL;
    }

    free_node->clean();
    delete free_node;
  }
  // we should ensure that MySQLNormalMergeNode::clean should handle the fetch
  // node first, and ensure the fetch node has free the resource, then handle
  // the clean of modify node.
  MySQLInnerNode::clean();

  while (!modify_node_children_for_explain_stmt.empty()) {
    free_node = modify_node_children_for_explain_stmt.front();
    modify_node_children_for_explain_stmt.pop_front();
    free_node->clean();
    delete free_node;
  }
}

void MySQLNormalMergeNode::select_node_children_execute() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = select_node_children.begin(); it != select_node_children.end();
       ++it) {
    (*it)->execute();
    (*it)->start_thread();
  }
  it = select_node_children.begin();
  if (strcmp((*it)->get_executenode_name(), "MySQLSparkNode")) {
    set_select_node_children_thread_status_start();
    plan->start_all_bthread();
  }
}

void MySQLNormalMergeNode::set_select_node_children_thread_status_start() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = select_node_children.begin(); it != select_node_children.end();
       ++it) {
    (*it)->set_thread_status_started();
  }
}

void MySQLNormalMergeNode::wait_children_select_node() {
  /*Check the modify children nodes's status, if they got error, stop the
   * processing.*/
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); it++) {
    if ((*it)->status == EXECUTE_STATUS_COMPLETE) {
      /*If the status is EXECUTE_STATUS_COMPLETE, this modify node must
       * goterror, so invoke notify_parent() to throw the exception.*/
      (*it)->notify_parent();
    }
  }
  if (plan->session->get_keep_conn()) {
#ifdef DEBUG
    if (max_mergenode_ready_rows_size && received_packet_num > 0)
      ACE_ASSERT(max_mergenode_buffer_rows > 0);
#endif
    if (max_mergenode_ready_rows_size &&
        max_mergenode_buffer_rows <= received_packet_num &&
        received_packet_num > 0) {
      LOG_ERROR(
          "MySQLNormalMergeNode in kept_conn state, receive [%d] packets, "
          "larger than config size [%d],"
          "plz config option 'max-mergenode-ready-rows-size'\n",
          received_packet_num, max_mergenode_ready_rows_size);
      throw Error(
          "MySQLNormalMergeNode receive packets larger than config size.");
    }
  }
  bool ready_flag = false;
  unsigned int finished_child = 0;
  while (!ready_flag) {
    finished_child = 0;
    for (it = select_node_children.begin(); it != select_node_children.end();
         ++it) {
      if ((*it)->notify_parent()) {
        ready_flag = true;
        status = EXECUTE_STATUS_SELECT_HANDLE;
      } else if ((*it)->is_finished()) {
        finished_child++;
      }
    }

    /* check if all chilren are finishd */
    if (!ready_flag && finished_child == select_node_children.size()) {
      status = EXECUTE_STATUS_SELECT_COMPLETE;
      break;
    }
  }
}

bool is_column_type_present_as_str(MySQLColumnType type) {
  switch (type) {
    case MYSQL_TYPE_TIMESTAMP:
    case MYSQL_TYPE_DATE:
    case MYSQL_TYPE_TIME:
    case MYSQL_TYPE_DATETIME:
    case MYSQL_TYPE_YEAR:
    case MYSQL_TYPE_NEWDATE:
    case MYSQL_TYPE_VARCHAR:
    case MYSQL_TYPE_ENUM:
    case MYSQL_TYPE_SET:
    case MYSQL_TYPE_TINY_BLOB:
    case MYSQL_TYPE_MEDIUM_BLOB:
    case MYSQL_TYPE_LONG_BLOB:
    case MYSQL_TYPE_BLOB:
    case MYSQL_TYPE_VAR_STRING:
    case MYSQL_TYPE_STRING:
    case MYSQL_TYPE_JSON:
      return true;
    default:
      return false;
  }
  return false;
}

void MySQLNormalMergeNode::init_column_as_str() {
  if (!init_column_flag) {
    list<Packet *> *field_packets =
        select_node_children.front()->get_field_packets();
    list<Packet *>::iterator it_field;
    for (it_field = field_packets->begin(); it_field != field_packets->end();
         it_field++) {
      MySQLColumnResponse col_resp(*it_field);
      col_resp.unpack();
      column_as_str.push_back(
          is_column_type_present_as_str(col_resp.get_column_type()));
    }

    LOG_DEBUG(
        "MySQLNormalMergeNode Init column to check as_str :"
        " columns = %d\n",
        column_as_str.size());
    init_column_flag = true;
  }
}

void MySQLNormalMergeNode::execute() {
  try {
    while (status != EXECUTE_STATUS_COMPLETE) {
      // 1. execute select_node_children, 2. wait_children select node
      // children, 3. assemble one modify sql, 4. children add modify sql, 5.
      // execute children;
      //
      // loop 1->5 until select_node_children all finish
      //
      // 6. wait_children children 7. return ok packets to parent node
      //
      // loop 6->7 until children all finish and all packets are passed

      switch (status) {
        case EXECUTE_STATUS_START: {
          list<MySQLExecuteNode *>::iterator it;
          init_row_map();
          for (it = select_node_children.begin();
               it != select_node_children.end(); ++it) {
            row_map[*it] = new AllocList<Packet *, Session *,
                                         StaticAllocator<Packet *> >();
            // TODO: add init value for row_map_size and
            // turn need_restrict_row_map_size on when need restrict row_map's
            // size.
            row_map[*it]->set_session(plan->session);
          }

          status = EXECUTE_STATUS_SELECT_FETCH_DATA;
          break;
        }
        case EXECUTE_STATUS_SELECT_FETCH_DATA: {
          select_node_children_execute();
          status = EXECUTE_STATUS_SELECT_WAIT;
        }
        case EXECUTE_STATUS_SELECT_WAIT: {
          try {
            wait_children_select_node();
          } catch (ExecuteNodeError &e) {
            children_modify_end();
            LOG_ERROR(
                "MySQLNormalMergeNode wait_children_select_node get error "
                "[%s]\n",
                e.what());
            string msg =
                "MySQLNormalMergeNode wait_children_select_node get error ";
            msg += e.what();
            record_migrate_error_message(plan, msg);
            status = EXECUTE_STATUS_COMPLETE;
            throw e;
          }
          break;
        }
        case EXECUTE_STATUS_SELECT_HANDLE: {
          // for update set, wait for all fetch node finished
          if (!replace_set_value) {
            init_column_as_str();
            handle_select_node_children();
          } else {
            for (auto it = select_node_children.begin();
                 it != select_node_children.end(); ++it) {
              if (row_map[*it]->size() > 1) {
                LOG_ERROR("Subquery returns more than 1 row\n");
                throw ExecuteNodeError("Subquery returns more than 1 row.");
              }
            }
          }
#ifndef DBSCALE_TEST_DISABLE
          /*just for test coverage*/
          Backend *bk = Backend::instance();
          dbscale_test_info *test_info = bk->get_dbscale_test_info();
          if (!strcasecmp(test_info->test_case_name.c_str(),
                          "insert_select_test") &&
              !strcasecmp(test_info->test_case_operation.c_str(),
                          "handle_select_get_error")) {
            throw Error("handle_select_get_error");
          }
#endif
          if (!replace_set_value) children_execute();
          status = EXECUTE_STATUS_SELECT_FETCH_DATA;
          break;
        }
        case EXECUTE_STATUS_SELECT_COMPLETE:
          LOG_DEBUG("Node %@ finish the select part.\n", this);
          if (replace_set_value) {
            init_column_as_str();
            handle_select_node_children();
            children_execute();
          }
          handle_select_complete();
          select_complete_handled = true;
          status = EXECUTE_STATUS_WAIT;
        case EXECUTE_STATUS_FETCH_DATA:
          status = EXECUTE_STATUS_WAIT;
        case EXECUTE_STATUS_WAIT:
          try {
            wait_children();
          } catch (ExecuteNodeError &e) {
            children_modify_end();
            LOG_ERROR("MySQLNormalMergeNode wait_children get error [%s]\n",
                      e.what());
            status = EXECUTE_STATUS_COMPLETE;
            throw e;
          }
          break;
        case EXECUTE_STATUS_HANDLE:
#ifdef DEBUG
          node_start_timing();
#endif
          handle_children();
#ifdef DEBUG
          node_end_timing();
#endif
          if (ready_rows->empty() && all_children_finished) {
            LOG_DEBUG(
                "Modify merge node read_rows is 0 and all children "
                "finished.\n");
            status = EXECUTE_STATUS_COMPLETE;
          } else {
            status = EXECUTE_STATUS_FETCH_DATA;
            LOG_DEBUG("Modify merge node gets ready rows %d.\n",
                      ready_rows->size());
            return;
          }
          break;
        case EXECUTE_STATUS_BEFORE_COMPLETE:
          status = EXECUTE_STATUS_COMPLETE;
        case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
          LOG_DEBUG("MySQLNormalMergeNode %@ cost %d ms\n", this,
                    node_cost_time);
#endif
          break;

        default:
          break;
      }
    }
  } catch (exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    LOG_ERROR("MySQLNormalMergeNode execute get error [%s]\n", e.what());
    string err("MySQLNormalMergeNode execute get error:");
    err.append(e.what());
    record_migrate_error_message(plan, err.c_str());
    throw;
  }
}

void MySQLNormalMergeNode::handle_children() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if (!row_map[*it]->empty()) {
      handle_child(*it);
    }
  }
}

void MySQLNormalMergeNode::handle_child(MySQLExecuteNode *child) {
  Packet *packet = NULL;
  while (!row_map[child]->empty()) {
    packet = row_map[child]->front();
    row_map[child]->pop_front();
    ready_rows->push_back(packet);
  }
}

Packet *MySQLNormalMergeNode::get_error_packet() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if ((*it)->get_error_packet()) {
      return (*it)->get_error_packet();
    }
  }

  for (it = select_node_children.begin(); it != select_node_children.end();
       ++it) {
    if ((*it)->get_error_packet()) {
      return (*it)->get_error_packet();
    }
  }

  return NULL;
}

void MySQLNormalMergeNode::rotate_sql(MySQLModifyNode *node, string *sql,
                                      string *added_sql) {
  child_add_sql(node, sql->c_str(), get_sql_length(sql));
  sql->clear();
  sql->append(modify_sql.c_str());
  if (added_sql) sql->append(added_sql->c_str());
}

void MySQLNormalMergeNode::append_field_to_sql(string *sql, int field_pos,
                                               MySQLRowResponse *row,
                                               bool handle_hex) {
  uint64_t field_length = (uint64_t)0;
  const char *field = NULL;
  bool is_field_as_str = false;
  bool is_null = false;
  is_null = row->field_is_null(field_pos);
  if (!is_null) {
    field = row->get_str(field_pos, &field_length);
    is_field_as_str = column_as_str[field_pos];
    add_str_before_field_value(sql, is_null);

    if (handle_hex) {
      sql->append("0x");
      sql->append(field, field_length);
    } else {
      if (is_field_as_str) {
        sql->append("'");
        string tmp_str(field, field_length);
        CharsetType ctype = session->get_client_charset_type();
        deal_with_str_column_value(&tmp_str, ctype);
        sql->append(tmp_str.c_str());
        sql->append("'");
      } else
        sql->append(field, field_length);
    }
  } else {
    add_str_before_field_value(sql, is_null);
    sql->append("NULL");
  }
}

/* class MySQLModifySelectNode */

MySQLModifySelectNode::MySQLModifySelectNode(
    ExecutePlan *plan, const char *modify_sql, vector<string *> *columns,
    vector<ExecuteNode *> *nodes, bool execute_quick, bool is_replace_set_value)
    : MySQLNormalMergeNode(plan, modify_sql, nodes),
      columns(columns),
      execute_quick(execute_quick) {
  this->name = "MySQLModifySelectNode";
  execute_sql.append(this->modify_sql.c_str());
  migrate_iter = children.begin();
  replace_set_value = is_replace_set_value;
}

void MySQLModifySelectNode::clean() {
  MySQLNormalMergeNode::clean();
  vector<string *>::iterator it = columns->begin();
  for (; it != columns->end(); it++) {
    delete *it;
  }
  columns->clear();
  delete columns;
}

void MySQLModifySelectNode::handle_select_node_child(MySQLExecuteNode *child,
                                                     string *sql) {
  LOG_DEBUG("UpdateSelectNode %@ start to assemble modify sql.\n", this);
  Packet *packet = NULL;

  if (replace_set_value) {
    string replace_value;
    if (column_as_str.size() > 1) {
      LOG_ERROR("Operand should contain 1 column\n");
      throw ExecuteNodeError("Operand should contain 1 column(s)");
    }
    if (row_map[child]->size() > 1) {
      LOG_ERROR("Subquery returns more than 1 row\n");
      throw ExecuteNodeError("Subquery returns more than 1 row.");
    } else if (row_map[child]->empty()) {
      replace_value = "= NULL";
    } else {
      packet = row_map[child]->front();
      row_map[child]->pop_front();
      MySQLRowResponse row(packet);
      bool is_null = row.field_is_null(0);
      if (is_null)
        replace_value = "= NULL";
      else
        append_field_to_sql(&replace_value, 0, &row);
      delete packet;
    }
    size_t pos = sql->find(DBSCALE_TMP_COLUMN_VALUE);
    if (pos != string::npos) {
      sql->replace(pos, strlen(DBSCALE_TMP_COLUMN_VALUE), replace_value);
    }
    return;
  }
  while (!row_map[child]->empty()) {
    packet = row_map[child]->front();
    row_map[child]->pop_front();
    MySQLRowResponse row(packet);
    unsigned int i = 0;
    string tmp(" ");
    tmp.append("(");
    tmp.append(columns->at(i)->c_str());
    append_field_to_sql(&tmp, i, &row);
    i++;
    for (; i < field_num; i++) {
      tmp.append(" and ");
      tmp.append(columns->at(i)->c_str());
      append_field_to_sql(&tmp, i, &row);
    }
    tmp.append(") OR");
    delete packet;
    if (!is_sql_len_valid(sql->length() + tmp.length())) {
      if (execute_quick && is_sql_len_valid(tmp.length()))
        rotate_modify_sql(sql, &tmp);
      else {
        LOG_ERROR(
            "Generated temporary SQL for %s is too large, dbscale unsupport"
            "currently.\n",
            plan->statement->get_sql());
        throw ExecuteNodeError(
            "Unsupport sql for dbscale, due to too large temporary SQL, \
                               please check the log for detail.");
      }
    } else
      sql->append(tmp.c_str());
  }
}

void MySQLModifySelectNode::rotate_modify_sql(string *sql, string *added_sql) {
  if (plan->get_migrate_tool()) {
    MySQLModifyNode *node = get_next_migrate_node();
    rotate_sql(node, sql, added_sql);
  } else {
    rotate_sql((MySQLModifyNode *)children.front(), sql, added_sql);
  }
}
MySQLModifyNode *MySQLModifySelectNode::get_next_migrate_node() {
  migrate_iter++;
  if (migrate_iter == children.end()) {
    migrate_iter = children.begin();
  }
  return (MySQLModifyNode *)*migrate_iter;
}

void MySQLModifySelectNode::handle_select_node_children() {
  if (!field_num) get_filed_num();

  list<MySQLExecuteNode *>::iterator it;
  for (it = select_node_children.begin(); it != select_node_children.end();
       ++it) {
    if (!row_map[*it]->empty() || replace_set_value)
      handle_select_node_child(*it, &execute_sql);
  }
  if (execute_quick && (execute_sql.size() > modify_sql.size())) {
    rotate_modify_sql(&execute_sql, NULL);
  }
  LOG_DEBUG("UpdateSelectNode %@ assemble sql [%s].\n", this,
            execute_sql.c_str());
}

/* class MySQLInsertSelectNode */

MySQLInsertSelectNode::MySQLInsertSelectNode(ExecutePlan *plan,
                                             const char *modify_sql,
                                             vector<ExecuteNode *> *nodes)
    : MySQLNormalMergeNode(plan, modify_sql, nodes) {
  this->name = "MySQLInsertSelectNode";
  modify_sql_exe.clear();
  modify_sql_exe.append(modify_sql);
  migrate_iter = children.begin();
  cross_join_max_rows = (uint64_t)(
      session->get_session_option("max_cross_join_moved_rows").ulong_val);
  select_row_num = 0;
}

void MySQLInsertSelectNode::handle_select_node_child(MySQLExecuteNode *child,
                                                     string *sql) {
  Packet *packet = NULL;
  select_row_num += row_map[child]->size();
  if (plan->statement->is_cross_node_join() &&
      select_row_num > cross_join_max_rows) {
    status = EXECUTE_STATUS_COMPLETE;
    LOG_ERROR(
        "Reach the max rows of [%u] for cross node join max moved rows.\n",
        select_row_num);
    throw ExecuteNodeError(
        "Reach the max row number of cross node join max moved rows.");
  }
  set<int> &hex_pos = plan->statement->get_hex_pos();
  while (!row_map[child]->empty()) {
    packet = row_map[child]->front();
    row_map[child]->pop_front();
    MySQLRowResponse row(packet);
    unsigned int i = 0;
    string tmp("");
    tmp.append("(");
    append_field_to_sql(&tmp, i, &row, hex_pos.count(i));
    i++;
    for (; i < field_num; i++) {
      tmp.append(",");
      append_field_to_sql(&tmp, i, &row, hex_pos.count(i));
    }
    tmp.append("),");
    delete packet;
    if (!is_sql_len_valid(sql->length() + tmp.length()))
      if (is_sql_len_valid(tmp.length())) {
        rotate_insert_sql(sql, &tmp);
      } else {
        LOG_ERROR(
            "Generated temporary SQL for %s is too large, dbscale unsupport"
            " currently.\n",
            plan->statement->get_sql());
        throw ExecuteNodeError(
            "Unsupport sql for dbscale,  due to too large temporary SQL, \
                               please check the log for detail.");
      }
    else
      sql->append(tmp.c_str());
  }
}

void MySQLInsertSelectNode::rotate_insert_sql(string *sql, string *added_sql) {
  if (plan->get_migrate_tool()) {
    MySQLModifyNode *node = get_next_migrate_node();
    rotate_sql(node, sql, added_sql);
  } else {
    rotate_sql((MySQLModifyNode *)children.front(), sql, added_sql);
  }
}
MySQLModifyNode *MySQLInsertSelectNode::get_next_migrate_node() {
  migrate_iter++;
  if (migrate_iter == children.end()) {
    migrate_iter = children.begin();
  }
  return (MySQLModifyNode *)*migrate_iter;
}
void MySQLInsertSelectNode::handle_select_node_children() {
  LOG_DEBUG("InsertSelectNode %@ start to assemble modify sql.\n", this);

  if (!field_num) get_filed_num();

  list<MySQLExecuteNode *>::iterator it;
  for (it = select_node_children.begin(); it != select_node_children.end();
       ++it) {
    if (!row_map[*it]->empty()) handle_select_node_child(*it, &modify_sql_exe);
  }
  if (is_sql_len_enough(modify_sql_exe.length()))
    rotate_insert_sql(&modify_sql_exe, NULL);
  LOG_DEBUG("InsertSelectNode %@ assemble sql [%s].\n", this,
            modify_sql_exe.c_str());
}

/* class MySQLPartitionMergeNode*/

MySQLPartitionMergeNode::MySQLPartitionMergeNode(ExecutePlan *plan,
                                                 const char *modify_sql,
                                                 vector<ExecuteNode *> *nodes,
                                                 PartitionedTable *par_table,
                                                 PartitionMethod *method)
    : MySQLNormalMergeNode(plan, modify_sql, nodes),
      par_table(par_table),
      method(method) {
  this->name = "MySQLPartitionMergeNode";
  if (plan->session->is_in_explain()) {
    unsigned int partition_num = par_table->get_real_partition_num();
    for (unsigned int i = 0; i < partition_num; i++) {
      DataSpace *space = par_table->get_partition(i);
      MySQLModifyNode *modify = new MySQLModifyNode(this->plan, space);
      add_modify_child_for_explain_stmt(modify);
    }
  }
}

unsigned int MySQLPartitionMergeNode::get_par_id_one_row(
    MySQLRowResponse *row) {
  vector<const char *> keys;
  map<unsigned int, string> key_value_string;
  uint64_t field_length = (uint64_t)0;
  const char *field = NULL;
  vector<unsigned int>::iterator it;
  for (it = key_pos_vec.begin(); it != key_pos_vec.end(); it++) {
    field = row->get_str(*it, &field_length);
    key_value_string[*it].clear();
    key_value_string[*it].append(field, field_length);
    keys.push_back(key_value_string[*it].c_str());
  }

  string sql_replace_char = plan->session->get_query_sql_replace_null_char();
  return method->get_partition_id(&keys, sql_replace_char);
}

unsigned int MySQLPartitionMergeNode::get_par_id_one_row(
    MySQLRowResponse *row, int auto_inc_key_pos, int64_t curr_auto_inc_val) {
  vector<const char *> keys;
  map<unsigned int, string> key_value_string;
  uint64_t field_length = (uint64_t)0;
  const char *field = NULL;
  vector<unsigned int>::iterator it;
  for (it = key_pos_vec.begin(); it != key_pos_vec.end(); it++) {
    if ((int)*it == auto_inc_key_pos || *it == field_num) {
      // *it == field_num means has auto_increment field but not specified in
      // column_list.
      char auto_inc_str_val[21];
      snprintf(auto_inc_str_val, sizeof(auto_inc_str_val), "%ld",
               curr_auto_inc_val);
      key_value_string[*it].clear();
      key_value_string[*it].append(auto_inc_str_val, strlen(auto_inc_str_val));
      keys.push_back(key_value_string[*it].c_str());
    } else {
      field = row->get_str(*it, &field_length);
      key_value_string[*it].clear();
      key_value_string[*it].append(field, field_length);
      keys.push_back(key_value_string[*it].c_str());
    }
  }

  string sql_replace_char = plan->session->get_query_sql_replace_null_char();
  return method->get_partition_id(&keys, sql_replace_char);
}

void MySQLPartitionMergeNode::children_add_sql() {
  map<unsigned int, MySQLModifyNode *>::iterator it;
  for (it = modify_node_map.begin(); it != modify_node_map.end(); it++) {
    unsigned int id = it->first;
    if (node_sql_map[id].length() > modify_sql.length()) {
      MySQLModifyNode *node = it->second;
      child_add_sql(node, node_sql_map[id].c_str(),
                    get_sql_length(&node_sql_map[id]));
      node_sql_map[id].clear();
      node_sql_map[id].append(modify_sql.c_str());
    }
  }
}

/* MySQLPartitionModifySelectNode */
MySQLPartitionModifySelectNode::MySQLPartitionModifySelectNode(
    ExecutePlan *plan, const char *modify_sql, vector<ExecuteNode *> *nodes,
    PartitionedTable *par_table, PartitionMethod *method,
    vector<string *> *columns, bool execute_quick)
    : MySQLPartitionMergeNode(plan, modify_sql, nodes, par_table, method),
      columns(columns),
      execute_quick(execute_quick) {
  this->name = "MySQLPartitionModifySelectNode";
  fulfill_key_pos_one_row(&key_pos_vec);
}

void MySQLPartitionModifySelectNode::handle_select_node_child(
    MySQLExecuteNode *child) {
  Packet *packet = NULL;
  unsigned int par_id;
  unsigned int i;
  string *sql;

  while (!row_map[child]->empty()) {
    packet = row_map[child]->front();
    row_map[child]->pop_front();
    MySQLRowResponse row(packet);
    par_id = get_par_id_one_row(&row);
    par_id = par_table->get_real_par_id_from_virtual_id(par_id);
    get_partiton_modify_node(par_id);
    sql = &node_sql_map[par_id];
    string tmp("");
    i = 0;
    tmp.append(" ");

    tmp.append("(");
    tmp.append(columns->at(i)->c_str());
    append_field_to_sql(&tmp, i, &row);
    i++;
    for (; i < field_num; i++) {
      tmp.append(" and ");
      tmp.append(columns->at(i)->c_str());
      append_field_to_sql(&tmp, i, &row);
    }
    tmp.append(") OR");
    delete packet;
    if (!is_sql_len_valid(sql->length() + tmp.length())) {
      if (execute_quick && is_sql_len_valid(tmp.length()))
        rotate_sql(modify_node_map[par_id], sql, &tmp);
      else {
        LOG_ERROR(
            "Generated temporary SQL for %s is too large, dbscale unsupport"
            " currently.\n",
            plan->statement->get_sql());
        throw ExecuteNodeError(
            "Unsupport sql for dbscale,  due to too large temporary SQL, \
                               please check the log for detail.");
      }
    } else
      sql->append(tmp.c_str());
  }
}

void MySQLPartitionModifySelectNode::handle_select_node_children() {
  if (!field_num) get_filed_num();

  list<MySQLExecuteNode *>::iterator it;
  for (it = select_node_children.begin(); it != select_node_children.end();
       ++it) {
    if (!row_map[*it]->empty()) handle_select_node_child(*it);
  }

  if (execute_quick) {
    children_add_sql();
  }
}

/* MySQLPartitionInsertSelectNode */
MySQLPartitionInsertSelectNode::MySQLPartitionInsertSelectNode(
    ExecutePlan *plan, const char *modify_sql, vector<ExecuteNode *> *nodes,
    PartitionedTable *par_table, PartitionMethod *method,
    vector<unsigned int> &key_pos, const char *schema_name,
    const char *table_name, bool is_duplicated)
    : MySQLPartitionMergeNode(plan, modify_sql, nodes, par_table, method),
      is_duplicated(is_duplicated) {
  this->name = "MySQLPartitionInsertSelectNode";
  key_pos_vec = key_pos;
  schema_name_insert.append(schema_name);
  table_name_insert.append(table_name);
  auto_increment_key_pos = plan->statement->get_auto_increment_key_pos();
  need_update_last_insert_id = false;
  need_auto_inc_lock = true;
  stmt_lock = NULL;

  has_init_duplicated_modify_node = false;
  cross_join_max_rows = (uint64_t)(
      session->get_session_option("max_cross_join_moved_rows").ulong_val);
  select_row_num = 0;
}

void MySQLPartitionInsertSelectNode::auto_inc_prepare() {
  if (plan->statement->get_auto_inc_status() != NO_AUTO_INC_FIELD) {
    stmt_lock = par_table->get_stmt_autoinc_lock(
        plan->statement->get_full_table_name());
    if (stmt_lock) stmt_lock->acquire();
    int enable_last_insert_id_session =
        session->get_session_option("enable_last_insert_id").int_val;
    if (enable_last_insert_id_session) {
      need_update_last_insert_id = true;
      plan->statement->set_old_last_insert_id(
          plan->handler->get_session()->get_last_insert_id());
    }
  }
  need_auto_inc_lock = false;
}

int64_t MySQLPartitionInsertSelectNode::get_auto_increment_value(
    MySQLRowResponse *row) {
  if (plan->statement->get_auto_inc_status() == AUTO_INC_NOT_SPECIFIED) {
    return plan->statement->get_auto_inc_value_multi_insert_row(
        par_table, plan, schema_name_insert.c_str(), table_name_insert.c_str(),
        NULL);
  } else {
    uint64_t field_length = 0;
    string field_string;
    if (row->field_is_null(auto_increment_key_pos)) {
      field_string.append("NULL");
    } else {
      const char *field_str =
          row->get_str(auto_increment_key_pos, &field_length);
      field_string.append(field_str, field_length);
    }
    StrExpression field_expr = StrExpression(field_string.c_str(), EXPR_STRING);
    return plan->statement->get_auto_inc_value_multi_insert_row(
        par_table, plan, schema_name_insert.c_str(), table_name_insert.c_str(),
        &field_expr);
  }
}

unsigned int MySQLPartitionInsertSelectNode::get_part_id(
    MySQLRowResponse *row) {
  unsigned int par_id = 0;
  AutoIncStatus auto_inc_status = plan->statement->get_auto_inc_status();
  if (auto_inc_status != NO_AUTO_INC_FIELD &&
      auto_inc_status != AUTO_INC_NO_NEED_MODIFY) {
    par_id = get_par_id_one_row(row, auto_increment_key_pos, curr_auto_inc_val);
  } else {
    par_id = get_par_id_one_row(row);
  }
  par_id = par_table->get_real_par_id_from_virtual_id(par_id);
  return par_id;
}

void MySQLPartitionInsertSelectNode::build_value_list_str(
    string &str, MySQLRowResponse *row) {
  unsigned int i = 0;
  str.append("(");
  AutoIncStatus auto_inc_status = plan->statement->get_auto_inc_status();
  for (; i < field_num; i++) {
    if (auto_inc_status == AUTO_INC_VALUE_NULL &&
        auto_increment_key_pos == (int)i) {
      char auto_inc_str_val[21];
      snprintf(auto_inc_str_val, sizeof(auto_inc_str_val), "%ld",
               curr_auto_inc_val);
      str.append(auto_inc_str_val);
    } else {
      append_field_to_sql(&str, i, row);
    }
    str.append(",");
  }
  if (auto_inc_status == AUTO_INC_NOT_SPECIFIED) {
    char auto_inc_str_val[21];
    snprintf(auto_inc_str_val, sizeof(auto_inc_str_val), "%ld",
             curr_auto_inc_val);
    str.append(auto_inc_str_val);
  } else {
    boost::erase_tail(str, 1);
  }
  str.append("),");
}

void MySQLPartitionInsertSelectNode::update_last_insert_id() {
  AutoIncStatus auto_inc_status = plan->statement->get_auto_inc_status();
  if (auto_inc_status == AUTO_INC_VALUE_NULL ||
      auto_inc_status == AUTO_INC_NOT_SPECIFIED) {
    plan->handler->get_session()->set_last_insert_id(curr_auto_inc_val);
    need_update_last_insert_id = false;
  }
}

void MySQLPartitionInsertSelectNode::handle_select_node_child(
    MySQLExecuteNode *child) {
  Packet *packet = NULL;
  unsigned int par_id;
  string *sql;
  select_row_num += row_map[child]->size();
  if (plan->statement->is_cross_node_join() &&
      select_row_num > cross_join_max_rows) {
    status = EXECUTE_STATUS_COMPLETE;
    LOG_ERROR(
        "Reach the max rows of [%u] for cross node join max moved rows.\n",
        select_row_num);
    throw ExecuteNodeError(
        "Reach the max row number of cross node join max moved rows.");
  }
  while (!row_map[child]->empty()) {
    packet = row_map[child]->front();
    row_map[child]->pop_front();
    MySQLRowResponse row(packet);
    string tmp;
    try {
      if (plan->statement->get_auto_inc_status() != NO_AUTO_INC_FIELD) {
        curr_auto_inc_val = get_auto_increment_value(&row);
      }
      par_id = get_part_id(&row);
      get_partiton_modify_node(par_id);
      sql = &node_sql_map[par_id];

      build_value_list_str(tmp, &row);
      delete packet;
    } catch (...) {
      delete packet;
      if (plan->statement->get_auto_inc_status() != NO_AUTO_INC_FIELD) {
        plan->statement->rollback_auto_inc_params(plan, par_table);
      }
      throw;
    }

    if (need_update_last_insert_id) update_last_insert_id();

    if (!is_sql_len_valid(sql->length() + tmp.length()))
      if (is_sql_len_valid(tmp.length()))
        rotate_sql(modify_node_map[par_id], sql, &tmp);
      else {
        LOG_ERROR(
            "Generated temporary SQL for %s is too large, dbscale unsupport"
            " currently.\n",
            plan->statement->get_sql());
        throw ExecuteNodeError(
            "Unsupport sql for dbscale,  due to too large temporary SQL, \
                               please check the log for detail.");
      }
    else
      sql->append(tmp.c_str());

    if (is_sql_len_enough(sql->length()))
      rotate_sql(modify_node_map[par_id], sql, NULL);
  }
}

void MySQLPartitionInsertSelectNode::handle_select_node_child_duplicated(
    MySQLExecuteNode *child) {
  if (!has_init_duplicated_modify_node) {
    unsigned int num = par_table->get_real_partition_num();
    for (unsigned int i = 0; i < num; i++) {
      DataSpace *space = par_table->get_partition(i);
      if (space->get_virtual_machine_id() > 0) continue;

      get_partiton_modify_node(i);
    }
    has_init_duplicated_modify_node = true;
  }
  Packet *packet = NULL;
  string *sql;
  select_row_num += row_map[child]->size();
  if (plan->statement->is_cross_node_join() &&
      select_row_num > cross_join_max_rows) {
    status = EXECUTE_STATUS_COMPLETE;
    LOG_ERROR(
        "Reach the max rows of [%u] for cross node join max moved rows.\n",
        select_row_num);
    throw ExecuteNodeError(
        "Reach the max row number of cross node join max moved rows.");
  }
  while (!row_map[child]->empty()) {
    packet = row_map[child]->front();
    row_map[child]->pop_front();
    MySQLRowResponse row(packet);
    string tmp;
    build_value_list_str(tmp, &row);
    try {
      map<unsigned int, MySQLModifyNode *>::iterator it =
          modify_node_map.begin();
      for (; it != modify_node_map.end(); it++) {
        sql = &node_sql_map[it->first];
        if (!is_sql_len_valid(sql->length() + tmp.length()))
          if (is_sql_len_valid(tmp.length()))
            rotate_sql(it->second, sql, &tmp);
          else {
            LOG_ERROR(
                "Generated temporary SQL for %s is too large, dbscale unsupport"
                " currently.\n",
                plan->statement->get_sql());
            throw ExecuteNodeError(
                "Unsupport sql for dbscale,  due to too large temporary SQL, \
                                   please check the log for detail.");
          }
        else
          sql->append(tmp.c_str());

        if (is_sql_len_enough(sql->length())) rotate_sql(it->second, sql, NULL);
      }
      delete packet;
    } catch (...) {
      delete packet;
      if (plan->statement->get_auto_inc_status() != NO_AUTO_INC_FIELD) {
        plan->statement->rollback_auto_inc_params(plan, par_table);
      }
      throw;
    }
  }
}

void MySQLPartitionInsertSelectNode::handle_select_node_children() {
  LOG_DEBUG("PartitionInsertSelectNode %@ start to assemble modify sql.\n",
            this);

  if (!field_num) get_filed_num();

  if (need_auto_inc_lock) auto_inc_prepare();

  list<MySQLExecuteNode *>::iterator it;
  for (it = select_node_children.begin(); it != select_node_children.end();
       ++it) {
    if (!row_map[*it]->empty()) {
      if (is_duplicated)
        handle_select_node_child_duplicated(*it);
      else
        handle_select_node_child(*it);
    }
  }
}

/* some global functions */

void handle_max_min(Packet *packet, Packet *field_packet,
                    vector<MySQLRowResponse *> *mysql_rows,
                    unsigned column_index, MySQLColumnType column_type,
                    bool is_max) {
  ACE_UNUSED_ARG(field_packet);
  list<MySQLExecuteNode *>::iterator it;
  MySQLRowResponse *max_row = NULL;
  MySQLRowResponse *row = NULL;
  unsigned int i, n;
  n = mysql_rows->size();
  for (i = 0; i < n; i++) {
    row = (*mysql_rows)[i];
    if (!row->field_is_null(column_index)) {
      max_row = row;
      break;
    }
  }

  if (max_row) {
    SortDesc sort_desc;
    sort_desc.column_index = column_index;
    sort_desc.sort_order = is_max ? ORDER_DESC : ORDER_ASC;
    sort_desc.is_cs = true;
    sort_desc.ctype = CHARSET_TYPE_OTHER;
    for (i++; i < n; i++) {
      row = (*mysql_rows)[i];
      if (!row->field_is_null(column_index)) {
        if (MySQLColumnCompare::compare(max_row, row, sort_desc, column_type) ==
            -1) {
          max_row = row;
        }
      }
    }
  } else {
    max_row = (*mysql_rows)[0];
  }

  char *max_data;
  uint64_t max_data_len;
  uint8_t max_header_len;
  max_row->set_current(column_index);
  max_data = max_row->get_current_data();
  max_data_len = max_row->get_current_length();
  max_header_len = max_row->get_current_header_length();
  max_row->move_next();

  if (packet->size() - (packet->wr_ptr() - packet->base()) <
      1 + max_header_len + max_data_len) {
    uint64_t resize_len =
        1 + max_header_len + max_data_len + packet->wr_ptr() - packet->base();
    LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
              packet->size(), resize_len);
    packet->size(resize_len);
  }
  packet->packdata(max_data - max_header_len, max_header_len + max_data_len);
}

void copy_column_to_packet(MySQLRowResponse *row, unsigned int column_index,
                           Packet *packet) {
  char *start_pos, *end_pos;
  row->set_current(column_index);
  start_pos = row->get_current_data() - row->get_current_header_length();
  end_pos = row->get_current_data() + row->get_current_length();
  if (packet->size() - (packet->wr_ptr() - packet->base()) <
      (unsigned int)(end_pos - start_pos + 1)) {
    uint64_t resize_len =
        end_pos - start_pos + 1 + packet->wr_ptr() - packet->base();
    LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
              packet->size(), resize_len);
    packet->size(resize_len);
  }
  packet->packdata(start_pos, end_pos - start_pos);
  row->move_next();
}

void handle_certain_max(Packet *packet, Packet *field_packet,
                        vector<MySQLRowResponse *> *mysql_rows,
                        unsigned column_index, MySQLColumnType column_type) {
  ACE_UNUSED_ARG(field_packet);
  list<MySQLExecuteNode *>::iterator it;
  MySQLRowResponse *max_row = NULL;
  MySQLRowResponse *row = NULL;
  unsigned int i, rows_count;
  rows_count = mysql_rows->size();
  for (i = 0; i < rows_count; i++) {
    row = (*mysql_rows)[i];
    if (!row->field_is_null(column_index)) {
      max_row = row;
      break;
    }
  }

  if (max_row) {
    SortDesc sort_desc;
    sort_desc.column_index = column_index;
    sort_desc.sort_order = ORDER_DESC;
    sort_desc.is_cs = true;
    sort_desc.ctype = CHARSET_TYPE_OTHER;
    for (i++; i < rows_count; i++) {
      row = (*mysql_rows)[i];
      if (!row->field_is_null(column_index)) {
        if (MySQLColumnCompare::compare(max_row, row, sort_desc, column_type) ==
            -1) {
          max_row = row;
        }
      }
    }
  } else {
    max_row = (*mysql_rows)[0];
  }
  char *data;
  char *max_data;
  uint64_t max_data_len;
  data = max_row->get_row_data();
  max_row->set_current(column_index);
  max_data = max_row->get_current_data();
  max_data_len = max_row->get_current_length();

  if (packet->size() < max_data - data + max_data_len) {
    uint64_t resize_len = 1 + max_data - data + max_data_len;
    LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
              packet->size(), resize_len);
    packet->size(resize_len);
  }
  packet->rewind();
  packet->wr_ptr(PACKET_HEADER_SIZE);
  for (unsigned int i = 0; i <= column_index; i++) {
    copy_column_to_packet(max_row, i, packet);
  }
}

void handle_max(Packet *packet, Packet *field_packet,
                vector<MySQLRowResponse *> *mysql_rows, unsigned column_index,
                MySQLColumnType column_type) {
  return handle_max_min(packet, field_packet, mysql_rows, column_index,
                        column_type, true);
}

void handle_min(Packet *packet, Packet *field_packet,
                vector<MySQLRowResponse *> *mysql_rows, unsigned column_index,
                MySQLColumnType column_type) {
  return handle_max_min(packet, field_packet, mysql_rows, column_index,
                        column_type, false);
}

void handle_sum(Packet *packet, Packet *field_packet,
                vector<MySQLRowResponse *> *mysql_rows, unsigned column_index,
                MySQLColumnType column_type) {
  ACE_UNUSED_ARG(column_type);
  mpf_t sum, tmp_mpf;
  mpf_init2(sum, DECIMAL_STORE_BIT);
  mpf_init2(tmp_mpf, DECIMAL_STORE_BIT);
  MySQLRowResponse *row;
  bool all_sum_is_null = true;
  unsigned int n = mysql_rows->size();
  for (unsigned int i = 0; i < n; i++) {
    row = (*mysql_rows)[i];
    if (!row->field_is_null(column_index)) {
      all_sum_is_null = false;
      row->get_mpf(column_index, tmp_mpf);
      mpf_add(sum, sum, tmp_mpf);
      char str2[200];
      size_t len = gmp_sprintf(str2, "%.Ff", tmp_mpf);
      str2[len] = '\0';
      char str3[200];
      len = gmp_sprintf(str3, "%.Ff", sum);
      str3[len] = '\0';
    }
  }
  if (!all_sum_is_null) {
    char str[200];
    size_t len = gmp_sprintf(str, "%.Ff", sum);
    if (len > 33) {
      if (mpf_sgn(sum) == 0) {
        len = sprintf(str, "%0.2f", 0.00);
      } else {
        if (field_packet) {
          MySQLColumnResponse col_resp(field_packet);
          col_resp.unpack();
          int scale = int(col_resp.get_decimals());
          char formator[10];
          sprintf(formator, "%%.%dFf", scale);
          len = gmp_sprintf(str, formator, sum);
        } else {
          len = gmp_sprintf(str, "%.30Ff", sum);
        }
        str[len] = '\0';
        if (!strcmp(str, "-0.000000000000000000000000000000") ||
            !strcmp(str, "0.000000000000000000000000000000"))
          len = sprintf(str, "%0.2f", 0.00);
      }
    }
    if (packet->size() - (packet->wr_ptr() - packet->base()) < 10 + len) {
      uint64_t resize_len = 10 + len + packet->wr_ptr() - packet->base();
      LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
                packet->size(), resize_len);
      packet->size(resize_len);
    }
    packet->pack_lenenc_int(len);
    packet->packdata(str, len);
  } else {
    char *start_pos, *end_pos;
    row = (*mysql_rows)[0];
    row->set_current(column_index);
    start_pos = row->get_current_data() - row->get_current_header_length();
    end_pos = row->get_current_data() + row->get_current_length();
    packet->packdata(start_pos, end_pos - start_pos);
    row->move_next();
  }
  mpf_clear(sum);
  mpf_clear(tmp_mpf);
}

void handle_count(Packet *packet, Packet *field_packet,
                  vector<MySQLRowResponse *> *mysql_rows, unsigned column_index,
                  MySQLColumnType column_type) {
  ACE_UNUSED_ARG(field_packet);
  ACE_UNUSED_ARG(column_type);
  uint64_t sum = 0;
  MySQLRowResponse *row;
  unsigned int n = mysql_rows->size();
  for (unsigned int i = 0; i < n; i++) {
    row = (*mysql_rows)[i];
    sum += row->get_uint(column_index);
  }

  char str[32];
  size_t len = sprintf(str, "%ld", sum);
  if (packet->size() - (packet->wr_ptr() - packet->base()) < 10 + len) {
    uint64_t resize_len = 10 + len + packet->wr_ptr() - packet->base();
    LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
              packet->size(), resize_len);
    packet->size(resize_len);
  }
  packet->pack_lenenc_int(len);
  packet->packdata(str, len);
}

void handle_normal(Packet *packet, Packet *field_packet,
                   vector<MySQLRowResponse *> *mysql_rows,
                   unsigned column_index, MySQLColumnType column_type) {
  ACE_UNUSED_ARG(field_packet);
  ACE_UNUSED_ARG(column_type);
  MySQLRowResponse *row;

  row = (*mysql_rows)[0];
  copy_column_to_packet(row, column_index, packet);
}

void init_column_handlers(unsigned int column_num,
                          list<AggregateDesc> &aggregate_desc,
                          vector<aggregate_handler> &column_handlers,
                          bool is_certain) {
  unsigned int i;
  list<AggregateDesc>::iterator it;
  for (i = 0; i < column_num; i++) {
    for (it = aggregate_desc.begin(); it != aggregate_desc.end(); it++) {
      if ((unsigned)it->column_index == i) {
        switch (it->type) {
          case AGGREGATE_TYPE_MAX:
            if (is_certain) {
              column_handlers.push_back(handle_certain_max);
              is_certain = false;
            } else
              column_handlers.push_back(handle_max);
            break;
          case AGGREGATE_TYPE_MIN:
            column_handlers.push_back(handle_min);
            break;
          case AGGREGATE_TYPE_COUNT:
            column_handlers.push_back(handle_count);
            break;
          case AGGREGATE_TYPE_SUM:
            column_handlers.push_back(handle_sum);
            break;
          default:
            break;
        }
        break;
      }
    }

    if (it == aggregate_desc.end()) column_handlers.push_back(handle_normal);
  }
}

/* class MySQLAggregateNode */

MySQLAggregateNode::MySQLAggregateNode(ExecutePlan *plan,
                                       list<AggregateDesc> &aggregate_desc)
    : MySQLInnerNode(plan) {
  this->name = "MySQLAggregateNode";
  this->aggregate_desc = aggregate_desc;
  status = EXECUTE_STATUS_START;
  node_can_swap = session->is_may_backend_exec_swap_able();
  result_packet = NULL;
}

void MySQLAggregateNode::init_mysql_rows() {
  max_row_size = 0;
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if (row_map[*it]->empty()) {
      continue;
    }
    MySQLRowResponse *row = new MySQLRowResponse(row_map[*it]->front());
    mysql_rows.push_back(row);
    if (max_row_size < row_map[*it]->front()->size()) {
      max_row_size = row_map[*it]->front()->size();
    }
  }
}

void MySQLAggregateNode::handle() {
  list<Packet *> *field_packets = get_field_packets();
  init_column_types(field_packets, column_types, column_num, NULL);
  init_column_handlers(column_num, aggregate_desc, column_handlers);
  init_mysql_rows();

  if (max_row_size >= (unsigned int)row_packet_size) {
    result_packet->size((unsigned int)(max_row_size * 1.2));
    LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
              row_packet_size, int(max_row_size * 1.2));
  } else if ((unsigned int)(row_packet_size * 0.9) < max_row_size) {
    result_packet->size((unsigned int)(row_packet_size * 1.2));
    LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
              row_packet_size, row_packet_size * 1.2);
  }

  list<Packet *>::iterator it_field = field_packets->begin();
  unsigned int i;
  for (i = 0; i < column_num; i++) {
    column_handlers[i](result_packet, *it_field, &mysql_rows, i,
                       column_types[i]);
    ++it_field;
  }

  // set the header of the result set
  size_t load_length = result_packet->length() - PACKET_HEADER_SIZE;
  pack_header(result_packet, load_length);

  ready_rows->push_back(result_packet);
  result_packet = NULL;
}

void MySQLAggregateNode::execute() {
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START:
        result_packet = Backend::instance()->get_new_packet(row_packet_size);
        result_packet->wr_ptr(PACKET_HEADER_SIZE);
        init_row_map();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        if (node_can_swap && plan->plan_can_swap()) {
          handle_swap_connections();
          return;
        }
        status = EXECUTE_STATUS_WAIT;

      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;

      case EXECUTE_STATUS_HANDLE:
#ifdef DEBUG
        node_start_timing();
#endif
        handle();
#ifdef DEBUG
        node_end_timing();
#endif
        status = EXECUTE_STATUS_FETCH_DATA;
        return;
        break;

      case EXECUTE_STATUS_BEFORE_COMPLETE:
        status = EXECUTE_STATUS_COMPLETE;

      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        LOG_DEBUG("MySQLAggregateNode %@ cost %d ms\n", this, node_cost_time);
#endif
        break;

      default:
        ACE_ASSERT(0);
        break;
    }
  }
}

void MySQLAggregateNode::do_clean() {
  MySQLRowResponse *row;
  while (!mysql_rows.empty()) {
    row = mysql_rows.back();
    mysql_rows.pop_back();
    delete row;
  }

  if (result_packet) {
    delete result_packet;
    result_packet = NULL;
  }
}

/* class MySQLWiseGroupNode */

MySQLWiseGroupNode::MySQLWiseGroupNode(ExecutePlan *plan,
                                       list<SortDesc> *group_desc,
                                       list<AggregateDesc> &aggregate_desc)
    : MySQLSortNode(plan, group_desc) {
  this->name = "MySQLWiseGroupNode";
  this->aggregate_desc = aggregate_desc;
  init_column_handler_flag = false;
  max_row_size = 0;
  group_size = 0;
  max_group_buffer_rows = 0;
  max_group_buffer_rows_size = max_wise_group_size * 1024;
}

void MySQLWiseGroupNode::handle_before_complete() {
  group_by();
  if (!group_rows.empty()) {
    Packet *new_row = merge_group();
    ready_rows->push_back(new_row);
  }
}

void MySQLWiseGroupNode::group_by() {
  Packet *packet;
  while (!ready_nodes->empty()) {
    packet = ready_nodes->front();
    if (add_to_group(packet)) {
      ready_nodes->pop_front();
      group_size++;
      if ((group_size & 0x03FF) == 0 || max_group_buffer_rows == 0) {
        max_group_buffer_rows =
            max_group_buffer_rows_size / packet->total_capacity();
        if (!max_group_buffer_rows) max_group_buffer_rows = 1000;
      }
      if (group_size > max_group_buffer_rows) {
        LOG_DEBUG("Wise group size is larger than max_wise_group_size.\n");
        throw Error("Wise group size is larger than max_wise_group_size.");
      }
    } else {
      group_size = 0;
      Packet *new_row = merge_group();
      ready_rows->push_back(new_row);
    }
  }
}

bool MySQLWiseGroupNode::add_to_group(Packet *packet) {
  if (group_rows.empty() ||
      MySQLColumnCompare::compare(group_rows[0]->get_packet(), packet,
                                  &sort_desc, &column_types) == 0) {
    MySQLRowResponse *row = new MySQLRowResponse(packet);
    group_rows.push_back(row);
    if (max_row_size < packet->size()) {
      max_row_size = packet->size();
    }
    return true;
  } else {
    return false;
  }
}

void MySQLWiseGroupNode::handle() {
  init_column_types(get_field_packets(), column_types, column_num,
                    &column_inited_flag);
  check_column_valid();
  if (!init_column_handler_flag) {
    init_column_handlers(column_num, aggregate_desc, column_handlers);
    init_column_handler_flag = true;
  }
  group_by();
}

Packet *MySQLWiseGroupNode::merge_group() {
  MySQLRowResponse *row;
  Packet *result_packet;

  if (group_rows.size() == 1) {
    result_packet = group_rows.back()->get_packet();
    row = group_rows.back();
    group_rows.pop_back();
    delete row;
  } else {
    result_packet = Backend::instance()->get_new_packet(row_packet_size);
    result_packet->wr_ptr(PACKET_HEADER_SIZE);
    if (max_row_size >= (unsigned int)row_packet_size) {
      LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
                row_packet_size, int(max_row_size * 1.2));
      result_packet->size((unsigned int)(max_row_size * 1.2));
    } else if ((unsigned int)(row_packet_size * 0.9) < max_row_size) {
      result_packet->size((unsigned int)(row_packet_size * 1.2));
      LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
                row_packet_size, int(row_packet_size * 1.2));
    }
    for (unsigned int i = 0; i < column_num; i++) {
      column_handlers[i](result_packet, NULL, &group_rows, i, column_types[i]);
    }

    size_t load_length = result_packet->length() - PACKET_HEADER_SIZE;
    pack_header(result_packet, load_length);

    while (!group_rows.empty()) {
      row = group_rows.back();
      group_rows.pop_back();
      delete row->get_packet();
      delete row;
    }
  }

  return result_packet;
}

void MySQLWiseGroupNode::execute() {
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START:
        init_row_map();
        ready_nodes = row_map[*(children.begin())];
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        if (node_can_swap && plan->plan_can_swap()) {
          handle_swap_connections();
          return;
        }
        status = EXECUTE_STATUS_WAIT;

      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;

      case EXECUTE_STATUS_HANDLE:
#ifdef DEBUG
        node_start_timing();
#endif
        handle();
#ifdef DEBUG
        node_end_timing();
#endif
        status = EXECUTE_STATUS_FETCH_DATA;
        if (!ready_rows->empty()) return;
        break;

      case EXECUTE_STATUS_BEFORE_COMPLETE:
        handle_before_complete();
        status = EXECUTE_STATUS_COMPLETE;

      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        LOG_DEBUG("MySQLWiseGroupNode %@ cost %d ms\n", this, node_cost_time);
#endif
        break;

      default:
        ACE_ASSERT(0);
        break;
    }
  }
}

void MySQLWiseGroupNode::do_clean() {
  MySQLRowResponse *row;
  while (!group_rows.empty()) {
    row = group_rows.back();
    group_rows.pop_back();
    delete row->get_packet();
    delete row;
  }
}

/* class MySQLPagesNode */
MySQLPagesNode::MySQLPagesNode(ExecutePlan *plan, list<SortDesc> *group_desc,
                               list<AggregateDesc> &aggregate_desc,
                               int page_size)
    : MySQLWiseGroupNode(plan, group_desc, aggregate_desc) {
  this->name = "MySQLPagesNode";
  this->page_size = page_size;
  rownum = 0;
  count_index = -1;
  list<AggregateDesc>::iterator it = aggregate_desc.begin();
  for (; it != aggregate_desc.end(); ++it) {
    if (it->type == AGGREGATE_TYPE_COUNT) count_index = it->column_index;
  }
}

void MySQLPagesNode::group_by() {
  Packet *packet;
  while (!ready_nodes->empty()) {
    packet = ready_nodes->front();
    ready_nodes->pop_front();
    ++rownum;

    MySQLRowResponse *row = new MySQLRowResponse(packet);
    group_rows.push_back(row);
    if (max_row_size < packet->size()) {
      max_row_size = packet->size();
    }
    ++group_size;
    if ((group_size & 0x03FF) == 0 || max_group_buffer_rows == 0) {
      max_group_buffer_rows =
          max_group_buffer_rows_size / packet->total_capacity();
      if (!max_group_buffer_rows) max_group_buffer_rows = 1000;
    }
    if (group_size > max_group_buffer_rows) {
      LOG_DEBUG("Wise group size is larger than max_wise_group_size.\n");
      throw Error("Wise group size is larger than max_wise_group_size.");
    }

    if (rownum % page_size == 0) {
      Packet *new_row = merge_group();
      ready_rows->push_back(new_row);
    }
  }
}

Packet *MySQLPagesNode::merge_group() {
  MySQLRowResponse *row;
  Packet *result_packet;

  result_packet = Backend::instance()->get_new_packet(row_packet_size);
  result_packet->wr_ptr(PACKET_HEADER_SIZE);
  if (max_row_size >= (unsigned int)row_packet_size) {
    LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
              row_packet_size, int(max_row_size * 1.2));
    result_packet->size((unsigned int)(max_row_size * 1.2));
  } else if ((unsigned int)(row_packet_size * 0.9) < max_row_size) {
    result_packet->size((unsigned int)(row_packet_size * 1.2));
    LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
              row_packet_size, int(row_packet_size * 1.2));
  }
  for (unsigned int i = 0; i < column_num; i++) {
    if (i == (unsigned int)count_index) {
      char str[32];
      size_t len = sprintf(str, "%ld", rownum);
      if (result_packet->size() -
              (result_packet->wr_ptr() - result_packet->base()) <
          10 + len) {
        uint64_t resize_len =
            10 + len + result_packet->wr_ptr() - result_packet->base();
        LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
                  result_packet->size(), resize_len);
        result_packet->size(resize_len);
      }
      result_packet->pack_lenenc_int(len);
      result_packet->packdata(str, len);
    } else {
      column_handlers[i](result_packet, NULL, &group_rows, i, column_types[i]);
    }
  }

  size_t load_length = result_packet->length() - PACKET_HEADER_SIZE;
  pack_header(result_packet, load_length);

  while (!group_rows.empty()) {
    row = group_rows.back();
    group_rows.pop_back();
    delete row->get_packet();
    delete row;
  }
  group_size = 0;

  return result_packet;
}

/* class MySQLGroupNode */

MySQLGroupNode::MySQLGroupNode(ExecutePlan *plan, list<SortDesc> *sort_desc,
                               list<AggregateDesc> &aggregate_desc)
    : MySQLSortNode(plan, sort_desc) {
  this->name = "MySQLGroupNode";
  this->aggregate_desc = aggregate_desc;
  init_column_handler_flag = false;
  this->sort_ready_nodes = &this->ready_nodes;
  max_row_size = 0;
  node_can_swap = session->is_may_backend_exec_swap_able();
}

bool MySQLGroupNode::add_to_group(Packet *packet) {
  if (group_rows.empty() ||
      MySQLColumnCompare::compare(group_rows[0]->get_packet(), packet,
                                  &sort_desc, &column_types) == 0) {
    MySQLRowResponse *row = new MySQLRowResponse(packet);
    group_rows.push_back(row);
    if (max_row_size < packet->size()) {
      max_row_size = packet->size();
    }
    return true;
  } else {
    return false;
  }
}

Packet *MySQLGroupNode::merge_group() {
  MySQLRowResponse *row;
  Packet *result_packet;
#ifdef DEBUG
  LOG_DEBUG("merge group with %d rows.\n", group_rows.size());
#endif
  if (group_rows.size() == 1) {
    result_packet = group_rows.back()->get_packet();
    row = group_rows.back();
    group_rows.pop_back();
    delete row;
  } else {
    result_packet = Backend::instance()->get_new_packet(row_packet_size);
    result_packet->wr_ptr(PACKET_HEADER_SIZE);
    if (max_row_size >= (unsigned int)row_packet_size) {
      LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
                row_packet_size, int(max_row_size * 1.2));
      result_packet->size((unsigned int)(max_row_size * 1.2));
    } else if ((unsigned int)(row_packet_size * 0.9) < max_row_size) {
      result_packet->size((unsigned int)(row_packet_size * 1.2));
      LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
                row_packet_size, int(row_packet_size * 1.2));
    }
    for (unsigned int i = 0; i < column_num; i++) {
      column_handlers[i](result_packet, NULL, &group_rows, i, column_types[i]);
    }

    // set the header of the result set
    size_t load_length = result_packet->length() - PACKET_HEADER_SIZE;
    pack_header(result_packet, load_length);

    while (!group_rows.empty()) {
      row = group_rows.back();
      group_rows.pop_back();
      delete row->get_packet();
      delete row;
    }
  }

  return result_packet;
}

void MySQLGroupNode::group_by() {
  Packet *packet;
  while (!ready_nodes.empty()) {
    packet = ready_nodes.front();

    if (add_to_group(packet)) {
      ready_nodes.pop_front();
    } else {
      Packet *new_row = merge_group();
      ready_rows->push_back(new_row);
    }
  }
}

void MySQLGroupNode::handle_before_complete() {
  last_sort();
  group_by();
  // deal with remaining ready_rows
  if (!group_rows.empty()) {
    Packet *new_row = merge_group();
    ready_rows->push_back(new_row);
  }
}

void MySQLGroupNode::handle() {
  init_column_types(get_field_packets(), column_types, column_num,
                    &column_inited_flag);
  adjust_column_index();
  check_column_valid();
  if (!init_column_handler_flag) {
    if (plan->statement->get_stmt_node()->execute_max_count_certain)
      init_column_handlers(column_num, aggregate_desc, column_handlers, true);
    else
      init_column_handlers(column_num, aggregate_desc, column_handlers);
    init_column_handler_flag = true;
  }
  sort();
  group_by();
}

void MySQLGroupNode::execute() {
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START:
        init_row_map();
        init_merge_sort_variables();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        if (node_can_swap && plan->plan_can_swap()) {
          handle_swap_connections();
          return;
        }
        status = EXECUTE_STATUS_WAIT;

      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;

      case EXECUTE_STATUS_HANDLE:
#ifdef DEBUG
        node_start_timing();
#endif
        handle();
#ifdef DEBUG
        node_end_timing();
#endif
        status = EXECUTE_STATUS_FETCH_DATA;
        if (!ready_rows->empty()) return;
        break;

      case EXECUTE_STATUS_BEFORE_COMPLETE:
        handle_before_complete();
        status = EXECUTE_STATUS_COMPLETE;

      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        LOG_DEBUG("MySQLGroupNode %@ cost %d ms\n", this, node_cost_time);
#endif
        break;

      default:
        ACE_ASSERT(0);
        break;
    }
  }
}

void MySQLGroupNode::do_clean() {
  MySQLRowResponse *row;
  while (!group_rows.empty()) {
    row = group_rows.back();
    group_rows.pop_back();
    delete row->get_packet();
    delete row;
  }

  Packet *packet;
  while (!ready_nodes.empty()) {
    packet = ready_nodes.back();
    ready_nodes.pop_back();
    if (packet) delete packet;
  }
}

/* class MySQLSingleSortNode */

MySQLSingleSortNode::MySQLSingleSortNode(ExecutePlan *plan,
                                         list<SortDesc> *sort_desc)
    : MySQLSortNode(plan, sort_desc) {
  this->name = "MySQLSingleSortNode";
  this->sort_ready_nodes = &this->ready_nodes;
  node_can_swap = session->is_may_backend_exec_swap_able();
  max_single_sort_buffer_rows = 0;
  max_single_sort_buffer_rows_size =
      session->get_session_option("max_single_sort_rows").ulong_val * 1024;
}

void MySQLSingleSortNode::init_heap() {
  list<MySQLExecuteNode *>::iterator it;

  /* dummy node */
  rows_heap.push_back(NULL);
  heap_size = 0;
}

void MySQLSingleSortNode::max_heapify(unsigned int i) {
  unsigned int left = i << 1;
  unsigned int right = left + 1;
  unsigned int largest;

  if (left <= heap_size &&
      MySQLColumnCompare::compare(rows_heap[left], rows_heap[i], &sort_desc,
                                  &column_types) == 1) {
    largest = left;
  } else {
    largest = i;
  }

  if (right <= heap_size &&
      MySQLColumnCompare::compare(rows_heap[right], rows_heap[largest],
                                  &sort_desc, &column_types) == 1) {
    largest = right;
  }

  if (i != largest) {
    Packet *packet;
    packet = rows_heap[i];
    rows_heap[i] = rows_heap[largest];
    rows_heap[largest] = packet;
    max_heapify(largest);
  }
}

void MySQLSingleSortNode::heap_insert(MySQLExecuteNode *node) {
  unsigned int i;
  Packet *packet;
  while (!row_map[node]->empty()) {
    i = ++heap_size;
    if ((heap_size & 0x03FF) == 0 || max_single_sort_buffer_rows == 0) {
      max_single_sort_buffer_rows = max_single_sort_buffer_rows_size /
                                    row_map[node]->front()->total_capacity();
      if (!max_single_sort_buffer_rows) max_single_sort_buffer_rows = 1000;
    }
    if (heap_size > max_single_sort_buffer_rows) {
      vector<Packet *>::iterator it = rows_heap.begin();
      for (; it != rows_heap.end(); it++) {
        delete (*it);
      }
      rows_heap.clear();

      LOG_ERROR(
          "Reach the max allowed single sort rows with row num %d, refuse this "
          "sql to protect dbscale.\n",
          heap_size);
      LOG_ERROR(
          "To solve this problem, you can change the sql from 'select * from "
          "tbl group by c1 order by c2' to 'select * from (select * from tbl "
          "group by c1) T order by c2'. In other words, move the group by part "
          "into a table subquery.\n");
      throw Error(
          "Reach the max allowed single sort rows. Please check manual and the "
          "error message in the log.");
    }
    rows_heap.push_back(row_map[node]->front());
    while (i > 1 &&
           MySQLColumnCompare::compare(rows_heap[i >> 1], rows_heap[i],
                                       &sort_desc, &column_types) == -1) {
      packet = rows_heap[i];
      rows_heap[i] = rows_heap[i >> 1];
      rows_heap[i >> 1] = packet;
      i = i >> 1;
    }

    row_map[node]->pop_front();
  }
}

void MySQLSingleSortNode::sort() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if (row_map[*it]->size()) {
      heap_insert(*it);
    }
  }
}

void MySQLSingleSortNode::handle_before_complete() {
  while (heap_size) {
    ready_rows->push_back(rows_heap[1]);
    rows_heap[1] = rows_heap[heap_size--];
    rows_heap.pop_back();
    max_heapify(1);
  }
}

void MySQLSingleSortNode::execute() {
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START:
        init_row_map();
        init_heap();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        if (node_can_swap && plan->plan_can_swap()) {
          handle_swap_connections();
          return;
        }
        status = EXECUTE_STATUS_WAIT;

      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;

      case EXECUTE_STATUS_HANDLE:
#ifdef DEBUG
        node_start_timing();
#endif
        init_column_types(get_field_packets(), column_types, column_num,
                          &column_inited_flag);
        adjust_column_index();
        check_column_valid();
        sort();
#ifdef DEBUG
        node_end_timing();
#endif
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_BEFORE_COMPLETE:
        handle_before_complete();
        status = EXECUTE_STATUS_COMPLETE;
        break;

      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        LOG_DEBUG("MySQLAggregateNode %@ cost %d ms\n", this, node_cost_time);
#endif
        break;

      default:
        ACE_ASSERT(0);
        break;
    }
  }
}

void MySQLSingleSortNode::do_clean() {
  Packet *packet;
  while (!ready_nodes.empty()) {
    packet = ready_nodes.back();
    ready_nodes.pop_back();
    if (packet) delete packet;
  }
}

/* class MySQLLoadLocalExternal */
MySQLLoadLocalExternal::MySQLLoadLocalExternal(ExecutePlan *plan,
                                               DataSpace *dataspace,
                                               DataServer *dataserver,
                                               const char *sql)
    : MySQLExecuteNode(plan, dataspace), dataserver(dataserver), sql(sql) {}

void MySQLLoadLocalExternal::execute() {
#ifdef DEBUG
  node_start_timing();
#endif

  int fifo_id = Backend::instance()->create_fifo();
  sprintf(fifo_name, "/tmp/tmp_fifo_%d", fifo_id);

  build_command();
  spawn_command();
  send_file_request_to_client();

  // feed data into FIFO
  Packet packet;
  size_t load_length = 0;
  bool before_file_close = false;
  try {
    do {
      packet.rewind();
      handler->receive_from_client(&packet);
      if (!driver->is_empty_packet(&packet)) {
        if (param.has_got_error() || param.get_external_load_flag() == FINISH) {
          throw ExecuteNodeError(
              "Got error when execute statement, check log for more "
              "information.");
        }
        load_length = packet.unpack3uint();
        LOG_DEBUG("MySQLLoadLocalExternal %@ get packet %@ from client.\n",
                  this, &packet);
        file.write(packet.base() + PACKET_HEADER_SIZE, load_length);
      }
    } while (!driver->is_empty_packet(&packet));
    before_file_close = true;
    file.close();
    ACE_Thread::join(t_handle);
  } catch (...) {
    status = EXECUTE_STATUS_COMPLETE;
    if (!before_file_close) {
      file.close();
      ACE_Thread::join(t_handle);
    }
    throw;
  }

  status = EXECUTE_STATUS_COMPLETE;
#ifdef DEBUG
  node_end_timing();
#endif

  if (param.has_got_error()) {
    LOG_ERROR("Got error when execute statement, %Q rows inserted.\n",
              param.insert_rows);
    throw ExecuteNodeError(
        "Got error when execute statement, check log for more information.");
  } else {
    send_ok_packet_to_client(handler, param.insert_rows, 0);
#ifdef DEBUG
    LOG_DEBUG("MySQLLoadLocalExternal %@ cost %d ms\n", this, node_cost_time);
#endif
  }
}

void MySQLLoadLocalExternal::send_file_request_to_client() {
  stmt_node *st = plan->statement->get_stmt_node();
  const char *filename = st->sql->load_oper->filename;
  Packet *res = Backend::instance()->get_new_packet(row_packet_size);
  res->wr_ptr(res->base() + PACKET_HEADER_SIZE);
  char load_flag;
  load_flag = 0xfb;
  res->packdata((const char *)&load_flag, 1);
  res->packdata(filename, strlen(filename));
  pack_header(res, strlen(filename) + 1);
  handler->send_to_client(res);
  delete res;
}

void MySQLLoadLocalExternal::build_command() {
  stmt_node *st = plan->statement->get_stmt_node();
  table_link *table = st->table_list_head;
  const char *field_terminate = st->sql->load_oper->field_terminate;
  const char *field_enclose = st->sql->load_oper->field_enclose;
  if (strlen(field_terminate) > 1 || strlen(field_enclose) > 1) {
    LOG_ERROR(
        "DBScale does not support FIELDS TERMINATED/ENCLOSED BY with multiple "
        "characters "
        "for external LOAD DATA.\n");
    throw NotImplementedError(
        "external LOAD DATA FIELDS TERMINATED/ENCLOSED BY has multiple "
        "characters");
  }
  const char *schema_name = table->join->schema_name ? table->join->schema_name
                                                     : session->get_schema();
  const char *table_name = table->join->table_name;
  if (!schema_name || schema_name[0] == '\0') {
    status = EXECUTE_STATUS_COMPLETE;
    throw ExecuteNodeError("No database selected.");
  }
  const char *remote_host = dataserver->get_host_direct();
  const char *remote_user = dataserver->get_remote_user();
  const char *remote_password = dataserver->get_remote_password();
  int remote_port = dataserver->get_remote_port();
  const char *external_load_script = dataserver->get_external_load_script();

  ExternalLoadRemoteExecute ep_ssh(external_load_script, schema_name,
                                   table_name, field_terminate, field_enclose,
                                   fifo_name, remote_host, remote_port,
                                   remote_user, remote_password);
  ep_ssh.build_full_command();
  load_local_external_cmd = ep_ssh.get_command();
  LOG_DEBUG("full command [%s].\n", load_local_external_cmd.c_str());
}

void MySQLLoadLocalExternal::spawn_command() {
  fd = popen(load_local_external_cmd.c_str(), "r");
  if (fd == NULL) {
    fprintf(stderr, "execute command failed");
    LOG_ERROR("Fail to execute command %s.\n", load_local_external_cmd.c_str());
    throw HandlerError("Fail to execute command.");
  }

  param.set_got_error(false);
  param.insert_rows = 0;
  param.fd = fd;
  param.set_external_load_flag(UNDEFINE);
  int ret =
      ACE_Thread::spawn((ACE_THR_FUNC)monitor_worker_via_fifo, &param,
                        THR_JOINABLE | THR_SCHED_DEFAULT, &t_id, &t_handle);
  if (ret == -1) {
    LOG_ERROR("Error when execute command %s.\n",
              load_local_external_cmd.c_str());
    throw HandlerError("Error when execute command.");
  }
  file.open(fifo_name);
  while (param.get_external_load_flag() == UNDEFINE) {
    LOG_DEBUG("Waiting for external load script report load file status.\n");
    ACE_OS::sleep(1);
  }
  if (param.get_external_load_flag() == FILE_NOT_OPEN) {
    file.close();
    ACE_Thread::join(t_handle);
    throw ExecuteNodeError(
        "Failed to execute statement, fail to open file, check log for more "
        "information.");
  }
}

void MySQLLoadLocalExternal::clean() {
  for (int i = 0; i < 3; i++) {  // delete fifo, try 3 times.
    if (!remove(fifo_name)) {    // return 0, delete fifo successful.
      break;
    }
    if (i == 2) {  // try 3 times but failed.
      LOG_ERROR("Error deleting fifo file.\n");
    }
  }
}

/* class MySQLLoadDataInfileExternal */

MySQLLoadDataInfileExternal::MySQLLoadDataInfileExternal(ExecutePlan *plan,
                                                         DataSpace *dataspace,
                                                         DataServer *dataserver)
    : MySQLExecuteNode(plan, dataspace), dataserver(dataserver) {}

void MySQLLoadDataInfileExternal::execute() {
#ifdef DEBUG
  node_start_timing();
#endif
  const char *remote_host = dataserver->get_host_direct();
  const char *remote_user = dataserver->get_remote_user();
  const char *remote_password = dataserver->get_remote_password();
  int remote_port = dataserver->get_remote_port();
  const char *external_load_script = dataserver->get_external_load_script();

  stmt_node *st = plan->statement->get_stmt_node();

  const char *file_name = st->sql->load_oper->filename;
  table_link *table = st->table_list_head;
  const char *schema_name = table->join->schema_name ? table->join->schema_name
                                                     : session->get_schema();
  const char *table_name = table->join->table_name;

  const char *field_terminate = st->sql->load_oper->field_terminate;
  const char *field_enclose = st->sql->load_oper->field_enclose;
  if (strlen(field_terminate) > 1 || strlen(field_enclose) > 1) {
    LOG_ERROR(
        "DBScale does not support FIELDS TERMINATED/ENCLOSED BY with multiple "
        "characters "
        "for external LOAD DATA.\n");
    throw NotImplementedError(
        "external LOAD DATA FIELDS TERMINATED/ENCLOSED BY has multiple "
        "characters");
  }

  if (!schema_name || schema_name[0] == '\0') {
    status = EXECUTE_STATUS_COMPLETE;
    throw ExecuteNodeError("No database selected.");
  }

  ExternalLoadResultHandler exp_handler;
  try {
    ExternalLoadRemoteExecute ep_ssh(external_load_script, schema_name,
                                     table_name, field_terminate, field_enclose,
                                     file_name, remote_host, remote_port,
                                     remote_user, remote_password);
    ep_ssh.set_result_handler(&exp_handler);
    ep_ssh.exec_remote_command();
  } catch (dbscale::sql::SQLError &e) {
    status = EXECUTE_STATUS_COMPLETE;
    throw ExecuteNodeError("Table is not exist for external load.");
  } catch (ExecuteFail &e) {
    status = EXECUTE_STATUS_COMPLETE;
    throw ExecuteNodeError(
        "Fail to execute LOAD DATA due to execution failure.");
  } catch (Exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    throw ExecuteNodeError(e.what());
  }

  const char *inserted_rows = exp_handler.get_inserted_rows();
  uint64_t affect_rows = atoll(inserted_rows);
  LOG_DEBUG("Get affect rows of GOS load %s %d.\n", inserted_rows, affect_rows);
  Packet ok_packet;
  MySQLOKResponse ok(affect_rows, 0);
  ok.pack(&ok_packet);
  handler->deal_autocommit_with_ok_eof_packet(&ok_packet);
  handler->record_affected_rows(&ok_packet);
  handler->send_to_client(&ok_packet);

  status = EXECUTE_STATUS_COMPLETE;
#ifdef DEBUG
  node_end_timing();
#endif
#ifdef DEBUG
  LOG_DEBUG("MySQLLoadDataInfileExternalNode %@ cost %d ms\n", this,
            node_cost_time);
#endif
}

/* class MySQLLoadLocalNode */

MySQLLoadLocalNode::MySQLLoadLocalNode(ExecutePlan *plan, DataSpace *dataspace,
                                       const char *sql)
    : MySQLExecuteNode(plan, dataspace), sql(sql) {
  this->name = "MySQLLoadLocalNode";
  load_data_node = plan->statement->get_stmt_node()->sql->load_oper;
  field_terminate = load_data_node->field_terminate;
  field_escape = load_data_node->field_escape[0];
  has_field_enclose = load_data_node->has_field_enclose;
  if (has_field_enclose) field_enclose = load_data_node->field_enclose[0];
  line_terminate = load_data_node->line_terminate;
  has_line_starting = load_data_node->has_line_starting;
  if (has_line_starting) {
    line_starting = load_data_node->line_starting;
  }
  warning_count = 0;
  warning_packet_list_size = 0;
  affected_row_count = 0;
  has_add_warning_packet = false;
  warning_packet_list = NULL;
}

void MySQLLoadLocalNode::execute() {
#ifdef DEBUG
  node_start_timing();
#endif
  Packet packet;
  MySQLQueryRequest query(sql);
  query.set_sql_replace_char(plan->session->get_query_sql_replace_null_char());
  Packet exec_query;
  query.pack(&exec_query);
  Connection *conn = NULL;
  try {
    conn = handler->send_to_server_retry(
        dataspace, &exec_query, session->get_schema(), session->is_read_only());
    conn->set_load(true);
    // since load data may execuate a long times.
    TimeValue timeout = TimeValue(UINT_MAX, 0);
    handler->receive_from_server(conn, &packet, &timeout);
    if (driver->is_error_packet(&packet)) {
      status = EXECUTE_STATUS_COMPLETE;
      handler->send_to_client(&packet);
      handler->put_back_connection(dataspace, conn);
      conn = NULL;
      session->set_cur_stmt_execute_fail(true);
      return;
    }
    MySQLResultSetHeaderResponse res(&packet);
    res.unpack();
    if (!res.is_columns_null_length()) {
      const char *errmsg = "Unexpected packet received for LOAD DATA response";
      LOG_ERROR("%s\n", errmsg);
      if (session->defined_lock_need_kept_conn(dataspace))
        session->remove_defined_lock_kept_conn(dataspace);
      handler->clean_dead_conn(&conn, dataspace);
      handler->send_to_client(&packet);
      status = EXECUTE_STATUS_COMPLETE;
      throw ExecuteNodeError(errmsg);
    }
    handler->send_to_client(&packet);
    unsigned long packet_num = 0;
    unsigned long load_once_packet_num =
        session->get_session_option("max_load_once_packet_num").ulong_val;
    do {
      packet.rewind();
      handler->receive_from_client(&packet);
      packet_num++;
      if (!driver->is_empty_packet(&packet)) {
        if (load_once_packet_num && packet_num >= load_once_packet_num) {
          size_t incomplete_row_len = check_first_row_complete(
              &packet, field_terminate, line_terminate, has_field_enclose,
              field_enclose, field_escape, 0);
          while (incomplete_row_len == packet.length()) {
            handler->send_to_server(conn, &packet);
            handler->receive_from_client(&packet);
            if (driver->is_empty_packet(&packet)) break;
            incomplete_row_len = check_first_row_complete(
                &packet, field_terminate, line_terminate, has_field_enclose,
                field_enclose, field_escape, 0);
          }
          if (!driver->is_empty_packet(&packet) && incomplete_row_len) {
            Packet *pf = NULL, *pr = NULL;
            divide_packet_by_first_row(&packet, incomplete_row_len, &pf, &pr);
            if (pf && pr) {
              handler->send_to_server(conn, pf);
              redo_load(conn, sql, handler, driver, affected_row_count,
                        warning_count, warning_packet_list_size,
                        &warning_packet_list, has_add_warning_packet);
              handler->send_to_server(conn, pr);
              packet_num = 0;
              delete pf;
              delete pr;
              pf = NULL;
              pf = NULL;
              continue;
            }
          }
        }
      }
      handler->send_to_server(conn, &packet);
    } while (!driver->is_empty_packet(&packet));
    packet.rewind();
    handler->receive_from_server(conn, &packet, &timeout);
    if (driver->is_ok_packet(&packet)) {
      MySQLOKResponse ok(&packet);
      ok.unpack();
      uint64_t warnings = ok.get_warnings();
      uint64_t affected_rows = ok.get_affected_rows();
      affected_row_count += affected_rows;
      warning_count += warnings;
      if (support_show_warning &&
          (warning_packet_list_size < MAX_LOAD_WARNING_PACKET_LIST_SIZE) &&
          warnings)
        store_warning_packet(conn, handler, driver, &warning_packet_list,
                             has_add_warning_packet, warning_packet_list_size);
      ok.set_affected_rows(affected_row_count);
      ok.set_warnings(warning_count);
      ok.pack(&packet);
    }
    handler->deal_autocommit_with_ok_eof_packet(&packet);
    handler->record_affected_rows(&packet);
    handler->send_to_client(&packet);
    if (warning_packet_list) {
      session->add_load_warning_packet_list(warning_packet_list, warning_count);
      warning_packet_list = NULL;
    }
    status = EXECUTE_STATUS_COMPLETE;
  } catch (...) {
    LOG_ERROR("got exception while handling LOAD DATA\n");
    if (session->defined_lock_need_kept_conn(dataspace))
      session->remove_defined_lock_kept_conn(dataspace);
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace);
    }
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  }
  handler->put_back_connection(dataspace, conn);
  session->record_xa_modified_conn(conn);
  conn = NULL;
#ifdef DEBUG
  node_end_timing();
#endif
#ifdef DEBUG
  LOG_DEBUG("MySQLLoadLocalNode %@ cost %d ms\n", this, node_cost_time);
#endif
}
/* class MySQLLoadDataInfile */

MySQLLoadDataInfile::MySQLLoadDataInfile()
    : max_size(1024L * 16L - 1), new_sql(NULL) {
  data_buffer = new char[max_size];
}
MySQLLoadDataInfile::~MySQLLoadDataInfile() { delete[] data_buffer; }
void MySQLLoadDataInfile::sql_add_local(ExecutePlan *plan, const char *sql) {
  new_sql = new string(sql);
  unsigned int pos =
      plan->statement->get_stmt_node()->sql->load_oper->infile_pos;
  new_sql->insert(pos - 1, "LOCAL ");
  LOG_DEBUG("The SQL [%s] has changed into [%s] in LoadDataInfile.\n", sql,
            new_sql->c_str());
}

void MySQLLoadDataInfile::build_error_packet(Packet &packet, const char *file) {
  string error_message = "Can't get stat of '";
  char buffer[PATH_MAX + 1];
  if (realpath(file, buffer)) {
    LOG_DEBUG("can't get the current working directory.\n");
    error_message = error_message + file;
  } else {
    error_message = error_message + buffer;
  }
  error_message = error_message + "' (Errcode: 2)";
  MySQLErrorResponse error_packet(13, error_message.c_str(), "HY000");
  error_packet.pack(&packet);
}
bool MySQLLoadDataInfile::open_local_file(const char *file_path) {
  filestr.open(file_path, ios::binary);
  if (!filestr)
    return false;
  else
    return true;
}
void MySQLLoadDataInfile::init_file_buf() {
  pbuf = filestr.rdbuf();
  size = pbuf->pubseekoff(0, ios::end, ios::in);
  size_remain = size;
  size_temp = 0;
  pbuf->pubseekpos(0, ios::in);
}
void MySQLLoadDataInfile::read_file_into_buf(Packet &buffer) {
  if (size_remain > max_size) {
    size_remain -= max_size;
    size_temp = max_size;
  } else if (size_remain <= max_size && size_remain > 0) {
    size_temp = size_remain;
    size_remain = 0;
  } else {
    size_temp = 0;
  }
  buffer.size(size_temp + PACKET_HEADER_SIZE + 1);
  try {
    filestr.read(buffer.base() + PACKET_HEADER_SIZE, size_temp);
  } catch (...) {
    throw;
  }
}
void MySQLLoadDataInfile::init_file_buf_for_load_insert() {
  pbuf = filestr.rdbuf();
  pbuf->pubseekpos(0, ios::in);
  size_temp = 0;
  size_remain = 1;
}
void MySQLLoadDataInfile::read_file_into_buf_for_load_insert(Packet &buffer) {
  size_temp = 0;
  char *cbuffer = data_buffer;
  int pos = pbuf->sgetc();
  while (pos != EOF) {
    char ch = pbuf->sgetc();
    cbuffer[size_temp] = ch;
    size_temp++;
    if (size_temp == max_size) {
      pbuf->sbumpc();
      break;
    }
    pos = pbuf->snextc();
  }

  buffer.size(size_temp + PACKET_HEADER_SIZE + 1);

  try {
    memcpy(buffer.base() + PACKET_HEADER_SIZE, cbuffer, size_temp);
  } catch (...) {
    throw;
  }
}
/* class MySQLLoadDataInfileNode */

MySQLLoadDataInfileNode::MySQLLoadDataInfileNode(ExecutePlan *plan,
                                                 DataSpace *dataspace,
                                                 const char *sql)
    : MySQLExecuteNode(plan, dataspace), sql(sql) {
  this->name = "MySQLLoadDataInfileNode";
  load_data_node = plan->statement->get_stmt_node()->sql->load_oper;
  field_terminate = load_data_node->field_terminate;
  field_escape = load_data_node->field_escape[0];
  has_field_enclose = load_data_node->has_field_enclose;
  if (has_field_enclose) field_enclose = load_data_node->field_enclose[0];
  line_terminate = load_data_node->line_terminate;
  has_line_starting = load_data_node->has_line_starting;
  if (has_line_starting) {
    line_starting = load_data_node->line_starting;
  }
  warning_count = 0;
  warning_packet_list_size = 0;
  affected_row_count = 0;
  has_add_warning_packet = false;
  warning_packet_list = NULL;
  error_packet = NULL;
}

void MySQLLoadDataInfileNode::execute() {
#ifdef DEBUG
  node_start_timing();
#endif
  Packet *packet = Backend::instance()->get_new_packet();
  Packet buffer;
  sql_add_local(plan, sql);
  MySQLQueryRequest query(new_sql->c_str());
  query.set_sql_replace_char(plan->session->get_query_sql_replace_null_char());
  Packet exec_query;
  query.pack(&exec_query);
  Connection *conn = NULL;
  // since this may execuate a long time, use UINT_MAX
  // to avoid max_conn_execute_time
  TimeValue timeout = TimeValue(UINT_MAX, 0);
  try {
    const char *file_path =
        plan->statement->get_stmt_node()->sql->load_oper->filename;
    if (!open_local_file(file_path)) {
      build_error_packet(*packet, file_path);
      status = EXECUTE_STATUS_COMPLETE;
      if (!plan->statement->is_load_insert_select()) {
        handler->send_to_client(packet);
        delete packet;
        session->set_cur_stmt_execute_fail(true);
        session->acquire_has_send_client_error_packet_mutex();
        session->set_has_send_client_error_packet();
        session->release_has_send_client_error_packet_mutex();
      } else {
        session->set_result_packet(packet);
      }
      throw ExecuteNodeError("open local file failed");
    }
    conn = handler->send_to_server_retry(
        dataspace, &exec_query, session->get_schema(), session->is_read_only());
    conn->set_load(true);
    handler->receive_from_server(conn, packet, &timeout);
    if (driver->is_error_packet(packet)) {
      status = EXECUTE_STATUS_COMPLETE;
      MySQLErrorResponse error(packet);
      error.unpack();
      char err_message[1000];
      sprintf(err_message, "%d (%s) %s", error.get_error_code(),
              error.get_sqlstate(), error.get_error_message());
      if (!plan->statement->is_load_insert_select()) {
        error_packet = packet;
      } else {
        session->set_result_packet(packet);
      }
      handler->put_back_connection(dataspace, conn);
      session->set_cur_stmt_execute_fail(true);
      throw ErrorPacketException((const char *)err_message);
    }
    MySQLResultSetHeaderResponse res(packet);
    res.unpack();
    if (!res.is_columns_null_length()) {
      const char *errmsg =
          "Unexpected packet received for non local LOAD DATA response";
      LOG_ERROR("%s\n", errmsg);
      if (session->defined_lock_need_kept_conn(dataspace))
        session->remove_defined_lock_kept_conn(dataspace);
      handler->clean_dead_conn(&conn, dataspace);
      if (!plan->statement->is_load_insert_select()) {
        handler->send_to_client(packet);
        delete packet;
      } else {
        session->set_result_packet(packet);
      }
      status = EXECUTE_STATUS_COMPLETE;
      throw ExecuteNodeError(errmsg);
    }
    if (plan->statement->is_load_insert_select())
      init_file_buf_for_load_insert();
    else
      init_file_buf();
    unsigned long packet_num = 0;
    unsigned long load_once_packet_num =
        session->get_session_option("max_load_once_packet_num").ulong_val;
    do {
      try {
        if (plan->statement->is_load_insert_select())
          read_file_into_buf_for_load_insert(buffer);
        else
          read_file_into_buf(buffer);
      } catch (...) {
        LOG_ERROR("file read error in load data infile\n");
        throw;
      }
      pack_header(&buffer, size_temp);
      packet_num++;
      if (!driver->is_empty_packet(&buffer)) {
        if (load_once_packet_num && packet_num >= load_once_packet_num) {
          size_t incomplete_row_len = check_first_row_complete(
              &buffer, field_terminate, line_terminate, has_field_enclose,
              field_enclose, field_escape, 0);
          while (incomplete_row_len == buffer.length()) {
            handler->send_to_server(conn, &buffer);
            try {
              if (plan->statement->is_load_insert_select())
                read_file_into_buf_for_load_insert(buffer);
              else
                read_file_into_buf(buffer);
            } catch (...) {
              LOG_ERROR("file read error in load data infile\n");
              throw;
            }
            pack_header(&buffer, size_temp);
            if (driver->is_empty_packet(&buffer)) break;
            incomplete_row_len = check_first_row_complete(
                &buffer, field_terminate, line_terminate, has_field_enclose,
                field_enclose, field_escape, 0);
          }
          if (!driver->is_empty_packet(&buffer) && incomplete_row_len) {
            Packet *pf = NULL, *pr = NULL;
            divide_packet_by_first_row(&buffer, incomplete_row_len, &pf, &pr);
            if (pf && pr) {
              handler->send_to_server(conn, pf);
              redo_load(conn, *new_sql, handler, driver, affected_row_count,
                        warning_count, warning_packet_list_size,
                        &warning_packet_list, has_add_warning_packet);
              handler->send_to_server(conn, pr);
              packet_num = 0;
              delete pf;
              delete pr;
              pf = NULL;
              pr = NULL;
              continue;
            }
          }
        }
      }
      handler->send_to_server(conn, &buffer);
    } while (!driver->is_empty_packet(&buffer));
    handler->receive_from_server(conn, packet, &timeout);
    if (driver->is_ok_packet(packet)) {
      MySQLOKResponse ok(packet);
      ok.unpack();
      uint64_t warnings = ok.get_warnings();
      uint64_t affected_rows = ok.get_affected_rows();
      affected_row_count += affected_rows;
      warning_count += warnings;
      if (support_show_warning &&
          (warning_packet_list_size < MAX_LOAD_WARNING_PACKET_LIST_SIZE) &&
          warnings)
        store_warning_packet(conn, handler, driver, &warning_packet_list,
                             has_add_warning_packet, warning_packet_list_size);
      ok.set_affected_rows(affected_row_count);
      ok.set_warnings(warning_count);
      ok.pack(packet);
      handler->record_affected_rows(packet);
      handler->deal_autocommit_with_ok_eof_packet(packet);
    }
    if (!plan->statement->is_load_insert_select()) {
      handler->send_to_client(packet);
    } else {
      session->set_result_packet(packet);
    }
    if (warning_packet_list) {
      session->add_load_warning_packet_list(warning_packet_list, warning_count);
      warning_packet_list = NULL;
    }
    if (!plan->statement->is_load_insert_select()) delete packet;
    status = EXECUTE_STATUS_COMPLETE;
    session->record_xa_modified_conn(conn);
  } catch (...) {
    LOG_ERROR("got exception while handling LOAD DATA\n");
    if (session->defined_lock_need_kept_conn(dataspace))
      session->remove_defined_lock_kept_conn(dataspace);
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace);
    }
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  }
  handler->put_back_connection(dataspace, conn);
  conn = NULL;
#ifdef DEBUG
  node_end_timing();
#endif
#ifdef DEBUG
  LOG_DEBUG("MySQLLOadDataInfileNode %@ cost %d ms\n", this, node_cost_time);
#endif
}
/* class MySQLLoadLocalPartitionNode */

MySQLLoadLocalPartitionNode::MySQLLoadLocalPartitionNode(
    ExecutePlan *plan, MySQLExecuteNode *table_node, DataSpace *dataspace,
    const char *sql)
    : MySQLExecuteNode(plan, dataspace),
      step(SENDING_QUERY),
      table_node(table_node),
      sql(sql),
      full_cond(full_lock) {
  this->MySQLExecuteNode::name = "MySQLLoadLocalPartitionNode";
  ready_packets = &ready_packets2;
  using_packets = &ready_packets1;
  ready_packet_size = 0;
  parent_is_waiting = false;
  child_is_waiting = false;
  is_stop = false;
  table_node_has_get_empty_packet = false;
  ready_to_finish = false;
  task_is_finish = false;
  max_ready_packets_local = max_load_ready_packets;
  has_add_warning_packet = false;
  warning_count = 0;
  warning_packet_list_size = 0;
  affected_row_count = 0;
  warning_packet_list = NULL;
}

void MySQLLoadLocalPartitionNode::execute() {
  const char *set_sql = "SET @@NET_READ_TIMEOUT = 31536000";
  MySQLQueryRequest set_query(set_sql);
  set_query.set_sql_replace_char(
      plan->session->get_query_sql_replace_null_char());
  Packet exec_query;
  set_query.pack(&exec_query);
  Connection *conn = NULL;
  Packet *packet = NULL;
  try {
    conn = handler->send_to_server_retry(
        dataspace, &exec_query, session->get_schema(), session->is_read_only());
    conn->set_load(true);
    handler->receive_from_server(conn, &result);
    if (driver->is_error_packet(&result)) {
      MySQLErrorResponse error(&result);
      error.unpack();
      char err_message[1000];
      sprintf(err_message, "%d (%s) %s", error.get_error_code(),
              error.get_sqlstate(), error.get_error_message());

      LOG_ERROR(
          "send SET @@NET_READ_TIMEOUT before LOAD DATA query to partition "
          "return error packet  [%s]\n",
          err_message);

      handler->put_back_connection(dataspace, conn);
      conn = NULL;
      throw ErrorPacketException((const char *)err_message);
    }
    conn->reset();

    const char *used_sql = sql;
    string new_sql_tmp;
    if (dataspace->get_virtual_machine_id()) {
      Parser *parser = MySQLParser::instance();
      Statement *new_stmt_for_shard = parser->parse(
          sql, plan->statement->get_allow_dot_in_ident(), true, NULL, NULL,
          NULL, handler->get_session()->get_client_charset_type());

      adjust_virtual_machine_schema(
          dataspace->get_virtual_machine_id(), dataspace->get_partition_id(),
          sql, plan->statement->get_schema(),
          new_stmt_for_shard->get_stmt_node(),
          plan->statement->get_record_scan_all_table_spaces_map(), new_sql_tmp);
      used_sql = new_sql_tmp.c_str();
      new_stmt_for_shard->free_resource();
      delete new_stmt_for_shard;
      new_stmt_for_shard = NULL;
    }
    LOG_DEBUG("warning used sql: [%s]\n", used_sql);

    MySQLQueryRequest query(used_sql);
    query.set_sql_replace_char(
        plan->session->get_query_sql_replace_null_char());
    query.pack(&exec_query);
    handler->send_to_server(conn, &exec_query);
    handler->receive_from_server(conn, &result);
    if (driver->is_error_packet(&result)) {
      MySQLErrorResponse error(&result);
      error.unpack();
      char err_message[1000];
      sprintf(err_message, "%d (%s) %s", error.get_error_code(),
              error.get_sqlstate(), error.get_error_message());

      LOG_ERROR("send LOAD DATA query to partition return error packet [%s]\n",
                err_message);
      handler->put_back_connection(dataspace, conn);
      conn = NULL;
      throw ErrorPacketException((const char *)err_message);
    }
    MySQLResultSetHeaderResponse res(&result);
    res.unpack();
    if (!res.is_columns_null_length()) {
      const char *errmsg = "Unexpected packet received for LOAD DATA response";
      LOG_ERROR("%s\n", errmsg);
      handler->clean_dead_conn(&conn, dataspace);

      throw ExecuteNodeError(errmsg);
    }
    step = REQUESTING_FILE;
    notify_parent();
#ifndef DBSCALE_TEST_DISABLE
    int loop_count = 0;
#endif
    unsigned long packet_num = 0;
    for (;;) {
      packet = get_packet_from_ready_list();
      if (!packet) {
        LOG_WARN(
            "The load data process is stopped for sql %s for partition load "
            "node %@.\n",
            sql, this);
        // TODO: handle the conn more suitable in issue #3499
        handler->clean_dead_conn(&conn, dataspace);
#ifdef DEBUG
        node_end_timing();
#endif
        return;
      }

#ifndef DBSCALE_TEST_DISABLE
      loop_count++;
      if (loop_count == 2) {
        Backend *bk = Backend::instance();
        dbscale_test_info *test_info = bk->get_dbscale_test_info();
        if (test_info->test_case_name.length() &&
            !strcasecmp(test_info->test_case_name.c_str(), "load_select") &&
            !strcasecmp(test_info->test_case_operation.c_str(),
                        "load_select_partition_fail")) {
          throw Exception("dbscale test fail.");
        }
      }
#endif

#ifdef DEBUG
      node_start_timing();
#endif
      // The mblk should be a packet of file data
      handler->send_to_server(conn, packet);
      if (driver->is_empty_packet(packet)) {
        LOG_DEBUG("Partition node %@ get the empty packet.\n", this);
        // Got empty packet, no more data to send, waiting for response from
        // server
        handler->receive_from_server(conn, &result);
        if (driver->is_error_packet(&result)) {
          MySQLErrorResponse error(&result);
          error.unpack();
          LOG_ERROR("LOAD DATA on partition return error packet: %d (%s) %s\n",
                    error.get_error_code(), error.get_sqlstate(),
                    error.get_error_message());
          delete packet;
          packet = NULL;
          handler->put_back_connection(dataspace, conn);
          conn = NULL;
          char err_message[1000];
          sprintf(err_message, "%d (%s) %s", error.get_error_code(),
                  error.get_sqlstate(), error.get_error_message());
          throw ErrorPacketException((const char *)err_message);
        }
        MySQLOKResponse ok(&result);
        ok.unpack();
        uint64_t warnings = ok.get_warnings();
        uint64_t affected_rows = ok.get_affected_rows();
        affected_row_count += affected_rows;
        warning_count += warnings;
        step = FETCHED_RESULT;
        delete packet;
        packet = NULL;
        if (support_show_warning &&
            (warning_packet_list_size < MAX_LOAD_WARNING_PACKET_LIST_SIZE) &&
            warnings)
          store_warning_packet(conn, handler, driver, &warning_packet_list,
                               has_add_warning_packet,
                               warning_packet_list_size);
        break;
      }
      delete packet;
      packet = NULL;
#ifdef DEBUG
      node_end_timing();
#endif
      notify_parent();
      packet_num++;
      unsigned long load_once_packet_num =
          session->get_session_option("max_load_once_packet_num").ulong_val;
      if (load_once_packet_num && packet_num >= load_once_packet_num) {
        redo_load(conn, used_sql, handler, driver, affected_row_count,
                  warning_count, warning_packet_list_size, &warning_packet_list,
                  has_add_warning_packet);
        packet_num = 0;
      }
    }
  } catch (Exception &e) {
    LOG_ERROR("MySQLLoadLocalPartitionNode %@::execute get exception %s.\n",
              this, e.what());
    if (packet) {
      delete packet;
      packet = NULL;
    }
    handler->clean_dead_conn(&conn, dataspace);
    throw enable_current_exception(e);
  }
  const char *server_name = conn->get_server()->get_name();
  record_modify_server(plan, session, server_name,
                       dataspace->get_virtual_machine_id(), false);
  session->record_xa_modified_conn(conn);
  if (warning_packet_list) {
    session->add_load_warning_packet_list(warning_packet_list, warning_count);
    warning_packet_list = NULL;
  }
  handler->put_back_connection(dataspace, conn);
  conn = NULL;
#ifdef DEBUG
  LOG_DEBUG("MySQLLoadLocalPartitionNode %@ cost %d ms\n", this,
            node_cost_time);
#endif
}

int MySQLLoadLocalPartitionNode::svc() {
  try {
    execute();
  } catch (...) {
    exception = boost::current_exception();
    LOG_ERROR("MySQLLoadLocalPartitionNode::svc get exception.\n");
  }
  status = EXECUTE_STATUS_COMPLETE;
  /*re-ensure ready_to_finish to be true, in case the
   * MySQLLoadLocalPartitionNode get exception without set ready_to_finish to
   * be true and parent node block at wait_ready_to_finish*/
  ready_to_finish = true;
  wake_up_parent_is_waiting();
  notify_parent();
  task_is_finish = true;
  LOG_DEBUG("MySQLLoadLocalPartitionNode::svc fin.\n");
  return 0;
}

bool MySQLLoadLocalPartitionNode::notify_parent() {
  if (!strcmp(table_node->get_executenode_name(),
              "MySQLLoadDataInfilePartTableNode")) {
    ((MySQLLoadDataInfilePartTableNode *)table_node)->signal();
  } else if (!strcmp(table_node->get_executenode_name(),
                     "MySQLLoadLocalPartTableNode")) {
    ((MySQLLoadLocalPartTableNode *)table_node)->signal();
  } else if (!strcmp(table_node->get_executenode_name(),
                     "MySQLLoadSelectPartitionNode")) {
    ((MySQLLoadSelectPartitionNode *)table_node)->signal();
  }
  return true;
}

void MySQLLoadLocalPartitionNode::add_packet_to_ready_list(Packet *p) {
  if (get_task_is_finish()) {
    delete p;
    throw ExecuteNodeError(
        "The MySQLLoadLocalPartitionNode has already finished unexpected. "
        "Abort the load task.\n");
  }
  ACE_Guard<ACE_Thread_Mutex> guard(full_lock);
#ifdef DEBUG
  LOG_DEBUG(
      "MySQLLoadLocalPartitionNode %@::add_packet_to_ready_list packet %@ to "
      "list %@.\n",
      this, p, ready_packets);
#endif
  ready_packets->push_back(p);
  ready_packet_size++;
  if (ready_packet_size >= max_ready_packets_local && !ready_to_finish) {
    /*The ready packet_size is too many, the parent should be blocked until
     * the child handle these ready packets.*/
    parent_is_waiting = true;
    full_cond.wait();
    parent_is_waiting = false;
    if (ACE_Reactor::instance()->reactor_event_loop_done()) {
      Backend::instance()->record_latest_important_dbscale_warning(
          "DBScale exit during load data sql %s.\n", sql);
      throw ExecuteNodeError("DBScale is exiting.");
    }
  }
  if (child_is_waiting && ready_packet_size) full_cond.signal();
}

bool MySQLLoadLocalPartitionNode::rotate_ready_list() {
  list<Packet *> *tmp = NULL;
  full_lock.acquire();
  if (ready_packet_size == 0) {
    if (table_node_has_get_empty_packet && !ready_to_finish) {
      LOG_DEBUG("Partition node %@ is ready to finish.\n", this);
      ready_to_finish = true;
      if (parent_is_waiting) full_cond.signal();
    } else {
      if (parent_is_waiting)
        LOG_WARN(
            "MySQLLoadLocalPartitionNode::rotate_ready_list "
            "parent_is_waiting\n");
    }
    if (check_stop()) {
      full_lock.release();
      return true;
    }
    child_is_waiting = true;
    full_cond.wait();
    child_is_waiting = false;
    if (check_stop()) {
      full_lock.release();
      return true;
    }
  }
  tmp = ready_packets;
  ready_packets = using_packets;
  using_packets = tmp;
  ready_packet_size = 0;
  if (parent_is_waiting) full_cond.signal();
  full_lock.release();
  return false;
}

Packet *MySQLLoadLocalPartitionNode::get_packet_from_ready_list() {
  if (using_packets->empty()) {
    if (rotate_ready_list()) return NULL;
  }
#ifdef DEBUG
  ACE_ASSERT(!using_packets->empty());
#endif
  Packet *ret = using_packets->front();
  using_packets->pop_front();
  return ret;
}

void MySQLLoadLocalPartitionNode::stop_execute() {
  full_lock.acquire();
  is_stop = true;
  if (child_is_waiting) full_cond.signal();
  full_lock.release();
}

void MySQLLoadLocalPartitionNode::clean() {
  full_lock.acquire();
  list<Packet *>::iterator it = ready_packets1.begin();
  for (; it != ready_packets1.end(); it++) {
    delete (*it);
  }
  it = ready_packets2.begin();
  for (; it != ready_packets2.end(); it++) {
    delete (*it);
  }
  ready_packets1.clear();
  ready_packets2.clear();
  if (warning_packet_list) {
    size_t len = warning_packet_list->size();
    for (size_t i = 0; i < len; i++) delete warning_packet_list->at(i);
    delete warning_packet_list;
    warning_packet_list = NULL;
  }
#ifdef DEBUG
  LOG_DEBUG("Finish clean up MySQLLoadLocalPartitionNode %@.\n", this);
#endif
  full_lock.release();
}

/* class MySQLLoadLocalPartTableNode */

MySQLLoadLocalPartTableNode::MySQLLoadLocalPartTableNode(
    ExecutePlan *plan, PartitionedTable *dataspace, const char *sql,
    const char *_schema_name, const char *_table_name)
    : MySQLExecuteNode(plan, dataspace),
      sql(sql),
      cond(lock),
      need_update_last_insert_id(false),
      has_not_support_terminate(false),
      analysis_sync_cond(analysis_sync_lock) {
  this->name = "MySQLLoadLocalPartTableNode";
  set_schema_name(_schema_name);
  set_table_name(_table_name);
  splice_full_table_name(_schema_name, _table_name, full_table_name);
  partition_num = dataspace->get_real_partition_num();
  // TODO: only support single char terminates
  load_data_node = plan->statement->get_stmt_node()->sql->load_oper;
  field_terminate = load_data_node->field_terminate;
  field_escape = load_data_node->field_escape[0];
  has_field_enclose = load_data_node->has_field_enclose;
  if (has_field_enclose) field_enclose = load_data_node->field_enclose[0];
  line_terminate = load_data_node->line_terminate;
  has_line_starting = load_data_node->has_line_starting;
  if (has_line_starting) {
    line_starting = load_data_node->line_starting;
  }
  table_fields_num = 0;

  auto_inc_at_last_field = false;

  analysis_num = load_analysis_num;
  has_got_empty_packet = false;
  need_to_feed_analysis = false;
  has_end_send_data = false;
  analysis_finish_num = 0;
  analysis_loop_count = 0;
  last_added_analysis_node_pos = analysis_num - 1;
  max_ready_packets_local = max_load_ready_packets;
#ifndef DBSCALE_TEST_DISABLE
  max_analysis_wait_size_local = 2;
#else
  max_analysis_wait_size_local = max_load_analysis_wait_size;
#endif
#ifdef DEBUG
  ACE_ASSERT(analysis_num >= 1);
  ACE_ASSERT(max_analysis_wait_size_local >= 2);
#endif
  sql_with_auto_inc_name = NULL;
  sql_specify_column_list = false;
  auto_inc_key_pos = -1;
  max_packet_size = max_load_packet_size;
  specified_column_count = 0;
  load_data_strict_mode_this_stmt = load_data_strict_mode;
}

void MySQLLoadLocalPartTableNode::clean_up_analysis_nodes() {
  vector<MySQLLoadPacketAnalysisNode *>::iterator it = analysis_vec.begin();
  for (; it != analysis_vec.end(); it++) {
    if (*it) {
      (*it)->set_is_stop();
      delete (*it);
    }
  }
  analysis_vec.clear();
}

void MySQLLoadLocalPartTableNode::clean() {
  LOG_INFO("Clean of MySQLLoadLocalPartTableNode.\n");
  status = EXECUTE_STATUS_COMPLETE;
  if (has_not_support_terminate) {
    return;
  }
  LOG_DEBUG("Clean up partition nodes.\n");
  if (!partition_nodes.empty()) {
    int p = partition_nodes.size();
    for (int i = 0; i < p; i++) {
      if (partition_nodes[i]) {
        if (!partition_nodes[i]->is_finished()) {
          partition_nodes[i]->stop_execute();
        }
        partition_nodes[i]->wait();
        partition_nodes[i]->clean();
        delete partition_nodes[i];
      }
    }
  }
  partition_nodes.clear();
  LOG_DEBUG("Clean up analysis nodes.\n");
  clean_up_analysis_nodes();
  vector<list<Packet *> *>::iterator it2 = ready_packets.begin();
  for (; it2 != ready_packets.end(); it2++) {
    list<Packet *>::iterator it3 = (*it2)->begin();
    for (; it3 != (*it2)->end(); it3++) {
      delete (*it3);
    }
    delete (*it2);
  }
  ready_packets.clear();
  if (sql_with_auto_inc_name) delete sql_with_auto_inc_name;
}

Packet *MySQLLoadLocalPartTableNode::get_error_packet() {
  unsigned int num = partition_nodes.size();
  for (unsigned int i = 0; i < num; i++) {
    Packet *result = partition_nodes[i]->get_result();
    if (driver->is_error_packet(result)) {
      return result;
    }
  }
  return NULL;
}

void MySQLLoadLocalPartTableNode::send_query() {
  // Intialize packet buffer for all partitions
  for (int i = 0; i < partition_num; i++) {
    DataSpace *part_ds = get_partition(i);
    partition_nodes.push_back(
        new MySQLLoadLocalPartitionNode(plan, this, part_ds, sql));
    if (partition_nodes[i] == NULL) {
      LOG_ERROR("cannot allocate memory for LOAD DATA partition node\n");
      throw NoMemoryError("allocate memory for LAOD DATA partition node");
    }
    partition_nodes[i]->set_max_ready_packets_local(max_ready_packets_local);

    list<Packet *> *tmp_list = new list<Packet *>();
    ready_packets.push_back(tmp_list);
  }

  // Start processing all partitions in there own threads
  for (int i = 0; i < partition_num; i++) {
    partition_nodes[i]->activate();
  }
}

void MySQLLoadLocalPartTableNode::add_to_ready_packets_from_Analysis(
    unsigned int part_id, Packet *packet) {
  try {
    ACE_Guard<ACE_Thread_Mutex> guard(ready_packet_lock);
    ready_packets[part_id]->push_back(packet);
    wakeup_analysis_sync_cond();
  } catch (...) {
    wakeup_analysis_sync_cond();
  }
}

void MySQLLoadLocalPartTableNode::flush_ready_packets_to_child() {
  vector<list<Packet *> *> tmp_vec;
  size_t ready_packets_size = ready_packets.size();
  {
    ACE_Guard<ACE_Thread_Mutex> guard(ready_packet_lock);
    for (size_t i = 0; i < ready_packets_size; i++) {
      tmp_vec.push_back(ready_packets[i]);
      ready_packets[i] = new list<Packet *>();
    }
  }
  try {
    for (size_t i = 0; i < ready_packets_size; i++) {
      list<Packet *>::iterator it = tmp_vec[i]->begin();
      for (; it != tmp_vec[i]->end(); it++) {
        try {
          add_packet_to_child(partition_nodes[i], (*it));
          *it = NULL;
        } catch (...) {
          *it = NULL;
          throw;
        }
      }
      tmp_vec[i]->clear();
      delete tmp_vec[i];
      tmp_vec[i] = NULL;
    }
  } catch (...) {
    for (size_t i = 0; i < ready_packets_size; i++) {
      if (!tmp_vec[i]) {
        continue;
      }
      list<Packet *>::iterator it = tmp_vec[i]->begin();
      for (; it != tmp_vec[i]->end(); it++) {
        if (*it) {
          delete *it;
          *it = NULL;
        }
      }
      tmp_vec[i]->clear();
      delete tmp_vec[i];
      tmp_vec[i] = NULL;
    }
    throw;
  }
}

void MySQLLoadLocalPartTableNode::request_file_and_send_data_to_server() {
  // result is the file requesting packet from the first partition, we
  // send it to client
  Packet *result = partition_nodes[0]->get_result();
#ifdef DEBUG
  ASSERT(!driver->is_error_packet(result));
#endif
  handler->send_to_client(result);

  while (1) {
    try {
      fill_packets_to_analysis();
      bool analysis_fin = wait_and_check_analysis();
      if (analysis_fin) {
        LOG_DEBUG("All analysis has finished.\n");
        break;
      }
    } catch (exception &e) {
      LOG_ERROR(
          "Get exception in "
          "MySQLLoadLocalPartTableNode::request_file_and_send_data_to_server, "
          "load_data_quick_error=%d.\n",
          load_data_quick_error);
      has_end_send_data = true;
      if (!has_got_empty_packet && load_data_quick_error == 0 &&
          !(session->get_is_killed())) {
        eat_all_packet_from_client();
      }
      // and then rethrow the exception
      if (load_data_quick_error > 0) {
        LOG_ERROR(
            "load data quick error due to [%s] for table [%s.%s] of sql "
            "[%s].\n",
            e.what(), schema_name, table_name, sql);
      }
      throw;
    }
    if (ACE_Reactor::instance()->reactor_event_loop_done()) {
      LOG_INFO("DBScale is exiting, so stop this load data task.\n");
      throw Error("DBScale is exiting.");
    }
  }
  flush_all();
}
bool MySQLLoadLocalPartTableNode::wait_and_check_analysis() {
  bool has_finish = true;
  vector<MySQLLoadPacketAnalysisNode *>::iterator it = analysis_vec.begin();
  for (; it != analysis_vec.end(); it++) {
    (*it)->check_exception();
  }
  flush_ready_packets_to_child();
  wait_children();
  analysis_sync_lock.acquire();
  if (!need_to_feed_analysis && !has_got_empty_packet) {
    LOG_DEBUG(
        "Load data table node %@ start to wait due to need_to_feed_analysis is "
        "0.\n",
        this);
    analysis_sync_cond.wait();
  } else if (has_got_empty_packet) {
    if (analysis_finish_num < analysis_num) {
      LOG_DEBUG(
          "Load data table node %@ start to wait due to"
          " analysis_finish_num is %d while analysis_num is %d.\n",
          this, analysis_finish_num, analysis_num);
      analysis_sync_cond.wait();
    }
  }
  need_to_feed_analysis = false;
  analysis_sync_lock.release();
  flush_ready_packets_to_child();
  wait_children();

  it = analysis_vec.begin();
  for (; it != analysis_vec.end(); it++) {
    if (!(*it)->has_finish()) {
      has_finish = false;
      break;
    }
  }
  if (has_finish) {
    /*here all analysis node has finish the packet handling*/
    vector<MySQLLoadPacketAnalysisNode *>::iterator it = analysis_vec.begin();
    for (; it != analysis_vec.end(); it++) {
      (*it)->check_exception();
    }
    flush_ready_packets_to_child();
    wait_children();
  }
  return has_finish;
}

Packet *MySQLLoadLocalPartTableNode::read_one_packet() {
  if (session->get_is_killed()) {
    LOG_DEBUG("Load data local is killed, so cancel read_one_packet()\n");
    has_end_send_data = true;
    throw Error("Load data local is killed");
  }
  Packet *packet = Backend::instance()->get_new_packet(LOAD_CLIENT_PACKET_SIZE);
  packet->rewind();
  try {
    handler->receive_from_client(packet);
  } catch (...) {
    delete packet;
    throw;
  }
  return packet;
}

/*This function will check whether the first row of this packet is a complete
 * row.*/
size_t MySQLLoadLocalPartTableNode::is_first_row_complete(Packet *packet) {
#ifdef DEBUG
  ACE_ASSERT(!driver->is_empty_packet(packet));
#endif
  return check_first_row_complete(packet, field_terminate, line_terminate,
                                  has_field_enclose, field_enclose,
                                  field_escape, table_fields_num);
}

void MySQLLoadLocalPartTableNode::fill_packets_to_analysis() {
  if (has_got_empty_packet) return;
  size_t n = 0, tmp = 0, tmp_wait_size = 0;
  Packet *packet;
  need_to_feed_analysis = false;
  for (; n < analysis_num; n++) {
    tmp_wait_size = analysis_vec[analysis_loop_count]->get_waiting_size();
    if (tmp_wait_size >= max_analysis_wait_size_local) {
      analysis_loop_count++;
      if (analysis_loop_count == analysis_num) analysis_loop_count = 0;
      continue;
    }
    tmp = max_analysis_wait_size_local - tmp_wait_size;
    size_t i = 0;
    bool skip_i_plus = false;
    for (; i < tmp; i++) {
      if (skip_i_plus) {
        skip_i_plus = false;
        i--;
      }
      packet = read_one_packet();
      if (driver->is_empty_packet(packet)) {
        delete packet;
        add_empty_pkt_to_analysis_node();
        return;
      }
      if (i == 0 && analysis_num > 1) {
        /*For the first packet, we should check whether it contains an
         * in-complete row of previous packet.*/
        try {
          size_t incomplete_row_len = is_first_row_complete(packet);
          while (incomplete_row_len == packet->length()) {
            /*This packet does not reach the end of packet, so put it to the
             * previous analysis node and read one more to check first row
             * complete.*/
            analysis_vec[last_added_analysis_node_pos]->add_packet_to_analysis(
                packet);
            packet = NULL;
            packet = read_one_packet();
            if (driver->is_empty_packet(packet)) {
              delete packet;
              add_empty_pkt_to_analysis_node();
              return;
            }

            incomplete_row_len = is_first_row_complete(packet);
          }
          if (incomplete_row_len) {
            /*find a incomplete row, add it to the previous analysis node.*/
            Packet *pf = NULL, *pr = NULL;
            divide_packet_by_first_row(packet, incomplete_row_len, &pf, &pr);
            if (!pf) {
              analysis_vec[last_added_analysis_node_pos]
                  ->add_packet_to_analysis(packet);
              packet = NULL;
              skip_i_plus = true;
              continue;
            }
            analysis_vec[last_added_analysis_node_pos]->add_packet_to_analysis(
                pf);
            delete packet;
            packet = pr;
          }
        } catch (...) {
          if (packet) delete packet;
          packet = NULL;
          throw;
        }
      }

      analysis_vec[analysis_loop_count]->add_packet_to_analysis(packet);
      last_added_analysis_node_pos = analysis_loop_count;

      packet = NULL;
    }
    analysis_loop_count++;
    if (analysis_loop_count == analysis_num) analysis_loop_count = 0;
  }
}

void MySQLLoadLocalPartTableNode::report_result_to_client() {
  Packet *result = NULL;
  uint64_t affected_rows = 0;
  uint64_t warnings = 0;
  for (int i = 0; i < partition_num; i++) {
    result = partition_nodes[i]->get_result();
    ASSERT(!driver->is_error_packet(result));
    if (!driver->is_ok_packet(result)) {
      LOG_ERROR("Got unexpected result packet from server\n");
      throw ExecuteNodeError("Got unexpected packet from server");
    }
    affected_rows += partition_nodes[i]->get_affected_row_count();
    warnings += partition_nodes[i]->get_warning_count();
  }
  // Use the last result packet to store the merged result
  MySQLOKResponse ok(result);
  ok.set_affected_rows(affected_rows);
  ok.set_warnings(warnings);
  Packet *ok_packet = Backend::instance()->get_new_packet();
  ok.pack(ok_packet);
  handler->deal_autocommit_with_ok_eof_packet(ok_packet);
  handler->record_affected_rows(ok_packet);
  if (!plan->statement->is_load_insert_select()) {
    handler->send_to_client(ok_packet);
    delete ok_packet;
  } else {
    session->set_result_packet(ok_packet);
  }
}

unsigned int MySQLLoadLocalPartTableNode::get_table_fields(string schema_name,
                                                           string table_name) {
  DataSpace *ds_part0 = part_space->get_partition(0);
  string fields_sql;
  fields_sql.append(
      "SELECT COUNT(*) FROM INFORMATION_SCHEMA.COLUMNS WHERE table_schema = "
      "\'");
  fields_sql.append(schema_name);
  fields_sql.append("\' AND table_name = \'");
  fields_sql.append(table_name);
  fields_sql.append("\'");
  unsigned int fields_num = 0;

  Packet packet;
  Packet exec_packet;
  MySQLQueryRequest query(fields_sql.c_str());
  query.set_sql_replace_char(plan->session->get_query_sql_replace_null_char());
  query.pack(&exec_packet);

  Connection *conn0 = NULL;
  // since this may execute a long times, use UINT_MAX
  // to avoid max_conn_execute_time.
  TimeValue timeout = TimeValue(UINT_MAX, 0);
  try {
    conn0 = handler->send_to_server_retry(
        ds_part0, &exec_packet, session->get_schema(), session->is_read_only());
    handler->receive_from_server(conn0, &packet, &timeout);
    if (driver->is_result_set_header_packet(&packet)) {
      // fields packets
      handler->receive_from_server(conn0, &packet, &timeout);
      while (!driver->is_eof_packet(&packet)) {
        handler->receive_from_server(conn0, &packet, &timeout);
      }

      // row data packets
      handler->receive_from_server(conn0, &packet, &timeout);
      while (!driver->is_eof_packet(&packet)) {
        MySQLRowResponse row(&packet);
        fields_num = row.get_uint(0);
        handler->receive_from_server(conn0, &packet, &timeout);
      }
    }
    if (conn0) {
      handler->put_back_connection(ds_part0, conn0);
      conn0 = NULL;
    }
    if (fields_num == 0) {
      LOG_ERROR("Failed to get table's field number.");
      throw ExecuteNodeError("Failed to get table's field number.");
    }
  } catch (exception &e) {
    if (conn0) {
      handler->clean_dead_conn(&conn0, ds_part0);
    }
    throw;
  }
  return fields_num;
}

void MySQLLoadLocalPartTableNode::execute() {
#ifdef DEBUG
  node_start_timing();
#endif
  sql_specify_column_list = (load_data_node->column_name_list != NULL);
  specified_column_count = 0;

  vector<const char *> *key_names =
      ((PartitionedTable *)dataspace)->get_key_names();
#ifdef DEBUG
  ACE_ASSERT(key_names->size() == 1);
#endif
  const char *key_name = (*key_names)[0];

  if (sql_specify_column_list) {
    name_item *column_name_list_head = load_data_node->column_name_list->next;
    name_item *column_name = column_name_list_head;
    do {
      /*if partition_key is not found in column_name_list,
       *maybe it is a tailed auto increment column and not specified
       *we will check it later.
       */
      if (!strcasecmp(key_name, column_name->name)) {
        if (!key_pos_vec.empty()) {
          string msg;
          msg.append("load data sql specify partition key [");
          msg.append(key_name);
          msg.append("] more than once.");
          LOG_ERROR("%s\n", msg.c_str());
          throw Error(msg.c_str());
        }
        key_pos_vec.push_back(specified_column_count);
      }
      specified_column_count++;
      column_name = column_name->next;
    } while (column_name != column_name_list_head);
  } else {
    ((PartitionedTable *)dataspace)
        ->get_key_pos_vec(schema_name, table_name, key_pos_vec, plan->session);
  }
  part_space = static_cast<PartitionedTable *>(dataspace);
  partition_method = part_space->get_partition_method();
  stmt = plan->statement;
  table_has_auto_inc_column =
      (stmt->get_auto_inc_status() != NO_AUTO_INC_FIELD);

  if (has_line_starting) {
    has_not_support_terminate = true;
    LOG_ERROR(
        "DBScale does not support LOAD DATA partitioned table "
        "with LINES STARTING BY separator argument.\n");
    throw NotImplementedError(
        "LOAD DATA partitioned table with LINES STARTING BY");
  }
  if (line_terminate[0] == field_terminate[0]) {
    LOG_ERROR(
        "DBScale does not support LINE TERMINATED and FIELDS TERMINATED has "
        "the same first character.\n");
    throw NotImplementedError(
        "DBScale does not support LINE TERMINATED and FIELDS TERMINATED has "
        "the same first character.");
  }

  ACE_Thread_Mutex *stmt_lock = NULL;
  try {
    table_fields_num = get_table_fields(schema_name, table_name);
    if (table_has_auto_inc_column) {
      need_update_last_insert_id =
          session->get_session_option("enable_last_insert_id").int_val;
      stmt->set_auto_increment_key_pos(
          part_space->get_auto_increment_key_pos(full_table_name));
      auto_inc_key_pos = stmt->get_auto_increment_key_pos();
      if ((int)table_fields_num == auto_inc_key_pos + 1) {
        auto_inc_at_last_field = true;
      }
      stmt_lock = part_space->get_stmt_autoinc_lock(full_table_name);
      if (stmt_lock) stmt_lock->acquire();
      part_space->init_auto_inc_value(plan->handler, full_table_name,
                                      schema_name, table_name, true);
    }
    if (table_has_auto_inc_column && sql_specify_column_list) {
      // check whether need append auto_inc column into sql's column name list
      string auto_inc_key_name =
          part_space->get_auto_increment_key(full_table_name);

      name_item *column_name_list_head = load_data_node->column_name_list->next;
      name_item *column_name = column_name_list_head;
      bool column_list_specify_auto_inc_column = false;
      int auto_inc_column_index_in_column_list = 0;
      do {
        if (!strcasecmp(auto_inc_key_name.c_str(), column_name->name)) {
          column_list_specify_auto_inc_column = true;
          break;
        }
        column_name = column_name->next;
        auto_inc_column_index_in_column_list++;
      } while (column_name != column_name_list_head);
      if (!column_list_specify_auto_inc_column) {
        sql_with_auto_inc_name = new string(sql);
        sql_with_auto_inc_name->insert(
            load_data_node->column_name_list_end_pos - 1, ", ");
        sql_with_auto_inc_name->insert(
            load_data_node->column_name_list_end_pos + 1, auto_inc_key_name);
        sql = sql_with_auto_inc_name->c_str();
        if (key_pos_vec.empty() &&
            !strcasecmp(key_name, auto_inc_key_name.c_str())) {
          key_pos_vec.push_back(specified_column_count);
        }
      }
      stmt->set_auto_increment_key_pos(auto_inc_column_index_in_column_list);
      auto_inc_key_pos = auto_inc_column_index_in_column_list;
    }

    if (key_pos_vec.empty()) {
      string msg;
      msg.append("partition key [");
      msg.append(key_name);
      msg.append("] not found.");
      LOG_ERROR("%s\n", msg.c_str());
      throw Error(msg.c_str());
    }

    status = EXECUTE_STATUS_START;
    send_query();
    size_t n = 0;
    LOG_DEBUG("Init the analysis with num %d.\n", analysis_num);
    if (load_data_check_auto_inc && table_has_auto_inc_column &&
        !is_column_list_has_append_auto_inc()) {
      LOG_DEBUG(
          "Cause need check auto increment value, so adjust the analysis_num "
          "from %d to 1.\n",
          analysis_num);
      analysis_num = 1;
      last_added_analysis_node_pos = 0;
    }
    for (; n < analysis_num; n++) {
      analysis_vec.push_back(new MySQLLoadPacketAnalysisNode(
          this, driver, session, handler, plan));
      analysis_vec[n]->init_analysis(partition_num);
      analysis_vec[n]->start_thread();
    }
    if (status == EXECUTE_STATUS_COMPLETE) {
      if (stmt_lock) {
        stmt_lock->release();
        stmt_lock = NULL;
      }
      return;
    }
    wait_children();
    status = EXECUTE_STATUS_FETCH_DATA;
    request_file_and_send_data_to_server();
    status = EXECUTE_STATUS_BEFORE_COMPLETE;
    wait_children();
    report_result_to_client();
    status = EXECUTE_STATUS_COMPLETE;
    if (stmt_lock) {
      stmt_lock->release();
      stmt_lock = NULL;
    }
  } catch (...) {
    if (stmt_lock) {
      stmt_lock->release();
      stmt_lock = NULL;
    }
    throw;
  }
#ifdef DEBUG
  node_end_timing();
#endif
#ifdef DEBUG
  LOG_DEBUG("MySQLLoadLocalPartTableNode %@ cost %d ms\n", this,
            node_cost_time);
#endif
}

void MySQLLoadLocalPartTableNode::wait_children() {
  ACE_GUARD_REACTION(ACE_Thread_Mutex, guard, lock,
                     LOG_ERROR("%p\n", "acquire LOAD DATA partitions lock");
                     throw HandlerError("acquire LOAD DATA partitions failed"));
  switch (status) {
    case EXECUTE_STATUS_START:
      for (int i = 0; i < partition_num; i++) {
        while (!partition_nodes[i]->requesting_file() &&
               !partition_nodes[i]->get_task_is_finish())
          cond.wait();
      }
      break;
    case EXECUTE_STATUS_FETCH_DATA:
      for (int i = 0; i < partition_num; i++) {
        partition_nodes[i]->check_exception();
      }
      break;
    case EXECUTE_STATUS_BEFORE_COMPLETE:
      for (int i = 0; i < partition_num; i++) {
        while (!partition_nodes[i]->fetched_result() &&
               !partition_nodes[i]->get_task_is_finish())
          cond.wait();

        partition_nodes[i]->check_exception();
      }
      break;
    default:
      ASSERT(0);
  }
}

void MySQLLoadLocalPartTableNode::add_packet_to_child(
    MySQLLoadLocalPartitionNode *child, Packet *packet) {
  char *p = packet->base();
  size_t len = packet->length();
#ifdef DEBUG
  ASSERT(len >= PACKET_HEADER_SIZE);
#endif
  Packet::pack3int(p, len - PACKET_HEADER_SIZE);
  if (!driver->is_empty_packet(packet)) {
    child->add_packet_to_ready_list(packet);
  } else {
    delete packet;
  }
}

void MySQLLoadLocalPartTableNode::flush_all() {
  /*before we send last empty packet to partition_node, we should first ensure
   * that all partition node has already finish the data packet handling,
   * otherwise there may be partial execution of the load, which means some
   * node has data,some not.*/
  LOG_DEBUG("Start to flush all for MySQLLoadLocalPartTableNode %@.\n", this);
  for (int i = 0; i < partition_num; i++) {
    partition_nodes[i]->set_table_node_has_get_empty_packet(true);
  }
  wait_children();
  for (int i = 0; i < partition_num; i++) {
    // prepare the last empty packet
    partition_nodes[i]->wait_ready_to_finish();
    LOG_DEBUG("Partition node %@ done for wait ready to finish.\n",
              partition_nodes[i]);
    wait_children();
  }
  /*Before send the last empty packet, we should re-ensure that all partition
   * load node work without exception.*/
  for (int i = 0; i < partition_num; i++) {
    partition_nodes[i]->check_exception();
  }

  for (int i = 0; i < partition_num; i++) {
    // prepare the last empty packet
    Packet *tmp = Backend::instance()->get_new_packet();
    tmp->rewind();
    Packet::pack3int(tmp->base(), 0);
    driver->set_packet_number(tmp, 0);
    tmp->wr_ptr(PACKET_HEADER_SIZE);
    partition_nodes[i]->add_packet_to_ready_list(tmp);
  }
}

/* class MySQLLoadPacketAnalysisNode*/
MySQLLoadPacketAnalysisNode::~MySQLLoadPacketAnalysisNode() {
  if (thread_status == THREAD_STARTED) {
    LOG_DEBUG("MySQLLoadPacketAnalysisNode %@ cleaning wait.\n", this);
    this->wait_for_cond();
    LOG_DEBUG("MySQLLoadPacketAnalysisNode %@ cleaning wait finish.\n", this);
  } else if (thread_status == THREAD_CREATED) {
    bthread->re_init_backend_thread();
    bthread->get_pool()->add_back_to_free(bthread);
  }

  LOG_DEBUG("MySQLLoadPacketAnalysisNode %@ has stopped.\n", this);
  free_stmt_node_mem(&tmp_st);
  column_values.expr_list_head = column_values.expr_list_tail = NULL;
  for (int i = 0; i < partition_num; i++) {
    if (buffers[i]) {
      delete buffers[i];
    }
  }
  buffers.clear();

  list<Packet *>::iterator it = waiting_packets.begin();
  for (; it != waiting_packets.end(); it++) {
    delete (*it);
  }
  waiting_packets.clear();
  delete[] line_buffer;
  if (tmp_buffer) {
    delete tmp_buffer;
    tmp_buffer = NULL;
  }
  if (previous_packet_escaped_pos_vec) {
    delete previous_packet_escaped_pos_vec;
    previous_packet_escaped_pos_vec = NULL;
  }
}

void MySQLLoadPacketAnalysisNode::start_thread() {
  // start the new thread
  if (thread_status == THREAD_STOP) {
    Backend *backend = Backend::instance();
    bthread = backend->get_backend_thread_pool()->get_one_from_free();
    if (!bthread) {
      LOG_ERROR(
          "Fail to get thread for "
          "MySQLLoadPacketAnalysisNode::start_thread.\n");
      throw Error(
          "Fail to get a backend thread from pool, so stop execute the sql");
    }
    thread_status = THREAD_CREATED;
    bthread->set_task(this);
    thread_status = THREAD_STARTED;
    bthread->wakeup_handler_thread();
  }
}

void MySQLLoadPacketAnalysisNode::init_analysis(int partition_num) {
  line_buffer = new char[DEFAULT_BUFFER_SIZE];
  tmp_buffer = Backend::instance()->get_new_packet();
  previous_packet_escaped_pos_vec = new vector<int>();
  this->partition_num = partition_num;
  int i = 0;
  max_packet_size = table_node->get_max_packet_size();
  for (; i < partition_num; i++) {
    buffers.push_back(Backend::instance()->get_new_packet(max_packet_size));
    buffers[i]->rewind();
    Packet::pack3int(buffers[i]->base(), 0);
    driver->set_packet_number(buffers[i], 0);
    buffers[i]->wr_ptr(PACKET_HEADER_SIZE);
  }
  field_terminate = table_node->get_field_terminate();
  field_escape = table_node->get_field_escape();
  field_enclose = table_node->get_field_enclose();
  line_terminate = table_node->get_line_terminate();
  line_starting = table_node->get_line_starting();
  has_field_enclose = table_node->get_has_field_enclose();
  has_line_starting = table_node->get_has_line_starting();
  key_pos_vec = table_node->get_key_pos_vec();
  schema_name = table_node->get_schema_name();
  table_name = table_node->get_table_name();
  need_update_last_insert_id = table_node->get_need_update_last_insert_id();
  auto_inc_key_pos = table_node->get_auto_inc_key_pos();
  partition_method = table_node->get_partition_method();
  full_table_name = table_node->get_full_table_name();
  auto_inc_at_last_field = table_node->get_auto_inc_at_last_field();
  need_append_auto_inc_column = false;
  column_list_has_append_auto_inc =
      table_node->is_column_list_has_append_auto_inc();
  table_has_auto_inc_column = table_node->is_table_has_auto_inc_column();
  sql_specify_column_list = table_node->is_sql_specify_column_list();
  table_fields_num = table_node->get_table_fields_num();
  part_space = table_node->get_part_space();
  stmt = table_node->get_stmt();
  max_analysis_wait_size_local = table_node->get_max_analysis_wait_size_local();
  has_set_auto_inc_status = false;
  load_data_strict_mode_this_stmt =
      table_node->get_load_data_strict_mode_this_stmt();
}

int MySQLLoadPacketAnalysisNode::svc() {
  Packet *packet = NULL;
  LOG_DEBUG("MySQLLoadPacketAnalysisNode %@ start to run.\n", this);
  try {
    while (1) {
      if (table_node->is_has_end_send_data() || is_stop ||
          ACE_Reactor::instance()->reactor_event_loop_done() ||
          session->get_is_killed())
        break;
      packet = get_packet_from_waiting_packets();
      if (table_node->is_has_end_send_data() || is_stop ||
          ACE_Reactor::instance()->reactor_event_loop_done() ||
          session->get_is_killed()) {
        if (packet) {
          delete packet;
          packet = NULL;
        }
        break;
      }

      try {
        send_file_data(packet);
      } catch (Exception &e) {
        delete packet;
        packet = NULL;

        LOG_DEBUG("Analysis %@ get Exception %s.\n", this, e.what());
        throw enable_current_exception(e);
      }
      delete packet;
      packet = NULL;

      if (table_node_has_get_empty_packet && waiting_size == 0) {
        LOG_DEBUG("MySQLLoadPacketAnalysisNode %@ has finished.\n", this);
        break;
      }
    }
  } catch (...) {
    if (packet) {
      delete packet;
      packet = NULL;
    }

    exception = boost::current_exception();
    table_node->wakeup_analysis_sync_cond();
  }
  has_finished = true;
  table_node->wakeup_analysis_sync_for_fin();
  LOG_DEBUG("MySQLLoadPacketAnalysisNode %@ finish running.\n", this);
  return FINISHED;
}

Packet *MySQLLoadPacketAnalysisNode::get_packet_from_waiting_packets() {
  acquire_sync_lock();
  if (waiting_size == 0) {
#ifdef DEBUG
    ACE_ASSERT(waiting_packets.empty());
#endif
    analysis_is_waiting = true;
#ifdef DEBUG
    LOG_DEBUG(
        "MySQLLoadPacketAnalysisNode %@ wait for packets with "
        "table_node_has_get_empty_packet %d waiting_size %d.\n",
        this, table_node_has_get_empty_packet ? 1 : 0, waiting_size);
#endif
    wait_sync_cond();
    analysis_is_waiting = false;
  }
  if (is_stop || ACE_Reactor::instance()->reactor_event_loop_done() ||
      session->get_is_killed()) {
    release_sync_lock();
    return NULL;
  }
#ifdef DEBUG
  ACE_ASSERT(!waiting_packets.empty());
#endif
  Packet *ret = waiting_packets.front();
  waiting_packets.pop_front();
#ifdef DEBUG
  ACE_ASSERT(waiting_size >= 1);
#endif
  waiting_size--;
  if (waiting_size < max_analysis_wait_size_local / 2 &&
      !table_node_has_get_empty_packet) {
    table_node->wakeup_analysis_sync_cond_for_feed();
  }
  release_sync_lock();
  return ret;
}

void MySQLLoadPacketAnalysisNode::flush_rows() {
  /*Currently, this function will only be invoked to flush all rows to child
   * before put the last empty packet.*/
  for (int i = 0; i < partition_num; i++) {
    if (!buffers[i]) {
      LOG_ERROR(
          "MySQLLoadPacketAnalysisNode::flush_rows fail due to buffers[%d] is "
          "NULL.\n",
          i);
      throw Error("MySQLLoadPacketAnalysisNode::flush_rows fail.");
    }
    char *p = buffers[i]->base();
    size_t len = buffers[i]->length();
#ifdef DEBUG
    ASSERT(len >= PACKET_HEADER_SIZE);
#endif
    Packet::pack3int(p, len - PACKET_HEADER_SIZE);
    if (!driver->is_empty_packet(buffers[i])) {
      table_node->add_to_ready_packets_from_Analysis(i, buffers[i]);
      buffers[i] = NULL;
    }
  }
}

void MySQLLoadPacketAnalysisNode::send_file_data(Packet *packet) {
  if (session->get_is_killed()) return;
  const char *pos = packet->base() + PACKET_HEADER_SIZE;
  const char *end = packet->base() + packet->length();

  if (driver->is_empty_packet(packet)) {
    flush_tmp_buffer();
    return;
  }

  // If previous line not terminated
  size_t line_len = tmp_buffer->wr_ptr() - tmp_buffer->base();
  vector<int> *escaped_pos_vec = new vector<int>();
  try {
    if (line_len > 0) {
      const char *packet_data_start_pos = pos;
      int line_term_len = line_terminate.length();
      int field_term_len = field_terminate.length();
      string::size_type sub_str_pos_line_term = string::npos;
      string::size_type sub_str_pos_field_term = string::npos;
      const char *field_end_pos = NULL;
      bool enclose_field_end_spec = false;
      bool need_handle_line_term = false;
      bool need_handle_field_term = false;
      bool is_first_part_enough = false;

      if (line_term_len > 1) {
        handle_load_local_params param;
        param.pos = pos;
        param.packet_data_start_pos = packet_data_start_pos;
        param.line_term_len = line_term_len;
        param.field_term_len = field_term_len;
        param.sub_str_pos_line_term = sub_str_pos_line_term;
        param.sub_str_pos_field_term = sub_str_pos_field_term;
        param.field_end_pos = field_end_pos;
        param.enclose_field_end_spec = enclose_field_end_spec;
        param.need_handle_line_term = need_handle_line_term;
        param.need_handle_field_term = need_handle_field_term;
        param.is_first_part_enough = is_first_part_enough;

        check_tmp_buffer_tail_contain_line_term_prefix(param, packet, line_len);

        pos = param.pos;
        packet_data_start_pos = param.packet_data_start_pos;
        line_term_len = param.line_term_len;
        field_term_len = param.field_term_len;
        sub_str_pos_line_term = param.sub_str_pos_line_term;
        sub_str_pos_field_term = param.sub_str_pos_field_term;
        field_end_pos = param.field_end_pos;
        enclose_field_end_spec = param.enclose_field_end_spec;
        need_handle_line_term = param.need_handle_line_term;
        need_handle_field_term = param.need_handle_field_term;
        is_first_part_enough = param.is_first_part_enough;
      }

      if (field_term_len > 1 && !need_handle_line_term) {
        handle_load_local_params param;
        param.pos = pos;
        param.packet_data_start_pos = packet_data_start_pos;
        param.line_term_len = line_term_len;
        param.field_term_len = field_term_len;
        param.sub_str_pos_line_term = sub_str_pos_line_term;
        param.sub_str_pos_field_term = sub_str_pos_field_term;
        param.field_end_pos = field_end_pos;
        param.enclose_field_end_spec = enclose_field_end_spec;
        param.need_handle_line_term = need_handle_line_term;
        param.need_handle_field_term = need_handle_field_term;
        param.is_first_part_enough = is_first_part_enough;

        check_tmp_buffer_tail_contain_field_term_prefix(param, packet,
                                                        line_len);

        pos = param.pos;
        packet_data_start_pos = param.packet_data_start_pos;
        line_term_len = param.line_term_len;
        field_term_len = param.field_term_len;
        sub_str_pos_line_term = param.sub_str_pos_line_term;
        sub_str_pos_field_term = param.sub_str_pos_field_term;
        field_end_pos = param.field_end_pos;
        enclose_field_end_spec = param.enclose_field_end_spec;
        need_handle_line_term = param.need_handle_line_term;
        need_handle_field_term = param.need_handle_field_term;
        is_first_part_enough = param.is_first_part_enough;
      }

      if (need_handle_line_term || need_handle_field_term) {
        const char *ptr = tmp_buffer->rd_ptr();
        bool is_char_escaped = false;
        while (ptr < field_end_pos) {
          if (is_char_escaped) {
            is_char_escaped = false;
          } else if (*ptr == field_escape) {
            is_char_escaped = true;
            escaped_pos_vec->push_back(int(ptr - tmp_buffer->rd_ptr()));
          }
          ptr++;
        }
      }

      if (need_handle_line_term) {
        LOG_DEBUG(
            "found multi-character line-terminate-string half in previous "
            "tmp_buffer and half in next packet\n");
        add_column_value_shell_1(&tmp_st, tmp_buffer->rd_ptr(),
                                 field_end_pos + 1, *escaped_pos_vec,
                                 enclose_field_end_spec);
        if (is_first_part_enough) {
          handle_line_term_first_part_enough(line_len, sub_str_pos_line_term,
                                             pos);
          pos += sub_str_pos_line_term + 1;
        } else {
          handle_line_term_first_part_not_enough(line_len, line_term_len,
                                                 sub_str_pos_line_term, pos);
          pos += line_term_len + sub_str_pos_line_term - line_len;
        }
        line_len = tmp_buffer->wr_ptr() - tmp_buffer->base();
      } else {
        if (need_handle_field_term) {
          LOG_DEBUG(
              "found multi-character field-terminate-string half in previous "
              "tmp_buffer and half in next packet\n");
          add_column_value_shell_2(&tmp_st, tmp_buffer->rd_ptr(),
                                   field_end_pos + 1, *escaped_pos_vec,
                                   enclose_field_end_spec);
          if (is_first_part_enough) {
            handle_field_term_first_part_enough(line_len,
                                                sub_str_pos_field_term, pos);
            pos += sub_str_pos_field_term + 1;
          } else {
            handle_field_term_first_part_not_enough(
                line_len, field_term_len, sub_str_pos_field_term, pos);
            pos += field_term_len + sub_str_pos_field_term - line_len;
          }
          line_len = tmp_buffer->wr_ptr() - tmp_buffer->base();
          escaped_pos_vec->clear();
        }

        const char *start = pos;
        bool is_char_escaped = false;
        bool need_enclose = false;
        bool enclose_field_end = false;
        unsigned int fields_count =
            previous_packet_fields_count + (need_handle_field_term ? 1 : 0);

        if (!need_handle_field_term &&
            !previous_packet_just_found_field_before_end) {
          escaped_pos_vec->clear();
          if (!previous_packet_escaped_pos_vec->empty()) {
            *escaped_pos_vec = *previous_packet_escaped_pos_vec;
            previous_packet_escaped_pos_vec->clear();
          }
          is_char_escaped = previous_packet_is_char_escaped;
          need_enclose = previous_packet_need_enclose;
          enclose_field_end = previous_packet_enclose_field_end;
        } else {
          /* rectify value of need_enclose only when
           * need_handle_field_term==true or
           * previous_packet_just_found_field_before_end==true, because in such
           * situation, the value of pos/start has changed, it has move to the
           * start of next field.
           */
          if (has_field_enclose && field_enclose == *start) {
            need_enclose = true;
            enclose_field_end = false;
          }
        }

        handle_char_one_by_one_params param;
        param.start = start;
        param.pos = pos;
        param.end = end;
        param.packet_data_start_pos = packet_data_start_pos;
        param.is_char_escaped = is_char_escaped;
        param.need_enclose = need_enclose;
        param.enclose_field_end = enclose_field_end;
        param.fields_count = fields_count;
        param.line_len = line_len;
        param.line_term_len = line_term_len;
        param.field_term_len = field_term_len;

        handle_previous_line_char_one_by_one(param, escaped_pos_vec);

        start = param.start;
        pos = param.pos;
        end = param.end;
        packet_data_start_pos = param.packet_data_start_pos;
        is_char_escaped = param.is_char_escaped;
        need_enclose = param.need_enclose;
        enclose_field_end = param.enclose_field_end;
        fields_count = param.fields_count;
        line_len = param.line_len;

        if (fields_count > table_fields_num
#ifndef DBSCALE_TEST_DISABLE
            && instance_option_value["check_column_load_assp"].int_val == 1
#endif
        ) {
          LOG_ERROR(
              "Fields count %d is larger than table_fields_num %d, plz check "
              "line terminate char.\n",
              fields_count, table_fields_num);
          throw Error(
              "Fields count is larger than table_fields_num, plz check line "
              "terminate char.");
        }

        if (pos == end) {
          handle_char_one_by_one_params param;
          param.start = start;
          param.pos = pos;
          param.is_char_escaped = is_char_escaped;
          param.need_enclose = need_enclose;
          param.enclose_field_end = enclose_field_end;
          param.fields_count = fields_count;
          param.line_len = line_len;
          save_current_line_status(param, escaped_pos_vec);
          delete escaped_pos_vec;
          return;
        }
        if (*pos == line_terminate[0] && (end - pos >= line_term_len) &&
            (string(pos, line_term_len) == line_terminate)) {
          handle_char_one_by_one_params param;
          param.start = start;
          param.pos = pos;
          param.fields_count = fields_count;
          param.line_len = line_len;
          param.enclose_field_end = enclose_field_end;
          param.line_term_len = line_term_len;

          concatenate_one_row_from_previous_pkt_and_cur_pkt_to_handle(
              param, escaped_pos_vec);

          pos = param.pos;
        }
      }
    }
    tmp_buffer->rewind();
    const char *field_start = pos;
    const char *line_start = pos;
    unsigned int fields_count = 0;
    bool is_char_escaped = false;
    bool need_enclose = false;
    if (has_field_enclose && field_enclose == *field_start) need_enclose = true;
    bool enclose_field_end = false;
    escaped_pos_vec
        ->clear();  // offset of escaped_char to current field's start pos

    handle_char_one_by_one_params param;
    param.pos = pos;
    param.end = end;
    param.is_char_escaped = is_char_escaped;
    param.need_enclose = need_enclose;
    param.enclose_field_end = enclose_field_end;
    param.fields_count = fields_count;
    param.line_len = line_len;
    param.field_start = field_start;
    param.line_start = line_start;

    handle_char_in_packet_one_by_one(param, escaped_pos_vec);

    pos = param.pos;
    end = param.end;
    is_char_escaped = param.is_char_escaped;
    need_enclose = param.need_enclose;
    enclose_field_end = param.enclose_field_end;
    fields_count = param.fields_count;
    line_len = param.line_len;
    field_start = param.field_start;
    line_start = param.line_start;

    if (fields_count > table_fields_num
#ifndef DBSCALE_TEST_DISABLE
        && instance_option_value["check_column_load_assp"].int_val == 1
#endif
    ) {
      LOG_ERROR(
          "Fields count %d is larger than table_fields_num %d, plz check line "
          "terminate char.\n",
          fields_count, table_fields_num);
      throw Error(
          "Fields count is larger than table_fields_num, plz check line "
          "terminate char.");
    }

    if (line_start != end) {
      save_to_previous_buff(end, line_start, field_start, fields_count,
                            need_enclose, enclose_field_end, is_char_escaped,
                            escaped_pos_vec);
    }
  } catch (...) {
    if (escaped_pos_vec) delete escaped_pos_vec;
    throw;
  }
  delete escaped_pos_vec;
}

void MySQLLoadPacketAnalysisNode::fill_field_value(
    char *field, size_t pos_vec_size, const char *begin, const char *start,
    const char *end, int enclose, size_t len, vector<int> &pos_vec) {
  for (size_t i = 0; i < pos_vec_size; i++) {
    int pos = begin - start - i - enclose;
    if ((int)len < pos ||
        pos_vec[i] - (int)(begin - start) > (int)(len - pos)) {
      LOG_ERROR(
          "MySQLLoadPacketAnalysisNode::add_column_value memcpy target wrong "
          "with len [%d] pos [%d] begin-start [%d] pos_vec[i] [%d].\n",
          len, pos, begin - start, pos_vec[i]);
      throw Error(
          "MySQLLoadPacketAnalysisNode::add_column_value memcpy target wrong, "
          "please try other value of fields escaped/enclosed/terminated by or "
          "lines terminated by");
    }
    if (end < begin || pos_vec[i] < (int)(begin - start) ||
        (int)(end - begin) < pos_vec[i] - (int)(begin - start)) {
      LOG_ERROR(
          "MySQLLoadPacketAnalysisNode::add_column_value memcpy source wrong "
          "with end-begin [%d] pos_vec[i] [%d] begin-start [%d].\n",
          end - begin, pos_vec[i], begin - start);
      throw Error(
          "MySQLLoadPacketAnalysisNode::add_column_value memcpy source wrong, "
          "please try other value of fields escaped/enclosed/terminated by or "
          "lines terminated by");
    }

    if (len > 0) {
      int len = pos_vec[i] - (int)(begin - start);
      memcpy(field + pos, begin, (size_t)len);
    }
    begin = begin + pos_vec[i] - (begin - start) + 1;
  }
  int pos = begin - start - pos_vec_size - enclose;
  if ((int)len < pos || (size_t)(end - begin - enclose) > len - pos) {
    LOG_ERROR(
        "MySQLLoadPacketAnalysisNode::add_column_value memcpy target wrong "
        "with end-begin [%d], enclose [%d], len [%d], pos [%d].\n",
        end - begin, enclose, len, pos);
    throw Error(
        "MySQLLoadPacketAnalysisNode::add_column_value memcpy target wrong on "
        "second check, please try other value of fields "
        "escaped/enclosed/terminated by or lines terminated by");
  } else if (begin > end - enclose) {
    LOG_ERROR(
        "MySQLLoadPacketAnalysisNode::add_column_value memcpy source wrong "
        "with begin > end - enclose.\n");
    throw Error(
        "MySQLLoadPacketAnalysisNode::add_column_value memcpy source wrong on "
        "second check, please try other value of fields "
        "escaped/enclosed/terminated by or lines terminated by");
  }

  if (len > 0) {
    size_t tmp = end - begin - enclose;
    memcpy(field + pos, begin, tmp);
  }
}

char *MySQLLoadPacketAnalysisNode::get_field_replace_null_str(stmt_node *st,
                                                              size_t len,
                                                              char *field) {
  char *field_replace_null = NULL;
  set<char> candidate_null_char_set =
      Backend::instance()->get_dbscale_replace_null_char_set();
  vector<size_t> null_pos;
  size_t null_count = 0;
  for (size_t i = 0; i < len; i++) {
    if (field[i] == 0) {
      null_pos.push_back(i);
      null_count++;
    }
    if (candidate_null_char_set.count(field[i]))
      candidate_null_char_set.erase(field[i]);
  }
  if (!candidate_null_char_set.empty()) {
    col_replace_null_char = string(1, *(candidate_null_char_set.begin()));
    field_replace_null = static_cast<char *>(assign_mem_from_stmt(st, len + 1));
    if (!field_replace_null) {
      LOG_ERROR("Cannot allocate memory for field_replace_null with len %d\n",
                len + 1);
      throw NoMemoryError("allocate memory for field_replace_null");
    }
    memcpy(field_replace_null, field, len + 1);
    for (size_t i = 0; i < null_count; i++)
      field_replace_null[null_pos[i]] = col_replace_null_char[0];
    field_replace_null[len] = 0;
    LOG_DEBUG(
        "NUL replace char is [%c], column with NUL character is modified to "
        "[%s]\n",
        col_replace_null_char[0], field_replace_null);
  } else {
    // single-char is not enough, try multi-char
    const char *dbscale_replace_null_char_candidate = DBSCALE_REPLACE_NULL_CHAR;
    size_t dbscale_replace_null_char_candidate_len =
        strlen(dbscale_replace_null_char_candidate);
    size_t max_candiate_len = 10;
    bool found_candidate = false;
    string field_str = string(field, len);
    for (size_t m = 2; m <= max_candiate_len; m++) {
      for (size_t i = 0; i < dbscale_replace_null_char_candidate_len; i++) {
        col_replace_null_char = string(
            m, dbscale_replace_null_char_candidate[i]);  // TODO build
                                                         // more candiates
        if (field_str.find(col_replace_null_char) == std::string::npos) {
          found_candidate = true;
          for (size_t j = 0; j < null_count; j++) {
            field_str.replace(null_pos[j] + j * (m - 1), 1,
                              col_replace_null_char);
          }
          size_t real_len = len + null_count * (m - 1);
#ifdef DEBUG
          ACE_ASSERT(real_len == field_str.size());
#endif
          field_replace_null =
              static_cast<char *>(assign_mem_from_stmt(st, real_len + 1));
          if (!field_replace_null) {
            LOG_ERROR(
                "Cannot allocate memory for field_replace_null with len %d\n",
                real_len + 1);
            throw NoMemoryError("allocate memory for field_replace_null");
          }
          memcpy(field_replace_null, field_str.c_str(), real_len);
          field_replace_null[real_len] = 0;
          LOG_DEBUG(
              "NUL replace char is multi-char [%s], query sql with NUL "
              "character is modified to [%s]\n",
              col_replace_null_char.c_str(), field_str.c_str());
          break;
        }
      }
      if (found_candidate) break;
    }
    if (!found_candidate) {
      LOG_ERROR(
          "the row which partition column contains NUL character can not be "
          "handled, related column is [%s]\n",
          field);
      throw Error(
          "the row which partition column contains NUL character can not be "
          "handled");
    }
  }
  return field_replace_null;
}

Expression *MySQLLoadPacketAnalysisNode::get_column_expr(
    stmt_node *st, size_t pos_vec_size, size_t len, char *field,
    bool need_replace_internal_nul_char, char *field_replace_null) {
  Expression *expr = NULL;
  if (pos_vec_size == 0 || !(len == 1 && field[0] == 'N')) {
    if (!need_replace_internal_nul_char)
      expr = new (st) StrExpression(field, EXPR_STRING);
    else
      expr =
          new (st) StrExpression((const char *)field_replace_null, EXPR_STRING);
  } else {
    expr = new (st) NullExpression();
  }
  if (expr == NULL) {
    LOG_ERROR("Cannot allocate memory for column value\n");
    throw NoMemoryError("allocate memory for column value");
  }
  return expr;
}

void MySQLLoadPacketAnalysisNode::add_expr_to_column_values(stmt_node *st,
                                                            Expression *expr) {
  if (column_values.expr_list_head == NULL) {
    column_values.expr_list_head =
        (expr_list_item *)assign_mem_for_struct(st, sizeof(expr_list_item));
    if (column_values.expr_list_head == NULL) {
      LOG_ERROR("Cannot allocate memory for column value\n");
      throw NoMemoryError("allocate memory for column value");
    }
    column_values.expr_list_head->expr = expr;
    expr->parent = &column_values;
    column_values.expr_list_head->next = column_values.expr_list_head;
    column_values.expr_list_tail = column_values.expr_list_head;
  } else {
    column_values.expr_list_tail->next =
        (expr_list_item *)assign_mem_for_struct(st, sizeof(expr_list_item));
    if (column_values.expr_list_tail->next == NULL) {
      LOG_ERROR("Cannot allocate memory for column value\n");
      throw NoMemoryError("allocate memory for column value");
    }
    column_values.expr_list_tail->next->expr = expr;
    expr->parent = &column_values;
    column_values.expr_list_tail->next->next = column_values.expr_list_head;
    column_values.expr_list_tail = column_values.expr_list_tail->next;
  }
}

void MySQLLoadPacketAnalysisNode::add_column_value(stmt_node *st,
                                                   const char *start,
                                                   const char *end,
                                                   vector<int> &pos_vec,
                                                   bool field_has_enclose) {
  bool need_check_internal_nul_char = false;
  if (column_pos_one_row ==
      (*key_pos_vec)[0])  // we assume key_pos_vec.size() == 1
    need_check_internal_nul_char = true;
  if (st->mem_head == NULL)
    if (init_stmt_node(st)) {
      LOG_ERROR(
          "Fail to assign memory for "
          "MySQLLoadPacketAnalysisNode::add_column_value for "
          "init_stmt_node.\n");
      throw Error(
          "Fail to assign memory for "
          "MySQLLoadPacketAnalysisNode::add_column_value.");
    }

  size_t pos_vec_size = pos_vec.size();
  if ((int)(end - start) < 0 || (size_t)(end - start) < pos_vec_size)
    throw Error(
        "MySQLLoadPacketAnalysisNode::add_column_value memcpy fail due to "
        "invalid len, please try other value of fields "
        "escaped/enclosed/terminated by or lines terminated by");
  size_t len = end - start - pos_vec_size;

  if (load_data_quick_error > 0 && len >= 16777216 /*16MB*/) {
    LOG_ERROR(
        "load_data_quick_error > 0, load data add_column_value detect column "
        "length larger than 16MB\n");
    throw Error(
        "load_data_quick_error > 0, load data add_column_value detect column "
        "length larger than 16MB");
  }

  const char *begin = start;
  int enclose = 0;
  if (field_has_enclose) {
    if (len < 2)
      throw Error(
          "MySQLLoadPacketAnalysisNode::add_column_value memcpy fail due to "
          "invalid len, please try other value of fields "
          "escaped/enclosed/terminated by or lines terminated by");
    len = len - 2;
    begin = start + 1;
    enclose = 1;
  }
  // add 1 for '\0' terminate
  char *field = static_cast<char *>(assign_mem_from_stmt(st, len + 1));
  if (!field) {
    LOG_ERROR(
        "Fail to assign memory for "
        "MySQLLoadPacketAnalysisNode::add_column_value with len %d.\n",
        len);
    throw Error(
        "Fail to assign memory for "
        "MySQLLoadPacketAnalysisNode::add_column_value.");
  }
  if (!begin) {
    throw Error(
        "MySQLLoadPacketAnalysisNode::add_column_value memcpy fail with begin "
        "NULL, please try other value of fields escaped/enclosed/terminated by "
        "or lines terminated by");
  }

  if (pos_vec_size == 0) {
    if ((int)len < 0 || end - begin < (int)len) {
      throw Error(
          "MySQLLoadPacketAnalysisNode::add_column_value memcpy fail with "
          "vec_size 0, please try other value of fields "
          "escaped/enclosed/terminated by or lines terminated by");
    }
    if (len > 0) memcpy(field, begin, len);
  } else {
    fill_field_value(field, pos_vec_size, begin, start, end, enclose, len,
                     pos_vec);
  }
  field[len] = '\0';
  pos_vec.clear();
  char *field_replace_null = NULL;
  bool need_replace_internal_nul_char = false;
  if (need_check_internal_nul_char &&
      (len > strlen(field))) {  // there is at least one NUL character in the
                                // field content.
    need_replace_internal_nul_char = true;
    field_replace_null = get_field_replace_null_str(st, len, field);
  }
  Expression *expr =
      get_column_expr(st, pos_vec_size, len, field,
                      need_replace_internal_nul_char, field_replace_null);
  add_expr_to_column_values(st, expr);
  column_pos_one_row += 1;
}

void MySQLLoadPacketAnalysisNode::handle_row(const char *line_start,
                                             const char *line_end) {
  if (line_end - line_start == 1) {
    LOG_DEBUG(
        "Find an empty row, which only contains the line terminate char, skip "
        "it.\n");
    // line ended, free all memory allocate for the line
    free_stmt_node_mem(&tmp_st);
    column_values.expr_list_head = column_values.expr_list_tail = NULL;

    return;
  }
  int part_id;
  int64_t auto_inc_val = 0;
  if (need_append_auto_inc_column) {
    auto_inc_val = get_auto_inc_value(line_start, line_end);
  } else if (load_data_check_auto_inc && table_has_auto_inc_column) {
    auto_inc_val = check_auto_inc_value(line_start, line_end);
  }
  try {
    if (need_append_auto_inc_column && !has_set_auto_inc_status) {
      stmt->set_auto_inc_status(AUTO_INC_VALUE_NULL);
      has_set_auto_inc_status = true;
    }
    part_id = stmt->get_partition_one_insert_row(
        &column_values, key_pos_vec, auto_inc_val, -1, partition_method,
        col_replace_null_char);
    part_id = part_space->get_real_par_id_from_virtual_id(part_id);
  } catch (...) {
    LOG_ERROR("Got unsupported row data for LOAD DATA.\n");
    throw ExecuteNodeError(
        "Got unsupported row data for LOAD DATA, due to fail to get partition "
        "for one insert row.");
  }
  if (need_update_last_insert_id &&
      stmt->get_auto_inc_status() == AUTO_INC_VALUE_NULL) {
    plan->handler->get_session()->set_last_insert_id(auto_inc_val);
    need_update_last_insert_id = false;
  }
  Packet *buffer = buffers[part_id];
  size_t len = buffer->length();
  size_t line_len = line_end - line_start;

  if (len + line_len + 2048 > max_packet_size) {
    /*only put the packet to child when it is full.*/
    table_node->add_to_ready_packets_from_Analysis(part_id, buffers[part_id]);
    buffers[part_id] = NULL;
    buffers[part_id] = Backend::instance()->get_new_packet(max_packet_size);
    buffers[part_id]->rewind();
    Packet::pack3int(buffers[part_id]->base(), 0);
    driver->set_packet_number(buffers[part_id], 0);
    buffers[part_id]->wr_ptr(PACKET_HEADER_SIZE);
    buffer = buffers[part_id];
    len = buffer->length();
  }

  int alt_len = 0;
  unsigned int line_term_len = line_terminate.length();
  if (need_append_auto_inc_column) {
    if (line_term_len > line_len ||
        buffer->size() - (buffer->wr_ptr() - buffer->base()) <
            line_len - line_term_len) {
      LOG_ERROR(
          "MySQLLoadPacketAnalysisNode::handle_row memcpy error with "
          "line_term_len [%d] line_len [%d] buff_len [%d] write_len [%d].\n",
          line_term_len, line_len, buffer->size(),
          (buffer->wr_ptr() - buffer->base()));
      throw Error("MySQLLoadPacketAnalysisNode::handle_row memcpy error");
    }

    memcpy(buffer->wr_ptr(), line_start, line_len - line_term_len);
    char *ptr = buffer->wr_ptr() + (line_len - line_term_len);
    if (has_field_enclose) {
      alt_len =
          sprintf(ptr, "%s%c%ld%c%s", field_terminate.c_str(), field_enclose,
                  auto_inc_val, field_enclose, line_terminate.c_str()) -
          line_term_len;
    } else {
      alt_len = sprintf(ptr, "%s%ld%s", field_terminate.c_str(), auto_inc_val,
                        line_terminate.c_str()) -
                line_term_len;
    }
    line_len += alt_len;
  } else {
    char *tmp_line = NULL;
    size_t tmp_line_len = line_len + 21;
    if (line_len + 21 > DEFAULT_BUFFER_SIZE) {
      char *new_line = new char[line_len + 21];
      tmp_line = new_line;
    } else {
      tmp_line = line_buffer;
    }
    char *tmp_line_begin = tmp_line;
    tmp_line[0] = '\0';
    unsigned int field_term_len = field_terminate.length();
    if (load_data_check_auto_inc && table_has_auto_inc_column &&
        stmt->get_auto_inc_status() != AUTO_INC_NO_NEED_MODIFY) {
      memset(tmp_line, 0, tmp_line_len);
      int i = 0, j = 0;
      const char *c = line_start;
      for (; i < auto_inc_key_pos; i++) {
        do {
          tmp_line[j++] = *c++;
        } while (*c != field_terminate[0] ||
                 (line_end - c >= field_term_len &&
                  field_terminate != string(c, field_term_len)));
        for (unsigned int k = 0; k < field_term_len; k++)
          tmp_line[j++] = *c++;  // eat terminate flag
      }
      alt_len = sprintf(&tmp_line[j], "%ld", auto_inc_val);
      while (tmp_line[j]) {
        j++;
      }
      while (*c &&
             (*c != field_terminate[0] ||
              (line_end - c >= field_term_len &&
               field_terminate != string(c, field_term_len))) &&
             (line_end - c > line_term_len)) {
        c++;
        alt_len--;
      }
      if (line_end < c || (int)line_len + 21 - j < (int)(line_end - c)) {
        LOG_ERROR(
            "MySQLLoadPacketAnalysisNode::handle_row memcpy error2 with "
            "line_len [%d] line_end [%d] c [%d].\n",
            line_len, line_end, c);
        throw Error("MySQLLoadPacketAnalysisNode::handle_row memcpy error");
      }

      memcpy(&tmp_line[j], c, line_end - c);
    }
    line_len += alt_len;
    if (line_len > buffer->size()) {
      size_t len_now = buffer->wr_ptr() - buffer->base();
      buffer->size(line_len * 2);
      buffer->rewind();
      buffer->wr_ptr(len_now);
    }
    memcpy(buffer->wr_ptr(), *tmp_line ? tmp_line : line_start, line_len);
    if (tmp_line_len > DEFAULT_BUFFER_SIZE) {
      delete[] tmp_line_begin;
    }
  }
  buffer->wr_ptr(line_len);
  // line ended, free all memory allocate for the line
  free_stmt_node_mem(&tmp_st);
  column_values.expr_list_head = column_values.expr_list_tail = NULL;
  column_pos_one_row = 0;
  col_replace_null_char.clear();
}

/* class MySQLLoadDataInfilePartTableNode */
MySQLLoadDataInfilePartTableNode::MySQLLoadDataInfilePartTableNode(
    ExecutePlan *plan, PartitionedTable *dataspace, const char *sql,
    const char *schema_name, const char *table_name)
    : MySQLLoadLocalPartTableNode(plan, dataspace, sql, schema_name,
                                  table_name) {
  this->name = "MySQLLoadDataInfilePartTableNode";
  delete_flag = false;
}

Packet *MySQLLoadDataInfilePartTableNode::read_one_packet() {
  if (session->get_is_killed()) {
    LOG_DEBUG("Load data is killed, so cancel read_one_packet()\n");
    has_end_send_data = true;
    throw Error("Load data is killed");
  }
  Packet *packet = Backend::instance()->get_new_packet(LOAD_CLIENT_PACKET_SIZE);
  packet->rewind();
  try {
    if (plan->statement->is_load_insert_select())
      read_file_into_buf_for_load_insert(*packet);
    else
      read_file_into_buf(*packet);
  } catch (...) {
    delete packet;
    LOG_ERROR("file read error in load data infile\n");
    throw;
  }
  pack_header(packet, size_temp);
  return packet;
}

void MySQLLoadDataInfilePartTableNode::request_file_and_send_data_to_server() {
#ifdef DEBUG
  Packet *result = partition_nodes[0]->get_result();
  ASSERT(!driver->is_error_packet(result));
#endif

  if (plan->statement->is_load_insert_select())
    init_file_buf_for_load_insert();
  else
    init_file_buf();
  while (1) {
    try {
      fill_packets_to_analysis();
      bool analysis_fin = wait_and_check_analysis();
      if (analysis_fin) {
        LOG_DEBUG("All analysis has finished.\n");
        break;
      }
    } catch (...) {
      LOG_ERROR(
          "Get exception in "
          "MySQLLoadDataInfilePartTableNode::request_file_and_send_data_to_"
          "server.\n");
      has_end_send_data = true;
      throw;
    }
    if (ACE_Reactor::instance()->reactor_event_loop_done()) {
      LOG_INFO("DBScale is exiting, so stop this load data task.\n");
      throw Error("DBScale is exiting.");
    }
  }
  flush_all();
}

void MySQLLoadDataInfilePartTableNode::send_query() {
  sql_add_local(plan, sql);
  sql = new_sql->c_str();
  Packet packet;
  const char *file_path =
      plan->statement->get_stmt_node()->sql->load_oper->filename;
  if (!open_local_file(file_path)) {
    build_error_packet(packet, file_path);
    status = EXECUTE_STATUS_COMPLETE;
    delete_flag = false;
    handler->send_to_client(&packet);
    session->set_cur_stmt_execute_fail(true);
    session->acquire_has_send_client_error_packet_mutex();
    session->set_has_send_client_error_packet();
    session->release_has_send_client_error_packet_mutex();
    throw ExecuteNodeError("open local file failed");
  }
  delete_flag = true;
  MySQLLoadLocalPartTableNode::send_query();
}

void MySQLLoadDataInfilePartTableNode::clean() {
  if (delete_flag) {
    MySQLLoadLocalPartTableNode::clean();
  } else {
    clean_up_analysis_nodes();
  }
  if (new_sql) delete new_sql;
  file_close();
}
/* class MySQLDistinctNode */

MySQLDistinctNode::MySQLDistinctNode(ExecutePlan *plan,
                                     list<int> &column_indexes)
    : MySQLSortNode(plan) {
  this->name = "MySQLDistinctNode";
  this->column_indexes = column_indexes;
  node_can_swap = false;
}

// TODO: improve the algorithm with bi-partitioned insertion algorithm
void MySQLDistinctNode::insert_row(Packet *row) {
  list<Packet *>::iterator it = ready_rows->begin();
  for (; it != ready_rows->end(); it++) {
    if (MySQLColumnCompare::compare(row, *it, &column_indexes, &column_types) ==
        0) {
      delete row;
      return;
    }
  }
  ready_rows->push_back(row);
}

void MySQLDistinctNode::handle_child(MySQLExecuteNode *child) {
  Packet *row;
  while (!row_map[child]->empty()) {
    row = row_map[child]->front();
    insert_row(row);
    row_map[child]->pop_front();
  }
}

void MySQLDistinctNode::handle_children() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if (!row_map[*it]->empty()) handle_child(*it);
  }
}

void MySQLDistinctNode::execute() {
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START:
        init_row_map();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        status = EXECUTE_STATUS_WAIT;

      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;

      case EXECUTE_STATUS_HANDLE:
#ifdef DEBUG
        node_start_timing();
#endif
        init_column_types(get_field_packets(), column_types, column_num,
                          &column_inited_flag);
        adjust_column_index();
        check_column_valid();
        handle_children();
#ifdef DEBUG
        node_end_timing();
#endif
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_BEFORE_COMPLETE:
        status = EXECUTE_STATUS_COMPLETE;
        break;

      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        LOG_DEBUG("MySQLDistinctNode %@ cost %d ms\n", this, node_cost_time);
#endif
        break;

      default:
        ACE_ASSERT(0);
        break;
    }
  }
}

void MySQLDistinctNode::do_clean() {
  Packet *packet;
  while (!ready_nodes.empty()) {
    packet = ready_nodes.back();
    ready_nodes.pop_back();
    if (packet) delete packet;
  }
}

/* class MySQLDBScaleShowPartitionNode */

MySQLDBScaleShowPartitionNode::MySQLDBScaleShowPartitionNode(ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowPartitionNode";
}

void MySQLDBScaleShowPartitionNode::init_data() {
  if (set_info()) {
    set_head_packet(4);
    const char *catalog = "def";
    const char *column = "Sequence";
    const char *org_column = "Sequence";
    add_column_packet(catalog, schema_name, table_name, table_name, column,
                      org_column, 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0,
                      0);
    column = "Type";
    org_column = "Type";
    add_column_packet(catalog, schema_name, table_name, table_name, column,
                      org_column, 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0,
                      0);
    column = "Expresion";
    org_column = "Expresion";
    add_column_packet(catalog, schema_name, table_name, table_name, column,
                      org_column, 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0,
                      0);

    column = "Data Source";
    org_column = "Data Source";
    add_column_packet(catalog, schema_name, table_name, table_name, column,
                      org_column, 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0,
                      0);
    int number = 0;
    char num[12];
    while (!data_source_names.empty()) {
      list<const char *> row_data;
      sprintf(num, "%d", number);
      row_data.push_back(num);
      row_data.push_back(part_type_name.c_str());
      const char *expresion_name = expresion.c_str();
      row_data.push_back(expresion_name);
      const char *source_name = data_source_names.front();
      row_data.push_back(source_name);
      add_row_packet(row_data);
      data_source_names.pop_front();
      number++;
    }
  } else {
    status = EXECUTE_STATUS_COMPLETE;
    set_error_packet(1109, "Unknown Table", "42S02");
    throw ErrorPacketException();
  }
}

bool MySQLDBScaleShowPartitionNode::set_info() {
  table_name =
      plan->statement->get_stmt_node()->table_list_tail->join->table_name;
  schema_name =
      plan->statement->get_stmt_node()->table_list_tail->join->schema_name;

  LOG_DEBUG("MySQLDBScaleShowPartitionNode schema:%s table:%s.\n", schema_name,
            table_name);

  if (NULL == schema_name) {
    schema_name = session->get_schema();
  }

  Backend *backend = Backend::instance();

  Table *table_tmp = backend->get_table_by_name(schema_name, table_name);
  if (table_tmp) {
    if (table_tmp->is_partitioned()) {
      PartitionedTable *table = (PartitionedTable *)table_tmp;
      PartitionScheme *partition_schema = table->get_partition_scheme();
      PartitionType part_type = table->get_partition_type();

      switch (part_type) {
        case PARTITION_TYPE_HASH: {
          part_type_name.clear();
          expresion.clear();
          part_type_name.append("Hash");
          vector<const char *> partition_keys = *table->get_key_names();
          expresion.append(partition_keys[0]);
          for (unsigned int i = 1; i < partition_keys.size(); i++) {
            expresion.append(", ");
            expresion.append(partition_keys[i]);
          }
          break;
        }
        case PARTITION_TYPE_MOD: {
          part_type_name.clear();
          expresion.clear();
          part_type_name.append("Mod");
          vector<const char *> partition_keys = *table->get_key_names();
          expresion.append(partition_keys[0]);
          for (unsigned int i = 1; i < partition_keys.size(); i++) {
            expresion.append(", ");
            expresion.append(partition_keys[i]);
          }
          break;
        }
        default:
          part_type_name.clear();
          part_type_name.append(" ");
          break;
      }

      for (unsigned int i = 0; i < partition_schema->get_partition_num(); i++) {
        data_source_names.push_back(
            partition_schema->get_partition(i)->get_data_source()->get_name());
      }
    } else {
      part_type_name.clear();
      expresion.clear();
      part_type_name.append("Normal Table");
      expresion.append(" ");
      data_source_names.push_back(table_tmp->get_data_source()->get_name());
    }
    return true;
  }
  return false;
}

/* class MySQLRowsNode */
MySQLRowsNode::MySQLRowsNode(ExecutePlan *plan, list<list<string> *> *row_list)
    : MySQLInnerNode(plan) {
  this->name = "MySQLRowsNode";
  this->row_list = row_list;
  build_row();
  node_can_swap = false;
}
void MySQLRowsNode::handle_child(MySQLExecuteNode *child) {
  Packet *row;
  while (!row_map[child]->empty()) {
    row = row_map[child]->front();
    ready_rows->push_back(row);
    row_map[child]->pop_front();
  }
}
void MySQLRowsNode::build_row() {
  if (row_list != NULL) {
    list<list<string> *>::iterator it = row_list->begin();
    for (; it != row_list->end(); it++) {
      int packet_len = 0;
      int len = 0;
      new_row = Backend::instance()->get_new_packet(row_packet_size);
      new_row->wr_ptr(new_row->base() + PACKET_HEADER_SIZE);
      list<string>::iterator itinner = (*it)->begin();
      unsigned int needed_length = 1;  // for terminate '\0'
      for (; itinner != (*it)->end(); itinner++) {
        needed_length += strlen((*itinner).c_str()) + 1;
      }
      if (new_row->size() - (new_row->wr_ptr() - new_row->base()) <
          needed_length) {
        uint64_t resize_len =
            needed_length + new_row->wr_ptr() - new_row->base();
        LOG_DEBUG("Packet size not enough, resize packet size from %d to %d\n",
                  new_row->size(), resize_len);
        new_row->size(resize_len);
      }
      itinner = (*it)->begin();
      for (; itinner != (*it)->end(); itinner++) {
        len = strlen((*itinner).c_str());
        new_row->pack_lenenc_int(len);
        new_row->packstr((*itinner).c_str());
        packet_len = packet_len + len + 1;
      }
      pack_header(new_row, packet_len);
      ready_rows->push_back(new_row);
    }
  }
}
void MySQLRowsNode::handle_children() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if (!row_map[*it]->empty()) handle_child(*it);
  }
}
void MySQLRowsNode::execute() {
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START:
        init_row_map();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        status = EXECUTE_STATUS_WAIT;

      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;

      case EXECUTE_STATUS_HANDLE:
#ifdef DEBUG
        node_start_timing();
#endif
        handle_children();
#ifdef DEBUG
        node_end_timing();
#endif
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_BEFORE_COMPLETE:
        status = EXECUTE_STATUS_COMPLETE;
        break;

      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        LOG_DEBUG("MySQLRowsNode %@ cost %d ms\n", this, node_cost_time);
#endif
        break;

      default:
        ACE_ASSERT(0);
        break;
    }
  }
}
void MySQLRowsNode::do_clean() {
  list<string> *del_list = NULL;
  if (row_list != NULL) {
    list<list<string> *>::iterator it = row_list->begin();
    for (; it != row_list->end(); it++) {
      del_list = *it;
      del_list->clear();
      if (del_list->empty()) {
        delete del_list;
      }
    }
    row_list->clear();
    if (row_list->empty()) {
      delete row_list;
    }
  }
}

// class MySQLFilterNode

void MySQLFilterNode::handle_child(MySQLExecuteNode *child) {
  Packet *row;
  while (!row_map[child]->empty()) {
    row = row_map[child]->front();
    row_map[child]->pop_front();
    if (filter(row)) {
      ready_rows->push_back(row);
    } else {
      delete row;
    }
  }
}

void MySQLFilterNode::handle_children() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if (!row_map[*it]->empty()) handle_child(*it);
  }
}

void MySQLFilterNode::execute() {
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START:
        init_row_map();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;
      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        status = EXECUTE_STATUS_WAIT;
      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;
      case EXECUTE_STATUS_HANDLE: {
#ifdef DEBUG
        node_start_timing();
#endif
        handle_children();
#ifdef DEBUG
        node_end_timing();
#endif
        status = EXECUTE_STATUS_FETCH_DATA;
      } break;
      case EXECUTE_STATUS_BEFORE_COMPLETE:
        status = EXECUTE_STATUS_COMPLETE;
        break;
      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        LOG_DEBUG("MySQLFilterNode %@ cost %d ms\n", this, node_cost_time);
#endif
        break;
      default:
        ACE_ASSERT(0);
        break;
    }
  }
}

/* class MySQLRegexFilterNode */
MySQLRegexFilterNode::MySQLRegexFilterNode(ExecutePlan *plan,
                                           enum FilterFlag filter_way,
                                           int column_index,
                                           list<const char *> *pattern)
    : MySQLFilterNode(plan) {
  this->name = "MySQLRegexFilterNode";
  this->filter_way = filter_way;
  this->column_index = column_index;
  this->pattern = pattern;
}

bool MySQLRegexFilterNode::filter(Packet *row) {
  bool match = false;
  char store_temp;
  uint64_t length;

  MySQLRowResponse row_rev(row);
  row_rev.set_current(column_index);
  length = row_rev.get_current_length();
  char *current_data = row_rev.get_current_data();
  store_temp = current_data[length];
  current_data[length] = '\0';
  list<const char *>::iterator it = pattern->begin();

  for (; it != pattern->end(); it++) {
    try {
      boost::regex reg(*it);
      if (regex_match(current_data, reg)) {
        match = true;
        break;
      }
    } catch (exception &e) {
      LOG_ERROR(
          "MySQLRegexFilterNode::filter get exception %s with current_data "
          "[%s] reg [%s], ignore this reg.\n",
          e.what(), current_data, *it);
      continue;
    }
  }

  current_data[length] = store_temp;
  return (filter_way == FILTER_FLAG_MATCH ? match : !match);
}

void MySQLRegexFilterNode::do_clean() {
  if (pattern != NULL) {
    pattern->clear();
    delete pattern;
  }
}

// class MySQLAvgShowTableStatusNode

MySQLAvgShowTableStatusNode::MySQLAvgShowTableStatusNode(ExecutePlan *plan)
    : MySQLInnerNode(plan) {
  this->name = "MySQLAvgShowTableStatusNode";
  node_can_swap = false;
  packet = NULL;
  field_packet = NULL;
  is_simple =
      plan->statement->get_stmt_node()->sql->show_tables_oper->is_simple;
}

Packet *MySQLAvgShowTableStatusNode::get_header_packet() {
  if (!is_simple) {
    MySQLExecuteNode *child = children.front();
    return child->get_header_packet();
  } else {
    if (!packet) {
      packet = Backend::instance()->get_new_packet();
      MySQLResultSetHeaderResponse result_set_header(5, 0);
      result_set_header.pack(packet);
    }
    return packet;
  }
}

list<Packet *> *MySQLAvgShowTableStatusNode::get_field_packets() {
  if (!is_simple) {
    MySQLExecuteNode *child = children.front();
    return child->get_field_packets();
  } else {
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "consistence_point";
    const char *org_table = "consistence_point";

    if (!field_packet) {
      field_packet = new list<Packet *>();
      list<const char *> columns;
      columns.push_back("Name");
      columns.push_back("Rows");
      columns.push_back("Avg_row_length");
      columns.push_back("Data_length");
      columns.push_back("Max_data_length");
      while (!columns.empty()) {
        MySQLColumnResponse field(catalog, schema, table, org_table,
                                  columns.front(), columns.front(), 8,
                                  NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
        Packet *packet_tmp = Backend::instance()->get_new_packet();
        field.pack(packet_tmp);
        field_packet->push_back(packet_tmp);
        columns.pop_front();
      }
    }
    return field_packet;
  }
}

MySQLAvgShowTableStatusNode::~MySQLAvgShowTableStatusNode() {
  if (packet) delete packet;
  packet = NULL;
  if (field_packet) {
    list<Packet *>::iterator it_packet = field_packet->begin();
    for (; it_packet != field_packet->end(); it_packet++) {
      delete *it_packet;
    }
    delete field_packet;
    field_packet = NULL;
  }
}

void MySQLAvgShowTableStatusNode::handle_child(MySQLExecuteNode *child) {
  Packet *row;

  if (is_simple) {
    while (!row_map[child]->empty()) {
      row = row_map[child]->front();
      MySQLShowTableSimpleStatusResponse row_response(row);
      Packet *new_row = Backend::instance()->get_new_packet(row_packet_size);
      row_response.unpack();
      if (row_response.is_view()) {
        row_response.pack(new_row);
        ready_rows->push_back(new_row);
        delete row;
      } else {
        if (row_response.rows != 0) {
          if (row_response.avg_data_length !=
              row_response.data_length / row_response.rows) {
            row_response.avg_data_length =
                row_response.data_length / row_response.rows;
            row_response.pack(new_row);
            ready_rows->push_back(new_row);
            delete row;
          } else {
            row_response.pack(new_row);
            ready_rows->push_back(new_row);
            delete row;
          }
        } else {
          row_response.pack(new_row);
          ready_rows->push_back(new_row);
          delete row;
        }
      }
      row_map[child]->pop_front();
    }
  } else {
    while (!row_map[child]->empty()) {
      row = row_map[child]->front();
      MySQLShowTableStatusResponse row_response(row);
      row_response.unpack();
      if (row_response.is_view()) {
        ready_rows->push_back(row);
      } else {
        if (row_response.rows != 0) {
          if (row_response.avg_data_length !=
              row_response.data_length / row_response.rows) {
            row_response.avg_data_length =
                row_response.data_length / row_response.rows;
            Packet *new_row =
                Backend::instance()->get_new_packet(row_packet_size);
            row_response.pack(new_row);
            ready_rows->push_back(new_row);
            delete row;
          } else {
            ready_rows->push_back(row);
          }
        } else {
          ready_rows->push_back(row);
        }
      }
      row_map[child]->pop_front();
    }
  }
}

void MySQLAvgShowTableStatusNode::handle_children() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if (!row_map[*it]->empty()) {
      handle_child(*it);
    }
  }
}

void MySQLAvgShowTableStatusNode::execute() {
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START:
        init_row_map();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;
      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        status = EXECUTE_STATUS_WAIT;
      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;
      case EXECUTE_STATUS_HANDLE:
#ifdef DEBUG
        node_start_timing();
#endif
        handle_children();
#ifdef DEBUG
        node_end_timing();
#endif
        status = EXECUTE_STATUS_FETCH_DATA;
        break;
      case EXECUTE_STATUS_BEFORE_COMPLETE:
#ifdef DEBUG
        LOG_DEBUG("MySQLAvgShowTableStatusNode %@ cost %d ms\n", this,
                  node_cost_time);
#endif
        status = EXECUTE_STATUS_COMPLETE;
        break;
      case EXECUTE_STATUS_COMPLETE:
        break;
      default:
        ACE_ASSERT(0);
        break;
    }
  }
}

MySQLExprFilterNode::MySQLExprFilterNode(ExecutePlan *plan,
                                         Expression *filter_expr)
    : MySQLFilterNode(plan) {
  this->name = "MySQLExprFilterNode";
  this->filter_expr = filter_expr;
  this->inited_filter_expr = false;
  this->inited_result_type = false;
}

void get_expression_type(list<Packet *> *field_packets,
                         FieldExpression *expression, ResultType &result_type) {
  list<Packet *>::iterator it;
  string expr_str;
  int col_index = 0;

  for (it = field_packets->begin(); it != field_packets->end();
       it++, col_index++) {
    if (col_index == expression->get_column_index()) {
      MySQLColumnResponse col_resp(*it);
      col_resp.unpack();
      MySQLColumnType col_type = col_resp.get_column_type();
      result_type = get_result_type_from_column_type(col_type);
      return;
    }
  }
  throw UnSupportPartitionSQL(
      "Can not find corresponding column in select item list.");
}

void MySQLExprFilterNode::init_filter() {
  if (!inited_filter_expr) {
    init_expr_low(filter_expr, HAVING_EXPR, get_field_packets(),
                  field_expr_list);
    inited_filter_expr = true;
  }
}

bool MySQLExprFilterNode::filter(Packet *row) {
  init_filter();
  list<FieldExpression *>::iterator it;
  for (it = field_expr_list.begin(); it != field_expr_list.end(); it++) {
    (*it)->set_row(row);
  }

  ExpressionValue expr_value;
  filter_expr->get_expression_value(&expr_value);

  switch (expr_value.value_type) {
    case RESULT_TYPE_NULL: {
      return false;
    } break;
    case RESULT_TYPE_NUM: {
      return (mpf_cmp_ui(expr_value.gmp_value.value, 0) != 0);
    } break;
    case RESULT_TYPE_BOOL: {
      return expr_value.bool_value;
    } break;
    default:
      LOG_ERROR("Unsupport result type for MySQLExprFilterNode.\n");
      throw ExecuteNodeError("Unsupport result type for MySQLExprFilterNode.");
  }
  return false;
}

/* class MySQLComQueryPrepareNode */
MySQLComQueryPrepareNode::MySQLComQueryPrepareNode(ExecutePlan *plan,
                                                   DataSpace *space,
                                                   const char *query_sql,
                                                   const char *name,
                                                   const char *sql)
    : MySQLExecuteNode(plan), dataspace(space), prepare_name(name), sql(sql) {
  this->name = "MySQLComQueryPrepareNode";
  packet = NULL;
  error_packet = NULL;
  size_t len1 = strlen(query_sql);
  prepare_sql = new char[len1 + 1];
  strncpy(prepare_sql, query_sql, len1 + 1);
  prepare_sql[len1] = '\0';
}
void MySQLComQueryPrepareNode::clean() {
  if (packet) {
    if (packet == error_packet) error_packet = NULL;
    delete packet;
    packet = NULL;
  }
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
  if (prepare_sql) {
    delete[] prepare_sql;
    prepare_sql = NULL;
  }
}

void MySQLComQueryPrepareNode::handle_error_packet(Packet *packet) {
  LOG_DEBUG("Got error packet when execute PREPARE in MYSQL_QUERY.\n");
  /*
   *  if prepare fail will clear the
   *  prepare with the same name
   */
  session->clean_prepare_item(prepare_name);
  if (!session->is_call_store_procedure()) {
    handler->send_to_client(packet);
  } else {
    error_packet = packet;
    throw ErrorPacketException();
  }
}

void MySQLComQueryPrepareNode::execute() {
  packet = Backend::instance()->get_new_packet();
  Packet exec_packet;
  Packet tmp_packet;
  Connection *conn = NULL;
  MySQLPrepareRequest prepare_request(prepare_sql);
  LOG_DEBUG("query PREPARE to command PREPARE\n");
  prepare_request.pack(&exec_packet);
  try {
#ifndef DBSCALE_TEST_DISABLE
    Session *s = plan->session;
    dbscale_test_info *test_info = s->get_dbscale_test_info();
    if (!strcasecmp(test_info->test_case_name.c_str(),
                    "send_to_server_retry") &&
        !strcasecmp(test_info->test_case_operation.c_str(),
                    "receive_error_packet")) {
      const char *sql = "unknown sql statement";
      MySQLQueryRequest query(sql);
      query.set_sql_replace_char(
          plan->session->get_query_sql_replace_null_char());
      query.pack(&exec_packet);
    }
#endif

    MySQLPrepareItem *tmp_prepare_item =
        session->get_prepare_item(prepare_name);
    if (tmp_prepare_item != NULL) {
      // drop prepare if it exists
      uint32_t statement_id = tmp_prepare_item->get_w_stmt_id();
      MySQLCloseRequest close_prepare(statement_id);
      Packet drop_packet;
      close_prepare.pack(&drop_packet);
      conn = handler->send_to_server_retry(dataspace, &drop_packet,
                                           session->get_schema(), false, false);
      session->clean_prepare_item(prepare_name);
      if (!conn->dec_prepare_num(prepare_name)) {
        LOG_ERROR(
            "There is no prepare statement in connection [%@] when execute "
            "drop prepare statement.\n",
            conn);
        throw ExecuteNodeError("No prepare statement when drop.");
      }
    }

    if (!conn) {
      conn = handler->send_to_server_retry(dataspace, &exec_packet,
                                           session->get_schema(),
                                           session->is_read_only(), false);
    } else {
      conn->reset();
      handler->send_to_server(conn, &exec_packet);
    }

#ifndef DBSCALE_TEST_DISABLE
    if (!strcasecmp(test_info->test_case_name.c_str(),
                    "send_to_server_retry") &&
        !strcasecmp(test_info->test_case_operation.c_str(),
                    "throw_execute_node_error")) {
      throw ExecuteNodeError("dbscale test execute node error.");
    }
#endif
    handler->receive_from_server(conn, packet);
    if (driver->is_error_packet(packet)) {
      handle_error_packet(packet);
    } else {
      if (!conn->inc_prepare_num(prepare_name)) {
        LOG_ERROR("prepare_num in connection [%@] has overflowed.\n", conn);
        throw ExecuteNodeError("prepare_num has overflowed.");
      }
      if (!session->is_dataspace_keeping_connection(dataspace) &&
          session->get_space_from_kept_space_map(dataspace) == NULL) {
        session->set_kept_connection(dataspace, conn);
      }
      MySQLPrepareResponse prep_resp(packet);
      prep_resp.unpack();
      MySQLPrepareItem *prepare_item =
          new MySQLPrepareItem(prepare_sql, 0, prep_resp.statement_id,
                               prep_resp.num_params, prep_resp.num_columns);
      if (!support_prepare_rwsplit) prepare_item->set_read_only(false);

      /* Insert the prepare item into map in this session */
      LOG_DEBUG("Session %@ The prepare query of statement[%d] is %s.\n",
                session, prep_resp.statement_id, prepare_sql);
      session->set_prepare_item(prepare_name, prepare_item);
      if (prep_resp.num_params > 0) {
        handler->receive_from_server(conn, &tmp_packet);
        while (!driver->is_eof_packet(&tmp_packet)) {
          MySQLColumnResponse column_resp(&tmp_packet);
          column_resp.unpack();
          prepare_item->get_param_type_list()->push_back(
              column_resp.get_column_type());
          handler->receive_from_server(conn, &tmp_packet);
        }
      }
      if (prep_resp.num_columns > 0) {
        handler->receive_from_server(conn, &tmp_packet);
        while (!driver->is_eof_packet(&tmp_packet)) {
          MySQLColumnResponse column_resp(&tmp_packet);
          column_resp.unpack();
          prepare_item->get_column_type_list()->push_back(
              column_resp.get_column_type());
          handler->receive_from_server(conn, &tmp_packet);
        }
      }
      if (!session->is_call_store_procedure()) {
        MySQLOKResponse ok_response(0, prep_resp.warning_count);
        ok_response.pack(&tmp_packet);
        handler->deal_autocommit_with_ok_eof_packet(&tmp_packet);
        handler->send_to_client(&tmp_packet);
      }
    }
    if (conn) {
      handler->put_back_connection(dataspace, conn);
    }
  } catch (...) {
    LOG_ERROR("got exception when execute sql [%s].\n", prepare_sql);
    if (session->defined_lock_need_kept_conn(dataspace))
      session->remove_defined_lock_kept_conn(dataspace);
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace);
    }
    session->clean_prepare_item(prepare_name);
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  }
#ifdef DEBUG
  node_end_timing();
#endif
#ifdef DEBUG
  LOG_DEBUG("PrepareNode %@ cost %d ms\n", this, node_cost_time);
#endif
  status = EXECUTE_STATUS_COMPLETE;
}
/* class MySQLComQueryExecPrepareNode */
MySQLComQueryExecPrepareNode::MySQLComQueryExecPrepareNode(
    ExecutePlan *plan, const char *name, var_item *var_item_list)
    : MySQLExecuteNode(plan), prepare_name(name), parameters(var_item_list) {
  this->name = "MySQLComQueryExecPrepareNode";
  packet = NULL;
  error_packet = NULL;
}
void MySQLComQueryExecPrepareNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
  }
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}

void MySQLComQueryExecPrepareNode::handle_execute_prepare(
    MySQLPrepareItem *prepare_item, string &query_sql) {
  list<execute_param> param_list;
  var_item *tmp = parameters;
  map<string, string> *user_var_map = session->get_user_var_map();
  while (tmp != NULL) {
    execute_param tmp_param;
    string value(tmp->value);
    boost::to_upper(value);
    tmp_param.type = MYSQL_TYPE_END;
    if (user_var_map->count(value)) {
      tmp_param.value = (*user_var_map)[value];
      tmp_param.is_null = false;
    } else {
      tmp_param.value = "NULL";
      tmp_param.is_null = true;
    }
    param_list.push_front(tmp_param);
    tmp = tmp->next;
  }
  query_sql = prepare_item->get_query();
  handler->execute_replace_str(query_sql, &param_list);
}
void MySQLComQueryExecPrepareNode::execute() {
  packet = Backend::instance()->get_new_packet();
  var_item *tmp = parameters;
  uint16_t i = 0;
  while (tmp != NULL) {
    i++;
    tmp = tmp->next;
  }
  try {
    MySQLPrepareItem *prepare_item = session->get_prepare_item(prepare_name);
    if (prepare_item == NULL) {
      string err_info("Unknown prepared statement handler (");
      err_info += prepare_name;
      err_info += (") given to EXECUTE");

      MySQLErrorResponse error(ERROR_UNKOWN_PREPARE_CODE, err_info.c_str());
      error_packet = Backend::instance()->get_new_packet();
      error.pack(error_packet);
      if (!session->is_call_store_procedure()) {
        // send error packet
        handler->send_to_client(error_packet);
      } else {
        throw ErrorPacketException();
      }
    } else if (i != prepare_item->get_num_params()) {
      MySQLErrorResponse error(
          ERROR_PREPARE_INCORRECT_ARGUS_CODE,
          dbscale_err_msg[ERROR_PREPARE_INCORRECT_ARGUS_CODE]);
      error_packet = Backend::instance()->get_new_packet();
      error.pack(error_packet);
      if (!session->is_call_store_procedure()) {
        // send error packet
        handler->send_to_client(error_packet);
      } else {
        throw ErrorPacketException();
      }
    } else {
      string query_sql;
      handle_execute_prepare(prepare_item, query_sql);
      handler->handle_query_command(query_sql.c_str(), query_sql.length());
      status = EXECUTE_STATUS_COMPLETE;
    }
    status = EXECUTE_STATUS_COMPLETE;
  } catch (...) {
    LOG_ERROR("got exception when execute prepare [%s].\n", prepare_name);
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  }
}
/* class MySQLComQueryDropPrepareNode */
MySQLComQueryDropPrepareNode::MySQLComQueryDropPrepareNode(
    ExecutePlan *plan, DataSpace *dataspace, const char *name,
    const char *prepare_sql)
    : MySQLExecuteNode(plan),
      prepare_name(name),
      drop_prepare_sql(prepare_sql),
      dataspace(dataspace) {
  this->name = "MySQLComQueryDropPrepareNode";
  packet = NULL;
  error_packet = NULL;
}
void MySQLComQueryDropPrepareNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
  }
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}
void MySQLComQueryDropPrepareNode::execute() {
  packet = Backend::instance()->get_new_packet();
  Connection *conn = NULL;

  try {
    MySQLPrepareItem *prepare_item = session->get_prepare_item(prepare_name);
    if (prepare_item == NULL) {
      // send error packet
      string err_info("Unknown prepared statement handler (");
      err_info += prepare_name;
      err_info += (") given to DEALLOCATE PREPARE");

      MySQLErrorResponse error(ERROR_UNKOWN_PREPARE_CODE, err_info.c_str());
      error_packet = Backend::instance()->get_new_packet();
      error.pack(error_packet);
      if (!session->is_call_store_procedure()) {
        handler->send_to_client(error_packet);
      } else {
        throw ErrorPacketException();
      }
    } else {
      uint32_t statement_id = prepare_item->get_w_stmt_id();
      MySQLCloseRequest close_prepare(statement_id);
      Packet exec_packet;
      close_prepare.pack(&exec_packet);
      conn = handler->send_to_server_retry(dataspace, &exec_packet,
                                           session->get_schema(), false, false);
      session->clean_prepare_item(prepare_name);
      if (!session->is_call_store_procedure()) {
        MySQLOKResponse ok_response(0, 0);
        ok_response.pack(packet);
        handler->deal_autocommit_with_ok_eof_packet(packet);
        handler->send_to_client(packet);
      }
      if (!conn->dec_prepare_num(prepare_name)) {
        LOG_ERROR(
            "There is no prepare statement in connection [%@] when execute "
            "drop prepare statement.\n",
            conn);
        throw ExecuteNodeError("No prepare statement when drop.");
      }
    }
    if (conn) {
      handler->put_back_connection(dataspace, conn);
    }
    status = EXECUTE_STATUS_COMPLETE;
  } catch (...) {
    LOG_ERROR("got exception  when execute drop prepare [%s].\n", prepare_name);
    if (session->defined_lock_need_kept_conn(dataspace))
      session->remove_defined_lock_kept_conn(dataspace);
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace);
    }
    session->clean_prepare_item(prepare_name);
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  }
}

/* class MySQLDBScaleGlobalConsistencePointNode */
MySQLDBScaleGlobalConsistencePointNode::MySQLDBScaleGlobalConsistencePointNode(
    ExecutePlan *plan)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLDBScaleGlobalConsistencePointNode";
  packet = NULL;
  error_packet = NULL;
}
void MySQLDBScaleGlobalConsistencePointNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
}

bool start_sync_get_consistence_point() {
  LOG_DEBUG("start_sync_get_consistence_point\n");
  char param[1024] = {0};
  sprintf(param, "%d", TYPE_GET_CONSISTENCE_POINT);

  Backend *backend = Backend::instance();
  // start sync, if success, slaves have do wait_, if failed, slaves have
  // cancaled.
  return backend->start_dynamic_operation_topic(
      GET_CONSISTENCE_POINT_TOPIC_NAME, param);
}

void fin_sync_get_consistence_point() {
  LOG_DEBUG("fin_sync_get_consistence_point\n");
  char param[1024] = {0};
  sprintf(param, "%d", TYPE_GET_CONSISTENCE_POINT);

  MultipleManager *mul = MultipleManager::instance();
  MultipleSyncTool *sync_tool = mul->get_sync_tool();
  try {
    sync_tool->get_sync_topic()->set_sync_info_op_param(param);
    sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param, NULL);
    sync_tool->publish_sync_message();
    sync_tool->wait_all_children_sync();
  } catch (Exception &e) {
    LOG_ERROR("fin_sync_get_consistence_point failed due to %s.\n", e.what());
  }
}
void MySQLDBScaleGlobalConsistencePointNode::execute() {
  packet = Backend::instance()->get_new_packet();

  try {
    LOG_DEBUG("MySQLDBScaleGlobalConsistencePointNode start!\n");
    MySQLResultSetHeaderResponse result_set_header(4, 0);
    result_set_header.pack(packet);
    handler->send_to_client(packet);

    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "consistence_point";
    const char *org_table = "consistence_point";

    list<const char *> columns;
    columns.push_back("Server_name");
    columns.push_back("File");
    columns.push_back("Position");
    columns.push_back("gtid");

    map<string, DataSource *> config_file_datasource;
    Backend::instance()->get_config_file_datasource(config_file_datasource);
    list<Connection *> list_connection;
    map<string, DataSource *>::iterator its = config_file_datasource.begin();
    for (; its != config_file_datasource.end(); its++) {
      if (its->second->get_data_source_type() == DATASOURCE_TYPE_ODBC) continue;
      Connection *conn = its->second->get_connection(NULL, false);
      if (!conn) {
        put_conn_list_back_to_free(&list_connection);
        throw ExecuteNodeError("Failed to get connection.");
      }
      list<Connection *>::iterator itc = list_connection.begin();
      bool is_contain_server = false;
      for (; itc != list_connection.end(); itc++) {
        if (is_share_same_server((*itc)->get_server(), conn->get_server())) {
          is_contain_server = true;
          break;
        }
      }
      if (!is_contain_server)
        list_connection.push_back(conn);
      else
        conn->get_pool()->add_back_to_free(conn);
    }

    int server_num = list_connection.size();
    list<const char *> row_data[server_num];
    char file[server_num][200];
    uint64_t pos;
    char pos_str[server_num][30];
    char gtid[server_num][5000];
    if (multiple_mode) {
      if (!start_sync_get_consistence_point()) {
        LOG_ERROR("start_sync_get_consistence_point failed\n");
        throw Error("start_sync_get_consistence_point failed");
      }
    }

    if (!Backend::instance()->wait_for_transaction_for_consistence_point()) {
      Backend::instance()->release_consistence_point_mutex();
      if (multiple_mode) {
        fin_sync_get_consistence_point();
      }
      throw Error("get_consistence_point failed");
    }
    Connection *conn = NULL;
    list<Connection *>::iterator it = list_connection.begin();
    try {
      int index = 0;
      for (; it != list_connection.end(); it++) {
        conn = *it;
        if (conn->get_server()->get_master_info(file[index], &pos, true,
                                                gtid[index])) {
          LOG_ERROR("get consistence_point, [%s] get binlog pos failed\n",
                    conn->get_server()->get_name());
          throw Error("get binlog pos failed.");
        }
        snprintf(pos_str[index], sizeof(pos_str[index]), "%ld", pos);
        row_data[index].push_back(conn->get_server()->get_name());
        row_data[index].push_back(file[index]);
        row_data[index].push_back(pos_str[index]);
        row_data[index].push_back(gtid[index]);
        index++;
      }
    } catch (...) {
      Backend::instance()->release_consistence_point_mutex();
      if (conn) {
        conn->get_pool()->add_back_to_dead(conn);
        conn = NULL;
        list_connection.erase(it);
      }
      put_conn_list_back_to_free(&list_connection);
      LOG_ERROR("update master info for get consistence_point failed! \n");
      if (multiple_mode) {
        fin_sync_get_consistence_point();
      }
      throw;
    }
    Backend::instance()->release_consistence_point_mutex();
    if (multiple_mode) {
      fin_sync_get_consistence_point();
    }
    put_conn_list_back_to_free(&list_connection);
    while (!columns.empty()) {
      MySQLColumnResponse field(catalog, schema, table, org_table,
                                columns.front(), columns.front(), 8,
                                NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
      field.pack(packet);
      handler->send_to_client(packet);
      columns.pop_front();
    }

    MySQLEOFResponse eof;
    eof.pack(packet);
    handler->deal_autocommit_with_ok_eof_packet(packet);
    handler->send_to_client(packet);

    for (int i = 0; i < server_num; i++) {
      MySQLRowResponse row(row_data[i]);
      row.pack(packet);
      handler->send_to_client(packet);
    }

    MySQLEOFResponse end;
    end.pack(packet);
    handler->deal_autocommit_with_ok_eof_packet(packet);
    handler->send_to_client(packet);
  } catch (exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE,
                             "Connection get error!");
    error.pack(packet);
    error_packet = packet;
    throw ErrorPacketException();
  }
  status = EXECUTE_STATUS_COMPLETE;
}

/*class MySQLDBScaleDynamicUpdateWhiteNode*/
MySQLDBScaleDynamicUpdateWhiteNode::MySQLDBScaleDynamicUpdateWhiteNode(
    ExecutePlan *plan, bool is_add, const char *ip,
    const ssl_option_struct &ssl_option_value, const char *comment,
    const char *user_name)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLDBScaleDynamicUpdateWhiteNode";
  this->is_add = is_add;
  this->user_name = string(user_name);
  this->ip = ip;
  this->comment = comment ? comment : "";
  this->ssl_option_value = ssl_option_value;
  error_packet = NULL;
}

void MySQLDBScaleDynamicUpdateWhiteNode::do_execute() {
  try {
    LOG_DEBUG("dbscale start update white ip info\n");
    Backend::instance()->update_white_user_info(user_name, ip, ssl_option_value,
                                                is_add, comment);
  } catch (exception &e) {
    LOG_ERROR("%s\n", e.what());
    error_packet = Backend::instance()->get_new_packet();
    string err_msg = "Got error when update white list.";
    err_msg.append(e.what());
    MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE, err_msg.c_str());
    error.pack(error_packet);
    throw Error(err_msg.c_str());
  }
}

/* class MySQLDBScaleFlushConfigToFileNode */
MySQLDBScaleFlushConfigToFileNode::MySQLDBScaleFlushConfigToFileNode(
    ExecutePlan *plan, const char *file_name, bool flush_all)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLDBScaleFlushConfigToFileNode";
  this->file_name = file_name;
  this->flush_all = flush_all;
  error_packet = NULL;
}

void MySQLDBScaleFlushConfigToFileNode::do_execute() {
  try {
    LOG_DEBUG("Start flush DBScale configuration to file %s\n", file_name);
    Backend::instance()->flush_config_to_file(file_name, flush_all);
  } catch (exception &e) {
    LOG_ERROR("%s\n", e.what());
    error_packet = Backend::instance()->get_new_packet();
    MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE,
                             "Got error when flush config to file");
    error.pack(error_packet);
    throw Error("Got error when flush config to file");
  }
}

/* class MySQLDBScaleFlushTableInfoNode */
MySQLDBScaleFlushTableInfoNode::MySQLDBScaleFlushTableInfoNode(
    ExecutePlan *plan, const char *schema_name, const char *table_name)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLDBScaleFlushTableInfoNode";
  this->schema_name = schema_name;
  this->table_name = table_name;
  error_packet = NULL;
}

void MySQLDBScaleFlushTableInfoNode::do_execute() {
  try {
    if (table_name) {
      string full_table_name;
      splice_full_table_name(schema_name, table_name, full_table_name);
      TableInfoCollection *tic = TableInfoCollection::instance();
      if (!tic->is_table_info_exist(full_table_name)) {
        LOG_INFO("table info of [%s] does not exist, no need flush.\n",
                 full_table_name.c_str());
        return;
      }
      LOG_DEBUG("Start flush table info for table [%s]\n",
                full_table_name.c_str());
      TableInfo *ti = tic->get_table_info_for_write(full_table_name);
      ti->mark_table_info_out_of_date();
      ti->release_table_info_lock();

      string schema_str(schema_name);
      string table_str(table_name);
      driver->erase_lru_cache(schema_str, table_str);

      Backend::instance()->alter_table_flag_on(full_table_name);
      LOG_DEBUG("Finish flush table info for table [%s]\n",
                full_table_name.c_str());
    } else {
      set<string> full_table_name_set =
          TableInfoCollection::instance()->get_all_table_name_in_db(
              schema_name);
      if (full_table_name_set.empty()) {
        LOG_INFO("table info in schema [%s] does not exist, no need flush.\n",
                 schema_name);
        return;
      }
      LOG_DEBUG("Start flush table info for schema [%s]\n", schema_name);
      TableInfoCollection *tic = TableInfoCollection::instance();
      string all_table_name_str;
      set<string>::iterator it = full_table_name_set.begin();
      for (; it != full_table_name_set.end(); it++) {
        string full_table_name = *it;
        all_table_name_str.append(full_table_name);
        all_table_name_str.append(" ");
        TableInfo *ti = tic->get_table_info_for_write(full_table_name);
        ti->mark_table_info_out_of_date();
        ti->release_table_info_lock();

        string schema_str(schema_name);
        string table_str(table_name);
        driver->erase_lru_cache(schema_str, table_str);

        Backend::instance()->alter_table_flag_on(full_table_name);
      }
      LOG_DEBUG(
          "Finish flush table info for schema [%s], related tables are [%s]\n",
          schema_name, all_table_name_str.c_str());
    }
  } catch (exception &e) {
    LOG_ERROR("%s\n", e.what());
    error_packet = Backend::instance()->get_new_packet();
    string msg = "Got error when flush table info. ";
    msg.append(e.what());
    MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE, msg.c_str());
    error.pack(error_packet);
    throw Error(msg.c_str());
  }
}

/* class MySQLDBScaleFlushACLNode */
MySQLDBScaleFlushACLNode::MySQLDBScaleFlushACLNode(ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLDBScaleFlushACLNode";
  error_packet = NULL;
}

void MySQLDBScaleFlushACLNode::do_execute() {
  int ver = Backend::instance()->get_acl_version();
  if (session->get_acl_version() != ver) {
    session->set_acl_version(ver);
    try {
      LOG_DEBUG("Start flush DBScale ACL\n");
      session->flush_acl();
    } catch (exception &e) {
      LOG_ERROR("%s\n", e.what());
      error_packet = Backend::instance()->get_new_packet();
      MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE,
                               "Got error when flush ACL");
      error.pack(error_packet);
      throw Error("Got error when flush ACL");
    }
#ifdef DEBUG
  } else {
    LOG_DEBUG(
        "session acl version = backend acl version, no need do flush acl\n");
#endif
  }
}

/* class MySQLShowMonitorPointStatusNode*/
MySQLShowMonitorPointStatusNode::MySQLShowMonitorPointStatusNode(
    ExecutePlan *plan)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLShowMonitorPointStatusNode";
  packet = NULL;
  error_packet = NULL;
}

void MySQLShowMonitorPointStatusNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
}

void MySQLShowMonitorPointStatusNode::execute() {
#ifndef DBSCALE_TEST_DISABLE
  packet = Backend::instance()->get_new_packet();
  try {
    MySQLResultSetHeaderResponse result_set_header(3, 0);
    result_set_header.pack(packet);
    handler->send_to_client(packet);

    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";

    list<const char *> columns;
    columns.push_back("Monitor_point");
    columns.push_back("Cost_time");
    columns.push_back("%");
    while (!columns.empty()) {
      MySQLColumnResponse field(catalog, schema, table, org_table,
                                columns.front(), columns.front(), 8,
                                NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
      field.pack(packet);
      handler->send_to_client(packet);
      columns.pop_front();
    }
    MySQLEOFResponse eof;
    eof.pack(packet);
    handler->deal_autocommit_with_ok_eof_packet(packet);
    handler->send_to_client(packet);
    stmt_type type = plan->statement->get_stmt_node()->type;
    if (type == STMT_DBSCALE_MONITOR_POINT_STATUS) {
      Backend *backend = Backend::instance();
      backend->collect_monitor_point_value();

      map<string, ACE_Time_Value> *tmp_map = backend->get_monitor_point_map();
      string tmp("session_svc");
      float total = (*tmp_map)[tmp].msec() * 1.0;
      map<string, ACE_Time_Value>::iterator it = tmp_map->begin();

      for (; it != tmp_map->end(); it++) {
        list<const char *> row_data;
        row_data.push_back(it->first.c_str());
        char query_time[20];
        snprintf(query_time, sizeof(query_time), "%lu", it->second.msec());
        row_data.push_back(query_time);
        char per[20];
        if (total)
          snprintf(per, sizeof(query_time), "%'.2f",
                   it->second.msec() * 100 / total);
        else
          snprintf(per, sizeof(query_time), "%d", 0);
        row_data.push_back(per);
        MySQLRowResponse row(row_data);
        row.pack(packet);
        handler->send_to_client(packet);
      }
    } else if (type == STMT_DBSCALE_HANDLER_MONITOR_POINT_STATUS) {
      Backend *backend = Backend::instance();
      backend->collect_monitor_handler_point_value();

      map<string, ACE_Time_Value> *tmp_map =
          backend->get_monitor_handler_point_map();
      string tmp("handler_svc");
      float total = (*tmp_map)[tmp].msec() * 1.0;
      map<string, ACE_Time_Value>::iterator it = tmp_map->begin();

      for (; it != tmp_map->end(); it++) {
        list<const char *> row_data;
        row_data.push_back(it->first.c_str());
        char query_time[20];
        snprintf(query_time, sizeof(query_time), "%lu", it->second.msec());
        row_data.push_back(query_time);
        char per[20];
        if (total)
          snprintf(per, sizeof(query_time), "%'.2f",
                   it->second.msec() * 100 / total);
        else
          snprintf(per, sizeof(query_time), "%d", 0);
        row_data.push_back(per);
        MySQLRowResponse row(row_data);
        row.pack(packet);
        handler->send_to_client(packet);
      }
      list<const char *> row_data;
      row_data.push_back("num_of_recv");
      char num_recv[20];
      snprintf(num_recv, sizeof(num_recv), "%lu", backend->num_of_recv_monitor);
      row_data.push_back(num_recv);
      row_data.push_back("0");
      MySQLRowResponse row(row_data);
      row.pack(packet);
      handler->send_to_client(packet);

      row_data.clear();
      row_data.push_back("num_of_send");
      char num_send[20];
      snprintf(num_send, sizeof(num_send), "%lu", backend->num_of_send_monitor);
      row_data.push_back(num_send);
      row_data.push_back("0");
      MySQLRowResponse row2(row_data);
      row2.pack(packet);
      handler->send_to_client(packet);
    }

    MySQLEOFResponse end;
    end.pack(packet);
    handler->deal_autocommit_with_ok_eof_packet(packet);
    handler->send_to_client(packet);

  } catch (...) {
    status = EXECUTE_STATUS_COMPLETE;
    MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE,
                             "Connection get error!");
    error.pack(packet);
    error_packet = packet;
    throw ErrorPacketException();
  }
  status = EXECUTE_STATUS_COMPLETE;
#else
  status = EXECUTE_STATUS_COMPLETE;
  throw Error("Not Supported functionality.");
#endif
}

/* class MySQLShowGlobalMonitorPointStatusNode*/
MySQLShowGlobalMonitorPointStatusNode::MySQLShowGlobalMonitorPointStatusNode(
    ExecutePlan *plan)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLShowGlobalMonitorPointStatusNode";
  packet = NULL;
  error_packet = NULL;
}

void MySQLShowGlobalMonitorPointStatusNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
}

void MySQLShowGlobalMonitorPointStatusNode::execute() {
  packet = Backend::instance()->get_new_packet();
  try {
    MySQLResultSetHeaderResponse result_set_header(4, 0);
    result_set_header.pack(packet);
    handler->send_to_client(packet);

    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";

    list<const char *> columns;
    columns.push_back("Statistic_method");
    columns.push_back("Cost_time");
    columns.push_back("success_num");
    columns.push_back("fail_num");
    while (!columns.empty()) {
      MySQLColumnResponse field(catalog, schema, table, org_table,
                                columns.front(), columns.front(), 8,
                                NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
      field.pack(packet);
      handler->send_to_client(packet);
      columns.pop_front();
    }
    MySQLEOFResponse eof;
    eof.pack(packet);
    handler->deal_autocommit_with_ok_eof_packet(packet);
    handler->send_to_client(packet);
    Backend *backend = Backend::instance();
    backend->collect_monitor_global_point_value();

    map<string, unsigned long> num_map =
        backend->get_monitor_global_point_number_map();
    string succ_num = to_string(num_map["success"]);
    string fail_num = to_string(num_map["fail"]);
    map<string, ACE_Time_Value> *tmp_map =
        backend->get_monitor_global_point_map();
    map<string, ACE_Time_Value>::iterator it = tmp_map->begin();
    for (; it != tmp_map->end(); ++it) {
      list<const char *> row_data;
      row_data.push_back(it->first.c_str());
      char query_time[20];
      ACE_UINT64 usec;
      it->second.to_usec(usec);
      snprintf(query_time, sizeof(query_time), "%.2f", usec / 1000.0);
      row_data.push_back(query_time);
      if (!strcasecmp(it->first.c_str(), "total")) {
        row_data.push_back(succ_num.c_str());
        row_data.push_back(fail_num.c_str());
      } else {
        row_data.push_back("");
        row_data.push_back("");
      }
      MySQLRowResponse row(row_data);
      row.pack(packet);
      handler->send_to_client(packet);
    }
    MySQLEOFResponse end;
    end.pack(packet);
    handler->deal_autocommit_with_ok_eof_packet(packet);
    handler->send_to_client(packet);

  } catch (...) {
    status = EXECUTE_STATUS_COMPLETE;
    MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE,
                             "Connection get error!");
    error.pack(packet);
    error_packet = packet;
    throw ErrorPacketException();
  }
  status = EXECUTE_STATUS_COMPLETE;
}
/* class MySQLShowHistogramMonitorPointStatusNode*/
MySQLShowHistogramMonitorPointStatusNode::
    MySQLShowHistogramMonitorPointStatusNode(ExecutePlan *plan)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLShowHistogramMonitorPointStatusNode";
  packet = NULL;
  error_packet = NULL;
}

void MySQLShowHistogramMonitorPointStatusNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
}

void MySQLShowHistogramMonitorPointStatusNode::execute() {
  packet = Backend::instance()->get_new_packet();
  try {
    MySQLResultSetHeaderResponse result_set_header(2, 0);
    result_set_header.pack(packet);
    handler->send_to_client(packet);

    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";

    list<const char *> columns;
    columns.push_back("interval_time");
    columns.push_back("execute_num");
    while (!columns.empty()) {
      MySQLColumnResponse field(catalog, schema, table, org_table,
                                columns.front(), columns.front(), 8,
                                NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
      field.pack(packet);
      handler->send_to_client(packet);
      columns.pop_front();
    }
    MySQLEOFResponse eof;
    eof.pack(packet);
    handler->deal_autocommit_with_ok_eof_packet(packet);
    handler->send_to_client(packet);
    Backend *backend = Backend::instance();
    backend->collect_monitor_histogram_point_value();

    unsigned int inter_show =
        plan->statement->get_stmt_node()
            ->sql->show_histogram_monitor_oper->inter_time;
    unsigned long inter_conf = histogram_monitor_point_grad;
    if (inter_show % inter_conf != 0) {
      string str_err =
          "Please enter histogram interval integral multiple, the histogram "
          "intserval is ";
      str_err += to_string(inter_conf);
      str_err += " and show number is ";
      str_err += to_string(inter_show);
      LOG_ERROR("Configuration error: %s\n", str_err.c_str());
      throw Error(str_err.c_str());
    }

    map<unsigned long, unsigned long> *tmp_map =
        backend->get_monitor_histogram_point_map();
    map<unsigned long, unsigned long> inter_map;
    map<unsigned long, unsigned long>::iterator it = tmp_map->begin();
    unsigned long tmp_inter = it->first;
    inter_map[tmp_inter] = 0;
    for (; it != tmp_map->end(); ++it) {
      if (tmp_inter * inter_show >= it->first * inter_conf) {
        inter_map[tmp_inter] += it->second;
      } else {
        while (1) {
          tmp_inter += 1;
          if (tmp_inter * inter_show >= it->first * inter_conf) {
            break;
          }
        }
        inter_map[tmp_inter] = it->second;
      }
    }
    map<unsigned long, unsigned long>::iterator it2 = inter_map.begin();
    for (; it2 != inter_map.end(); ++it2) {
      list<const char *> row_data;
      unsigned long interval_start = it2->first * inter_show;
      unsigned long interval_end = interval_start + (inter_show);
      char inter_start[20];
      snprintf(inter_start, sizeof(inter_start), "%lu", interval_start);
      string tmp_str = string(inter_start);
      tmp_str.append("~");
      char inter_end[20];
      snprintf(inter_end, sizeof(inter_end), "%lu", interval_end);
      tmp_str.append(inter_end);
      row_data.push_back(tmp_str.c_str());
      char times_query[20];
      snprintf(times_query, sizeof(times_query), "%lu", it2->second);
      row_data.push_back(times_query);
      MySQLRowResponse row(row_data);
      row.pack(packet);
      handler->send_to_client(packet);
    }

    MySQLEOFResponse end;
    end.pack(packet);
    handler->deal_autocommit_with_ok_eof_packet(packet);
    handler->send_to_client(packet);

  } catch (std::exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
    error.pack(packet);
    error_packet = packet;
    throw ErrorPacketException();
  }
  status = EXECUTE_STATUS_COMPLETE;
}

MySQLShowOutlineMonitorInfoNode::MySQLShowOutlineMonitorInfoNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLShowOutlineMonitorInfo";
}

void MySQLShowOutlineMonitorInfoNode::init_data() {
  set_head_packet(9);
  const char *schema_name = "information_schema";
  const char *catalog = "def";
  const char *table_name = "Outline_Monitor_Info";

  const auto &columns = SQLOutlineMonitorInfoMap::columns;
  for (auto c : columns) {
    add_column_packet(catalog, schema_name, table_name, table_name, c, c, 8,
                      NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  }

  auto time_frame =
      plan->statement->get_stmt_node()->sql->show_monitor_info_oper->time_frame;
  auto sort_type =
      plan->statement->get_stmt_node()->sql->show_monitor_info_oper->sort_type;
  Backend *backend = Backend::instance();
  SQLOutlineMonitorInfoMap::OrderedOutlineMonitorInfo_Vector ret;
  if (enable_outline && backend->get_outline_monitor_time_handler()) {
    ret = backend->get_outline_monitor_time_handler()->get_monitor_info(
        time_frame, sort_type);
  }
  for (auto p : ret) {
    list<const char *> row_data;
    row_data.push_back(p.first.c_str());
    row_data.push_back(p.second.no_param_sql.c_str());
    string exec_times, parse_t, plan_g_t, plan_e_t, dbscale_e_t, total_e_t,
        avg_total_t;
    exec_times = std::to_string(p.second.execute_times);
    parse_t = p.second.get_sql_parse_time();
    plan_g_t = p.second.get_plan_generation_time();
    plan_e_t = p.second.get_plan_execution_time();
    dbscale_e_t = p.second.get_dbscale_execution_time();
    total_e_t = p.second.get_total_execution_time();
    avg_total_t = p.second.get_avg_total_execution_time();
    row_data.push_back(exec_times.c_str());
    row_data.push_back(parse_t.c_str());
    row_data.push_back(plan_g_t.c_str());
    row_data.push_back(dbscale_e_t.c_str());
    row_data.push_back(plan_e_t.c_str());
    row_data.push_back(total_e_t.c_str());
    row_data.push_back(avg_total_t.c_str());
    LOG_DEBUG("outline avg_total_t: [%s], sql: [%s]\n", avg_total_t.c_str(),
              p.second.no_param_sql.substr(0, 10).c_str());
    add_row_packet(row_data);
  }
}

/* class MySQLDBScaleCreateOutlineHintNode*/
MySQLDBScaleCreateOutlineHintNode::MySQLDBScaleCreateOutlineHintNode(
    ExecutePlan *plan, dbscale_operate_outline_hint_node *oper)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLDBScaleCreateOutlineHintNode";
  this->status = EXECUTE_STATUS_START;
  this->error_packet = nullptr;
  this->oper = oper;
}

void MySQLDBScaleCreateOutlineHintNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = nullptr;
  }
  this->oper = nullptr;
}

void MySQLDBScaleCreateOutlineHintNode::execute() {
  Backend *backend = Backend::instance();
  string cur_schema = session->get_schema();
  stmt_node st;
  init_stmt_node(&st);
  outline_hint_item oh_item;
  try {
    oh_item.hint_name = this->oper->hint_name;
    // get hint
    bool ret = do_fast_parse_get_hint(this->oper->raw_sql, &st, oh_item);
    if (!ret) {
      string error_message = this->oper->raw_sql;
      error_message =
          "Failed to get_outline_hint from the sql: '" + error_message + "'.";
      LOG_ERROR(
          "Get_outline_hint from sql %s error for "
          "MySQLDBScaleCreateOutlineHintNode::execute\n",
          this->oper->raw_sql);
      throw NotSupportedError(error_message.c_str());
    }
    if (cur_schema == "NULL") {
      LOG_ERROR(
          "Current database is NULL, fail to create hint for "
          "MySQLDBScaleCreateOutlineHintNode::execute.\n");
      throw ExecuteNodeError("No database selected.");
    }
    oh_item.schema = cur_schema;

    ret = backend->create_outline_hint(oh_item.sql_id, oh_item.schema,
                                       oh_item.hint_name, oh_item);
    if (ret) {
      send_ok_packet_to_client(handler, 0, 0);
    } else {
      LOG_ERROR(
          "Raw sql create hint failed for "
          "MySQLDBScaleCreateOutlineHintNode::execute.\n");
      throw ExecuteNodeError("Raw sql create hint failed.");
    }
  } catch (Exception &e) {
    error_packet = Backend::instance()->get_new_packet();
    MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
    error.pack(error_packet);
    handler->send_to_client(error_packet);
  }
  this->status = EXECUTE_STATUS_COMPLETE;
  free_stmt_node_mem(&st);
}

/* class MySQLDBScaleFlushOutlineHintNode*/
MySQLDBScaleFlushOutlineHintNode::MySQLDBScaleFlushOutlineHintNode(
    ExecutePlan *plan, dbscale_operate_outline_hint_node *oper)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLDBScaleFlushOutlineHintNode";
  this->status = EXECUTE_STATUS_START;
  this->error_packet = nullptr;
  this->oper = oper;
}

void MySQLDBScaleFlushOutlineHintNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = nullptr;
  }
  this->oper = nullptr;
}

void MySQLDBScaleFlushOutlineHintNode::execute() {
  Backend *backend = Backend::instance();
  string cur_schema = session->get_schema();

  try {
    if (cur_schema == "NULL") {
      LOG_ERROR(
          "Current database is NULL, fail to create hint for "
          "MySQLDBScaleFlushOutlineHintNode::execute\n");
      throw ExecuteNodeError("No database selected.");
    }
    bool ret = false;
    switch (oper->operate_type) {
      case ALL_HINT: {
        ret = backend->flush_outline_hint_all();
      } break;
      case ONLY_SQLID: {
        ret = backend->flush_outline_hint_by_sqlid(oper->sql_id, cur_schema);
      } break;
      case ONLY_NAME: {
        ret = backend->flush_outline_hint_by_hintname(oper->hint_name,
                                                      cur_schema);
      } break;
    }
    if (ret) {
      send_ok_packet_to_client(handler, 0, 0);
    } else {
      LOG_ERROR(
          "Flush hint is error, fail to flush hint for "
          "MySQLDBScaleFlushOutlineHintNode::execute.\n");
      throw ExecuteNodeError("Flush hint is error.");
    }
  } catch (Exception &e) {
    error_packet = Backend::instance()->get_new_packet();
    MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
    error.pack(error_packet);
    handler->send_to_client(error_packet);
  }
  this->status = EXECUTE_STATUS_COMPLETE;
}

/* class MySQLDBScaleDeleteOutlineHintNode*/
MySQLDBScaleDeleteOutlineHintNode::MySQLDBScaleDeleteOutlineHintNode(
    ExecutePlan *plan, dbscale_operate_outline_hint_node *oper)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLDBScaleDeleteOutlineHintNode";
  this->status = EXECUTE_STATUS_START;
  this->error_packet = nullptr;
  this->oper = oper;
}

void MySQLDBScaleDeleteOutlineHintNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = nullptr;
  }
  this->oper = nullptr;
}

void MySQLDBScaleDeleteOutlineHintNode::execute() {
  Backend *backend = Backend::instance();
  string cur_schema = session->get_schema();

  try {
    if (cur_schema == "NULL") {
      LOG_ERROR(
          "Current database is NULL, fail to create hint for "
          "MySQLDBScaleCreateOutlineHintNode::execute.\n");
      throw ExecuteNodeError("No database selected.");
    }

    bool ret = false;
    switch (oper->operate_type) {
      case ALL_HINT: {
        ret = backend->delete_outline_hint_all();
      } break;
      case ONLY_SQLID: {
        ret = backend->delete_outline_hint_by_sqlid(oper->sql_id, cur_schema);
      } break;
      case ONLY_NAME: {
        string tmp_sql_id;
        ret = backend->check_exist_by_hintname(oper->hint_name, cur_schema,
                                               tmp_sql_id);
        if (ret) {
          ret = backend->delete_outline_hint_by_sqlid(tmp_sql_id, cur_schema);
        } else {
          LOG_ERROR(
              "Failed to find outline hint [%s]:[%s] for "
              "MySQLDBScaleDeleteOutlineHintNode::execute.\n",
              oper->hint_name, cur_schema.c_str());
          throw ExecuteNodeError("Outline hint doesnt exist.");
        }
      } break;
    }
    if (ret) {
      send_ok_packet_to_client(handler, 0, 0);
    } else {
      LOG_ERROR(
          "Failed to delete outline hint for "
          "MySQLDBScaleDeleteOutlineHintNode::execute.\n");
      throw ExecuteNodeError("Failed to delete outline hint.");
    }
  } catch (Exception &e) {
    error_packet = Backend::instance()->get_new_packet();
    MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
    error.pack(error_packet);
    handler->send_to_client(error_packet);
  }
  this->status = EXECUTE_STATUS_COMPLETE;
}

/* class MySQLDBScaleShowOutlineHintNode*/
MySQLDBScaleShowOutlineHintNode::MySQLDBScaleShowOutlineHintNode(
    ExecutePlan *plan, dbscale_operate_outline_hint_node *oper)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowOutlineHintNode";
  this->oper = oper;
  columns.push_back("sqlid");
  columns.push_back("database");
  columns.push_back("hint_name");
  columns.push_back("no_param_sql");
  columns.push_back("hint_position");
  columns.push_back("hint_type");
  columns.push_back("hint_value");
}

void MySQLDBScaleShowOutlineHintNode::init_data() {
  Backend *backend = Backend::instance();
  string cur_schema = session->get_schema();
  if (cur_schema == "NULL") {
    LOG_ERROR(
        "Current database is NULL, fail to create hint for "
        "MySQLDBScaleShowOutlineHintNode::init_data\n");
    throw ExecuteNodeError("No database selected.");
  }

  set_head_packet(columns.size());
  const char *catalog = "def";
  const char *schema = session->get_schema();
  const char *table = "tables";
  const char *org_table = "tables";
  list<const char *>::iterator it = columns.begin();
  for (; it != columns.end(); it++) {
    add_column_packet(catalog, schema, table, org_table, *it, *it, 8,
                      NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  }

  vector<vector<string> > vec;
  switch (oper->operate_type) {
    case ALL_HINT: {
      backend->get_outline_hint_vec_all(vec);
    } break;
    case ONLY_SQLID: {
      backend->get_outline_hint_vec_by_sqlid(oper->sql_id, cur_schema, vec);
    } break;
    case ONLY_NAME: {
      string tmp_sql_id;
      if (backend->check_exist_by_hintname(oper->hint_name, cur_schema,
                                           tmp_sql_id)) {
        backend->get_outline_hint_vec_by_sqlid(tmp_sql_id, cur_schema, vec);
      }
    } break;
  }

  list<const char *> row_data;
  for (auto it = vec.begin(); it != vec.end(); it++) {
    if (it->size() != QUERY_HINT_SIZE) {
      LOG_ERROR("Query hint size error.\n");
      continue;
    }
    row_data.clear();
    row_data.push_back((*it)[0].c_str());
    row_data.push_back((*it)[1].c_str());
    row_data.push_back((*it)[2].c_str());
    row_data.push_back((*it)[3].c_str());
    row_data.push_back((*it)[4].c_str());
    row_data.push_back((*it)[5].c_str());
    row_data.push_back((*it)[6].c_str());
    add_row_packet(row_data);
  }
}

/* class MySQLDynamicRemoveOPNode */
MySQLDynamicRemoveOPNode::MySQLDynamicRemoveOPNode(ExecutePlan *plan,
                                                   const char *name)
    : MySQLExecuteNode(plan), op_name(name) {
  this->name = "MySQLDynamicRemoveOPNode";
  error_packet = NULL;
}
void MySQLDynamicRemoveOPNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}

bool MySQLDynamicRemoveOPNode::contain_unclean_table(const char *op_name) {
  int unclean_table_count = 0;
  string select_sql =
      "SELECT count(*) FROM dbscale.MIGRATE_CLEAN_SHARD WHERE source_name = '";
  select_sql.append(op_name);
  select_sql.append("'");
  Backend *backend = Backend::instance();
  DataSpace *data_space = backend->get_config_data_space();
  Connection *conn = NULL;
  try {
    conn = data_space->get_connection(session);
    if (!conn) throw;
    string value;
    conn->query_for_one_value(select_sql.c_str(), value, 0);
    unclean_table_count = atoi(value.c_str());
  } catch (...) {
    if (conn) {
      conn->get_pool()->add_back_to_dead(conn);
      conn = NULL;
    }
    throw DynamicOpFail(
        "The config source have problems, fail to query migrate_clean_shard, "
        "please check logs.");
  }
  if (conn) {
    conn->get_pool()->add_back_to_free(conn);
  }
  return unclean_table_count > 0;
}

void MySQLDynamicRemoveOPNode::dynamic_remove_source() {
  Backend *backend = Backend::instance();
#ifndef CLOSE_MULTIPLE
  DataSourceMessageHandler *source_handler =
      DataSourceMessageHandler::instance();

  if (multiple_mode) {
    source_handler->acquire_message_lock_write();
    int retry = 60;
    MultipleManager *mul = MultipleManager::instance();
    while (retry) {
      if (mul->check_stable_situation()) break;
      timespec_t t = (timespec_t)ACE_Time_Value(0, 1000000);  // sleep 1s
      ACE_OS::nanosleep(&t);
      retry--;
    }
    if (!retry) {
      source_handler->release_message_lock();
      LOG_ERROR(
          "DB is busy and state changing now, fail to wait stable for "
          "MySQLDynamicRemoveOPNode::dynamic_remove_source.\n");
      throw Error("DB is busy and state changing now, plz retry later.");
    }
  }
#endif

  DataSource *source = backend->find_data_source(op_name);
  try {
    if (!source) {
      throw DynamicOpFail("the source is not exist");
    }

    if (contain_unclean_table(op_name)) {
      throw DynamicOpFail(
          "the source contains uncleaned table, use \"DBSCALE SHOW MIGRATE "
          "CLEAN TABLES;\" to check.");
    }

    string err_msg;
    if (!source->check_remove_self(err_msg)) {
      LOG_ERROR("remove_data_source failed due to [%s]\n", err_msg.c_str());
      throw DynamicOpFail(err_msg.c_str());
    }
  } catch (...) {
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) {
      source_handler->release_message_lock();
    }
#endif
    throw;
  }
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    MultipleManager *mul = MultipleManager::instance();
    MultipleSyncTool *sync_tool = mul->get_sync_tool();
    char param[1024] = {0};
    sprintf(param, "%d %s", TYPE_DYNAMIC_REMOVE_SOURCE, op_name);

    DynamicOperationSyncCondition *cond = NULL;
    try {
      if (!backend->start_dynamic_operation_topic(
              DYNAMIC_REMOVE_SOURCE_TOPIC_NAME, param))
        throw Error("Fail to start_dynamic_operation_topic.\n");
      mul->acquire_dynamic_info_lock();
      unsigned long after_pub_version = mul->get_cur_config_info_version();
      mul->release_dynamic_info_lock();
      cond = new DynamicOperationSyncCondition();
      cond->prepare_condition(after_pub_version);

      sync_tool->get_sync_topic()->set_sync_info_op_param(param);
      sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param, cond);
      sync_tool->publish_sync_message();
      sync_tool->wait_all_children_sync();
      delete cond;
      cond = NULL;
    } catch (...) {
      source->cancel_remove_self();
      source_handler->release_message_lock();
      LOG_ERROR("Got error when dynamic remove datasource %s.\n", op_name);
      if (cond) {
        delete cond;
        cond = NULL;
      }
      throw;
    }
  }
#endif
  source->remove_self();
  backend->remove_config_file_datasource(source);
  backend->remove_data_source(source->get_name());
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    source_handler->release_message_lock();
  }
#endif
}
void MySQLDynamicRemoveOPNode::dynamic_remove_server() {
  Backend *backend = Backend::instance();
  DataServer *server = backend->find_data_server(op_name);
  if (!server) {
    throw DynamicOpFail("the server is not exist");
  }

  if (server->has_relate_datasource()) {
    string datasource_str = server->get_relate_datasource_str();
    string err_msg = "DataServer relate to DataSource:";
    err_msg += datasource_str;
    throw DynamicOpFail(err_msg.c_str());
  }

  bool is_multiple_dbscale = false;
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) is_multiple_dbscale = true;
#endif
  if (is_multiple_dbscale) {
    bool is_sync_finish = false;
    bool is_start_sync = false;
    MultipleManager *mul = MultipleManager::instance();
    MultipleSyncTool *sync_tool = mul->get_sync_tool();
    char param[1024] = {0};
    sprintf(param, "%d %s", TYPE_DYNAMIC_REMOVE_SERVER, op_name);

    is_start_sync = backend->start_dynamic_operation_topic(
        DYNAMIC_REMOVE_SERVER_TOPIC_NAME, param);
    if (!is_start_sync) {
      LOG_ERROR(
          "Fail to start_dynamic_operation_topic for dynamic_remove_server.\n");
      throw Error(
          "Fail to start_dynamic_operation_topic for dynamic_remove_server.");
    }

    DynamicOperationSyncCondition *cond = NULL;
    try {
      /*fin the sync topic*/
      mul->acquire_dynamic_info_lock();
      unsigned long after_pub_version = mul->get_cur_config_info_version();
      mul->release_dynamic_info_lock();
      cond = new DynamicOperationSyncCondition();
      // The fin condition should ensure the slave dbscale has get the new
      // config version.
      cond->prepare_condition(after_pub_version);

      sync_tool->get_sync_topic()->set_sync_info_op_param(param);
      sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param, cond);
      sync_tool->publish_sync_message();
      sync_tool->wait_all_children_sync();
      delete cond;
      cond = NULL;
      is_sync_finish = true;
    } catch (Exception &e) {
      try {
        sync_tool->cancel_all_children_sync(param, cond);
      } catch (...) {
        LOG_ERROR("Cancel remove server error.\n");
      }
      LOG_ERROR("error happen in dynamic add slave, due to %s.\n", e.what());
      if (cond) {
        delete cond;
        cond = NULL;
      }
    }

    if (!is_sync_finish) {
      throw Error("Fail to dynamic_remove_server.\n");
    }
  }

  backend->remove_server_monitor(server);
  backend->remove_data_server(server->get_name());
}

void MySQLDynamicRemoveOPNode::dynamic_remove_partition_scheme() {
  LOG_INFO("Start to dynamic remove scheme %s.\n", op_name);
  Backend *backend = Backend::instance();
  PartitionScheme *scheme = backend->find_partition_scheme(op_name);
  if (!scheme) {
    string err_message("the scheme is not exist:");
    err_message.append(op_name);
    throw DynamicOpFail(err_message.c_str());
  }

  if (scheme->has_relate_tables()) {
    string relate_tables_str = scheme->get_relate_table_str();
    string err_msg = "Scheme relate to table:";
    err_msg += relate_tables_str;
    throw DynamicOpFail(err_msg.c_str());
  }

  backend->remove_partition_scheme(scheme);
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    try {
      bool is_sync_finish = false;
      bool is_start_sync = false;
      MultipleManager *mul = MultipleManager::instance();
      MultipleSyncTool *sync_tool = mul->get_sync_tool();
      char param[1024] = {0};
      sprintf(param, "%d %s", TYPE_DYNAMIC_REMOVE_SCHEME, op_name);

      is_start_sync = backend->start_dynamic_operation_topic(
          DYNAMIC_REMOVE_SCHEME_TOPIC_NAME, param);
      if (!is_start_sync) {
        LOG_ERROR(
            "Fail to start_dynamic_operation_topic for "
            "dynamic_remove_scheme.\n");
        throw Error(
            "Fail to start_dynamic_operation_topic for dynamic_remove_scheme.");
      }

      DynamicOperationSyncCondition *cond = NULL;
      try {
        /*fin the sync topic*/
        mul->acquire_dynamic_info_lock();
        unsigned long after_pub_version = mul->get_cur_config_info_version();
        mul->release_dynamic_info_lock();
        cond = new DynamicOperationSyncCondition();
        // The fin condition should ensure the slave dbscale has get the new
        // config version.
        cond->prepare_condition(after_pub_version);

        sync_tool->get_sync_topic()->set_sync_info_op_param(param);
        sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param, cond);
        sync_tool->publish_sync_message();
        sync_tool->wait_all_children_sync();
        delete cond;
        cond = NULL;
        is_sync_finish = true;
      } catch (Exception &e) {
        try {
          sync_tool->cancel_all_children_sync(param, cond);
        } catch (...) {
          LOG_ERROR("Cancel remove scheme error.\n");
        }
        LOG_ERROR("error happen in dynamic_remove_scheme, due to %s.\n",
                  e.what());
        if (cond) {
          delete cond;
          cond = NULL;
        }
      }

      if (!is_sync_finish) {
        throw Error("Fail to dynamic_remove_scheme.\n");
      }
    } catch (...) {
      status = EXECUTE_STATUS_COMPLETE;
      throw;
    }
  }
#endif
}
void MySQLDynamicRemoveOPNode::execute() {
  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *dynamic_remove_mutex =
      backend->get_dynamic_modify_rep_mutex();
  {  // Guard scope start of dynamic_remove_mutex
    ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_remove_mutex);
    bool start_config_locked = false;
    try {
      if (multiple_mode) {
        acquire_start_config_lock(
            session->get_zk_start_config_lock_extra_info());
        start_config_locked = true;
      }
      stmt_type type = plan->statement->get_stmt_node()->type;
      switch (type) {
        case STMT_DBSCALE_DYNAMIC_REMOVE_DATASERVER:
          dynamic_remove_server();
          break;
        case STMT_DBSCALE_DYNAMIC_REMOVE_DATASOURCE:
          dynamic_remove_source();
          break;
        case STMT_DBSCALE_DYNAMIC_REMOVE_PARTITION_SCHEME:
          dynamic_remove_partition_scheme();
          break;
        default:
          throw Error("not support this type.");
      }
      if (multiple_mode) {
        release_start_config_lock();
        start_config_locked = false;
        Backend::instance()->flush_config_to_zoo(false);
      }
      send_ok_packet_to_client(handler, 0, 0);
    } catch (exception &e) {
      if (multiple_mode && start_config_locked) {
        try {
          release_start_config_lock();
        } catch (...) {
        }
      }
      LOG_DEBUG("connection get error in dynamic remove.\n");
      status = EXECUTE_STATUS_COMPLETE;
      MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
      error_packet = Backend::instance()->get_new_packet();
      error.pack(error_packet);
      throw;
    }
  }  // Guard scope end of dynamic_remove_mutex
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    MultipleManager *mul = MultipleManager::instance();
    mul->acquire_ka_update_generate_lock();
    mul->reset_ka_init_node_message();
    mul->release_ka_update_generate_lock();
  }
#endif

  status = EXECUTE_STATUS_COMPLETE;
  return;
}
/* class MySQLDynamicRemoveSlaveNode */
MySQLDynamicRemoveSlaveNode::MySQLDynamicRemoveSlaveNode(
    ExecutePlan *plan, dynamic_remove_slave_op_node *dynamic_remove_slave_oper)
    : MySQLExecuteNode(plan),
      master_name(dynamic_remove_slave_oper->master_name),
      slave_name(dynamic_remove_slave_oper->slave_name),
      target_ds(NULL),
      slave_ds(NULL) {
  this->name = "MySQLDynamicRemoveSlaveNode";
  is_force = dynamic_remove_slave_oper->is_force;
  packet = NULL;
  error_packet = NULL;
}
void MySQLDynamicRemoveSlaveNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
}
void MySQLDynamicRemoveSlaveNode::check_config() {
  Backend *backend = Backend::instance();
  target_ds = backend->find_data_source(master_name);
  if (!target_ds) {
    throw DynamicRemoveSlaveFail("the target datasource is not exist");
  }

  DataSourceType ds_type = target_ds->get_data_source_type();
  if (ds_type != DATASOURCE_TYPE_REPLICATION &&
      ds_type != DATASOURCE_TYPE_MGR) {
    throw DynamicRemoveSlaveFail(
        "the type of datasource is not REPLICATION or MGR");
  }

  DataServer *data_server = backend->find_data_server(slave_name);
  if (data_server && data_server->is_a_master_server()) {
    throw DynamicRemoveSlaveFail("the slave is master to other slaves");
  }
  if (data_server) {
    slave_ds = target_ds->get_slave_by_server(data_server);
  } else {
    slave_ds = ((LoadBalanceDataSource *)(((RWSplitDataSource *)target_ds)
                                              ->get_read_source()))
                   ->get_data_source_by_name(slave_name);
    if (slave_ds && ds_type == DATASOURCE_TYPE_MGR) {
      throw DynamicRemoveSlaveFail(
          "MGR datasource should not contain slave source");
    }
  }

  if (!slave_ds) {
    throw DynamicRemoveSlaveFail(
        "the server/datasource is not a slave of the master datasource");
  }

  // send pre-disaster-master info when remove disaster cluster
  if (slave_ds->get_data_source_type() == DATASOURCE_TYPE_SERVER &&
      ((ServerDataSource *)slave_ds)->get_server()->is_dbscale_server()) {
    // get host information
    string cluster_host = node_local_addr;
    cluster_host += ":" + to_string(driver->get_port());
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) {
      string master_host = cluster_host;
      vector<string> vec;
      ZooKeeperTool *zk = ZooKeeperTool::instance();
      zk->get_node_child(ZK_CLUSTER_NODES_PATH, &vec);
      for (auto it = vec.begin(); it != vec.end(); ++it) {
        string tmp_host = *it;
        tmp_host = tmp_host.substr(0, tmp_host.rfind(':'));
        if (master_host == tmp_host) continue;
        cluster_host += "," + tmp_host;
      }
    }
#endif
    LOG_DEBUG("MySQLDynamicRemoveSlaveNode get cluster_host:%s\n",
              cluster_host.c_str());

    // send pre-disaster-master information
    Connection *conn = NULL;
    try {
      conn = slave_ds->get_connection(NULL, false);
      if (!conn) {
        string err_mg = "fail to get conn from source ";
        err_mg += slave_name;
        throw HandlerError(err_mg.c_str());
      } else {
        // build sql
        string sql = "DBSCALE SET GLOBAL \"pre-disaster-master-info\" = \"";
        sql += cluster_host + "\"";
        conn->execute_one_modify_sql(sql.c_str());
        conn->get_pool()->add_back_to_free(conn);
        conn = NULL;
      }
    } catch (HandlerError &e) {
      // not throw due to not affect dynamic remove slave
      if (conn) {
        conn->get_pool()->add_back_to_dead(conn);
      }
      conn = NULL;
      LOG_ERROR(
          "MySQLDynamicRemoveSlaveNode fail to send pre-disaster-master "
          "parameters due to %s\n",
          e.what());
    }
  }
}

void MySQLDynamicRemoveSlaveNode::dynamic_remove_slave() {
#ifndef CLOSE_MULTIPLE
  DataSourceMessageHandler *source_handler =
      DataSourceMessageHandler::instance();

  if (multiple_mode) {
    source_handler->acquire_message_lock_write();
    int retry = 60;
    MultipleManager *mul = MultipleManager::instance();
    while (retry) {
      if (mul->check_stable_situation()) break;
      timespec_t t = (timespec_t)ACE_Time_Value(0, 1000000);  // sleep 1s
      ACE_OS::nanosleep(&t);
      retry--;
    }
    if (!retry) {
      source_handler->release_message_lock();
      LOG_ERROR(
          "DB is busy and state changing now, fail to wait stable for "
          "MySQLDynamicRemoveOPNode::dynamic_remove_source.\n");
      throw Error("DB is busy and state changing now, plz retry later.");
    }
  }
#endif
  target_ds->dynamic_remove_slave(slave_ds);
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    source_handler->release_message_lock();
  }
#endif
}

void MySQLDynamicRemoveSlaveNode::set_error_packet(uint16_t error_code,
                                                   const char *error_message,
                                                   const char *sqlstate,
                                                   uint8_t number) {
  error_packet = Backend::instance()->get_new_packet();
  MySQLErrorResponse error(error_code, error_message, sqlstate, number);
  error.pack(error_packet);
}

void MySQLDynamicRemoveSlaveNode::do_delay_remove_slave() {
  if (!enable_disaster_mode) {
    LOG_ERROR("delay disable cluster need set enable_disaster_mode.\n");
    throw DynamicRemoveSlaveFail(
        "delay disable cluster need set enable_disaster_mode = 1");
  }
  if (!reinforce_enable_read_only) {
    LOG_ERROR("delay disable cluster need set reinforce_enable_read_only.\n");
    throw DynamicRemoveSlaveFail(
        "delay disable cluster need set reinforce_enable_read_only = 1");
  }
  if (!send_slave_dbscale_disconnect) {
    LOG_ERROR(
        "delay disable cluster need set send_slave_dbscale_disconnect = 1\n");
    throw DynamicRemoveSlaveFail(
        "delay disable cluster need set send_slave_dbscale_disconnect = 1");
  }
  Backend *backend = Backend::instance();
  AsyncTaskDisconnectReplication *async_task_tool =
      backend->async_task_diconnection_replication_tool;
  if (slave_ds && slave_ds->get_is_slave_source() &&
      slave_ds->get_data_source_type() == DATASOURCE_TYPE_SERVER) {
    ServerDataSource *server_source = static_cast<ServerDataSource *>(slave_ds);
    DataServer *server = server_source->get_server();
    if (!server) {
      LOG_ERROR("execute disable_cluster source [%s] fail to find server\n",
                server_source->get_name());
      throw DynamicRemoveSlaveFail("the server is not exist");
    }
    if (!server->is_dbscale_server()) {
      LOG_ERROR(
          "execute disable_cluster source [%s] server is not dbscale server\n",
          server_source->get_name());
      throw DynamicRemoveSlaveFail("the server is not dbscale server");
    }
    slave_server_name = server->get_name();
  } else {
    LOG_ERROR("only the slave dbscale cluster datasource can delay remove\n");
    throw DynamicRemoveSlaveFail(
        "only the slave dbscale cluster datasource can delay remove");
  }
  try {
    ACE_Guard<ACE_Thread_Mutex> guard(async_task_tool->mutex);
    if (async_task_tool->data_server_exec_async_task_id > 0 ||
        async_task_tool->data_server_exec_async_task_id_second > 0) {
      throw Error("The async task is running");
    }
    string async_name = "DBSCALE_DELAY_DISABLE_CLUSTER&";
    async_name.append(slave_name);
    async_name.append("%");
    async_name.append(master_name);
    async_name.append("#");
    async_name.append(slave_server_name);
    async_name.append("|");
    AsyncTaskManager *atm = AsyncTaskManager::instance();
    int uid = atm->register_async_task(async_name.c_str(),
                                       "Start to disable Cluster");
    async_task_tool->data_server_exec_async_task_id = uid;
    async_task_tool->cond.signal();
  } catch (...) {
    LOG_ERROR(
        "The cluster disconnection command has been executed,please check "
        "async tasks\n");
    throw DynamicRemoveSlaveFail(
        "The cluster disconnection command has been executed,please check "
        "async tasks");
  }
}

void MySQLDynamicRemoveSlaveNode::execute() {
  LOG_INFO("begin to dynamic remove a slave to rep datasource[%s].\n",
           master_name);

  try {
    check_config();
    if (!is_force) {
      do_delay_remove_slave();
      status = EXECUTE_STATUS_COMPLETE;
      send_ok_packet_to_client(handler, 0, 0);
      return;
    }
    dynamic_remove_slave();
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) Backend::instance()->flush_config_to_zoo(false);
#endif
  } catch (DynamicRemoveSlaveFail &e) {
    status = EXECUTE_STATUS_COMPLETE;
    LOG_DEBUG("error happen in dynamic remove slave.\n");
    Packet error_packet;
    MySQLErrorResponse error(ERROR_DYNAMIC_REMOVE_SLAVE_CODE, e.what());
    error.pack(&error_packet);
    handler->send_to_client(&error_packet);
    return;
  }
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    MultipleManager *mul = MultipleManager::instance();
    mul->acquire_ka_update_generate_lock();
    mul->reset_ka_init_node_message();
    mul->release_ka_update_generate_lock();
  }
#endif

  status = EXECUTE_STATUS_COMPLETE;
  send_ok_packet_to_client(handler, 0, 0);
}

/* class MySQLDynamicRemoveSchemaNode */
MySQLDynamicRemoveSchemaNode::MySQLDynamicRemoveSchemaNode(
    ExecutePlan *plan, const char *schema_name, bool is_force)
    : MySQLExecuteNode(plan), schema_name(schema_name), is_force(is_force) {
  this->name = "MySQLDynamicRemoveSchemaNode";
  packet = NULL;
  error_packet = NULL;
  boost::to_lower(this->schema_name);
}
void MySQLDynamicRemoveSchemaNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
}
void MySQLDynamicRemoveSchemaNode::execute() {
  Backend *bk = Backend::instance();
  if (bk->is_system_schema(schema_name.c_str())) {
    LOG_ERROR(
        "Dynamic remove schema dataspace failed, cannot remove dbscale system "
        "schema.\n");
    status = EXECUTE_STATUS_COMPLETE;
    Packet error_packet;
    MySQLErrorResponse error(ERROR_DYNAMIC_REMOVE_SCHEMA_CODE,
                             "There has table dataspace inside it");
    error.pack(&error_packet);
    handler->send_to_client(&error_packet);
    return;
  }
  ACE_RW_Thread_Mutex *mutex = bk->get_dynamic_modify_rep_mutex();
  ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*mutex);
  Schema *schema = bk->find_schema(schema_name.c_str());
  if (schema == NULL) {
    LOG_DEBUG("Schema not config, no need to remove.\n");
    status = EXECUTE_STATUS_COMPLETE;
    send_ok_packet_to_client(handler, 0, 0);
    return;
  }

  if (schema->contain_table()) {
    LOG_ERROR(
        "Dynamic remove schema dataspace failed, there has table dataspace "
        "inside it.\n");
    status = EXECUTE_STATUS_COMPLETE;
    Packet error_packet;
    MySQLErrorResponse error(ERROR_DYNAMIC_REMOVE_SCHEMA_CODE,
                             "There has table dataspace inside it");
    error.pack(&error_packet);
    handler->send_to_client(&error_packet);
    return;
  }

  if (!is_force) {
    bool has_schema = false;
    Connection *conn = NULL;
    try {
      conn = schema->get_connection(session);
      if (!conn) {
        LOG_ERROR("Failed to get connection when dynamic_remove_schema.\n");
        throw Error("Failed to get connection when dynamic_remove_schema.");
      }
      const char *sql = "SHOW DATABASES";
      vector<string> vec;
      TimeValue tv(backend_sql_net_timeout);
      conn->query_for_one_column(sql, 0, &vec, &tv, true);
      vector<string>::iterator it;
      for (it = vec.begin(); it != vec.end(); it++) {
        if (*it == schema_name) {
          has_schema = true;
          break;
        }
      }
      conn->get_pool()->add_back_to_free(conn);
    } catch (...) {
      if (conn) {
        conn->get_pool()->add_back_to_dead(conn);
      }
      status = EXECUTE_STATUS_COMPLETE;
      throw;
    }
    if (has_schema) {
      LOG_ERROR(
          "Dynamic remove schema dataspace failed, the schema should drop "
          "first before dynamic remove schema dataspace.\n");
      status = EXECUTE_STATUS_COMPLETE;
      Packet error_packet;
      MySQLErrorResponse error(
          ERROR_DYNAMIC_REMOVE_SCHEMA_CODE,
          "Schema should drop first before dynamic remove schema dataspace.");
      error.pack(&error_packet);
      handler->send_to_client(&error_packet);
      return;
    }
  }
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    try {
      bool is_sync_finish = false;
      bool is_start_sync = false;
      MultipleManager *mul = MultipleManager::instance();
      MultipleSyncTool *sync_tool = mul->get_sync_tool();
      char param[1024] = {0};
      sprintf(param, "%d %s %d", TYPE_DYNAMIC_REMOVE_SCHEMA,
              schema_name.c_str(), is_force > 0 ? 1 : 0);

      is_start_sync = bk->start_dynamic_operation_topic(
          DYNAMIC_REMOVE_SCHEMA_TOPIC_NAME, param);
      if (!is_start_sync) {
        LOG_ERROR(
            "Fail to start_dynamic_operation_topic for "
            "dynamic_remove_schema.\n");
        throw Error(
            "Fail to start_dynamic_operation_topic for dynamic_remove_schema.");
      }

      DynamicOperationSyncCondition *cond = NULL;
      try {
        /*fin the sync topic*/
        mul->acquire_dynamic_info_lock();
        unsigned long after_pub_version = mul->get_cur_config_info_version();
        mul->release_dynamic_info_lock();
        cond = new DynamicOperationSyncCondition();
        // The fin condition should ensure the slave dbscale has get the new
        // config version.
        cond->prepare_condition(after_pub_version);

        sync_tool->get_sync_topic()->set_sync_info_op_param(param);
        sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param, cond);
        sync_tool->publish_sync_message();
        sync_tool->wait_all_children_sync();
        delete cond;
        cond = NULL;
        is_sync_finish = true;
      } catch (Exception &e) {
        try {
          sync_tool->cancel_all_children_sync(param, cond);
        } catch (...) {
          LOG_ERROR("Cancel remove schema error.\n");
        }
        LOG_ERROR("error happen in dynamic_remove_schema, due to %s.\n",
                  e.what());
        if (cond) {
          delete cond;
          cond = NULL;
        }
      }

      if (!is_sync_finish) {
        throw Error("Fail to dynamic_remove_schema.\n");
      }
    } catch (...) {
      status = EXECUTE_STATUS_COMPLETE;
      throw;
    }
  }
#endif

  bk->get_catalog()->remove_schema_to_discard(schema_name.c_str());
  Backend::instance()->remove_data_space(schema);
  status = EXECUTE_STATUS_COMPLETE;
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) bk->flush_config_to_zoo(false);
#endif

  Driver::get_driver()->get_config_helper()->remove_schema(schema_name.c_str());

  send_ok_packet_to_client(handler, 0, 0);
}
/* class MySQLDynamicRemoveTableNode */
MySQLDynamicRemoveTableNode::MySQLDynamicRemoveTableNode(ExecutePlan *plan,
                                                         const char *table_name,
                                                         bool is_force)
    : MySQLExecuteNode(plan), table_name(table_name), is_force(is_force) {
  this->name = "MySQLDynamicRemoveTableNode";
  packet = NULL;
  error_packet = NULL;
  boost::to_lower(this->table_name);
  this->mul_topic_type = TYPE_DYNAMIC_REMOVE_TABLE;
}
void MySQLDynamicRemoveTableNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
}

bool MySQLDynamicRemoveTableNode::check_table_exists() {
  size_t pos = table_name.find('.');
  if (pos == string::npos) {
    LOG_ERROR(
        "Remove table dataspace name format error, for safety consider, the "
        "format should be SCHEMA_NAME.TABLET_NAME.\n");
    status = EXECUTE_STATUS_COMPLETE;
    Packet error_packet;
    MySQLErrorResponse error(ERROR_DYNAMIC_REMOVE_TABLE_CODE,
                             "the name format of remove table dataspace should "
                             "be SCHEMA_NAME.TABLET_NAME");
    error.pack(&error_packet);
    handler->send_to_client(&error_packet);
    return false;
  }
  s_name = table_name.substr(0, pos);
  t_name = table_name.substr(pos + 1);
  Backend *bk = Backend::instance();
  ACE_RW_Thread_Mutex *mutex = bk->get_dynamic_modify_rep_mutex();
  ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*mutex);
  Schema *schema = bk->find_schema(s_name.c_str());
  Table *table = NULL;
  if (schema) table = schema->get_table_by_name(t_name.c_str());
  if (schema == NULL || table == NULL) {
    LOG_DEBUG("Table not config, no need to remove.\n");
    status = EXECUTE_STATUS_COMPLETE;
    send_ok_packet_to_client(handler, 0, 0);
    return false;
  }
  return true;
}

bool MySQLDynamicRemoveTableNode::check_space_has_tables(Table *table) {
  bool has_table = false;
  Connection *conn = NULL;
  bool is_partitioned_table = table->is_partitioned();
  try {
    if (is_partitioned_table) {
      PartitionedTable *part_table = (PartitionedTable *)table;
      conn = part_table->get_partition(0)->get_connection(session);
    } else {
      conn = table->get_connection(session);
    }
    if (!conn) {
      LOG_ERROR("Failed to get connection when dynamic_remove_table.\n");
      throw Error("Failed to get connection when dynamic_remove_table.");
    }
    const char *pattern = table->get_name_pattern();
    char sql[1024];
    if (pattern == NULL) {
      sprintf(sql,
              "SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES WHERE "
              "(TABLE_SCHEMA='%s' OR TABLE_SCHEMA LIKE '%s_dbscale') AND "
              "TABLE_NAME = '%s'",
              s_name.c_str(), s_name.c_str(), t_name.c_str());
      string value;
      conn->query_for_one_value(sql, value, 0);
      if (value != "0") {
        has_table = true;
      }
    } else {
      string pattern_name(pattern);
      boost::to_lower(pattern_name);
      boost::regex pattern_regex(pattern_name);
      sprintf(sql,
              "SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE "
              "TABLE_SCHEMA='%s' OR TABLE_SCHEMA LIKE '%s_dbscale'",
              s_name.c_str(), s_name.c_str());
      vector<string> vec;
      TimeValue tv(backend_sql_net_timeout);
      conn->query_for_one_column(sql, 0, &vec, &tv, true);
      for (vector<string>::iterator it = vec.begin(); it != vec.end(); it++) {
        string value = *it;
        boost::to_lower(value);
        if (regex_match(value, pattern_regex)) {
          Backend *backend = Backend::instance();
          DataSpace *tmp_s = backend->get_data_space_for_table(
              table->get_schema()->get_name(), value.c_str());
          if (tmp_s == (DataSpace *)table) {
            has_table = true;
            break;
          }
        }
      }
    }
    conn->get_pool()->add_back_to_free(conn);
  } catch (...) {
    if (conn) {
      conn->get_pool()->add_back_to_dead(conn);
    }
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  }
  return has_table;
}

void MySQLDynamicRemoveTableNode::sync_topic(char *param) {
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    try {
      bool is_sync_finish = false;
      bool is_start_sync = false;
      Backend *bk = Backend::instance();
      MultipleManager *mul = MultipleManager::instance();
      MultipleSyncTool *sync_tool = mul->get_sync_tool();

      is_start_sync = bk->start_dynamic_operation_topic(
          DYNAMIC_REMOVE_TABLE_TOPIC_NAME, param);
      if (!is_start_sync) {
        LOG_ERROR(
            "Fail to start_dynamic_operation_topic for "
            "dynamic_remove_table.\n");
        throw Error(
            "Fail to start_dynamic_operation_topic for dynamic_remove_table.");
      }

      DynamicOperationSyncCondition *cond = NULL;
      try {
        /*fin the sync topic*/
        mul->acquire_dynamic_info_lock();
        unsigned long after_pub_version = mul->get_cur_config_info_version();
        mul->release_dynamic_info_lock();
        cond = new DynamicOperationSyncCondition();
        // The fin condition should ensure the slave dbscale has get the new
        // config version.
        cond->prepare_condition(after_pub_version);

        sync_tool->get_sync_topic()->set_sync_info_op_param(param);
        sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param, cond);
        sync_tool->publish_sync_message();
        sync_tool->wait_all_children_sync();
        delete cond;
        cond = NULL;
        is_sync_finish = true;
      } catch (Exception &e) {
        try {
          sync_tool->cancel_all_children_sync(param, cond);
        } catch (...) {
          LOG_ERROR("Cancel remove schema error.\n");
        }
        LOG_ERROR("error happen in dynamic_remove_table, due to %s.\n",
                  e.what());
        if (cond) {
          delete cond;
          cond = NULL;
        }
      }

      if (!is_sync_finish) {
        throw Error("Fail to dynamic remove table oper.\n");
      }
    } catch (...) {
      status = EXECUTE_STATUS_COMPLETE;
      throw;
    }
  }
#endif
}

void MySQLDynamicRemoveTableNode::execute() {
  if (!check_table_exists()) return;
  Backend *bk = Backend::instance();
  ACE_RW_Thread_Mutex *mutex = bk->get_dynamic_modify_rep_mutex();
  ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*mutex);
  Schema *schema = bk->find_schema(s_name.c_str());
  Table *table = schema->get_table_by_name(t_name.c_str());

  bool is_partitioned_table = table->is_partitioned();
  if (!is_force) {
    bool has_table = check_space_has_tables(table);
    if (has_table) {
      LOG_ERROR(
          "Dynamic remove table dataspace failed, the table should drop first "
          "before dynamic remove schema dataspace.\n");
      status = EXECUTE_STATUS_COMPLETE;
      Packet error_packet;
      MySQLErrorResponse error(
          ERROR_DYNAMIC_REMOVE_TABLE_CODE,
          "Table should drop first before dynamic remove table dataspace.");
      error.pack(&error_packet);
      handler->send_to_client(&error_packet);
      return;
    }
  }

  char param[1024] = {0};
  sprintf(param, "%d %s", mul_topic_type, table_name.c_str());
  sync_topic(param);

  schema->remove_table_to_discard(t_name.c_str());
  Backend::instance()->remove_data_space(table);
  Backend::instance()->clean_join_table_space(table);
  status = EXECUTE_STATUS_COMPLETE;
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) Backend::instance()->flush_config_to_zoo(false);
#endif

  if (is_partitioned_table) {
    Driver::get_driver()->get_config_helper()->remove_partition_table(
        t_name.c_str(), schema->get_name());
  } else {
    Driver::get_driver()->get_config_helper()->remove_normal_table(
        t_name.c_str(), schema->get_name());
  }

  send_ok_packet_to_client(handler, 0, 0);
}

/* class MySQLDBScaleShowSlowSqlTopNNode*/
MySQLDBScaleShowSlowSqlTopNNode::MySQLDBScaleShowSlowSqlTopNNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowSlowSqlTopNNode";
  this->top_n = plan->statement->get_stmt_node()
                    ->sql->dbscale_show_slow_sql_top_n_oper->top_n;
}

/* class MySQLDynamicChangeTableSchemeNode */
MySQLDynamicChangeTableSchemeNode::MySQLDynamicChangeTableSchemeNode(
    ExecutePlan *plan, const char *table_name, const char *scheme_name)
    : MySQLDynamicRemoveTableNode(plan, table_name, true),
      scheme_name(scheme_name) {
  this->name = "MySQLDynamicChangeTableSchemeNode";
  packet = NULL;
  error_packet = NULL;
  boost::to_lower(this->scheme_name);
  this->mul_topic_type = TYPE_DYNAMIC_CHANGE_TABLE_SCHEME;
}

void MySQLDynamicChangeTableSchemeNode::execute() {
  if (!check_table_exists()) return;
  Backend *bk = Backend::instance();
  ACE_RW_Thread_Mutex *mutex = bk->get_dynamic_modify_rep_mutex();
  ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*mutex);
  Schema *schema = bk->find_schema(s_name.c_str());
  Table *table = schema->get_table_by_name(t_name.c_str());

  string err_msg = "";
  bool is_error = false;
  PartitionScheme *ps = NULL;
  PartitionedTable *part_table = NULL;
  bool is_partitioned_table = table->is_partitioned();
  if (!is_partitioned_table) {
    err_msg = "No need do dynamic remove table pattern for ";
    err_msg.append(table_name);
    err_msg.append(" it is not partition table.\n");
    is_error = true;
  }

  if (!is_error) {
    bool has_table = check_space_has_tables(table);
    if (has_table) {
      err_msg =
          "The table should drop first before dynamic change table scheme.\n";
      is_error = true;
    }
  }

  if (!is_error) {
    ps = Backend::instance()->get_partition_scheme(scheme_name.c_str());
    if (!ps) {
      err_msg = "Can not find scheme ";
      err_msg.append(scheme_name);
      err_msg.append(" for dynamic change table scheme.\n");
      is_error = true;
    }
  }

  if (!is_error) {
    part_table = (PartitionedTable *)table;
    PartitionedTable *new_scheme_table = ps->get_one_relate_part_table();
    if (new_scheme_table) {
      if (new_scheme_table->get_partition_num() !=
          part_table->get_partition_num()) {
        err_msg = "old partition scheme table partition num:" +
                  to_string(part_table->get_partition_num()) +
                  ", new partition scheme table partition num:" +
                  to_string(new_scheme_table->get_partition_num()) +
                  ", they should be same";
        is_error = true;
      }
    } else {
      err_msg = "partition scheme " + string(ps->get_name()) +
                " should has partition table.";
      is_error = true;
    }
  }

  if (is_error) {
    LOG_ERROR("%s\n", err_msg.c_str());
    status = EXECUTE_STATUS_COMPLETE;
    Packet error_packet;
    MySQLErrorResponse error(ERROR_DYNAMIC_REMOVE_TABLE_CODE, err_msg.c_str());
    error.pack(&error_packet);
    handler->send_to_client(&error_packet);
    return;
  }

  char param[1024] = {0};
  sprintf(param, "%d %s %s", mul_topic_type, table_name.c_str(),
          scheme_name.c_str());
  sync_topic(param);

  part_table->init_partition_scheme(ps);
  status = EXECUTE_STATUS_COMPLETE;
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) Backend::instance()->flush_config_to_zoo(false);
#endif
  send_ok_packet_to_client(handler, 0, 0);
}

bool compare_slow_log(const slow_log_info &a, const slow_log_info &b) {
  return a.time > b.time;
}

void MySQLDBScaleShowSlowSqlTopNNode::init_data() {
  if (top_n > 100) top_n = 100;
  try {
    set_head_packet(2);
    add_column_packet("def", "information_schema", "time(s)", "time(s)",
                      "time(s)", "time(s)", 8, NAME_LEN + 1,
                      MYSQL_TYPE_VAR_STRING, 0, 0, 0);
    add_column_packet("def", "information_schema", "sql", "sql", "sql", "sql",
                      8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
    vector<slow_log_info> info;
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) {
      LOG_DEBUG("try to get cluster slow sql top n\n");
      char sql[64];
      sprintf(sql, "dbscale request cluster slow sql top %lu", top_n);
      MultipleManager *mul_manager = MultipleManager::instance();
      vector<vector<string> > row_data_vec;
      mul_manager->get_all_slow_sql_info(sql, &row_data_vec);
      if (!row_data_vec.empty()) {
        for (size_t i = 0; i < row_data_vec.size(); i++) {
          slow_log_info tmp = {"", atof(row_data_vec[i][0].c_str()),
                               row_data_vec[i][1]};
          info.push_back(tmp);
        }
      }
    } else {
      Backend::instance()->get_top_n_slow_sql(info, top_n);
    }
#else
    Backend::instance()->get_top_n_slow_sql(info, top_n);
#endif
    if (!info.empty()) {
      sort(info.begin(), info.end(), compare_slow_log);
      size_t real_size = info.size();
      size_t n = real_size < top_n ? real_size : top_n;
      for (size_t i = 0; i < n; i++) {
        list<const char *> row_data;
        char tmp[20];
        sprintf(tmp, "%.3f", info[i].time);
        row_data.push_back(tmp);
        row_data.push_back(info[i].query_sql.c_str());
        add_row_packet(row_data);
      }
    }
  } catch (exception &e) {
    LOG_ERROR("got exception when execute dbscale show slow sql top %B.\n",
              top_n);
    throw;
  }
}

/* class MySQLDBScaleRequestSlowSqlTopNNode*/
MySQLDBScaleRequestSlowSqlTopNNode::MySQLDBScaleRequestSlowSqlTopNNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleRequestSlowSqlTopNNode";
  this->top_n = plan->statement->get_stmt_node()
                    ->sql->dbscale_show_slow_sql_top_n_oper->top_n;
}

void MySQLDBScaleRequestSlowSqlTopNNode::init_data() {
  try {
    set_head_packet(2);
    add_column_packet("def", "information_schema", "time(s)", "time(s)",
                      "time(s)", "time(s)", 8, NAME_LEN + 1,
                      MYSQL_TYPE_VAR_STRING, 0, 0, 0);
    add_column_packet("def", "information_schema", "sql", "sql", "sql", "sql",
                      8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
    vector<slow_log_info> info;
    Backend::instance()->get_top_n_slow_sql(info, top_n);
    if (!info.empty()) {
      size_t real_size = info.size();
      size_t n = real_size < top_n ? real_size : top_n;
      for (size_t i = 0; i < n; i++) {
        list<const char *> row_data;
        char tmp[20];
        sprintf(tmp, "%.3f", info[i].time);
        row_data.push_back(tmp);
        row_data.push_back(info[i].query_sql.c_str());
        add_row_packet(row_data);
      }
    }
  } catch (exception &e) {
    LOG_ERROR("got exception when execute dbscale request slow sql top %B.\n",
              top_n);
    throw;
  }
}

/* class MySQLDBScaleShowAuditUserListNode*/
MySQLDBScaleShowAuditUserListNode::MySQLDBScaleShowAuditUserListNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowAuditUserListNode";
}

void MySQLDBScaleShowAuditUserListNode::init_data() {
  try {
    set_head_packet(1);
    add_column_packet("def", "information_schema", "username", "username",
                      "username", "username", 8, NAME_LEN + 1,
                      MYSQL_TYPE_VAR_STRING, 0, 0, 0);
    set<string> audit_user_set;
    Backend::instance()->get_audit_user_set(audit_user_set);
    if (!audit_user_set.empty()) {
      set<string>::iterator it = audit_user_set.begin();
      for (; it != audit_user_set.end(); it++) {
        list<const char *> row_data;
        row_data.push_back(it->c_str());
        add_row_packet(row_data);
      }
    }
  } catch (exception &e) {
    LOG_ERROR("got exception when execute dbscale show audit user list.\n");
    throw;
  }
}

/* class MySQLDBScaleHelpNode */
MySQLDBScaleHelpNode::MySQLDBScaleHelpNode(ExecutePlan *plan, const char *name)
    : MySQLDBScaleShowNode(plan), cmd_name(name) {
  this->name = "MySQLDBScaleHelpNode";
}

void MySQLDBScaleHelpNode::init_data() {
  try {
    set_head_packet(3);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";
    list<const char *> columns;
    columns.push_back("Cmd");
    columns.push_back("Desc");
    columns.push_back("Extra");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
    for (unsigned int i = 0;
         i < sizeof(help_message) / sizeof(dbscale_help_str); i++) {
      if (cmd_name != NULL && strcasestr(help_message[i].cmd, cmd_name) == NULL)
        continue;
      list<const char *> row_data;
      row_data.push_back(help_message[i].cmd);
      row_data.push_back(help_message[i].des);
      row_data.push_back(help_message[i].extra);
      add_row_packet(row_data);
    }
  } catch (exception &e) {
    LOG_ERROR("got exception when execute dbscale help.\n");
    throw;
  }
}

/* class MySQLDBScaleShowShardPartitionNode*/
MySQLDBScaleShowShardPartitionNode::MySQLDBScaleShowShardPartitionNode(
    ExecutePlan *plan, const char *name)
    : MySQLDBScaleShowNode(plan), scheme_name(name) {
  this->name = "MySQLDBScaleShowShardPartitionNode";
}

void MySQLDBScaleShowShardPartitionNode::init_data() {
  set_head_packet(4);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "dataservers";
  const char *org_table = "dataservers";
  list<const char *> columns;
  columns.push_back("Table");
  columns.push_back("Scheme");
  columns.push_back("RealPartitions");
  columns.push_back("Shards");
  list<const char *>::iterator it = columns.begin();
  for (; it != columns.end(); it++) {
    add_column_packet(catalog, schema, table, org_table, *it, *it, 8,
                      NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  }
  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *dynamic_add_mutex =
      backend->get_dynamic_modify_rep_mutex();

  ACE_Read_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_add_mutex);
  Backend *bd = Backend::instance();
  map<PartitionScheme *, set<string> > real_table_map;
  try {
    if (scheme_name) {
      PartitionScheme *ps = bd->get_partition_scheme(scheme_name);
      if (ps) {
        set<string> tmp_set;
        bd->get_real_shard_table_list_from_scheme(&tmp_set, scheme_name);
        real_table_map[ps] = tmp_set;
      }
    } else {
      bd->get_real_shard_table_list_from_all_schemes(real_table_map);
    }
  } catch (Exception &e) {
    LOG_ERROR("Get exception for show shard partition tables due to %s.\n ",
              e.what());
    throw Error(e.what());
  }
  map<PartitionScheme *, set<string> >::iterator it3 = real_table_map.begin();
  for (; it3 != real_table_map.end(); it3++) {
    if (!it3->first->is_shard()) continue;
    char pnum[64];
    snprintf(pnum, sizeof(pnum), "%u", it3->first->get_real_partition_size());
    char snum[64];
    snprintf(snum, sizeof(snum), "%u", it3->first->get_partition_num());
    set<string> &tb_name_set = it3->second;
    set<string>::iterator it2 = tb_name_set.begin();
    for (; it2 != tb_name_set.end(); it2++) {
      list<const char *> row_data;
      row_data.push_back(it2->c_str());
      row_data.push_back(it3->first->get_name());
      row_data.push_back(pnum);
      row_data.push_back(snum);
      add_row_packet(row_data);
    }
  }
}

/* class MySQLDBScaleShowRebalanceWorkLoadNode*/
MySQLDBScaleShowRebalanceWorkLoadNode::MySQLDBScaleShowRebalanceWorkLoadNode(
    ExecutePlan *plan, const char *name, list<string> sources,
    const char *schema_name, int is_remove)
    : MySQLDBScaleShowNode(plan),
      scheme_name(name),
      sources(sources),
      schema_name(schema_name),
      is_remove(is_remove) {
  this->name = "MySQLDBScaleShowRebalanceWorkLoadNode";
}

string MySQLDBScaleShowRebalanceWorkLoadNode::get_extra_info_for_table(
    const char *schema_name, const char *table_name) {
  string extra_info;
  if (instance_option_value["migrate_method"].uint_val !=
      METHOD_MIGRATE_PHYSICAL) {
    Session *session = plan->session;
    TableInfoCollection *tic = TableInfoCollection::instance();
    TableInfo *ti = tic->get_table_info_for_read(schema_name, table_name);
    map<string, TableColumnInfo *, strcasecomp> *column_info_map;
    try {
      column_info_map =
          ti->element_table_column
              ->get_element_map_columnname_table_column_info(session);
    } catch (...) {
      LOG_ERROR(
          "Error occured when try to get table info column "
          "info(map_columnname_table_column_info) of table [%s.%s]\n",
          schema_name, table_name);
      ti->release_table_info_lock();
      throw;
    }

    string column_type;
    map<string, TableColumnInfo *, strcasecomp>::iterator it =
        column_info_map->begin();
    for (; it != column_info_map->end(); it++) {
      column_type = it->second->column_type;
      if (column_type.find("float") != string::npos ||
          column_type.find("double") != string::npos) {
        size_t pos = column_type.find("(");
        if (pos != string::npos) {
          string precision(column_type, pos + 1,
                           column_type.length() - pos - 2);
          vector<string> strs;
          boost::algorithm::split(strs, precision,
                                  boost::algorithm::is_any_of(","));
          if (strs.size() != 2) {
            extra_info.append(
                "Float/double type column without complete precision like "
                "'float(6,2)'");
            break;
          }
        } else {
          extra_info.append("Float/double type column without precision.");
          break;
        }
      }
    }

    ti->release_table_info_lock();
  }
  return extra_info;
}

void MySQLDBScaleShowRebalanceWorkLoadNode::init_data() {
  set_head_packet(6);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "dataservers";
  const char *org_table = "dataservers";
  list<const char *> columns;
  columns.push_back("Table");
  columns.push_back("Scheme");
  columns.push_back("FromSource");
  columns.push_back("ToSource");
  columns.push_back("Charset");
  columns.push_back("ExtraInfo");
  list<const char *>::iterator it = columns.begin();
  for (; it != columns.end(); it++) {
    add_column_packet(catalog, schema, table, org_table, *it, *it, 8,
                      NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  }
  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *dynamic_add_mutex =
      backend->get_dynamic_modify_rep_mutex();

  ACE_Read_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_add_mutex);
  Backend *bd = Backend::instance();
  map<PartitionScheme *, set<string> > real_table_map;
  try {
    if (scheme_name) {
      PartitionScheme *ps = bd->get_partition_scheme(scheme_name);
      if (ps) {
        set<string> tmp_set;
        bd->get_real_shard_table_list_from_scheme(&tmp_set, scheme_name);
        real_table_map[ps] = tmp_set;
      }
    } else {
      bd->get_real_shard_table_list_from_all_schemes(real_table_map);
    }
  } catch (Exception &e) {
    LOG_ERROR("Get exception for show shard partition tables due to %s.\n ",
              e.what());
    throw Error(e.what());
  }
  map<PartitionScheme *, set<string> >::iterator it3 = real_table_map.begin();
  try {
    for (; it3 != real_table_map.end(); it3++) {
      if (!it3->first->is_shard()) continue;
      list<pair<string, string> > work_load;
      if (!is_remove)
        ((ShardPartitionScheme *)it3->first)
            ->calculate_rebalance_work_add_source(&sources, &work_load);
      else
        ((ShardPartitionScheme *)it3->first)
            ->calculate_rebalance_work_remove_source(&sources, &work_load);
      set<string> &tb_name_set = it3->second;
      set<string>::iterator it2 = tb_name_set.begin();
      for (; it2 != tb_name_set.end(); it2++) {
        list<pair<string, string> >::iterator it4 = work_load.begin();
        for (; it4 != work_load.end(); it4++) {
          list<const char *> row_data;
          if (schema_name) {
            vector<string> strs;
            boost::split(strs, *it2, boost::is_any_of("."),
                         boost::token_compress_on);
            if (lower_case_compare(strs[0].c_str(), schema_name) == 0) {
              string enclosed_tb_name("`");
              enclosed_tb_name.append(strs[0].c_str());
              enclosed_tb_name.append("`.`");
              enclosed_tb_name.append(strs[1].c_str());
              enclosed_tb_name.append("`");
              row_data.push_back(enclosed_tb_name.c_str());
              row_data.push_back(it3->first->get_name());
              row_data.push_back((*it4).first.c_str());
              row_data.push_back((*it4).second.c_str());
              string charset;
              backend->get_charset_for_table(strs[0].c_str(), strs[1].c_str(),
                                             plan->session, &charset);
              row_data.push_back(charset.c_str());
              string extrainfo =
                  get_extra_info_for_table(strs[0].c_str(), strs[1].c_str());
              row_data.push_back(extrainfo.c_str());
              add_row_packet(row_data);
            }
          } else {
            vector<string> strs;
            boost::split(strs, *it2, boost::is_any_of("."),
                         boost::token_compress_on);
            string enclosed_tb_name("`");
            enclosed_tb_name.append(strs[0].c_str());
            enclosed_tb_name.append("`.`");
            enclosed_tb_name.append(strs[1].c_str());
            enclosed_tb_name.append("`");
            row_data.push_back(enclosed_tb_name.c_str());
            row_data.push_back(it3->first->get_name());
            row_data.push_back((*it4).first.c_str());
            row_data.push_back((*it4).second.c_str());
            string charset;
            backend->get_charset_for_table(strs[0].c_str(), strs[1].c_str(),
                                           plan->session, &charset);
            row_data.push_back(charset.c_str());
            string extrainfo =
                get_extra_info_for_table(strs[0].c_str(), strs[1].c_str());
            row_data.push_back(extrainfo.c_str());
            add_row_packet(row_data);
          }
        }
      }
    }
  } catch (exception &e) {
    LOG_ERROR("Get exception 2 for show shard partition tables due to %s.\n",
              e.what());
    throw;
  }
}

/* class MySQLDBScaleShowJoinNode */
MySQLDBScaleShowJoinNode::MySQLDBScaleShowJoinNode(ExecutePlan *plan,
                                                   const char *name)
    : MySQLDBScaleShowNode(plan), op_name(name) {
  this->name = "MySQLDBScaleShowJoinNode";
}

void MySQLDBScaleShowJoinNode::init_data() {
  if (!strcasecmp(op_name, "hotspot")) {
    set_head_packet(2);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "dataservers";
    const char *org_table = "dataservers";
    list<const char *> columns;
    columns.push_back("Servers");
    columns.push_back("Write-Read Times");
    list<const char *>::iterator it = columns.begin();
    for (; it != columns.end(); it++) {
      add_column_packet(catalog, schema, table, org_table, *it, *it, 8,
                        NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
    }
    Driver *driver = Driver::get_driver();
    driver->acquire_session_mutex();
    set<Session *, compare_session> *session_set = driver->get_session_set();
    set<Session *, compare_session>::iterator h_it = session_set->begin();
    for (; h_it != session_set->end(); h_it++) {
      (*h_it)->update_server_hotspot_count();
    }
    driver->release_session_mutex();
    map<string, DataServer *> data_servers;
    Backend::instance()->get_data_servers(data_servers);
    map<string, DataServer *>::iterator it_s = data_servers.begin();
    for (; it_s != data_servers.end(); it_s++) {
      char count[64];
      snprintf(count, sizeof(count), "%u", it_s->second->get_hotspot_count());
      list<const char *> row_data;
      row_data.push_back(it_s->second->get_name());
      row_data.push_back(count);
      add_row_packet(row_data);
    }
  } else {
    LOG_ERROR("Unknown status named [%s].\n", op_name);
    throw Error("Unknown join status.");
  }
}

/* class MySQLDynamicAddDataServerNode*/
MySQLDynamicAddDataServerNode::MySQLDynamicAddDataServerNode(
    ExecutePlan *plan,
    dynamic_add_data_server_op_node *dynamic_add_data_server_oper)
    : MySQLExecuteNode(plan), data_server(NULL) {
  this->name = "MySQLDynamicAddDataServerNode";
  packet = NULL;
  error_packet = NULL;
  char *tmp;
  SAVE_STR(server_name, dynamic_add_data_server_oper->server_name);
  SAVE_STR(server_user, dynamic_add_data_server_oper->server_user);
  SAVE_STR(server_password, dynamic_add_data_server_oper->server_password);
  SAVE_STR(server_host, dynamic_add_data_server_oper->server_host);
  SAVE_STR(server_alias_host, dynamic_add_data_server_oper->server_alias_host);
  SAVE_STR(remote_user, dynamic_add_data_server_oper->remote_user);
  SAVE_STR(remote_password, dynamic_add_data_server_oper->remote_password);
  SAVE_STR(local_load_script, dynamic_add_data_server_oper->local_load_script);
  SAVE_STR(external_load_script,
           dynamic_add_data_server_oper->external_load_script);
  is_master_backup = !dynamic_add_data_server_oper->is_master_backup;
  is_external_load = dynamic_add_data_server_oper->is_external_load;
  server_port = dynamic_add_data_server_oper->server_port;
  remote_port = dynamic_add_data_server_oper->remote_port;
  location_id = dynamic_add_data_server_oper->location_id;
  mgr_server = dynamic_add_data_server_oper->mgr_server;
  dbscale_server = dynamic_add_data_server_oper->dbscale_server;
  set_by_add_pre_disaster_master(
      dynamic_add_data_server_oper->by_add_pre_disaster_master);
}
void MySQLDynamicAddDataServerNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
}

void MySQLDynamicAddDataServerNode::check_config() {
  Backend *backend = Backend::instance();
  if (backend->find_data_server(server_name)) {
    throw DynamicOpFail("the dataserver is exist plz change its name");
  }
  if (dbscale_server && !enable_disaster_mode) {
    throw DynamicOpFail(
        "cannot dynamic add slave_dbscale_server when \"enable-disaster-mode\" "
        "is off.");
  }
  if (server_port == 0 && dbscale_server) {
    check_dbscale_server_host();
  } else if (server_port == 0 || dbscale_server) {
    throw DynamicOpFail(
        "cannot dynamic add data_server when server_port is error or "
        "dbscale_server value is error.\n");
  }
}
void MySQLDynamicAddDataServerNode::dynamic_add_data_server() {
  LOG_INFO("begin to dynamic init data server[%s].\n", server_name);
  Backend *backend = Backend::instance();
  Driver *driver = Driver::get_driver();
  if (!dbscale_server || (server_port != 0)) {
    data_server = new DataServer(
        server_name, driver, server_port, server_host, server_alias_host,
        server_user, server_password, is_master_backup, location_id);
  } else {
    data_server = new DataServer(
        server_name, driver, dbscale_server_port, dbscale_server_host.c_str(),
        dbscale_server_host.c_str(), server_user, server_password,
        is_master_backup, location_id);
    data_server->set_dbscale_server_host_ip(dbscale_host_ip_vec);
  }
  data_server->set_local_load_script(local_load_script);
  data_server->set_external_load_script(external_load_script);
  data_server->set_is_external_load(is_external_load);
  data_server->set_remote_user(remote_user);
  data_server->set_remote_password(remote_password);
  data_server->set_remote_port(remote_port);
  data_server->set_mgr_server(mgr_server);
  data_server->set_dbscale_server(dbscale_server);
  backend->add_data_server(data_server);
  LOG_INFO("end dynamic init data server[%s].\n", server_name);
}

void MySQLDynamicAddDataServerNode::check_dbscale_server_host() {
  Backend *backend = Backend::instance();
  if (!backend->get_dbscale_server_ip_port(server_host, dbscale_server_host,
                                           dbscale_server_port,
                                           dbscale_host_ip_vec))
    throw DynamicOpFail("dbscale dataserver host has bad format\n");
}

void MySQLDynamicAddDataServerNode::dynamic_init_data_server() {
  LOG_INFO("begin to dynamic add data server[%s].\n", server_name);
  Backend *backend = Backend::instance();
  if (data_server) {
    data_server->init_monitor_conn();
    backend->assign_monitor_for_dataserver(data_server);
  }
  LOG_INFO("end dynamic add data server[%s].\n", server_name);
}
void MySQLDynamicAddDataServerNode::execute() {
  LOG_DEBUG("MySQLDynamicAddDataServerNode::execute.\n");
  Backend *backend = Backend::instance();
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    acquire_start_config_lock(session->get_zk_start_config_lock_extra_info());
  }
#endif
  ACE_RW_Thread_Mutex *dynamic_add_mutex =
      backend->get_dynamic_modify_rep_mutex();
  packet = Backend::instance()->get_new_packet();
  dynamic_add_mutex->acquire_write();
  bool locked = true;

  try {
    check_config();
  } catch (DynamicOpFail &e) {
    status = EXECUTE_STATUS_COMPLETE;
    dynamic_add_mutex->release();
    locked = false;
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) release_start_config_lock();
#endif
    LOG_ERROR("error happen in dynamic add server, due to %s.\n", e.what());
    MySQLErrorResponse error(ERROR_DYNAMIC_OPERATION_CODE, e.what());
    error.pack(packet);
    error_packet = packet;
    throw ErrorPacketException();
  }

#ifndef CLOSE_MULTIPLE
  bool is_sync_finish = false;
#endif
  try {
#ifndef CLOSE_MULTIPLE
    bool is_start_sync = false;
    if (multiple_mode) {
      // TODO: start_sync
      MultipleManager *mul = MultipleManager::instance();
      MultipleSyncTool *sync_tool = mul->get_sync_tool();
      char param[1024] = {0};
      sprintf(param, "%d %s %s %s %d %s %s %d %s %s %d %s %s %d %d %d %d",
              TYPE_DYNAMIC_ADD_SERVER, server_name, server_host,
              server_alias_host ? server_alias_host : server_host, server_port,
              server_user, server_password ? server_password : "null",
              is_master_backup, local_load_script ? local_load_script : "null",
              external_load_script ? external_load_script : "null",
              is_external_load, remote_user ? remote_user : "null",
              remote_password ? remote_password : "null", remote_port,
              location_id, mgr_server, dbscale_server);
      try {
        is_start_sync = backend->start_dynamic_operation_topic(
            DYNAMIC_ADD_DATASERVER_TOPIC_NAME, param);
        if (!is_start_sync)
          throw Error("Fail to start_dynamic_operation_topic.");

      } catch (...) {
        LOG_ERROR("Start dynamic add dataserver [%s] topic error.\n",
                  server_name);
#ifndef CLOSE_MULTIPLE
        if (multiple_mode) release_start_config_lock();
#endif
        dynamic_add_mutex->release();
        locked = false;
        throw Error("Start dynamic add dataserver topic error");
      }
      if (is_start_sync) {
#endif
        dynamic_add_data_server();

#ifndef CLOSE_MULTIPLE
        DynamicOperationSyncCondition *cond = NULL;
        try {
          /*first store the modified conf to the zookeeper*/
          Backend::instance()->flush_config_to_zoo(false);

          /*Then fin the sync topic*/
          mul->acquire_dynamic_info_lock();
          unsigned long after_pub_version = mul->get_cur_config_info_version();
          mul->release_dynamic_info_lock();
          cond = new DynamicOperationSyncCondition();
          cond->prepare_condition(
              after_pub_version);  // The fin condition should ensure the slave
                                   // dbscale has get the new config version.

          sync_tool->get_sync_topic()->set_sync_info_op_param(param);
          sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param, cond);
          sync_tool->publish_sync_message();
          sync_tool->wait_all_children_sync();
          delete cond;
          cond = NULL;
          is_sync_finish = true;
#endif
          dynamic_init_data_server();
          dynamic_add_mutex->release();
          locked = false;
#ifndef CLOSE_MULTIPLE
          // TODO: delete the dataserver info from database if error happen in
          // issue #1580
        } catch (Exception &e) {
          LOG_ERROR("error happen in dynamic add server, due to %s.\n",
                    e.what());
          MySQLErrorResponse error(ERROR_DYNAMIC_OPERATION_CODE, e.what());
          error.pack(packet);
          error_packet = packet;
          dynamic_add_mutex->release();
          locked = false;
          if (cond) {
            delete cond;
            cond = NULL;
          }
        }
        if (multiple_mode) release_start_config_lock();
      }
    } else {
      dynamic_add_data_server();
      dynamic_init_data_server();
      dynamic_add_mutex->release();
      locked = false;
      if (multiple_mode) release_start_config_lock();
    }
#endif
  } catch (...) {
    if (locked) {
      dynamic_add_mutex->release();
      locked = false;
    }
    throw;
  }

#ifndef CLOSE_MULTIPLE
  if (multiple_mode && !is_sync_finish) {
    if (data_server) {
      Backend *backend = Backend::instance();
      backend->remove_data_server(server_name);
      data_server = NULL;
    }
    status = EXECUTE_STATUS_COMPLETE;
    throw ErrorPacketException();
  } else {
#endif
    if (!get_by_add_pre_disaster_master()) {
      Packet ok_packet;
      MySQLOKResponse ok(0, 0);
      ok.pack(&ok_packet);
      handler->deal_autocommit_with_ok_eof_packet(&ok_packet);
      handler->send_to_client(&ok_packet);
    }
    status = EXECUTE_STATUS_COMPLETE;
#ifndef CLOSE_MULTIPLE
  }
#endif
}

/* class MySQLDynamicAddSlaveNode */
MySQLDynamicAddSlaveNode::MySQLDynamicAddSlaveNode(
    ExecutePlan *plan, dynamic_add_slave_op_node *dynamic_add_slave_oper)
    : MySQLExecuteNode(plan),
      packet(NULL),
      error_packet(NULL),
      server_info(dynamic_add_slave_oper->slave_info->server_info),
      master_name(dynamic_add_slave_oper->master_name),
      slave_source_name(dynamic_add_slave_oper->slave_info->slave_source),
      is_add_slave_server(
          dynamic_add_slave_oper->slave_info->is_add_slave_server),
      target_ds(NULL),
      data_server(NULL),
      slave_source(NULL),
      ds_type(DATASOURCE_TYPE_REPLICATION) {
  this->name = "MySQLDynamicAddSlaveNode";
  set_by_add_pre_disaster_master(
      dynamic_add_slave_oper->by_add_pre_disaster_master);
}
void MySQLDynamicAddSlaveNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
}
void MySQLDynamicAddSlaveNode::check_config() {
  Backend *backend = Backend::instance();

  DataSource *ds = backend->find_data_source(master_name);
  if (!ds) {
    throw DynamicAddSlaveFail("the datasource is not exist");
  }
  ds_type = ds->get_data_source_type();
  if ((ds_type != DATASOURCE_TYPE_REPLICATION) &&
      (ds_type != DATASOURCE_TYPE_MGR)) {
    throw DynamicAddSlaveFail(
        "the type of datasource is not REPLICATION or MGR");
  }
  target_ds = (RWSplitDataSource *)ds;

  if (is_add_slave_server) {
    // add slave server
    data_server = backend->find_data_server(server_info->server_name);
    if (!data_server) {
      throw DynamicAddSlaveFail(
          "the dataserver is not exist, plz add it before");
    }
    if ((ds_type == DATASOURCE_TYPE_MGR) && !data_server->is_mgr_server()) {
      throw DynamicAddSlaveFail("the dataserver is not MGR server");
    }
    // check conn poll
    if (!(server_info->min <= server_info->low) ||
        !(server_info->low <= server_info->high) ||
        !(server_info->high <= server_info->max)) {
      throw DynamicAddSlaveFail("connection pool set error");
    }

    // the server can not be a slave
    if (data_server->is_a_slave_server()) {
      throw DynamicAddSlaveFail("the server is already a slave");
    }
    int unique_server_datasource = 1;
#ifndef DBSCALE_TEST_DISABLE
    Backend *bk = Backend::instance();
    dbscale_test_info *test_info = bk->get_dbscale_test_info();
    if (!test_info->test_case_name.length() ||
        strcasecmp(test_info->test_case_name.c_str(),
                   "unique_serverdatasource") ||
        strcasecmp(test_info->test_case_operation.c_str(), "on"))
      unique_server_datasource = 0;
#endif
    if (unique_server_datasource) {
      list<DataSource *> server_data_sources;
      backend->find_data_source_by_type(DATASOURCE_TYPE_SERVER,
                                        server_data_sources);
      list<DataSource *>::iterator it_source = server_data_sources.begin();
      for (; it_source != server_data_sources.end(); ++it_source) {
        DataServer *master_server = (*it_source)->get_master_server();
        if (master_server == data_server || master_server->equal(data_server)) {
          throw DynamicOpFail(
              "This DataServer has belong to another Server DataSource.");
        }
      }
    }
  } else {
    // add slave source
    slave_source = backend->find_data_source(slave_source_name);
    if (!slave_source) {
      throw DynamicAddSlaveFail("the slave datasource is not exist");
    }
    if (slave_source->is_parent(target_ds->get_read_source())) {
      throw DynamicAddSlaveFail(
          "the master datasource is already slave source's parent");
    }
    if (backend->source_contain_same_server(target_ds, slave_source)) {
      throw DynamicAddSlaveFail(
          "the master datasource and slave datasource contain same server.");
    }
  }
}
void MySQLDynamicAddSlaveNode::dynamic_add_slave() {
  LOG_INFO("begin to dynamic add a slave to rep datasource[%s].\n",
           master_name);
  if (is_add_slave_server) {
    string full_name;
    if (multiple_mode) {
      full_name = string(target_ds->get_name());
    } else {
      full_name = string(target_ds->get_read_source_name());
    }
    if (full_name.length()) full_name += "_";
    full_name += server_info->server_name;
    ServerDataSource *old_relate_server_source =
        data_server->get_relate_server_datasource();
    slave_source = new ServerDataSource(
        full_name.c_str(), data_server, NULL, NULL, server_info->min,
        server_info->max, server_info->low, server_info->high);
    try {
      target_ds->dynamic_add_slave(slave_source);
    } catch (DynamicAddSlaveFail &e) {
      LOG_ERROR(
          "dynamic add a slave error, maybe the status of datasource is not "
          "working\n");
      slave_source->unregister_event(DATASERVER_CHECK_PING);
      slave_source->unregister_event(DATASERVER_CHECK_SLAVE);
      data_server->remove_relate_datasource(slave_source);
      data_server->set_relate_server_datasource(old_relate_server_source);
      delete slave_source;
      slave_source = NULL;
      throw e;
    }

    Backend::instance()->add_data_source(slave_source);
  } else {
    try {
      if (target_ds ==
          Backend::instance()->get_auth_data_space()->get_data_source()) {
        // if dbscale is set as a server datasource,see issue #1888 for detail
        if (slave_source &&
            slave_source->get_data_source_type() == DATASOURCE_TYPE_SERVER) {
          DataServer *server = slave_source->get_master_server();
          if (server && (!server->is_dbscale_server())) {
            throw DynamicAddSlaveFail("auth datasource can not be master");
          }
        }
      } else if (slave_source == Backend::instance()
                                     ->get_auth_data_space()
                                     ->get_data_source()) {
        throw DynamicAddSlaveFail("auth datasource can not be slave");
      }
      target_ds->dynamic_add_slave(slave_source);
    } catch (DynamicAddSlaveFail &e) {
      LOG_ERROR(
          "dynamic add a slave error, maybe the status of datasource is not "
          "working\n");
      throw e;
    }
    slave_source->set_slave_source_flag(true);
    target_ds->register_init();
    Backend::instance()->add_rep_relation(target_ds, slave_source);
    if (!target_ds->get_write_source()->is_internal()) {
      Backend::instance()->add_rep_relation(target_ds->get_write_source(),
                                            slave_source);
    }
  }
}
void MySQLDynamicAddSlaveNode::rollback() {
  if (is_add_slave_server) {
    if (slave_source) {
      slave_source->unregister_event(DATASERVER_CHECK_PING);
      slave_source->unregister_event(DATASERVER_CHECK_SLAVE);
      slave_source->acquire_write_lock();
      slave_source->remove_self();
      target_ds->get_read_source()->dynamic_remove_slave(slave_source);
      Backend::instance()->remove_data_source(slave_source->get_name());
    }
  } else {
    if (slave_source) {
      slave_source->set_slave_source_flag(false);
      target_ds->get_read_source()->remove_data_source(slave_source);
    }
  }
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) Backend::instance()->flush_config_to_zoo(false);
#endif
}
void MySQLDynamicAddSlaveNode::execute() {
  bool need_rollback = false;
  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *dynamic_add_mutex =
      backend->get_dynamic_modify_rep_mutex();
  dynamic_add_mutex->acquire_write();
  bool locked = true;
  try {
    check_config();
#ifndef DBSCALE_TEST_DISABLE
    Backend *bk = Backend::instance();
    dbscale_test_info *test_info = bk->get_dbscale_test_info();
    if (!strcasecmp(test_info->test_case_name.c_str(),
                    "add_remove_slave_sync") &&
        !strcasecmp(test_info->test_case_operation.c_str(), "before_sync")) {
      throw DynamicAddSlaveFail("DBSCALE_TEST dynamic add slave error.");
    }
#endif
    need_rollback = true;
    dynamic_add_slave();
    if (get_by_add_pre_disaster_master() ||
        (slave_source &&
         slave_source->get_data_source_type() == DATASOURCE_TYPE_SERVER &&
         ((ServerDataSource *)slave_source)
             ->get_server()
             ->is_dbscale_server())) {
      LOG_DEBUG("reset pre_disaster_master_info start\n");
#ifndef CLOSE_MULTIPLE
      if (multiple_mode) {
        acquire_start_config_lock(
            session->get_zk_start_config_lock_extra_info());
      }
#endif
      try {
        OptionParser *config = OptionParser::instance();
        string option_name("pre_disaster_master_info");
        if (all_option_value.count(option_name)) {
          config->dynamic_set_option(DEFAULT_PRE_DISASTER_MASTER_INFO,
                                     all_option_value[option_name],
                                     VAR_SCOPE_GLOBAL);
          LOG_INFO("Dynamic configuration set %s to null.\n",
                   option_name.c_str());
        }
#ifndef CLOSE_MULTIPLE
        if (multiple_mode) {
          map<string, OptionValue> tmp;
          map<string, OptionValue>::iterator it = all_option_value.begin();
          for (; it != all_option_value.end(); it++) {
            if (it->second.dynamic_set) tmp[it->first] = it->second;
          }
          map<string, string> options;
          config->get_option_map(&options, &tmp);
          MultipleManager *mul = MultipleManager::instance();
          mul->pub_ka_dynamic_set_option_info(&options);
        }
#endif
      } catch (exception &e) {
        LOG_ERROR(
            "Invalid dynamic reset pre-disaster-master-info, due to %s.\n",
            e.what());
      }
#ifndef CLOSE_MULTIPLE
      if (multiple_mode) release_start_config_lock();
#endif
    }
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) Backend::instance()->flush_config_to_zoo(false);
#endif
  } catch (exception &e) {
    if (need_rollback) {
      rollback();
    }
    status = EXECUTE_STATUS_COMPLETE;
    dynamic_add_mutex->release();
    locked = false;
    string error_msg = e.what();
    if (get_by_add_pre_disaster_master()) {
      error_msg += ". please manually check and remove the datasource ";
      error_msg += slave_source_name;
      error_msg += " and its dataserver.";
    }
    LOG_ERROR("error happen in dynamic add slave, due to %s.\n",
              error_msg.c_str());
    packet = Backend::instance()->get_new_packet();
    MySQLErrorResponse error(ERROR_DYNAMIC_ADD_SLAVE_CODE, error_msg.c_str());
    error.pack(packet);
    error_packet = packet;
    throw ErrorPacketException();
  }

  bool is_multiple_dbscale = false;
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) is_multiple_dbscale = true;
#endif
  try {
    if (!is_multiple_dbscale) {
      dynamic_add_mutex->release();
      locked = false;
    } else {
      bool is_sync_finish = false;
      bool is_start_sync = false;
      MultipleManager *mul = MultipleManager::instance();
      MultipleSyncTool *sync_tool = mul->get_sync_tool();
      char param[1024] = {0};
      if (is_add_slave_server) {
        sprintf(param, "%d 1 %s %s %u %u %u %u", TYPE_DYNAMIC_ADD_SLAVE,
                master_name, server_info->server_name, server_info->min,
                server_info->max, server_info->low, server_info->high);
      } else {
        sprintf(param, "%d 0 %s %s", TYPE_DYNAMIC_ADD_SLAVE, master_name,
                slave_source_name);
      }

      try {
        is_start_sync = backend->start_dynamic_operation_topic(
            DYNAMIC_ADD_SLAVE_TOPIC_NAME, param);
        if (!is_start_sync)
          throw Error("Fail to start_dynamic_operation_topic.");
      } catch (exception &e) {
        rollback();
        dynamic_add_mutex->release();
        locked = false;
        LOG_ERROR("Start dynamic add slave topic error due to [%s].\n",
                  e.what());
        string err("Start dynamic add datasource topic error ");
        err.append(e.what());
        if (get_by_add_pre_disaster_master()) {
          err.append(". please manually check and remove the datasource ");
          err.append(slave_source_name);
          err.append(" and its dataserver.");
        }
        throw Error(err.c_str());
      }
      DynamicOperationSyncCondition *cond = NULL;
      try {
        /*fin the sync topic*/
        mul->acquire_dynamic_info_lock();
        unsigned long after_pub_version = mul->get_cur_config_info_version();
        mul->release_dynamic_info_lock();
        cond = new DynamicOperationSyncCondition();
        // The fin condition should ensure the slave dbscale has get the new
        // config version.
        cond->prepare_condition(after_pub_version);

        sync_tool->get_sync_topic()->set_sync_info_op_param(param);
        sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param, cond);
        sync_tool->publish_sync_message();
        sync_tool->wait_all_children_sync();
        delete cond;
        cond = NULL;
        is_sync_finish = true;
        dynamic_add_mutex->release();
        locked = false;
      } catch (Exception &e) {
        rollback();
        try {
          sync_tool->cancel_all_children_sync(param, cond);
        } catch (...) {
          LOG_ERROR("Cancel add slave sync error.\n");
        }
        dynamic_add_mutex->release();
        locked = false;
        string error_msg = e.what();
        if (get_by_add_pre_disaster_master()) {
          error_msg += ". please manually check and remove the datasource ";
          error_msg += slave_source_name;
          error_msg += " and its dataserver.";
        }
        LOG_ERROR("error happen in dynamic add slave, due to %s.\n",
                  error_msg.c_str());
        MySQLErrorResponse error(ERROR_DYNAMIC_OPERATION_CODE,
                                 error_msg.c_str());
        packet = Backend::instance()->get_new_packet();
        error.pack(packet);
        error_packet = packet;
        if (cond) {
          delete cond;
          cond = NULL;
        }
      }

      if (!is_sync_finish) {
        status = EXECUTE_STATUS_COMPLETE;
        throw ErrorPacketException();
      }
    }
  } catch (...) {
    if (locked) {
      dynamic_add_mutex->release();
      locked = false;
    }
    throw;
  }

  if (is_add_slave_server) {
    slave_source->register_event(DATASERVER_CHECK_PING);
    if (ds_type == DATASOURCE_TYPE_REPLICATION) {
      slave_source->register_event(DATASERVER_CHECK_SLAVE);
    }
    slave_source->set_belong_to_rep_group(true);
    slave_source->active_pull_message();
  }
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    MultipleManager *mul = MultipleManager::instance();
    mul->acquire_ka_update_generate_lock();
    mul->reset_ka_init_node_message();
    mul->release_ka_update_generate_lock();
  }
#endif

  status = EXECUTE_STATUS_COMPLETE;
  send_ok_packet_to_client(handler, 0, 0);
}
/* class MySQLDynamicChangeMasterNode */
MySQLDynamicChangeMasterNode::MySQLDynamicChangeMasterNode(
    ExecutePlan *plan,
    dynamic_change_master_op_node *dynamic_change_master_oper)
    : MySQLExecuteNode(plan),
      dynamic_change_master_oper(dynamic_change_master_oper) {
  this->name = "MySQLDynamicChangeMasterNode";
  packet = NULL;
  error_packet = NULL;
}
void MySQLDynamicChangeMasterNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
  }
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}
void MySQLDynamicChangeMasterNode::dynamic_change_master() {
  const char *rep_name = dynamic_change_master_oper->rep_name;
  const char *new_master = dynamic_change_master_oper->new_master;
  Backend *backend = Backend::instance();
  DataServer *server = backend->find_data_server(new_master);
  if (!server) {
    throw DynamicChangeMasterFail("the new master is not exist");
  }
  DataSource *rep_ds = backend->find_data_source(rep_name);
  if (!rep_ds) {
    throw DynamicChangeMasterFail("the replication datasource is not exist");
  }
  if (rep_ds->get_data_source_type() != DATASOURCE_TYPE_REPLICATION) {
    throw DynamicChangeMasterFail("the type of datasource is not REPLICATION");
  }
  DataSource *slave_ds = rep_ds->get_slave_by_server(server);

  if (!slave_ds) {
    throw DynamicChangeMasterFail(
        "the server is not a slave of the master server");
  }

  if (!server->is_master_backup()) {
    throw DynamicChangeMasterFail(
        "the server is not master backup, and not allowed to be master");
  }

  ((ReplicationDataSource *)rep_ds)->set_config_master(slave_ds);
  DataSource *read_source =
      ((ReplicationDataSource *)rep_ds)->get_read_source();
  if (!read_source->is_can_dynamic_change_master())
    throw DynamicChangeMasterFail("the read source is not working normal");

  string async_name("DynamicChangeMaster_");
  async_name.append(rep_ds->get_name());
  async_name.append("_MASTER_");
  async_name.append(server->get_name());

  rep_ds->get_async_task_worker()->register_async_task(
      async_name.c_str(), "Start to dynamic change master");

  LOG_DEBUG("Change master async task is registed.\n");

  ((ReplicationDataSource *)rep_ds)->set_config_master(slave_ds);

  unsigned int next_source_message_id = 0;
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    MultipleManager *mul = MultipleManager::instance();
    /* For multiple dbscale mode, notify parent operation should only be done by
     * the cluster master*/
    if (!mul->get_is_cluster_master()) return;
    next_source_message_id = mul->get_next_source_message_id();
  }
#endif
  // when do dynamic change master, should let old master xa_is_recover false
  DataServer *old_master_server = rep_ds->get_master_server();
  old_master_server->set_xa_is_recovered(false);

  DataSourceMessage *message = new DataSourceMessage(
      read_source, rep_ds, DATASOURCE_MESSAGE_SLAVE_STARTED);
  message->set_source_message_id(next_source_message_id);

#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    MultipleManager *mul = MultipleManager::instance();
    if (mul->get_is_cluster_master() &&
        mul->get_manager_state() == STATE_MASTER_UPDATE_INFO) {
      LOG_INFO("Start to collect ka update message.\n");
      mul->acquire_ka_update_generate_lock();
      mul->prepare_to_collect_ka_update_message();

      KASourceNotifyParentMessage *m = new KASourceNotifyParentMessage(
          next_source_message_id, read_source->get_name(), rep_ds->get_name(),
          DATASOURCE_MESSAGE_SLAVE_STARTED);
      MultipleManager *mul = MultipleManager::instance();
      message->set_has_send_notify_message(mul->collect_ka_update_message(m));
      mul->finish_to_collect_ka_update_message(NULL);
      mul->release_ka_update_generate_lock();
    }
  }
  /*Here we should first pub message to zookeeper, then send message. Cause
   * the send message will lead the data source message handler to send
   * KASourceConsumeMessageMessage. The KASourceConsumeMessageMessage message
   * must follow KASourceNotifyParentMessage.*/
#endif
  MessageSender::send(message);
}
void MySQLDynamicChangeMasterNode::execute() {
  LOG_DEBUG("begin to dynamic change master.\n");
  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *dynamic_change_mutex =
      backend->get_dynamic_modify_rep_mutex();
  bool is_lock = true;
  dynamic_change_mutex->acquire_write();
  try {
    try {
      dynamic_change_master();
      dynamic_change_mutex->release();
      is_lock = false;
      send_ok_packet_to_client(handler, 0, 0);
    } catch (DynamicChangeMasterFail &e) {
      status = EXECUTE_STATUS_COMPLETE;
      LOG_ERROR("error happen in dynamic change master, due to %s.\n",
                e.what());
      Packet error_packet;
      MySQLErrorResponse error(e.get_errno(), e.what());
      error.pack(&error_packet);
      handler->send_to_client(&error_packet);
      if (is_lock) {
        dynamic_change_mutex->release();
        is_lock = false;
      }
    }
  } catch (exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    if (is_lock) dynamic_change_mutex->release();
    LOG_ERROR("error happen in dynamic change master, due to %s.\n", e.what());
    throw;
  }
  LOG_DEBUG("end dynamic change master.\n");
  status = EXECUTE_STATUS_COMPLETE;
}

/* class MySQLDynamicChangeMultipleMasterActiveNode */
MySQLDynamicChangeMultipleMasterActiveNode ::
    MySQLDynamicChangeMultipleMasterActiveNode(
        ExecutePlan *plan,
        dynamic_change_multiple_master_active_op_node *dc_mm_active_oper)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLDynamicChangeMultipleMasterActiveNode";
  mm_ds_name = dc_mm_active_oper->mm_ds_name;
  mm_ds_new_active_name = dc_mm_active_oper->new_active_name;
  packet = NULL;
  error_packet = NULL;
}
void MySQLDynamicChangeMultipleMasterActiveNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
  }
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}
void MySQLDynamicChangeMultipleMasterActiveNode::
    dynamic_change_multiple_master_active() {
  Backend *backend = Backend::instance();
  DataSource *mm_ds = backend->find_data_source(mm_ds_name);
  if (!mm_ds) {
    throw DynamicChangeMultipleMasterActiveFail(
        "the multiple master datasource not exist");
  }
  if (mm_ds->get_data_source_type() != DATASOURCE_TYPE_MULTIPLEMASTER) {
    throw DynamicChangeMultipleMasterActiveFail(
        "the type of datasource is not MULTIPLEMASTER");
  }
  DataSource *mm_ds_new_active =
      backend->find_data_source(mm_ds_new_active_name);
  if (!mm_ds_new_active) {
    char msg[100];
    sprintf(msg, "no datasource named [%s]", mm_ds_new_active_name);
    throw DynamicChangeMultipleMasterActiveFail(msg);
  }
  if (mm_ds_new_active ==
      ((MultipleMasterDataSource *)mm_ds)->get_active_source()) {
    char msg[200];
    sprintf(msg,
            "active datasource of the multiple master datasource is already "
            "[%s], no need change",
            mm_ds_new_active_name);
    throw DynamicChangeMultipleMasterActiveFail(msg);
  }
  if (!(((MultipleMasterDataSource *)mm_ds)
            ->get_lb_source()
            ->is_lb_contain_working_source(mm_ds_new_active))) {
    char msg[300];
    sprintf(msg,
            "in multiple master datasource [%s], no datasource named [%s], or "
            "datasource is not working now",
            mm_ds_name, mm_ds_new_active_name);
    throw DynamicChangeMultipleMasterActiveFail(msg);
  }

  MultipleMasterDataSource *mds = (MultipleMasterDataSource *)mm_ds;
  mds->set_dynamic_change_active_candidate(mm_ds_new_active);
  handle_dynamic_result dynamic_change_active_result;
  mds->handle_dynamic_change_active(dynamic_change_active_result);
  if (dynamic_change_active_result == HANDLE_DYNAMIC_RESULT_ERROR) {
    char msg[300];
    sprintf(msg,
            "fail to change active for MultipleMaster datasource [%s], error "
            "occurred when try to change active node to [%s].",
            mm_ds_name, mm_ds_new_active_name);
    LOG_ERROR("%s\n", msg);
    throw DynamicChangeMultipleMasterActiveFail(msg);
  }
}
void MySQLDynamicChangeMultipleMasterActiveNode::execute() {
  LOG_DEBUG(
      "begin to dynamic change the active source of multiple_master datasource "
      "[%s].\n",
      mm_ds_name);
  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *dynamic_change_mutex =
      backend->get_dynamic_modify_rep_mutex();
  bool is_lock = true;
  dynamic_change_mutex->acquire_write();
  try {
    try {
      dynamic_change_multiple_master_active();
      dynamic_change_mutex->release();
      is_lock = false;
      send_ok_packet_to_client(handler, 0, 0);
    } catch (DynamicChangeMultipleMasterActiveFail &e) {
      status = EXECUTE_STATUS_COMPLETE;
      LOG_ERROR(
          "error happen in dynamic change multiple_master's active source due "
          "to [%s].\n",
          e.what());
      Packet error_packet;
      MySQLErrorResponse error(e.get_errno(), e.what());
      error.pack(&error_packet);
      handler->send_to_client(&error_packet);
      if (is_lock) {
        dynamic_change_mutex->release();
        is_lock = false;
      }
    }
  } catch (exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    LOG_ERROR(
        "error happen in dynamic change multiple_master's active source due to "
        "[%s].\n",
        e.what());
    if (is_lock) dynamic_change_mutex->release();
    throw;
  }
  LOG_DEBUG(
      "end dynamic change the active source of multiple_master datasource.\n");
  status = EXECUTE_STATUS_COMPLETE;
}

/* MySQLDynamicChangeDataServerSShNode */
MySQLDynamicChangeDataServerSShNode::MySQLDynamicChangeDataServerSShNode(
    ExecutePlan *plan, const char *server_name, const char *username,
    const char *pwd, int port)
    : MySQLExecuteNode(plan),
      server_name(server_name),
      username(username),
      pwd(pwd),
      port(port) {
  this->name = "MySQLDynamicChangeDataServerSShNode";
  packet = NULL;
  error_packet = NULL;
}

void MySQLDynamicChangeDataServerSShNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
  }
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}

void MySQLDynamicChangeDataServerSShNode::execute() {
  if (port > 65535) {
    LOG_ERROR("SSH port should between 1-65535.\n");
    status = EXECUTE_STATUS_COMPLETE;
    throw Error("SSH port should between 1-65535");
  }

  Backend *bk = Backend::instance();
  DataServer *ds = bk->find_data_server(server_name);
  if (ds == NULL) {
    LOG_ERROR(
        "Cannot find data-server %s for dynamic change dataserver remote ssh "
        "info.\n",
        server_name);
    status = EXECUTE_STATUS_COMPLETE;
    throw Error(
        "Cannot find data-server for dynamic change dataserver remote ssh "
        "info");
  }

  bool need_change = false;
  if (username != NULL) {
    if (ds->get_remote_user() == NULL ||
        strcmp(username, ds->get_remote_user())) {
      need_change = true;
      const char *new_user_name = NULL;
      char *tmp;
      SAVE_STR(new_user_name, username);
      ds->set_remote_user(new_user_name);
    }
  }
  if (pwd != NULL) {
    if (ds->get_remote_password() == NULL ||
        strcmp(pwd, ds->get_remote_password())) {
      need_change = true;
      const char *new_pwd = NULL;
      char *tmp;
      SAVE_STR(new_pwd, pwd);
      ds->set_remote_password(new_pwd);
    }
  }
  if (port != 0 && port != ds->get_remote_port()) {
    need_change = true;
    ds->set_remote_port(port);
  }

  try {
#ifndef CLOSE_MULTIPLE
    bool need_forward = false;
    if (multiple_mode) {
      MultipleManager *mul = MultipleManager::instance();
      bool is_master = mul->get_is_cluster_master();
      // only forward to slave when is master
      need_forward = is_master;
      if (is_master) {
        Backend::instance()->flush_config_to_zoo(false);
      }
    }
    if (need_change && need_forward) {
      // TODO: update slave dbscale
      int failed_count =
          MultipleManager::instance()->modify_ssh_info_for_data_server(
              server_name, username, pwd, port);
      if (failed_count > 0) {
        throw Error(
            "got error when notify all dbscale nodes to modify ssh info, node "
            "fail count: %d\n",
            failed_count);
      }
    }
#else
    ACE_UNUSED_ARG(need_change);
#endif
    send_ok_packet_to_client(handler, 0, 0);
  } catch (exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  }

  status = EXECUTE_STATUS_COMPLETE;
}

/* class MySQLDBScaleShowBackendThreadsNode */
MySQLDBScaleShowBackendThreadsNode::MySQLDBScaleShowBackendThreadsNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowBackendThreadsNode";
}

void MySQLDBScaleShowBackendThreadsNode::init_data() {
  try {
    set_head_packet(3);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";
    list<const char *> columns;
    columns.push_back("Thread_name");
    columns.push_back("Desc");
    columns.push_back("Extra");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
    int i = 0;
    for (i = 0; i < DBSCALE_BACKEND_THREADS_NUMS; i++) {
      list<const char *> row_data;
      row_data.push_back(threads_message[i].thread_name);
      row_data.push_back(threads_message[i].desc);
      row_data.push_back(threads_message[i].extra);
      add_row_packet(row_data);
    }
  } catch (exception &e) {
    LOG_ERROR("got exception when execute dbscale threads.\n");
    throw;
  }
}

/* class MySQLDBScaleShowSchemaNode */
MySQLDBScaleShowSchemaNode::MySQLDBScaleShowSchemaNode(ExecutePlan *plan,
                                                       const char *schema_name)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowSchemaNode";
  this->schema_name = schema_name;
}

void MySQLDBScaleShowSchemaNode::init_data() {
  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *show_mutex = backend->get_dynamic_modify_rep_mutex();
  ACE_Read_Guard<ACE_RW_Thread_Mutex> guard(*show_mutex);
  try {
    set_head_packet(6);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "partition_scheme";
    const char *org_table = "partition_scheme";
    list<const char *> columns;
    columns.push_back("scheme_name");
    columns.push_back("data-source");
    columns.push_back("pushdown_sp");
    columns.push_back("is_alias");
    columns.push_back("alias_schema");
    columns.push_back("table_space_num");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
    Backend *backend = Backend::instance();

    Catalog *ca = backend->get_default_catalog();
    map<string, Schema *, strcasecomp> schema_map;
    if (!schema_name) {
      ca->get_schema_map(schema_map);
    } else {
      Schema *sch = ca->get_schema(schema_name);
      if (sch) schema_map[schema_name] = sch;
    }
    map<string, Schema *, strcasecomp>::iterator it = schema_map.begin();
    for (; it != schema_map.end(); it++) {
      if (backend->is_system_schema(it->first.c_str())) continue;

      list<const char *> row_data;
      row_data.push_back(it->first.c_str());
      row_data.push_back(it->second->get_data_source()->get_name());
      int pushdown_value = it->second->get_schema_pushdown_sp_config_value();
      if (pushdown_value == 0) {
        row_data.push_back("no");
      } else if (pushdown_value == 1) {
        row_data.push_back("always");
      } else {
        row_data.push_back("dependent");
      }
      row_data.push_back(it->second->get_is_alias() ? "1" : "0");
      row_data.push_back(it->second->get_alias_real_name());
      size_t num = it->second->get_table_count();
      char num_str[10];
      snprintf(num_str, sizeof(num_str), "%d", (int)num);
      row_data.push_back(num_str);

      add_row_packet(row_data);
    }
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
    throw ErrorPacketException();
  }
}

/* class MySQLDBScaleShowTableNode */
MySQLDBScaleShowTableNode::MySQLDBScaleShowTableNode(ExecutePlan *plan,
                                                     const char *schema,
                                                     const char *table,
                                                     bool use_like)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowTableNode";
  this->schema_name = schema;
  this->table_name = table;
  this->use_like = use_like;
}

void release_all_schema_mutexs(list<ACE_RW_Thread_Mutex *> &locks) {
  list<ACE_RW_Thread_Mutex *>::iterator it = locks.begin();
  for (; it != locks.end(); it++) (*it)->release();
  locks.clear();
}

void MySQLDBScaleShowTableNode::init_data() {
  string schema_str;
  string table_str;
  Backend *backend = Backend::instance();
  if (lower_case_table_names) {
    if (schema_name) {
      schema_str.assign(schema_name);
      boost::to_lower(schema_str);
      schema_name = schema_str.c_str();
    }
    if (table_name) {
      table_str.assign(table_name);
      boost::to_lower(table_str);
      table_name = table_str.c_str();
    }
  }
  ACE_RW_Thread_Mutex *show_mutex = backend->get_dynamic_modify_rep_mutex();
  ACE_Read_Guard<ACE_RW_Thread_Mutex> guard(*show_mutex);
  list<ACE_RW_Thread_Mutex *> lock_list;
  try {
    set_head_packet(7);
    const char *catalog_name = "def";
    const char *schema = "information_schema";
    const char *table = "partition_scheme";
    const char *org_table = "partition_scheme";
    list<const char *> columns;
    columns.push_back("table_name");
    columns.push_back("schema_name");
    columns.push_back("pattern_name");
    columns.push_back("table_type");
    columns.push_back("data-source");
    columns.push_back("partition-scheme");
    columns.push_back("partitioin-key");
    while (!columns.empty()) {
      add_column_packet(catalog_name, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
    Backend *backend = Backend::instance();
    Catalog *catalog = backend->get_default_catalog();
    map<string, Table *, strcasecomp> table_map;
    if (schema_name == NULL) {
      map<string, Schema *, strcasecomp> schema_map;
      catalog->get_schema_map(schema_map);
      map<string, Schema *, strcasecomp>::iterator it;
      map<string, Table *, strcasecomp>::iterator it_tbl;
      for (it = schema_map.begin(); it != schema_map.end(); it++) {
        if (backend->is_system_schema(it->first.c_str())) continue;

        ACE_RW_Thread_Mutex *m = it->second->get_schema_mutex();
        m->acquire_read();
        lock_list.push_back(m);
        map<string, Table *, strcasecomp> tmp_table_map;
        it->second->add_all_tables_to_map(tmp_table_map, false);
        for (it_tbl = tmp_table_map.begin(); it_tbl != tmp_table_map.end();
             it_tbl++) {
          string tmp_name = it->first + it_tbl->first;
          table_map[tmp_name] = it_tbl->second;
        }
      }
    } else {
      Schema *sch = catalog->get_schema(schema_name);
      if (sch) {
        ACE_RW_Thread_Mutex *m = sch->get_schema_mutex();
        m->acquire_read();
        lock_list.push_back(m);

        if (table_name == NULL) {
          sch->get_tables_map(table_map, false);
        } else if (use_like) {
          map<string, Table *, strcasecomp> tmp_table_map;
          sch->get_tables_map(tmp_table_map, false);
          string option_name(table_name);
          boost::to_lower(option_name);
          size_t pos = option_name.find("%");
          while (pos != string::npos) {
            option_name.replace(pos, 1, ".*");
            pos += 2;
            pos = option_name.find("%", pos);
          }
          boost::regex pattern(option_name.c_str());
          map<string, Table *, strcasecomp>::iterator it;
          for (it = tmp_table_map.begin(); it != tmp_table_map.end(); it++) {
            if (regex_match(it->first.c_str(), pattern)) {
              table_map[it->first] = it->second;
            }
          }

        } else {
          Table *table = sch->get_table(table_name, false);
          if (table) {
            table_map[table_name] = table;
          }
        }
      }
    }

    map<string, Table *, strcasecomp>::iterator it;
    for (it = table_map.begin(); it != table_map.end(); it++) {
      Table *table = it->second;
      list<const char *> row_data;
      row_data.push_back(table->get_name());
      row_data.push_back(table->get_schema()->get_name());
      row_data.push_back(
          table->get_name_pattern() == NULL ? "" : table->get_name_pattern());
      if (table->is_partitioned()) {
        row_data.push_back("partition table");
        row_data.push_back("");
        PartitionedTable *part_table = (PartitionedTable *)table;
        row_data.push_back(part_table->get_partition_scheme()->get_name());
        row_data.push_back(part_table->get_key_names()->at(0));
      } else {
        row_data.push_back("normal table");
        row_data.push_back(table->get_data_source()->get_name());
        row_data.push_back("");
        row_data.push_back("");
      }
      add_row_packet(row_data);
    }
    release_all_schema_mutexs(lock_list);
  } catch (exception &e) {
    release_all_schema_mutexs(lock_list);
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
    throw ErrorPacketException();
  }
}

/* class MySQLDBScaleShowPartitionSchemeNode */
MySQLDBScaleShowPartitionSchemeNode::MySQLDBScaleShowPartitionSchemeNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowPartitionSchemeNode";
}

void MySQLDBScaleShowPartitionSchemeNode::init_data() {
  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *show_mutex = backend->get_dynamic_modify_rep_mutex();
  ACE_Read_Guard<ACE_RW_Thread_Mutex> guard(*show_mutex);
  try {
    set_head_packet(6);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "partition_scheme";
    const char *org_table = "partition_scheme";
    list<const char *> columns;
    columns.push_back("scheme_name");
    columns.push_back("type");
    columns.push_back("comment");
    columns.push_back("is_shard");
    columns.push_back("partitions");
    columns.push_back("relate_tables");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
    Backend *backend = Backend::instance();
    map<string, PartitionScheme *> scheme;
    backend->get_all_partition_scheme(scheme);
    map<string, PartitionScheme *>::iterator it = scheme.begin();
    for (; it != scheme.end(); it++) {
      list<const char *> row_data;
      row_data.push_back(it->first.c_str());
      PartitionType ptype = it->second->get_type();
      switch (ptype) {
        case PARTITION_TYPE_HASH:
          row_data.push_back("HASH");
          break;
        case PARTITION_TYPE_RANGE:
          row_data.push_back("RANGE");
          break;
        case PARTITION_TYPE_LIST:
          row_data.push_back("LIST");
          break;
        case PARTITION_TYPE_MOD:
          row_data.push_back("MOD");
          break;
      }
      const string &comment_str = it->second->get_comment();
      row_data.push_back(comment_str.c_str());
      if (it->second->is_shard())
        row_data.push_back("1");
      else
        row_data.push_back("0");
      string partitions_str = it->second->get_partitions_str();
      row_data.push_back(partitions_str.c_str());
      string relate_tables_str = it->second->get_relate_table_str();
      row_data.push_back(relate_tables_str.c_str());
      add_row_packet(row_data);
    }
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
    throw ErrorPacketException();
  }
}
/* class MySQLDBScaleShowDataServerNode */
MySQLDBScaleShowDataServerNode::MySQLDBScaleShowDataServerNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowDataServerNode";
}

void MySQLDBScaleShowDataServerNode::init_data() {
  Backend *backend = Backend::instance();
  bool count_server_dataflow_local = count_server_dataflow;
  ACE_RW_Thread_Mutex *show_mutex = backend->get_dynamic_modify_rep_mutex();
  show_mutex->acquire_read();
  bool locked = true;
  bool auto_flashback = auto_master_failover_flashback;
  bool has_alias_host = backend->get_has_dataserver_with_alias_host();
  bool has_server_with_location_id = backend->get_has_server_with_location_id();
  try {
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "dataservers";
    const char *org_table = "dataservers";
    list<const char *> columns;
    columns.push_back("servername");
    columns.push_back("host");
    if (has_alias_host) {
      columns.push_back("alias_host");
    }
    columns.push_back("port");
    columns.push_back("username");
    columns.push_back("status");
    if (auto_flashback) {
      columns.push_back("master-online-status");
    }
    columns.push_back("master_backup");
    if (backend->is_cluster_has_external_load_server()) {
      columns.push_back("external_load");
      columns.push_back("local_script");
      columns.push_back("external_script");
    }
    columns.push_back("remote_user");
    columns.push_back("remote_port");
    columns.push_back("max_needed_conn/max_mysql_conn");
    if (count_server_dataflow_local) {
      columns.push_back("net_in(Byte)");
      columns.push_back("net_out(Byte)");
    }
    if (has_server_with_location_id) {
      columns.push_back("location_id");
    }
    columns.push_back("master_priority");
    set_head_packet(columns.size());
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }

#ifndef CLOSE_MULTIPLE
    bool is_master = true;
    if (multiple_mode) {
      MultipleManager *mul = MultipleManager::instance();
      is_master = mul->get_is_cluster_master();
    }
    if (!is_master &&
        plan->statement->get_stmt_node()->forward_to_master != 2) {
      show_mutex->release();
      locked = false;
      backend->forward_query_to_master_role_node(
          plan->statement->get_sql(), session->get_schema(), row_list);
      return;
    }
#endif

    map<string, DataServer *> data_servers;
    backend->get_data_servers(data_servers);
    map<string, DataServer *>::iterator it = data_servers.begin();
    for (; it != data_servers.end(); it++) {
      list<const char *> row_data;
      row_data.push_back(it->second->get_name());
      row_data.push_back(it->second->get_host_ip());
      if (has_alias_host) {
        row_data.push_back(it->second->get_alias_host_ip());
      }
      snprintf(port, sizeof(port), "%d", it->second->get_port());
      row_data.push_back(port);
      if (it->second->get_user())
        row_data.push_back(it->second->get_user());
      else
        row_data.push_back("");

      int status = it->second->get_show_status();
      string status_str(it->second->get_server_status(status));
      string seconds = it->second->get_seconds_behind_master_str(status);
      if (seconds.length()) status_str.append(seconds.c_str());
      row_data.push_back(status_str.c_str());
      if (auto_flashback) {
        row_data.push_back(it->second->get_online_status_str());
      }
      if (it->second->is_master_backup())
        row_data.push_back("1");
      else
        row_data.push_back("0");
      if (backend->is_cluster_has_external_load_server()) {
        if (it->second->get_is_external_load())
          row_data.push_back("1");
        else
          row_data.push_back("0");
        if (it->second->get_local_load_script())
          row_data.push_back(it->second->get_local_load_script());
        else
          row_data.push_back("");
        if (it->second->get_external_load_script())
          row_data.push_back(it->second->get_external_load_script());
        else
          row_data.push_back("");
      }
      if (it->second->get_remote_user())
        row_data.push_back(it->second->get_remote_user());
      else
        row_data.push_back("");

      if (it->second->get_remote_port() == 0) {
        row_data.push_back("");
      } else {
        snprintf(remote_port, sizeof(remote_port), "%d",
                 it->second->get_remote_port());
        row_data.push_back(remote_port);
      }
      if (it->second->get_max_need_num()) {
        stringstream ss;
        ss << it->second->get_max_need_num() << "/"
           << it->second->get_max_mysql_num();
        server_connection = ss.str();
        row_data.push_back(server_connection.c_str());
      } else {
        row_data.push_back("");
      }

      string bytes_received;
      string bytes_sent;
      if (count_server_dataflow_local) {
        map<DataSource *, int> data_source_map;
        it->second->get_data_source_map(data_source_map);
        map<DataSource *, int>::iterator it2 = data_source_map.begin();
        DataSource *data_source = it2->first;
        if (!data_source ||
            data_source->get_data_source_type() == DATASOURCE_TYPE_ODBC) {
          row_data.push_back("NULL");
          row_data.push_back("NULL");
        } else {
          ServerDataSource *server_data_source =
              static_cast<ServerDataSource *>(data_source);
          Connection *conn = server_data_source->get_connection(NULL, true);
          if (!conn) {
            row_data.push_back("NULL");
            row_data.push_back("NULL");
          } else {
            try {
              conn->query_for_one_value(
                  "show global status like 'Bytes_received'", bytes_received,
                  1);
              conn->query_for_one_value("show global status like 'Bytes_sent'",
                                        bytes_sent, 1);
              conn->get_pool()->add_back_to_free(conn);
              row_data.push_back(bytes_received.c_str());
              row_data.push_back(bytes_sent.c_str());
            } catch (...) {
              if (conn) {
                conn->get_pool()->add_back_to_dead(conn);
              }
              row_data.push_back("NULL");
              row_data.push_back("NULL");
            }
          }
        }
      }
      if (has_server_with_location_id) {
        snprintf(location_id, sizeof(location_id), "%d",
                 it->second->get_location_id());
        row_data.push_back(location_id);
      }
      snprintf(master_priority, sizeof(master_priority), "%d",
               it->second->get_master_priority());
      row_data.push_back(master_priority);

      add_row_packet(row_data);
    }
    show_mutex->release();
    locked = false;
  } catch (exception &e) {
    if (locked) show_mutex->release();
    string err("Fail for show dataservers due to '");
    err += e.what();
    err += "'";
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, err.c_str(), "42S02");
    throw ErrorPacketException();
  }
}

/*class MySQLDBScaleEraseAuthInfoNode*/
MySQLDBScaleEraseAuthInfoNode::MySQLDBScaleEraseAuthInfoNode(
    ExecutePlan *plan, name_item *username_list, bool all_dbscale_node)
    : MySQLReturnOKNode(plan),
      username_list(username_list),
      all_dbscale_node(all_dbscale_node) {
  this->name = "MySQLDBScaleEraseAuthInfoNode";
}
void MySQLDBScaleEraseAuthInfoNode::do_execute() {
  if (!all_dbscale_node) {
    Backend::instance()->remove_auth_info_for_user(username_list);
  } else {
    vector<const char *> username_vec;
    name_item *username = username_list;
    do {
      username_vec.push_back(username->name);
      username = username->next;
    } while (username != username_list);
    Backend::instance()->notify_all_dbscale_remove_auth_info_for_users(
        &username_vec);
  }
  Backend::instance()->set_dbscale_to_dbscale_conn_new(true);
}

MySQLDBScaleBackendServerExecuteNode::MySQLDBScaleBackendServerExecuteNode(
    ExecutePlan *plan, const char *stmt_sql)
    : MySQLReturnOKNode(plan), stmt_sql(stmt_sql) {
  packet = NULL;
  this->name = "MySQLDBScaleBackendServerExecuteNode";
}

void MySQLDBScaleBackendServerExecuteNode::clean() {
  if (packet) {
    delete packet;
  }
}

void MySQLDBScaleBackendServerExecuteNode::do_execute() {
  Backend *backend = Backend::instance();
  string err_msg;
  map<string, DataServer *> data_servers;
  Parser *parser = MySQLParser::instance();
  MySQLStatement *stmt = NULL;
  stmt = static_cast<MySQLStatement *>(
      parser->parse(stmt_sql, plan->statement->get_allow_dot_in_ident(), true,
                    session->get_mem_alloc(), stmt, NULL,
                    session->get_client_charset_type()));
  stmt_type type = stmt->get_stmt_node()->type;
  if (type == STMT_SET || type == STMT_GRANT || type == STMT_SET_PASSWORD ||
      type == STMT_FLUSH_PRIVILEGES) {
    backend->get_data_servers(data_servers);
    backend->get_data_server_mutex();
    map<string, DataServer *>::iterator it = data_servers.begin();
    for (; it != data_servers.end(); it++) {
      try {
        int ret = it->second->do_exec(stmt_sql);
        if (ret) throw Error("fail to get conn for backend execute.");
      } catch (dbscale::sql::SQLError &e) {
        LOG_ERROR("Got error packet when backend server execute sql[%s]\n",
                  stmt_sql);
        err_msg.append("server ");
        err_msg += it->second->get_name();
        err_msg += " fail due to ";
        err_msg.append(e.what());
        err_msg += ";";
      } catch (exception &e) {
        LOG_ERROR("get exception during execute sql due to [%s]\n", e.what());
        err_msg.append("server ");
        err_msg += it->second->get_name();
        err_msg += " fail due to ";
        err_msg.append(e.what());
        err_msg += ";";
      }
    }
    backend->release_data_server_mutex();
  } else {
    stmt->free_resource();
    delete stmt;
    stmt = NULL;
    err_msg.append("Please use SET or GRANT statement!\n");
    throw Error(err_msg.c_str());
  }
  stmt->free_resource();
  delete stmt;
  stmt = NULL;
  if (!err_msg.empty()) {
    string err("get exception during execute sql fail: ");
    err.append(err_msg.c_str(),
               err_msg.length() > 200 ? 200 : err_msg.length());
    throw Error(err.c_str());
  }
}

MySQLDBScaleExecuteOnAllMasterserverExecuteNode::
    MySQLDBScaleExecuteOnAllMasterserverExecuteNode(ExecutePlan *plan,
                                                    const char *stmt_sql)
    : MySQLReturnOKNode(plan), stmt_sql(stmt_sql) {
  packet = NULL;
  this->name = "MySQLDBScaleExecuteOnAllMasterserverExecuteNode";
}

void MySQLDBScaleExecuteOnAllMasterserverExecuteNode::clean() {
  if (packet) {
    delete packet;
  }
}

void MySQLDBScaleExecuteOnAllMasterserverExecuteNode::
    adjust_data_source_by_type(map<string, DataSource *> &src_ds,
                               list<DataSource *> &dst_ds) {
  map<string, DataSource *>::iterator src_iter;
  for (src_iter = src_ds.begin(); src_iter != src_ds.end(); ++src_iter) {
    if (src_iter->second->get_data_source_type() == DATASOURCE_TYPE_SERVER ||
        src_iter->second->get_data_source_type() == DATASOURCE_TYPE_READ_ONLY ||
        src_iter->second->get_data_source_type() == DATASOURCE_TYPE_ODBC) {
      dst_ds.push_back(src_iter->second);
    } else if (src_iter->second->get_data_source_type() !=
               DATASOURCE_TYPE_REPLICATION) {
      dst_ds.push_front(src_iter->second);
    }
  }

  list<DataSource *> top_rep_data_source;
  ConfigSourceRepResource dataservers_rep_relation;
  Backend::instance()->get_dataservers_rep_relation(dataservers_rep_relation);
  for (src_iter = src_ds.begin(); src_iter != src_ds.end(); ++src_iter) {
    if (src_iter->second->get_data_source_type() ==
        DATASOURCE_TYPE_REPLICATION) {
      DataSource *ds =
          dataservers_rep_relation.get_real_top_source(src_iter->second);
      if (ds == src_iter->second) {
        // top data source must be in the front
        top_rep_data_source.push_back(src_iter->second);
      } else {
        // non-top data source,follow top data source..
        dst_ds.push_front(src_iter->second);
      }
    }
  }

  // push top data source front
  list<DataSource *>::iterator top_ds_iter;
  for (top_ds_iter = top_rep_data_source.begin();
       top_ds_iter != top_rep_data_source.end(); ++top_ds_iter) {
    dst_ds.push_front(*top_ds_iter);
  }
}

void MySQLDBScaleExecuteOnAllMasterserverExecuteNode::do_execute() {
  Backend *backend = Backend::instance();
  string err_msg;
  map<string, DataSource *> data_sources;
  set<DataSource *> already_alter_ds_set;
  set<DataSource *>::iterator ds_iter;
  list<DataSource *> data_source_list;
  data_source_list.clear();
  bool exist_flag = false;

  Parser *parser = MySQLParser::instance();
  MySQLStatement *stmt = NULL;
  stmt = static_cast<MySQLStatement *>(
      parser->parse(stmt_sql, false, true, session->get_mem_alloc(), stmt, NULL,
                    session->get_client_charset_type()));
  stmt_type type = stmt->get_stmt_node()->type;

  if (type == STMT_ALTER_ROTATE_KEY) {
    backend->get_data_sources(data_sources);
    adjust_data_source_by_type(data_sources, data_source_list);
    list<DataSource *>::iterator it = data_source_list.begin();

    backend->get_data_server_mutex();
    for (; it != data_source_list.end(); ++it) {
      try {
        DataServer *ds = (*it)->get_master_server();
        if (ds) {
          for (ds_iter = already_alter_ds_set.begin();
               ds_iter != already_alter_ds_set.end(); ++ds_iter) {
            if ((*ds_iter)->contain_data_server(ds)) {
              exist_flag = true;
              break;
            }
          }
          if (!exist_flag) {
            already_alter_ds_set.insert(*it);
            if (ds->do_exec(stmt_sql))
              throw Error("fail to get conn for backend execute.");
          } else {
            exist_flag = false;
          }
        }
      } catch (dbscale::sql::SQLError &e) {
        LOG_ERROR("Got error packet when backend server execute sql[%s]\n",
                  stmt_sql);
        err_msg.append("server ");
        err_msg += (*it)->get_name();
        err_msg += " fail due to ";
        err_msg.append(e.what());
        err_msg += ";";
      } catch (exception &e) {
        LOG_ERROR("get exception during execute sql due to [%s]\n", e.what());
        err_msg.append("server ");
        err_msg += (*it)->get_name();
        err_msg += " fail due to ";
        err_msg.append(e.what());
        err_msg += ";";
      }
    }
    backend->release_data_server_mutex();
  } else {
    stmt->free_resource();
    delete stmt;
    stmt = NULL;
    err_msg.append(
        "Please use statement \"alter instance rotate innodb master key\"!\n");
    throw Error(err_msg.c_str());
  }
  stmt->free_resource();
  delete stmt;
  stmt = NULL;
  if (!err_msg.empty()) {
    string err("get exception during execute sql fail: ");
    err.append(err_msg.c_str(), err_msg.length());
    throw Error(err.c_str());
  }
}

/* class MySQLDynamicAddDataSourceNode */
MySQLDynamicAddDataSourceNode::MySQLDynamicAddDataSourceNode(
    ExecutePlan *plan,
    dynamic_add_data_source_op_node *dynamic_add_data_source_oper,
    DataSourceType type)
    : MySQLExecuteNode(plan),
      packet(NULL),
      error_packet(NULL),
      server_list(dynamic_add_data_source_oper->server_list),
      server_list_size(dynamic_add_data_source_oper->server_list_size),
      group_id_c(dynamic_add_data_source_oper->group_id_c),
      group_id(dynamic_add_data_source_oper->group_id),
      type(type),
      source_name(dynamic_add_data_source_oper->source_name),
      semi_sync(dynamic_add_data_source_oper->semi_sync) {
  this->name = "MySQLDynamicAddDataSourceNode";
  lb_strategy = LOAD_BALANCE_STRATEGY_SLAVES;
  if (type == DATASOURCE_TYPE_REPLICATION || type == DATASOURCE_TYPE_MGR) {
    this->lb_strategy =
        (LoadBalanceStrategy)(dynamic_add_data_source_oper->lb_strategy);
  }
  set_by_add_pre_disaster_master(
      dynamic_add_data_source_oper->by_add_pre_disaster_master);
}
void MySQLDynamicAddDataSourceNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
}
bool MySQLDynamicAddDataSourceNode::check_group_id_char() {
  if (strlen(group_id_c) > 5) return false;
  return true;
}
void MySQLDynamicAddDataSourceNode::check_config() {
  Backend *backend = Backend::instance();
  if (backend->find_data_source(source_name)) {
    throw DynamicOpFail("the source name is already exist plz change it");
  }
  dynamic_add_data_source_server_info *it = server_list;
  for (; it != NULL; it = it->next) {
    if (!backend->find_data_server(it->server_name)) {
      string str = "the dataserver ";
      str += it->server_name;
      str += " is not exist plz add it before";
      throw DynamicOpFail(str.c_str());
    }
    // check conn pool
    if (!(it->min <= it->low) || !(it->low <= it->high) ||
        !(it->high <= it->max)) {
      string str = it->server_name;
      str += "connection pool set error";
      throw DynamicOpFail(str.c_str());
    }
  }
  try {
    if (Backend::instance()->is_centralized_cluster(false) &&
        type != DATASOURCE_TYPE_SERVER) {
      Backend::instance()->check_need_keep_centralized_without_auth();
    }
  } catch (exception &e) {
    string error_msg("cannot dynamic add new datasource due to ");
    error_msg += e.what();
    throw DynamicOpFail(error_msg.c_str());
  }
  if (!get_by_add_pre_disaster_master() &&
      (enable_xa_transaction || enable_event)) {
    if (group_id == 0)
      throw DynamicOpFail(
          "when enable_xa_transaction or enable_event, dynamic add datasource "
          "should set group_id");
  }
  if (group_id == 0) {
    int global_group_id = backend->get_global_group_id();
    global_group_id++;
    group_id = global_group_id;
  }
  if (!check_group_id_char()) {
    LOG_ERROR(
        "group_id value[%s] is out of range, should be between 0 and 65535\n",
        group_id_c);
    throw DynamicOpFail(
        "group_id value is out of range, should be between 0 and 65535");
  }
  OptionParser *config = OptionParser::instance();
  int ret = config->check_group_id(group_id);
  if (ret == 1) {
    throw DynamicOpFail(
        "group_id value is out of range, should be between 0 and 65535");
  } else if (ret == 2) {
    throw DynamicOpFail(
        "the group_id is same with other group_id, plz use another one");
  }
  backend->set_global_group_id(group_id);

  if (type == DATASOURCE_TYPE_REPLICATION) {
    if (server_list_size < 2)
      throw DynamicOpFail("the replication datasource must have slave server");
  } else if (type == DATASOURCE_TYPE_RWSPLIT) {
    if (server_list_size < 2)
      throw DynamicOpFail(
          "the rwsplit datasource must have two or more servers");
  } else if (type == DATASOURCE_TYPE_MGR) {
    if (server_list_size < 2)
      throw DynamicOpFail("the mgr datasource must have two or more servers");
  }

  int unique_server_datasource = 1;
#ifndef DBSCALE_TEST_DISABLE
  Backend *bk = Backend::instance();
  dbscale_test_info *test_info = bk->get_dbscale_test_info();
  if (!test_info->test_case_name.length() ||
      strcasecmp(test_info->test_case_name.c_str(),
                 "unique_serverdatasource") ||
      strcasecmp(test_info->test_case_operation.c_str(), "on"))
    unique_server_datasource = 0;
#endif
  if (type == DATASOURCE_TYPE_SERVER && unique_server_datasource) {
    list<DataSource *> server_data_sources;
    backend->find_data_source_by_type(DATASOURCE_TYPE_SERVER,
                                      server_data_sources);
    list<DataSource *>::iterator it_source = server_data_sources.begin();
    DataServer *svr = backend->find_data_server(server_list->server_name);

    for (; it_source != server_data_sources.end(); ++it_source) {
      DataServer *server = (*it_source)->get_master_server();
      if (server == svr || server->equal(svr)) {
        throw DynamicOpFail(
            "This DataServer has belong to another Server DataSource.");
      }
    }
  }
}

void MySQLDynamicAddDataSourceNode::execute() {
  LOG_DEBUG("MySQLDynamicAddDataSourceNode::execute.\n");
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    acquire_start_config_lock(session->get_zk_start_config_lock_extra_info());
  }
#endif

  Backend *backend = Backend::instance();
  packet = Backend::instance()->get_new_packet();
  ACE_RW_Thread_Mutex *dynamic_add_mutex =
      backend->get_dynamic_modify_rep_mutex();
  dynamic_add_mutex->acquire_write();
  try {
    check_config();
  } catch (exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    string error_msg = e.what();
    if (get_by_add_pre_disaster_master()) {
      error_msg += ". please manually check and remove the dataserver ";
      error_msg += server_list->server_name;
    }
    LOG_ERROR("error happen in dynamic add datasource due to %s.\n",
              error_msg.c_str());
    MySQLErrorResponse error(ERROR_DYNAMIC_OPERATION_CODE, error_msg.c_str());
    error.pack(packet);
    error_packet = packet;
    dynamic_add_mutex->release();
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) release_start_config_lock();
#endif

    throw ErrorPacketException();
  }

  DataSource *source = NULL;
#ifndef CLOSE_MULTIPLE
  bool is_sync_finish = false;
  bool is_start_sync = false;
  bool locked = true;
  try {
    if (multiple_mode) {
      // TODO: start_sync
      MultipleManager *mul = MultipleManager::instance();
      MultipleSyncTool *sync_tool = mul->get_sync_tool();
      char param[1024] = {0};
      int len = sprintf(param, "%d %s %d %d %d %u %d", TYPE_DYNAMIC_ADD_SOURCE,
                        source_name, type, group_id, lb_strategy,
                        server_list_size, semi_sync);
      dynamic_add_data_source_server_info *server = server_list;
      char *tmp = param;
      while (server) {
        tmp += len;
        len = sprintf(tmp, " %s %d %d %d %d", server->server_name, server->min,
                      server->max, server->low, server->high);
        server = server->next;
      }

      try {
        is_start_sync = backend->start_dynamic_operation_topic(
            DYNAMIC_ADD_DATASOURCE_TOPIC_NAME, param);
        if (!is_start_sync)
          throw Error("Fail to start_dynamic_operation_topic.");

      } catch (...) {
        LOG_ERROR("Start dynamic add datasource [%s] topic error.\n",
                  source_name);
        if (multiple_mode) release_start_config_lock();
        string error_msg = "Start dynamic add datasource topic error";
        if (get_by_add_pre_disaster_master()) {
          error_msg += ". please manually check and remove the dataserver ";
          error_msg += server_list->server_name;
        }
        throw Error(error_msg.c_str());
      }
      if (is_start_sync) {
#endif
        source = add_datasource(source_name, type, group_id, server_list,
                                lb_strategy, semi_sync);
        backend->add_config_file_data_source(source);

#ifndef CLOSE_MULTIPLE
        DynamicOperationSyncCondition *cond = NULL;
        try {
          /*first store the modified conf to the zookeeper*/
          Backend::instance()->flush_config_to_zoo(false);

          /*Then fin the sync topic*/
          mul->acquire_dynamic_info_lock();
          unsigned long after_pub_version = mul->get_cur_config_info_version();
          mul->release_dynamic_info_lock();
          cond = new DynamicOperationSyncCondition();
          cond->prepare_condition(
              after_pub_version);  // The fin condition should ensure the slave
                                   // dbscale has get the new config version.

          sync_tool->get_sync_topic()->set_sync_info_op_param(param);
          sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param, cond);
          sync_tool->publish_sync_message();
          sync_tool->wait_all_children_sync();
          delete cond;
          cond = NULL;
          is_sync_finish = true;
#endif
          dynamic_add_mutex->release();
#ifndef CLOSE_MULTIPLE
          locked = false;

        } catch (Exception &e) {
          string error_msg = e.what();
          if (get_by_add_pre_disaster_master()) {
            error_msg += ". please manually check and remove the dataserver ";
            error_msg += server_list->server_name;
          }
          LOG_ERROR("error happen in dynamic add source, due to %s.\n",
                    error_msg.c_str());
          MySQLErrorResponse error(ERROR_DYNAMIC_OPERATION_CODE,
                                   error_msg.c_str());
          error.pack(packet);
          error_packet = packet;
          dynamic_add_mutex->release();
          locked = false;
          if (cond) {
            delete cond;
            cond = NULL;
          }
        }
        if (multiple_mode) release_start_config_lock();
      }
    } else {
      source = add_datasource(source_name, type, group_id, server_list,
                              lb_strategy, semi_sync);
      backend->add_config_file_data_source(source);
      dynamic_add_mutex->release();
      locked = false;
      if (multiple_mode) release_start_config_lock();
    }
  } catch (...) {
    if (locked) dynamic_add_mutex->release();
    throw;
  }

  if (multiple_mode && !is_sync_finish) {
    if (source) {
      string err_msg;
      source->remove_self();
      Backend *backend = Backend::instance();
      backend->remove_data_source(source_name);
      backend->remove_config_file_datasource(source);
      source = NULL;
    }
    status = EXECUTE_STATUS_COMPLETE;
    throw ErrorPacketException();
  } else {
#endif
    if (!get_by_add_pre_disaster_master()) {
      Packet ok_packet;
      MySQLOKResponse ok(0, 0);
      ok.pack(&ok_packet);
      handler->deal_autocommit_with_ok_eof_packet(&ok_packet);
      handler->send_to_client(&ok_packet);
    }
    status = EXECUTE_STATUS_COMPLETE;
#ifndef CLOSE_MULTIPLE
  }
#endif
}

/* class MySQLDynamicAddDataSpaceNode */
MySQLDynamicAddDataSpaceNode::MySQLDynamicAddDataSpaceNode(
    ExecutePlan *plan,
    dynamic_add_data_space_op_node *dynamic_add_data_space_oper)
    : MySQLExecuteNode(plan),
      packet(NULL),
      error_packet(NULL),
      data_space_info(dynamic_add_data_space_oper) {
  this->name = "MySQLDynamicAddDataSpaceNode";
  if (data_space_info->type == PARTITION_TABLE_DATASPACE) {
    char *tmp;
    SAVE_STR(partition_key_name, data_space_info->partition_key_name);
    SAVE_STR(name_pattern, data_space_info->name_pattern);
  } else if (data_space_info->type == HASH_PARTITION_SCHEME_DATASPACE ||
             data_space_info->type == MOD_PARTITION_SCHEME_DATASPACE) {
    char *tmp;
    SAVE_STR(partition_scheme_name, data_space_info->partition_scheme_name);
  } else if (data_space_info->type == NORMAL_TABLE_DATASPACE) {
    char *tmp;
    SAVE_STR(name_pattern, data_space_info->name_pattern);
  }
  is_force = false;
}
void MySQLDynamicAddDataSpaceNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
}

void MySQLDynamicAddDataSpaceNode::check_config() {
  Backend *backend = Backend::instance();
  add_dataspace_type type = data_space_info->type;
  switch (type) {
    case SCHEMA_DATASPACE:
    case NORMAL_TABLE_DATASPACE: {
      if (!backend->find_data_source(data_space_info->data_source_name)) {
        LOG_ERROR("dynamicadddatasource the datasource is not exist\n");
        throw DynamicOpFail("the datasource is not exist");
      }
    } break;
    case MOD_PARTITION_SCHEME_DATASPACE:
    case HASH_PARTITION_SCHEME_DATASPACE: {
      partition_name_list *it = data_space_info->partition_names;
      if (it == NULL)
        throw DynamicOpFail("the partition list size at least be 1");
      for (; it != NULL; it = it->next) {
        DataSource *source = backend->find_data_source(it->name);
        if (!source) throw DynamicOpFail("the datasource is not exist");
        if (!source->get_is_conf_source())
          throw DynamicOpFail("the datasource should be config source");
      }
      if (backend->find_partition_scheme(
              data_space_info->partition_scheme_name) != NULL) {
        throw DynamicOpFail("the partition scheme is exist");
      }
    } break;
    case PARTITION_TABLE_DATASPACE: {
      if (backend->find_partition_scheme(
              data_space_info->partition_scheme_name) == NULL) {
        throw DynamicOpFail("the partition scheme is not exist");
      }
    } break;
  }
}

void MySQLDynamicAddDataSpaceNode::add_normal_table_dataspace() {
  Backend *backend = Backend::instance();
  check_add_table_dataspace();
  Schema *schema = backend->find_schema(schema_name.c_str());
  DataSource *source =
      backend->find_data_source(data_space_info->data_source_name);
  Table *tab = NULL;
  if (!source) throw DynamicOpFail("the datasource is not exist");

  if (!source->get_is_conf_source())
    throw DynamicOpFail("the datasource should be config source");

#ifndef CLOSE_MULTIPLE
  bool is_sync_finish = false;
  bool is_start_sync = false;
  if (multiple_mode) {
    // TODO: start_sync
    MultipleManager *mul = MultipleManager::instance();
    MultipleSyncTool *sync_tool = mul->get_sync_tool();
    char param[1024] = {0};
    sprintf(param, "%d %s %s %s %s %d", TYPE_DYNAMIC_ADD_NORM_TABLE,
            table_name.c_str(), schema_name.c_str(),
            data_space_info->data_source_name,
            name_pattern ? name_pattern : "null", is_force > 0 ? 1 : 0);

    try {
      is_start_sync = backend->start_dynamic_operation_topic(
          DYNAMIC_ADD_NORMTABLE_TOPIC_NAME, param);
      if (!is_start_sync) throw Error("Fail to start_dynamic_operation_topic.");

    } catch (...) {
      LOG_ERROR("Start dynamic add normtable [%s.%s] topic error.\n",
                schema_name.c_str(), table_name.c_str());
      throw Error("Start dynamic add normtable topic error");
    }
    if (is_start_sync) {
#endif

      tab = new Table(table_name.c_str(), source, schema, true, name_pattern);
      if (is_force) {
        schema->remove_table_to_discard(table_name.c_str());
        Table *old_tab =
            backend->get_table_by_name(schema_name.c_str(), table_name.c_str());
        if (old_tab) {
          backend->remove_data_space(old_tab);
        }
      }
      schema->add_table(tab);
      backend->add_data_space(tab);
      tab->set_from_config_source();

#ifndef CLOSE_MULTIPLE
      DynamicOperationSyncCondition *cond = NULL;
      try {
        /*first store the modified conf to the zookeeper*/
        Backend::instance()->flush_config_to_zoo(false);

        /*Then fin the sync topic*/
        mul->acquire_dynamic_info_lock();
        unsigned long after_pub_version = mul->get_cur_config_info_version();
        mul->release_dynamic_info_lock();
        cond = new DynamicOperationSyncCondition();
        cond->prepare_condition(
            after_pub_version);  // The fin condition should ensure the slave
                                 // dbscale has get the new config version.

        sync_tool->get_sync_topic()->set_sync_info_op_param(param);
        sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param, cond);
        sync_tool->publish_sync_message();
        sync_tool->wait_all_children_sync();
        delete cond;
        cond = NULL;
        is_sync_finish = true;

      } catch (Exception &e) {
        LOG_ERROR("error happen in dynamic add normtable, due to %s.\n",
                  e.what());
        MySQLErrorResponse error(ERROR_DYNAMIC_OPERATION_CODE, e.what());
        error.pack(packet);
        error_packet = packet;
        if (cond) {
          delete cond;
          cond = NULL;
        }
      }
    }

    if (multiple_mode && !is_sync_finish) {
      status = EXECUTE_STATUS_COMPLETE;
      throw ErrorPacketException();
    }
  } else {
    if (is_force) {
      schema->remove_table_to_discard(table_name.c_str());
      Table *old_tab =
          backend->get_table_by_name(schema_name.c_str(), table_name.c_str());
      if (old_tab) {
        backend->remove_data_space(old_tab);
      }
    }
    tab = new Table(table_name.c_str(), source, schema, true, name_pattern);
    schema->add_table(tab);
    backend->add_data_space(tab);
  }
#endif
  // record dynamic add norm_table into config source
  list<string> update_list;
  update_list.push_back(
      Driver::get_driver()->get_config_helper()->generate_table_config(
          tab, source->get_name()));
  Driver::get_driver()->get_config_helper()->update_config(update_list);
  tab->set_from_config_source();
}
void MySQLDynamicAddDataSpaceNode::add_partition_scheme_dataspace() {
  Backend *backend = Backend::instance();
  vector<string> partition_names;
  partition_name_list *it = data_space_info->partition_names;
  while (it != NULL) {
    partition_names.push_back(it->name);
    it = it->next;
  }
  PartitionType type = PARTITION_TYPE_HASH;
  if (data_space_info->type == MOD_PARTITION_SCHEME_DATASPACE)
    type = PARTITION_TYPE_MOD;
  PartitionScheme *sch = NULL;
  bool is_shard = data_space_info->is_shard;
  int shard_nums = data_space_info->shard_nums;
  if (shard_nums == -1) shard_nums = DEFAULT_SHARD_NUMS;
  if (is_shard && shard_nums < 1) {
    shard_nums = partition_names.size() * 3;
  }

#ifndef CLOSE_MULTIPLE
  bool is_sync_finish = false;
  bool is_start_sync = false;
  if (multiple_mode) {
    // TODO: start_sync
    MultipleManager *mul = MultipleManager::instance();
    MultipleSyncTool *sync_tool = mul->get_sync_tool();
    char param[1024] = {0};
    int len = sprintf(
        param, "%d %s %d %d %d", TYPE_DYNAMIC_ADD_SCHEME, partition_scheme_name,
        type == PARTITION_TYPE_HASH ? 0 : 1, is_shard ? 1 : 0, shard_nums);
    char *tmp = param;
    tmp += len;
    if (type == PARTITION_TYPE_HASH)
      len = sprintf(tmp, " %s", data_space_info->hash_method_type);
    else
      len = sprintf(tmp, " %d", data_space_info->is_simple ? 1 : 0);
    tmp += len;
    len = sprintf(tmp, " %d", (int)partition_names.size());
    vector<string>::iterator it = partition_names.begin();
    for (; it != partition_names.end(); it++) {
      tmp += len;
      len = sprintf(tmp, " %s", it->c_str());
    }

    string tmp_param = param;
    if (data_space_info->comment) {
      tmp_param.append(" 0 ");
      string coded_comment = data_space_info->comment;
      boost::replace_all(coded_comment, " ", "");
      tmp_param.append(coded_comment);
    } else {
      tmp_param.append(" -1 no_comment");
    }

    try {
      is_start_sync = backend->start_dynamic_operation_topic(
          DYNAMIC_ADD_SCHEME_TOPIC_NAME, tmp_param.c_str());
      if (!is_start_sync) throw Error("Fail to start_dynamic_operation_topic.");

    } catch (...) {
      LOG_ERROR("Start dynamic add scheme [%s] topic error.\n",
                partition_scheme_name);
      throw Error("Start dynamic add scheme topic error");
    }
    if (is_start_sync) {
#endif

      if (is_shard) {
        map<unsigned int, vector<unsigned int> > shard_partition_map;
        backend->fulfill_shard_partition_map(
            shard_partition_map, partition_names.size(), (size_t)shard_nums);
        sch = backend->create_shard_partition_scheme(
            partition_scheme_name, &partition_names, shard_partition_map,
            shard_nums, type);
      } else {
        sch = backend->create_partition_scheme(partition_scheme_name,
                                               &partition_names, type);
      }
      try {
        if (type == PARTITION_TYPE_HASH) {
          const char *type_str = data_space_info->hash_method_type;
          HashMethodType hash_type = change_string_to_hash_type(type_str);
          sch->set_hash_type(hash_type);
        } else if (type == PARTITION_TYPE_MOD) {
          bool is_simple = data_space_info->is_simple;
          sch->set_is_simple_mode(is_simple);
        }
      } catch (Error &e) {
        delete sch;
        throw;
      }
      if (data_space_info->comment)
        sch->set_comment(data_space_info->comment, false);
      backend->add_partition_scheme(sch);

#ifndef CLOSE_MULTIPLE
      DynamicOperationSyncCondition *cond = NULL;
      try {
        /*first store the modified conf to the zookeeper*/
        Backend::instance()->flush_config_to_zoo(false);

        /*Then fin the sync topic*/
        mul->acquire_dynamic_info_lock();
        unsigned long after_pub_version = mul->get_cur_config_info_version();
        mul->release_dynamic_info_lock();
        cond = new DynamicOperationSyncCondition();
        cond->prepare_condition(
            after_pub_version);  // The fin condition should ensure the slave
                                 // dbscale has get the new config version.

        sync_tool->get_sync_topic()->set_sync_info_op_param(param);
        sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param, cond);
        sync_tool->publish_sync_message();
        sync_tool->wait_all_children_sync();
        delete cond;
        cond = NULL;
        is_sync_finish = true;

      } catch (Exception &e) {
        LOG_ERROR("error happen in dynamic add scheme, due to %s.\n", e.what());
        MySQLErrorResponse error(ERROR_DYNAMIC_OPERATION_CODE, e.what());
        error.pack(packet);
        error_packet = packet;
        if (cond) {
          delete cond;
          cond = NULL;
        }
      }
    }

    if (multiple_mode && !is_sync_finish) {
      status = EXECUTE_STATUS_COMPLETE;
      throw ErrorPacketException();
    }
  } else {
    if (is_shard) {
      map<unsigned int, vector<unsigned int> > shard_partition_map;
      backend->fulfill_shard_partition_map(
          shard_partition_map, partition_names.size(), (size_t)shard_nums);
      sch = backend->create_shard_partition_scheme(
          partition_scheme_name, &partition_names, shard_partition_map,
          shard_nums, type);
    } else {
      sch = backend->create_partition_scheme(partition_scheme_name,
                                             &partition_names, type);
    }
    try {
      if (type == PARTITION_TYPE_HASH) {
        const char *type_str = data_space_info->hash_method_type;
        HashMethodType hash_type = change_string_to_hash_type(type_str);
        sch->set_hash_type(hash_type);
      } else if (type == PARTITION_TYPE_MOD) {
        bool is_simple = data_space_info->is_simple;
        sch->set_is_simple_mode(is_simple);
      }
    } catch (Error &e) {
      delete sch;
      sch = NULL;
      throw;
    }
    if (data_space_info->comment)
      sch->set_comment(data_space_info->comment, false);
    backend->add_partition_scheme(sch);
  }
#endif
}
void MySQLDynamicAddDataSpaceNode::check_add_table_dataspace() {
  Backend *backend = Backend::instance();
  string name = data_space_info->space_name;
  is_force = data_space_info->is_force;
  string::size_type dot_pos = name.find_first_of('.');
  string::size_type dot_pos1 = name.find_last_of('.');
  if (dot_pos == string::npos || dot_pos1 == string::npos ||
      dot_pos != dot_pos1) {
    throw DynamicOpFail("the table name should schema.table");
  }
  schema_name = name.substr(0, dot_pos);
  table_name = name.substr(dot_pos + 1);
  Schema *schema = backend->find_schema(schema_name.c_str());
  if (schema != NULL) {
    if (schema->get_table_by_name(table_name.c_str()))
      throw DynamicOpFail("the table is exist");
  } else {
    throw DynamicOpFail("before add a table the schema must exist");
  }

  if (!is_force) {
    bool has_table = false;
    Connection *conn = NULL;
    try {
      conn = schema->get_connection(session);
      if (!conn) {
        LOG_ERROR("Failed to get connection when dynamic_add_table.\n");
        throw Error("Failed to get connection when dynamic_add_table.");
      }
      char sql[1024];
      if (name_pattern == NULL) {
        sprintf(sql,
                "SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES WHERE "
                "TABLE_SCHEMA='%s' AND TABLE_NAME = '%s'",
                schema_name.c_str(), table_name.c_str());
        string value;
        conn->query_for_one_value(sql, value, 0);
        if (value != "0") {
          has_table = true;
        }
      } else {
        string pattern_name(name_pattern);
        boost::to_lower(pattern_name);
        boost::regex pattern_regex(pattern_name);
        sprintf(sql,
                "SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE "
                "TABLE_SCHEMA='%s'",
                schema_name.c_str());
        vector<string> vec;
        TimeValue tv(backend_sql_net_timeout);
        conn->query_for_one_column(sql, 0, &vec, &tv, true);
        for (vector<string>::iterator it = vec.begin(); it != vec.end(); it++) {
          string value = *it;
          boost::to_lower(value);
          if (!regex_match(value, pattern_regex)) {
            continue;
          }
          // if ths table name does not config yet, then it will return the
          // schema dataspace in this situation, if is not allowed, else, when
          // find a table dataspace, then means this table already have been
          // configured, that's ok, it will not use pattern table at all
          DataSpace *tmp_s = backend->get_data_space_for_table(
              schema_name.c_str(), value.c_str());
          if (tmp_s == schema) {
            has_table = true;
            break;
          }
        }
      }
      conn->get_pool()->add_back_to_free(conn);
    } catch (...) {
      if (conn) {
        conn->get_pool()->add_back_to_dead(conn);
      }
      status = EXECUTE_STATUS_COMPLETE;
      throw;
    }
    if (has_table) {
      LOG_ERROR(
          "Dynamic add table dataspace failed, the table already created.\n");
      throw DynamicOpFail("The table already created");
    }
  }
}
void MySQLDynamicAddDataSpaceNode::add_partition_table() {
  Backend *backend = Backend::instance();
  check_add_table_dataspace();
  Schema *schema = backend->find_schema(schema_name.c_str());
  PartitionScheme *sch =
      backend->find_partition_scheme(data_space_info->partition_scheme_name);
  PartitionedTable *tab = NULL;

#ifndef CLOSE_MULTIPLE
  bool is_sync_finish = false;
  bool is_start_sync = false;
  if (multiple_mode) {
    // TODO: start_sync
    MultipleManager *mul = MultipleManager::instance();
    MultipleSyncTool *sync_tool = mul->get_sync_tool();
    char param[1500] = {0};
    int raise_alter_table_flag = 0;
    sprintf(param, "%d %s %s %s %s %s %d %d", TYPE_DYNAMIC_ADD_PART_TABLE,
            table_name.c_str(), schema_name.c_str(), partition_key_name,
            data_space_info->partition_scheme_name,
            name_pattern ? name_pattern : "null", raise_alter_table_flag,
            is_force > 0 ? 1 : 0);

    try {
      is_start_sync = backend->start_dynamic_operation_topic(
          DYNAMIC_ADD_PARTTABLE_TOPIC_NAME, param);
      if (!is_start_sync) throw Error("Fail to start_dynamic_operation_topic.");
    } catch (...) {
      LOG_ERROR("Start dynamic add parttable [%s.%s] topic error.\n",
                schema_name.c_str(), table_name.c_str());
      throw Error("Start dynamic add parttable topic error");
    }
    if (is_start_sync) {
#endif
      tab = new PartitionedTable(table_name.c_str(), partition_key_name, sch,
                                 schema, sch->get_type(), true, name_pattern,
                                 DEFAULT_VIRTUAL_TIMES);
      if (is_force) {
        schema->remove_table_to_discard(table_name.c_str());
        Table *old_tab =
            backend->get_table_by_name(schema_name.c_str(), table_name.c_str());
        if (old_tab) {
          backend->remove_data_space(old_tab);
        }
      }
      schema->add_table(tab);
      backend->add_data_space(tab);
      tab->set_from_config_source();

#ifndef CLOSE_MULTIPLE
      DynamicOperationSyncCondition *cond = NULL;
      try {
        /*first store the modified conf to the zookeeper*/
        Backend::instance()->flush_config_to_zoo(false);

        /*Then fin the sync topic*/
        mul->acquire_dynamic_info_lock();
        unsigned long after_pub_version = mul->get_cur_config_info_version();
        mul->release_dynamic_info_lock();
        cond = new DynamicOperationSyncCondition();
        cond->prepare_condition(
            after_pub_version);  // The fin condition should ensure the slave
                                 // dbscale has get the new config version.

        sync_tool->get_sync_topic()->set_sync_info_op_param(param);
        sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param, cond);
        sync_tool->publish_sync_message();
        sync_tool->wait_all_children_sync();
        delete cond;
        cond = NULL;
        is_sync_finish = true;

      } catch (Exception &e) {
        LOG_ERROR("error happen in dynamic add parttable, due to %s.\n",
                  e.what());
        MySQLErrorResponse error(ERROR_DYNAMIC_OPERATION_CODE, e.what());
        error.pack(packet);
        error_packet = packet;
        if (cond) {
          delete cond;
          cond = NULL;
        }
      }
    }

    if (multiple_mode && !is_sync_finish) {
      status = EXECUTE_STATUS_COMPLETE;
      throw ErrorPacketException();
    }
  } else {
    tab = new PartitionedTable(table_name.c_str(), partition_key_name, sch,
                               schema, sch->get_type(), true, name_pattern,
                               DEFAULT_VIRTUAL_TIMES);
    if (is_force) {
      schema->remove_table_to_discard(table_name.c_str());
      Table *old_tab =
          backend->get_table_by_name(schema_name.c_str(), table_name.c_str());
      if (old_tab) {
        backend->remove_data_space(old_tab);
      }
    }
    schema->add_table(tab);
    backend->add_data_space(tab);
  }
#endif
  // record dynamic add part_table into config source
  list<string> config_list;
  const char *scheme_name = tab->get_partition_scheme()->get_name();
  config_list.push_back(Driver::get_driver()
                            ->get_config_helper()
                            ->generate_partition_table_config(
                                tab, scheme_name, tab->get_virtual_map()));
  Driver::get_driver()->get_config_helper()->update_config(config_list);
  tab->set_from_config_source();
}

void MySQLDynamicAddDataSpaceNode::execute() {
  LOG_DEBUG("MySQLDynamicAddDataSpaceNode::execute.\n");
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    acquire_start_config_lock(session->get_zk_start_config_lock_extra_info());
  }
#endif

  Backend *backend = Backend::instance();
  try {
    packet = Backend::instance()->get_new_packet();
    ACE_RW_Thread_Mutex *dynamic_add_mutex =
        backend->get_dynamic_modify_rep_mutex();
    ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_add_mutex);
    check_config();
    add_dataspace_type type = data_space_info->type;
    switch (type) {
      case SCHEMA_DATASPACE: {
        add_schema_dataspace();
      } break;
      case NORMAL_TABLE_DATASPACE: {
        add_normal_table_dataspace();
      } break;
      case HASH_PARTITION_SCHEME_DATASPACE:
      case MOD_PARTITION_SCHEME_DATASPACE: {
        add_partition_scheme_dataspace();
      } break;
      case PARTITION_TABLE_DATASPACE: {
        add_partition_table();
      } break;
    }
  } catch (exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    LOG_ERROR("error happen in dynamic add a data space, due to %s.\n",
              e.what());
    MySQLErrorResponse error(ERROR_DYNAMIC_OPERATION_CODE, e.what());
    error.pack(packet);
    error_packet = packet;
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) release_start_config_lock();
#endif
    throw ErrorPacketException();
  }
  try {
    Packet ok_packet;
    MySQLOKResponse ok(0, 0);
    ok.pack(&ok_packet);
    handler->deal_autocommit_with_ok_eof_packet(&ok_packet);
    handler->send_to_client(&ok_packet);
  } catch (exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    LOG_ERROR("error happen in dynamic add a data space, due to %s.\n",
              e.what());
    MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE,
                             "Connection get error!");
    error.pack(packet);
    error_packet = packet;
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) release_start_config_lock();
#endif
    throw ErrorPacketException();
  }
  status = EXECUTE_STATUS_COMPLETE;
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) release_start_config_lock();
#endif
}
void MySQLDynamicAddDataSpaceNode::add_schema_dataspace() {
  is_force = data_space_info->is_force;
  const char *space_name = data_space_info->space_name;
  const char *source_name = data_space_info->data_source_name;
  const char *alias_real_name = data_space_info->alias_real_name;
  int pushdown_stored_procedure = data_space_info->pushdown_stored_procedure;
  Backend *backend = Backend::instance();
  // check the there no table in that schema
  if (!is_force) {
    Schema *sch = backend->find_schema(space_name);
    if (sch != NULL) {
      throw DynamicOpFail("the schema is exist");
    }
    DataSource *source = backend->find_data_source(source_name);
    if (source == NULL) {
      LOG_ERROR("The data-source does not exist.\n");
      throw DynamicOpFail("The source not exist");
    }
    if (!source->get_is_conf_source())
      throw DynamicOpFail("the datasource should be config source");

    if (source == backend->get_catalog()->get_data_source()) {
#ifdef DEBUG
      LOG_DEBUG("Do not care about table created or not.\n");
#endif
    } else {
      bool has_not_config_table = false;
      Connection *conn = NULL;
      try {
        conn = backend->get_catalog()->get_connection(session);
        if (!conn) {
          LOG_ERROR("Failed to get connection when dynamic_add_schema.\n");
          throw Error("Failed to get connection when dynamic_add_schema.");
        }
        char sql[1024];
        sprintf(sql,
                "SELECT COUNT(*) FROM INFORMATION_SCHEMA.SCHEMATA WHERE "
                "SCHEMA_NAME='%s'",
                space_name);
        string value;
        TimeValue tv(backend_sql_net_timeout);
        conn->query_for_one_value(sql, value, 0);
        if (value != "0") {
          has_not_config_table = true;
        }
        conn->get_pool()->add_back_to_free(conn);
      } catch (...) {
        if (conn) {
          conn->get_pool()->add_back_to_dead(conn);
        }
        status = EXECUTE_STATUS_COMPLETE;
        throw;
      }
      if (has_not_config_table) {
        LOG_ERROR(
            "Dynamic add schema dataspace failed, the database already "
            "created.\n");
        throw DynamicOpFail(
            "The database should not exist when DYNAMIC ADD SCHEMA DATASPACE");
      }
    }
  }

  try {
    backend->add_schema_space_for_multiple(space_name, source_name,
                                           alias_real_name,
                                           pushdown_stored_procedure, is_force);
  } catch (ErrorPacketException &e) {
    status = EXECUTE_STATUS_COMPLETE;
    MySQLErrorResponse error(ERROR_DYNAMIC_OPERATION_CODE, e.what());
    error.pack(packet);
    error_packet = packet;
    throw;
  }
}

/* MySQLShowTableLoctionNode */
MySQLDBScaleShowTableLocationNode::MySQLDBScaleShowTableLocationNode(
    ExecutePlan *plan, const char *schema_name, const char *table_name)
    : MySQLDBScaleShowNode(plan),
      schema_name(schema_name),
      table_name(table_name) {
  this->name = "MySQLDBScaleShowTableLocationNode";
}

void MySQLDBScaleShowTableLocationNode::get_slave_info(
    LoadBalanceDataSource *cur_ds, list<const char *> &row_data, int group_id) {
  if (cur_ds == NULL) {
    row_data.push_back(" ");
    row_data.push_back(" ");
    row_data.push_back(" ");
    add_row_packet(row_data);
    return;
  }
  vector<DataSource *> data_sources = cur_ds->get_data_sources();
  char port[20];
  bool not_first_line = 0;
  bool has_push_back_row_data = false;
  if (!data_sources.empty()) {
    for (unsigned int i = 0; i < data_sources.size(); i++) {
      DataSource *source = data_sources[i];
      if (source->get_group_id() != group_id) continue;
      if (not_first_line) {
        row_data.push_back(" ");
        row_data.push_back(" ");
        row_data.push_back(" ");
        row_data.push_back(" ");
        row_data.push_back(" ");
      }
      row_data.push_back(source->get_master_server()->get_name());
      row_data.push_back(source->get_master_server()->get_host_ip());
      snprintf(port, sizeof(port), "%d",
               source->get_master_server()->get_port());
      row_data.push_back(port);
      not_first_line = 1;
      has_push_back_row_data = true;
      add_row_packet(row_data);
      row_data.clear();
    }
    if (!has_push_back_row_data) {
      row_data.push_back(" ");
      row_data.push_back(" ");
      row_data.push_back(" ");
      add_row_packet(row_data);
    }
  } else {
    row_data.push_back(" ");
    row_data.push_back(" ");
    row_data.push_back(" ");
    add_row_packet(row_data);
  }
}

void MySQLDBScaleShowTableLocationNode::get_table_location() {
  try {
    Backend *backend = Backend::instance();
    DataSpace *ds = backend->get_data_space_for_table(schema_name, table_name);
    DataSource *data_source = ds->get_data_source();
    set_head_packet(8);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasource";
    const char *org_table = "datasources";
    list<const char *> columns;
    columns.push_back("Server_name");
    columns.push_back("Server_ip");
    columns.push_back("Server_port");
    data_source ? columns.push_back("Data_source")
                : columns.push_back("PartitionScheme");
    columns.push_back("local_db_name");
    columns.push_back("slave_name");
    columns.push_back("slave_ip");
    columns.push_back("slave_port");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
    // Partition table's datasouce is NULL.
    // See constructor of partition table.
    if (data_source != NULL) {
      DataServer *dsv = data_source->get_master_server();
#ifdef DEBUG
      ACE_ASSERT(dsv);
#endif
      list<const char *> row_data;
      row_data.push_back(dsv->get_name());
      row_data.push_back(dsv->get_host_ip());
      char host_port[20];
      snprintf(host_port, sizeof(host_port), "%d", dsv->get_port());
      row_data.push_back(host_port);
      row_data.push_back(data_source->get_name());
      row_data.push_back(schema_name);
      int group_id = data_source->get_group_id();
      LoadBalanceDataSource *read;
      if (data_source->get_data_source_type() == DATASOURCE_TYPE_REPLICATION ||
          data_source->get_data_source_type() == DATASOURCE_TYPE_RWSPLIT ||
          data_source->get_data_source_type() == DATASOURCE_TYPE_MGR) {
        RWSplitDataSource *source = (RWSplitDataSource *)data_source;
        read = (LoadBalanceDataSource *)(source->get_read_source());
      } else {
        read = NULL;
      }
      get_slave_info(read, row_data, group_id);
    } else {
      // partition table
#ifdef DEBUG
      bool is_part = ((Table *)ds)->is_partitioned();
      ACE_ASSERT(is_part);
#endif
      PartitionScheme *ps = ((PartitionedTable *)ds)->get_partition_scheme();
      vector<Partition *> *m = ps->get_partitions_vector();
      for (vector<Partition *>::iterator it = m->begin(); it != m->end();
           it++) {
        list<const char *> row_data;
        row_data.push_back(
            (*it)->get_data_source()->get_master_server()->get_name());
        row_data.push_back(
            (*it)->get_data_source()->get_master_server()->get_host_ip());
        char host_port[20];
        snprintf(host_port, sizeof(host_port), "%d",
                 (*it)->get_data_source()->get_master_server()->get_port());
        row_data.push_back(host_port);
        row_data.push_back(ps->get_name());
        string shard_schema_name = schema_name;
        backend->get_shard_schema_string((*it)->get_virtual_machine_id(),
                                         (*it)->get_partition_id(),
                                         shard_schema_name);
        row_data.push_back(shard_schema_name.c_str());
        int group_id = (*it)->get_data_source()->get_group_id();
        LoadBalanceDataSource *read;
        if ((*it)->get_data_source()->get_data_source_type() ==
                DATASOURCE_TYPE_REPLICATION ||
            (*it)->get_data_source()->get_data_source_type() ==
                DATASOURCE_TYPE_RWSPLIT ||
            (*it)->get_data_source()->get_data_source_type() ==
                DATASOURCE_TYPE_MGR) {
          RWSplitDataSource *source =
              (RWSplitDataSource *)((*it)->get_data_source());
          read = (LoadBalanceDataSource *)(source->get_read_source());
        } else {
          read = NULL;
        }
        get_slave_info(read, row_data, group_id);
      }
    }
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Connection get error!",
                     "42S02");
    throw ErrorPacketException();
  }
}

void MySQLDBScaleShowTableLocationNode::init_data() {
  LOG_INFO("DBSCALE SHOW TABLE LOCATION %s.%s\n", this->schema_name,
           this->table_name);
  get_table_location();
}

void get_user_status_output(set<Session *, compare_session> *pset,
                            list<Packet *> &row_list, bool only_show_running) {
#ifndef CLOSE_MULTIPLE
  char cluster_id_str[20];
  int cluster_id = Backend::instance()->get_cluster_id();
  snprintf(cluster_id_str, sizeof(cluster_id_str), "%d", cluster_id);
#endif
  set<Session *, compare_session>::iterator it;
  for (it = pset->begin(); it != pset->end(); it++) {
    if (only_show_running && strcasecmp((*it)->get_query_sql(), IDLE_SQL) == 0)
      continue;
    list<const char *> row_data;
    char session_id[20];
    snprintf(session_id, sizeof(session_id), "%p", (*it));
    row_data.push_back(session_id);
    char thread_id[20];
    string tmp_s = "";
    snprintf(thread_id, sizeof(thread_id), "%d", (*it)->get_thread_id());
    row_data.push_back(thread_id);
    string schema, user_addr, query_sql;
    (*it)->get_schema(schema);
    (*it)->get_user_addr(user_addr);
    (*it)->get_query_sql(query_sql);
    row_data.push_back(schema.c_str());
    row_data.push_back(user_addr.c_str());
    row_data.push_back(query_sql.c_str());
    char query_time[20];
    snprintf(query_time, sizeof(query_time), "%lu", (*it)->get_query_time());
    row_data.push_back(query_time);
    bool in_prepare = (*it)->session_is_in_prepare();
    if ((*it)->is_in_lock()) {
      if (in_prepare) {
        row_data.push_back("in-lock-table & in-prepare");
      } else {
        row_data.push_back("in-lock-table");
      }
    } else if ((*it)->is_in_transaction()) {
      if (in_prepare) {
        row_data.push_back("in-transaction & in-prepare");
      } else {
        row_data.push_back("in-transaction");
      }
    } else if ((*it)->is_in_cluster_xa_transaction()) {
      tmp_s.append((*it)
                       ->cluster_xa_transaction_to_string(
                           (*it)->get_cluster_xa_transaction_state())
                       .c_str());
      if (in_prepare) {
        tmp_s.append(" & in-prepare");
        row_data.push_back(tmp_s.c_str());
      } else {
        row_data.push_back(tmp_s.c_str());
      }
    } else if (in_prepare) {
      row_data.push_back("in-prepare");
    } else if ((*it)->is_do_logining()) {
      row_data.push_back("doing login");
    } else {
      row_data.push_back("none");
    }
    ACE_Date_Time login_time((*it)->get_login_time());
    char login_time_str[100];
    snprintf(login_time_str, sizeof(login_time_str),
             "%04ld-%02ld-%02ld %02ld:%02ld:%02ld", login_time.year(),
             login_time.month(), login_time.day(), login_time.hour(),
             login_time.minute(), login_time.second());
    row_data.push_back(login_time_str);
    char conn_size[20];
    snprintf(conn_size, sizeof(conn_size), "%llu", (*it)->get_using_conn_num());
    row_data.push_back(conn_size);
    char net_in[20];
    char net_out[20];
    snprintf(net_in, sizeof(net_in), "%llu", (*it)->get_net_in());
    snprintf(net_out, sizeof(net_out), "%llu", (*it)->get_net_out());
    row_data.push_back(net_in);
    row_data.push_back(net_out);

#ifndef CLOSE_MULTIPLE
    if (multiple_mode) {
      row_data.push_back(cluster_id_str);
    }
#endif
    char in_transaction_query_time[20];
    snprintf(in_transaction_query_time, sizeof(in_transaction_query_time),
             "%lu", (*it)->get_in_transaction_time());
    row_data.push_back(in_transaction_query_time);

    Packet *packet = Backend::instance()->get_new_packet();
    MySQLRowResponse row(row_data);
    row.pack(packet);
    row_list.push_back(packet);
  }
}

void get_user_status_by_name(MySQLDriver *driver, const char *user_name,
                             list<Packet *> &row_list) {
  driver->acquire_session_mutex();
  set<Session *, compare_session> *pset = new set<Session *, compare_session>;
  driver->get_session_set_for_user(user_name, pset);
  get_user_status_output(pset, row_list, false);
  delete pset;
  driver->release_session_mutex();
}

void get_user_status(MySQLDriver *driver, list<Packet *> &row_list,
                     bool only_show_running) {
  driver->acquire_session_mutex();
  set<Session *, compare_session> *session_set = driver->get_session_set();
  get_user_status_output(session_set, row_list, only_show_running);
  driver->release_session_mutex();
}

void get_user_work_state(Session *session, list<const char *> &row_data,
                         string &extra_info) {
  extra_info.clear();
  char preparenum[10];
  string tmp_s = "";

  bool in_prepare = session->session_is_in_prepare();
  if (session->is_in_lock()) {
    lock_table_node *lock_node = session->get_lock_head();
    while (lock_node != NULL) {
      if (lock_node->item.operation == LOCK_TABLES) {
        extra_info = extra_info + lock_node->item.schema_name + "." +
                     lock_node->item.table_name + " ";
      }
      lock_node = lock_node->next;
    }
    if (in_prepare) {
      row_data.push_back("in-lock-table & in-prepare");
      snprintf(preparenum, sizeof(preparenum), "%d",
               session->get_prepare_num());
      extra_info = extra_info + ";" + preparenum;
    } else {
      row_data.push_back("in-lock-table");
    }
  } else if (session->is_in_transaction()) {
    session->get_extra_info(extra_info);
    if (in_prepare) {
      row_data.push_back("in-transaction & in-prepare");
      snprintf(preparenum, sizeof(preparenum), "%d",
               session->get_prepare_num());
      extra_info = extra_info + ";" + preparenum;
    } else {
      row_data.push_back("in-transaction");
    }
  } else if (session->is_in_cluster_xa_transaction()) {
    session->get_extra_info(extra_info);
    tmp_s.append(session
                     ->cluster_xa_transaction_to_string(
                         session->get_cluster_xa_transaction_state())
                     .c_str());
    if (in_prepare) {
      tmp_s.append(" & in-prepare");
      row_data.push_back(tmp_s.c_str());
      snprintf(preparenum, sizeof(preparenum), "%d",
               session->get_prepare_num());
      extra_info = extra_info + ";" + preparenum;
    } else {
      row_data.push_back(tmp_s.c_str());
    }
  } else if (in_prepare) {
    row_data.push_back("in-prepare");
    snprintf(preparenum, sizeof(preparenum), "%d", session->get_prepare_num());
    extra_info += preparenum;
  } else if (session->is_do_logining()) {
    row_data.push_back("doing login");
  } else {
    row_data.push_back("none");
  }
}
void get_user_status(MySQLDriver *driver, unsigned int thread_id,
                     list<Packet *> &row_list) {
  list<const char *> row_data;
  char thread_id_str[20];
  string extra_info = "";
  stringstream kept_conn_list;

  driver->acquire_session_mutex();
  Session *session = driver->get_session_by_thread_id(thread_id);
  if (session == NULL) {
    driver->release_session_mutex();
    return;
  }
  snprintf(thread_id_str, sizeof(thread_id_str), "%d", thread_id);
  row_data.push_back(thread_id_str);
  row_data.push_back(session->get_schema());
  get_user_work_state(session, row_data, extra_info);
  row_data.push_back(extra_info.c_str());
  session->get_conn_list_str(kept_conn_list);
  string tmp_str = kept_conn_list.str();
  if (tmp_str.size() == 0)
    row_data.push_back("");
  else
    row_data.push_back(tmp_str.c_str());
#ifndef CLOSE_MULTIPLE
  char cluster_id_str[20];
  int cluster_id = Backend::instance()->get_cluster_id();
  snprintf(cluster_id_str, sizeof(cluster_id_str), "%d", cluster_id);
  if (multiple_mode) {
    row_data.push_back(cluster_id_str);
  }
#endif

  Packet *packet = Backend::instance()->get_new_packet();
  MySQLRowResponse row(row_data);
  row.pack(packet);
  row_list.push_back(packet);
  driver->release_session_mutex();
}

MySQLDBScaleShowUserMemoryStatusNode::MySQLDBScaleShowUserMemoryStatusNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowUserMemoryStatusNode";
}

void MySQLDBScaleShowUserMemoryStatusNode::get_user_memory_status(
    list<Packet *> &row_list) {
  driver->acquire_session_mutex();
  set<Session *, compare_session> *session_set = driver->get_session_set();
  set<Session *, compare_session>::iterator it1;
  for (it1 = session_set->begin(); it1 != session_set->end(); it1++) {
    list<const char *> row_data;
    char thread_id[20];
    snprintf(thread_id, sizeof(thread_id), "%d", (*it1)->get_thread_id());
    row_data.push_back(thread_id);
    (*it1)->acquire_using_conn_mutex();
    set<Connection *> *conn_set = (*it1)->get_using_conn();
    set<Connection *>::iterator it2;
    int sum = 0;
    Connection *conn = NULL;
    for (it2 = conn_set->begin(); it2 != conn_set->end(); it2++) {
      conn = (*it2)->get_pool()->get_one_from_free();
      char user_id[20];
      snprintf(user_id, sizeof(user_id), "%d", (*it2)->get_thread_id());
      string sql =
          "select memory_used from information_schema.processlist where id = ";
      sql = sql + user_id + ";";
      string val;
      const char *s;
      try {
        conn->query_for_one_value(sql.c_str(), val, 0);
        s = val.c_str();
        sum += atoi(s);
        (*it2)->get_pool()->add_back_to_free(conn);
      } catch (exception &e) {
        (*it2)->get_pool()->add_back_to_dead(conn);
        (*it1)->release_using_conn_mutex();
        driver->release_session_mutex();
        throw ErrorPacketException();
      }
    }
    char memory_used[25];
    snprintf(memory_used, sizeof(memory_used), "%d", sum);
    row_data.push_back(memory_used);
    Packet *packet = Backend::instance()->get_new_packet();
    MySQLRowResponse row(row_data);
    row.pack(packet);
    row_list.push_back(packet);
    (*it1)->release_using_conn_mutex();
  }
  driver->release_session_mutex();
}

void MySQLDBScaleShowUserMemoryStatusNode::init_data() {
  int ret = 1;
  Backend *backend = Backend::instance();
  map<string, DataServer *> dataservers;
  backend->get_data_servers(dataservers);
  LOG_INFO("dbscale show user memory status\n");
  try {
    set_head_packet(2);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";
    list<const char *> columns;
    columns.push_back("user_id");
    columns.push_back("memory_used");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
    if (!Backend::instance()->get_is_mariadb_server()) {
      ret = 0;
      throw ErrorPacketException();
    }
    get_user_memory_status(row_list);
  } catch (exception &e) {
    if (ret)
      set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Connection get error!",
                       "");
    else
      set_error_packet(ERROR_BACKEND_NOT_MARIADB_CODE,
                       dbscale_err_msg[ERROR_BACKEND_NOT_MARIADB_CODE], "");
    throw ErrorPacketException();
  }
}

MySQLNavicateProfileSqlNode::MySQLNavicateProfileSqlNode(ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {}

void MySQLNavicateProfileSqlNode::init_data() {
  LOG_DEBUG("DBSCALE Navicate profile sql node\n");
  set_head_packet(2);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "datasource";
  const char *org_table = "datasources";
  list<const char *> columns;
  columns.push_back("QUERY_ID");
  columns.push_back("SUM_DURATION");
  while (!columns.empty()) {
    add_column_packet(catalog, schema, table, org_table, columns.front(),
                      columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    columns.pop_front();
  }
  list<const char *> row_data;
  row_data.push_back("10");
  row_data.push_back("10");
  add_row_packet(row_data);
}

MySQLDBScaleShowUserSqlCountNode::MySQLDBScaleShowUserSqlCountNode(
    ExecutePlan *plan, const char *user_id)
    : MySQLDBScaleShowNode(plan), user_id(user_id) {}

void MySQLDBScaleShowUserSqlCountNode::init_data() {
  LOG_DEBUG("DBSCALE SHOW user sql count!\n");
  try {
    set_head_packet(6);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasource";
    const char *org_table = "datasources";
    list<const char *> columns;
    columns.push_back("USER_ID");
    columns.push_back("SELECT");
    columns.push_back("INSERT/REPLACE");
    columns.push_back("INSERT_SELECT/REPLACE_SELECT");
    columns.push_back("UPDATE");
    columns.push_back("DELETE");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
    if (this->user_id == NULL)
      get_user_sql_count();
    else
      get_user_sql_count_by_id();
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Connection get error!",
                     "42S02");
    throw ErrorPacketException();
  }
}

void MySQLDBScaleShowUserSqlCountNode::output(Session *cur_session) {
  list<const char *> row_data;
  char thread_id[20];
  snprintf(thread_id, sizeof(thread_id), "%d", cur_session->get_thread_id());
  row_data.push_back(thread_id);
  char str1[20], str2[20], str3[20], str4[20], str5[20];
  snprintf(str1, sizeof(str1), "%d", cur_session->get_select());
  row_data.push_back(str1);
  snprintf(str2, sizeof(str2), "%d", cur_session->get_insert());
  row_data.push_back(str2);
  snprintf(str3, sizeof(str3), "%d", cur_session->get_insert_select());
  row_data.push_back(str3);
  snprintf(str4, sizeof(str4), "%d", cur_session->get_update());
  row_data.push_back(str4);
  snprintf(str5, sizeof(str5), "%d", cur_session->get_delete());
  row_data.push_back(str5);
  Packet *packet = Backend::instance()->get_new_packet();
  MySQLRowResponse row(row_data);
  row.pack(packet);
  row_list.push_back(packet);
}

void MySQLDBScaleShowUserSqlCountNode::output_historical_totally() {
  list<const char *> row_data;
  row_data.push_back("historical");
  DBScaleStatus *showstatus = DBScaleStatus::instance();
  map<int, StatusNode *> *dbscalestatus = showstatus->get_status();
  char str1[20], str2[20], str3[20], str4[20], str5[20];
  snprintf(str1, sizeof(str1), "%lu", (*dbscalestatus)[TIMES_SELECT]->value);
  row_data.push_back(str1);
  snprintf(str2, sizeof(str2), "%lu", (*dbscalestatus)[TIMES_INSERT]->value);
  row_data.push_back(str2);
  snprintf(str3, sizeof(str3), "%lu",
           (*dbscalestatus)[TIMES_INSERT_SELECT]->value);
  row_data.push_back(str3);
  snprintf(str4, sizeof(str4), "%lu", (*dbscalestatus)[TIMES_UPDATE]->value);
  row_data.push_back(str4);
  snprintf(str5, sizeof(str5), "%lu", (*dbscalestatus)[TIMES_DELETE]->value);
  row_data.push_back(str5);
  Packet *packet = Backend::instance()->get_new_packet();
  MySQLRowResponse row(row_data);
  row.pack(packet);
  row_list.push_back(packet);
}

void MySQLDBScaleShowUserSqlCountNode::get_user_sql_count() {
  driver->acquire_session_mutex();
  set<Session *, compare_session> *session_set = driver->get_session_set();
  set<Session *, compare_session>::iterator it;
  for (it = session_set->begin(); it != session_set->end(); it++) {
    output((*it));
  }
  driver->release_session_mutex();
  output_historical_totally();
}

void MySQLDBScaleShowUserSqlCountNode::get_user_sql_count_by_id() {
  list<const char *> row_data;
  driver->acquire_session_mutex();
  uint32_t thread_id = atoi(this->user_id);
  Session *session = driver->get_session_by_thread_id(thread_id);

  if (session == NULL) {
    driver->release_session_mutex();
    return;
  }
  output(session);
  driver->release_session_mutex();
}

/* class MySQLDBScaleShowSessionIdNode */
MySQLDBScaleShowSessionIdNode::MySQLDBScaleShowSessionIdNode(
    ExecutePlan *plan, const char *server_name, int connection_id)
    : MySQLDBScaleShowNode(plan),
      server_name(server_name),
      connection_id(connection_id) {
  if (plan->statement->get_stmt_node()->type == STMT_DBSCALE_SHOW_SESSION_ID) {
    this->name = "MySQLDBScaleShowSessionIdNode";
  } else {
    this->name = "MySQLDBScaleRequestSessionIdNode";
  }
}

void MySQLDBScaleShowSessionIdNode::init_data() {
  uint64_t head_num = 2;
  set_head_packet(head_num);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "datasources";
  const char *org_table = "datasources";
  list<const char *> columns;
  columns.push_back("Cluster id");
  columns.push_back("Session_id");
  while (!columns.empty()) {
    add_column_packet(catalog, schema, table, org_table, columns.front(),
                      columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    columns.pop_front();
  }

  if (plan->statement->get_stmt_node()->type == STMT_DBSCALE_SHOW_SESSION_ID) {
    try {
      if (multiple_mode) {
        char sql[1000];
        int len = sprintf(
            sql,
            "DBSCALE REQUEST SESSION ID WITH DATASERVER = %s CONNECTION = %d;",
            server_name, connection_id);
        sql[len] = '\0';
        MultipleManager *mul_manager = MultipleManager::instance();
        mul_manager->get_cluster_packet(sql, row_list);
      } else {
        get_session_info();
      }
    } catch (exception &e) {
      set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Connection get error!",
                       "42S02");
      throw ErrorPacketException();
    }
  } else if (plan->statement->get_stmt_node()->type ==
             STMT_DBSCALE_REQUEST_SESSION_ID) {
    get_session_info();
  } else {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Unknown SQL type",
                     "42S02");
    throw ErrorPacketException();
  }
}

void MySQLDBScaleShowSessionIdNode::get_session_info() {
  LOG_DEBUG("MySQLDBScaleShowSessionIdNode get_session_info\n");
  int session_id = -1;
  DataServer *server = Backend::instance()->find_data_server(server_name);
  LOG_DEBUG("MySQLDBScaleShowSessionIdNode acquire_session_mutex\n");
  driver->acquire_session_mutex();
  set<Session *, compare_session> *session_set = driver->get_session_set();
  set<Session *, compare_session>::iterator it;
  for (it = session_set->begin(); it != session_set->end(); it++) {
    LOG_DEBUG("MySQLDBScaleShowSessionIdNode contains_using_connection\n");
    if ((*it)->contains_using_connection(server, connection_id)) {
      session_id = (*it)->get_thread_id();
      LOG_DEBUG(
          "MySQLDBScaleShowSessionIdNode contains_using_connection server_id "
          "[%d]\n",
          session_id);
      break;
    }
  }
  LOG_DEBUG("MySQLDBScaleShowSessionIdNode release_session_mutex\n");
  driver->release_session_mutex();
  if (session_id >= 0) {
    list<const char *> row_data;
    char cluster_id[20];
    snprintf(cluster_id, sizeof(cluster_id), "%d",
             Backend::instance()->get_cluster_id());
    row_data.push_back(cluster_id);

    char session_id_str[20];
    snprintf(session_id_str, sizeof(session_id_str), "%d", session_id);
    row_data.push_back(session_id_str);
    add_row_packet(row_data);
  }
}

/* class MySQLDBScaleShowUserProcesslistNode */
MySQLDBScaleShowUserProcesslistNode::MySQLDBScaleShowUserProcesslistNode(
    ExecutePlan *plan, const char *cluster_id, const char *user_id, int local)
    : MySQLDBScaleShowNode(plan),
      cluster_id_str(cluster_id),
      user_id_str(user_id),
      local(local) {
  this->user_id = atoi(user_id_str);
}

void MySQLDBScaleShowUserProcesslistNode::init_data() {
  try {
    uint64_t head_num = 8;
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";
    list<const char *> columns;
#ifndef CLOSE_MULTIPLE
    if (multiple_mode && cluster_id_str) {
      head_num = 9;
      columns.push_back("Cluster Id");
    }
#endif
    set_head_packet(head_num);
    columns.push_back("User Id");
    columns.push_back("Server");
    columns.push_back("Connection Id");
    columns.push_back("db");
    columns.push_back("Command");
    columns.push_back("Time");
    columns.push_back("State");
    columns.push_back("Info");

    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }

#ifndef CLOSE_MULTIPLE
    Backend *backend = Backend::instance();
    int cluster_id_cur = backend->get_cluster_id();
    int cluster_id = cluster_id_cur;
    if (multiple_mode && cluster_id_str) cluster_id = atoi(cluster_id_str);
    if (!local && cluster_id != cluster_id_cur) {
      string sql("dbscale show processlist cluster ");
      sql.append(cluster_id_str);
      sql.append(" user ");
      sql.append(user_id_str);
      sql.append(" local");
      MultipleManager *mul_manager = MultipleManager::instance();
      mul_manager->get_user_processlist(sql.c_str(), row_list, cluster_id);
    } else if (cluster_id == cluster_id_cur) {
#endif
      get_user_processlist();
#ifndef CLOSE_MULTIPLE
    }
#endif
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Connection get error!",
                     "42S02");
    throw ErrorPacketException();
  }
}

void MySQLDBScaleShowUserProcesslistNode::get_user_processlist() {
  map<DataServer *, list<uint32_t> > thread_ids;
  driver->acquire_session_mutex();
  Session *session = driver->get_session_by_thread_id(user_id);
  if (session == NULL) {
    driver->release_session_mutex();
    return;
  }
  session->get_using_conn_ids(thread_ids);
  driver->release_session_mutex();

  map<DataServer *, list<uint32_t> >::iterator it = thread_ids.begin();
  for (; it != thread_ids.end(); it++) {
    list<uint32_t>::iterator it_l = it->second.begin();
    for (; it_l != it->second.end(); it_l++) {
      char thread_id_str[20];
      snprintf(thread_id_str, sizeof(thread_id_str), "%d", *it_l);
      string sql("SELECT ");
#ifndef CLOSE_MULTIPLE
      if (multiple_mode && cluster_id_str) {
        sql.append(cluster_id_str);
        sql.append(", ");
      }
#endif
      sql.append(user_id_str);
      sql.append(", '");
      sql.append(it->first->get_name());
      sql.append(
          "', ID, DB, COMMAND, TIME, STATE, INFO FROM "
          "INFORMATION_SCHEMA.PROCESSLIST WHERE ID = ");
      sql.append(thread_id_str);
      it->first->get_result_from_server(sql.c_str(), row_list);
    }
  }
}

/* class MySQLDBScaleShowUserStatusNode */
MySQLDBScaleShowUserStatusNode::MySQLDBScaleShowUserStatusNode(
    ExecutePlan *plan, const char *user_id, const char *user_name,
    bool only_show_running, bool instance, bool show_status_count)
    : MySQLDBScaleShowNode(plan),
      user_id(user_id),
      user_name(user_name),
      only_show_running(only_show_running),
      instance(instance),
      show_status_count(show_status_count) {
  this->name = "MySQLDBScaleShowUserStatusNode";
}

void MySQLDBScaleShowUserStatusNode::get_status_by_name() {
  try {
    uint64_t head_num = 12;
#ifndef CLOSE_MULTIPLE
    if (multiple_mode && !instance) {
      head_num = head_num + 1;
    }
#endif
    set_head_packet(head_num);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";
    list<const char *> columns;
    columns.push_back("Thread_id_Handler");
    columns.push_back("User_id");
    columns.push_back("Cur_schema");
    columns.push_back("User_info");
    columns.push_back("Executing SQL");
    columns.push_back("Executing time(ms)");
    columns.push_back("Working State");
    columns.push_back("Login time");
    columns.push_back("using_conn_num");
    columns.push_back("net_in(Byte)");
    columns.push_back("net_out(Byte)");
#ifndef CLOSE_MULTIPLE
    if (multiple_mode && !instance) {
      columns.push_back("Cluster id");
    }
#endif
    columns.push_back("Transaction executing time(ms)");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
#ifndef CLOSE_MULTIPLE
    if (multiple_mode && !instance) {
      const char *sql = "dbscale request cluster user status";
      MultipleManager *mul_manager = MultipleManager::instance();
      mul_manager->get_all_user_status_info(sql, row_list);
    } else {
#endif
      get_user_status_by_name(driver, this->user_name, row_list);
#ifndef CLOSE_MULTIPLE
    }
#endif
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Connection get error!",
                     "42S02");
    throw ErrorPacketException();
  }
}

void MySQLDBScaleShowUserStatusNode::get_all_users_status() {
  try {
    uint64_t head_num = 12;
#ifndef CLOSE_MULTIPLE
    if (multiple_mode && !instance) {
      head_num = head_num + 1;
    }
#endif
    set_head_packet(head_num);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";
    list<const char *> columns;
    columns.push_back("Thread_id_Handler");
    columns.push_back("User_id");
    columns.push_back("Cur_schema");
    columns.push_back("User_info");
    columns.push_back("Executing SQL");
    columns.push_back("Executing time(ms)");
    columns.push_back("Working State");
    columns.push_back("Login time");
    columns.push_back("using_conn_num");
    columns.push_back("net_in(Byte)");
    columns.push_back("net_out(Byte)");
#ifndef CLOSE_MULTIPLE
    if (multiple_mode && !instance) {
      columns.push_back("Cluster id");
    }
#endif
    columns.push_back("Transaction executing time(ms)");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
#ifndef CLOSE_MULTIPLE
    if (multiple_mode && !instance) {
      const char *sql = NULL;
      if (only_show_running)
        sql = "dbscale request cluster user status running";
      else
        sql = "dbscale request cluster user status";
      MultipleManager *mul_manager = MultipleManager::instance();
      mul_manager->get_all_user_status_info(sql, row_list);
    } else {
#endif
      get_user_status(driver, row_list, only_show_running);
#ifndef CLOSE_MULTIPLE
    }
#endif
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Connection get error!",
                     "42S02");
    throw ErrorPacketException();
  }
}

void get_user_status_count_output(set<Session *, compare_session> *pset,
                                  list<Packet *> &row_list, bool show_running) {
  list<const char *> row_data;
  char status_count[20];
  int real_status_count = 0;
  if (show_running) {
    set<Session *, compare_session>::iterator it;
    for (it = pset->begin(); it != pset->end(); ++it) {
      if (strcasecmp((*it)->get_query_sql(), IDLE_SQL) != 0) {
        real_status_count++;
      }
    }
  } else {
    real_status_count = pset->size();
  }
  snprintf(status_count, sizeof(status_count), "%d", real_status_count);
  row_data.push_back(status_count);
#ifndef CLOSE_MULTIPLE
  char cluster_id_str[20];
  int cluster_id = Backend::instance()->get_cluster_id();
  snprintf(cluster_id_str, sizeof(cluster_id_str), "%d", cluster_id);
  if (multiple_mode) {
    row_data.push_back(cluster_id_str);
  }
#endif
  Packet *packet = Backend::instance()->get_new_packet();
  MySQLRowResponse row(row_data);
  row.pack(packet);
  row_list.push_back(packet);
}

void get_user_status_running_count(MySQLDriver *driver,
                                   list<Packet *> &row_list,
                                   bool show_running) {
  driver->acquire_session_mutex();
  set<Session *, compare_session> *session_set = driver->get_session_set();
  get_user_status_count_output(session_set, row_list, show_running);
  driver->release_session_mutex();
}

void MySQLDBScaleShowUserStatusNode::get_user_status_count() {
  try {
    uint64_t head_num = 1;
#ifndef CLOSE_MULTIPLE
    if (multiple_mode && !instance) {
      head_num += 1;
    }
#endif
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";
    list<const char *> columns;
    set_head_packet(head_num);
    columns.push_back("Status count");
#ifndef CLOSE_MULTIPLE
    if (multiple_mode && !instance) {
      columns.push_back("Cluster id");
    }
#endif
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
#ifndef CLOSE_MULTIPLE
    if (multiple_mode && !instance) {
      char sql[64];
      int len;
      if (only_show_running) {
        len = sprintf(sql, "dbscale request cluster user status running count");
      } else {
        len = sprintf(sql, "dbscale request cluster user status count");
      }
      sql[len] = '\0';
      MultipleManager *mul_manager = MultipleManager::instance();
      mul_manager->get_all_user_status_info(sql, row_list);
    } else {
#endif
      get_user_status_running_count(driver, row_list, only_show_running);
#ifndef CLOSE_MULTIPLE
    }
#endif
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Connection get error!",
                     "42S02");
    throw ErrorPacketException();
  }
}

void MySQLDBScaleShowUserStatusNode::get_user_status_by_id() {
  try {
    uint64_t head_num = 5;
#ifndef CLOSE_MULTIPLE
    if (multiple_mode && !instance) {
      head_num = 6;
    }
#endif
    set_head_packet(head_num);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";
    list<const char *> columns;
    columns.push_back("User_id");
    columns.push_back("Cur_schema");
    columns.push_back("Working State");
    columns.push_back("Extra Info");
    columns.push_back("Kept Conn List");
#ifndef CLOSE_MULTIPLE
    if (multiple_mode && !instance) {
      columns.push_back("Cluster id");
    }
#endif
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
#ifndef CLOSE_MULTIPLE
    if (multiple_mode && !instance) {
      char sql[64];
      int len = sprintf(sql, "dbscale request cluster user status %s", user_id);
      sql[len] = '\0';
      MultipleManager *mul_manager = MultipleManager::instance();
      mul_manager->get_all_user_status_info(sql, row_list);
    } else {
#endif
      uint32_t thread_id = atoi(this->user_id);
      get_user_status(driver, thread_id, row_list);
#ifndef CLOSE_MULTIPLE
    }
#endif
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Connection get error!",
                     "42S02");
    throw ErrorPacketException();
  }
}

void MySQLDBScaleShowUserStatusNode::init_data() {
  if (this->show_status_count)
    get_user_status_count();
  else if (this->user_id == NULL && this->user_name == NULL)
    get_all_users_status();
  else if (user_id != NULL)
    get_user_status_by_id();
  else
    get_status_by_name();
}

/* class MySQLDBScaleShowDynamicOptionNode */
MySQLDBScaleShowDynamicOptionNode::MySQLDBScaleShowDynamicOptionNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowDynamicOptionNode";
  columns.push_back("option_name");
  columns.push_back("value");
}

void MySQLDBScaleShowDynamicOptionNode::get_row_datas() {
  map<string, OptionValue> tmp_option_value;
  map<string, OptionValue>::iterator it_v = all_option_value.begin();
  for (; it_v != all_option_value.end(); it_v++) {
    if (it_v->second.dynamic_set) tmp_option_value[it_v->first] = it_v->second;
  }
  OptionParser *config = OptionParser::instance();
  config->get_option_map(&option_val_map, &tmp_option_value);

  map<string, string>::iterator it = option_val_map.begin();
  for (; it != option_val_map.end(); it++) {
    LOG_DEBUG("Option value:%s = %s\n", it->first.c_str(), it->second.c_str());
    row_data.clear();
    string tmp(it->first);
    boost::replace_all(tmp, "_", "-");
    row_data.push_back(tmp.c_str());
    row_data.push_back(it->second.c_str());
    add_row_packet(row_data);
  }
}

void MySQLDBScaleShowDynamicOptionNode::init_data() {
  get_row_datas();
  set_head_packet(columns.size());
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "datasources";
  const char *org_table = "datasources";
  list<const char *>::iterator it = columns.begin();
  for (; it != columns.end(); it++) {
    add_column_packet(catalog, schema, table, org_table, *it, *it, 8,
                      NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  }
}

/* class MySQLDBScaleShowOptionNodeNode */
MySQLDBScaleShowOptionNode::MySQLDBScaleShowOptionNode(ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowOptionNode";
  columns.push_back("option_name");
  columns.push_back("value");
}

void MySQLDBScaleShowOptionNode::get_row_datas() {
  string err_msg;
  show_option_op_node *option_oper =
      plan->statement->get_stmt_node()->sql->show_option_oper;
  map<string, OptionValue> session_option_value =
      session->get_session_option_map();
  OptionParser *config = OptionParser::instance();
  config->get_option_map(&option_val_map, &all_option_value);
  var_scope_type vst = option_oper->vst;
  switch (vst) {
    case VAR_SCOPE_INSTANCE: {
      config->get_option_map(&option_val_map, &instance_option_value);
      break;
    }
    case VAR_SCOPE_SESSION: {
      config->get_option_map(&option_val_map, &instance_option_value);
      config->get_option_map(&option_val_map, &session_option_value);
      break;
    }
    default:
      break;
  }

  if (!option_oper->option_name) {
    map<string, string>::iterator it = option_val_map.begin();
    for (; it != option_val_map.end(); it++) {
      LOG_DEBUG("Option value:%s = %s\n", it->first.c_str(),
                it->second.c_str());
      row_data.clear();
      string tmp(it->first);
      boost::replace_all(tmp, "_", "-");
      row_data.push_back(tmp.c_str());
      row_data.push_back(it->second.c_str());
      add_row_packet(row_data);
    }
    return;
  }

  string option_name(option_oper->option_name);
  boost::to_lower(option_name);
  boost::replace_all(option_name, "-", "_");
  size_t pos = option_name.find("%");
  while (pos != string::npos) {
    option_name.replace(pos, 1, ".*");
    pos += 2;
    pos = option_name.find("%", pos);
  }
  boost::regex pattern(option_name.c_str());
  map<string, string>::iterator it = option_val_map.begin();
  for (; it != option_val_map.end(); it++) {
    if (regex_match(it->first.c_str(), pattern)) {
      row_data.clear();
      string tmp(it->first);
      boost::replace_all(tmp, "_", "-");
      row_data.push_back(tmp.c_str());
      row_data.push_back(it->second.c_str());
      add_row_packet(row_data);
    }
  }
}

void MySQLDBScaleShowOptionNode::init_data() {
  get_row_datas();
  set_head_packet(columns.size());
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "datasources";
  const char *org_table = "datasources";
  list<const char *>::iterator it = columns.begin();
  for (; it != columns.end(); it++) {
    add_column_packet(catalog, schema, table, org_table, *it, *it, 8,
                      NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  }
}

/* class MySQLShowChangedStartupConfigNode */
MySQLShowChangedStartupConfigNode::MySQLShowChangedStartupConfigNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLShowChangedStartupConfigNode";
  columns.push_back("option_name");
  columns.push_back("value");
}

void MySQLShowChangedStartupConfigNode::get_row_datas() {
  char buffer[ZK_KA_MESSAGE_LEN];
  int buf_len = zk_ka_message_len;
  struct Stat stat;
  try {
    ZooKeeperTool *zk = ZooKeeperTool::instance();
    zk->get_node_value(ZK_CHANGED_CONFIG_NODE, buffer, &buf_len, &stat);
    LOG_DEBUG("Get %d len buffer after get_node value for path %s.\n", buf_len,
              ZK_CHANGED_CONFIG_NODE);
  } catch (...) {
    LOG_ERROR("Get exception when get changed startup config message.\n");
    throw;
  }
  if (buf_len == 0) {
    LOG_INFO("Update node %s has no value.\n", ZK_CHANGED_CONFIG_NODE);
    return;
  }
  string message_str(buffer, buf_len);

  string option, value;
  map<string, string> tmp_options;
  istringstream iss(message_str.c_str());
  while (iss >> option) {
    iss >> value;
    value.erase(0, 1);
    value.erase(value.length() - 1, 1);
    tmp_options[option] = value;
  }

  map<string, string>::iterator it = tmp_options.begin();
  for (; it != tmp_options.end(); it++) {
    LOG_DEBUG("Option value:%s = %s\n", it->first.c_str(), it->second.c_str());
    row_data.clear();
    row_data.push_back(it->first.c_str());
    row_data.push_back(it->second.c_str());
    add_row_packet(row_data);
  }
}

void MySQLShowChangedStartupConfigNode::init_data() {
  get_row_datas();
  set_head_packet(columns.size());
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "configs";
  const char *org_table = "configs";
  list<const char *>::iterator it = columns.begin();
  for (; it != columns.end(); it++) {
    add_column_packet(catalog, schema, table, org_table, *it, *it, 8,
                      NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  }
}

/* class MySQLChangeStartupConfigNode */
MySQLChangeStartupConfigNode::MySQLChangeStartupConfigNode(ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLChangeStartupConfigNode";
}

void MySQLChangeStartupConfigNode::do_execute() {
  LOG_DEBUG("MySQLChangeStartupConfigNode::do_execute.\n");
  ZooKeeperTool *zk = ZooKeeperTool::instance();
  string option, value;
  change_startup_config_node *node =
      plan->statement->get_stmt_node()->sql->change_startup_config_oper;
  if (node->reset) {
    try {
      ZOO_ERRORS ret = zk->set_node_message(ZK_CHANGED_CONFIG_NODE, "", 0, -1);
      if (ret != ZOK) {
        LOG_ERROR("reset config to zookeeper failed\n");
        throw("reset config to zookeeper failed");
      }
    } catch (...) {
      LOG_ERROR("reset config to zookeeper failed.\n");
      throw;
    }
    return;
  }

  option.assign(node->name);
  value.assign(node->value);

  OptionParser *config = OptionParser::instance();
  Section *section =
      config->get_section("main", DEFAULT_SECTION_SUBTYPE, false);
  if (!section->has_option(option)) {
    LOG_ERROR("Invalid option [%s].\n", option.c_str());
    throw Error("Invalid option.");
  }
  if (value.find(' ') != string::npos) {
    LOG_ERROR("Option value should not contain space character ' '.\n");
    throw Error("Option value should not contain space character ' '.");
  }
  if (value.length() > 2048) {
    LOG_ERROR("Option value should not be lager then 2048.\n");
    throw Error("Option value should not be lager then 2048.");
  }

  char buffer[ZK_KA_MESSAGE_LEN];
  int buf_len = zk_ka_message_len;
  struct Stat stat;
  try {
    zk->get_node_value(ZK_CHANGED_CONFIG_NODE, buffer, &buf_len, &stat);
    LOG_DEBUG("Get %d len buffer after get_node value for path %s.\n", buf_len,
              ZK_CHANGED_CONFIG_NODE);
  } catch (...) {
    LOG_ERROR("Get exception when get changed startup config message.\n");
    throw;
  }
  if (buf_len == 0) {
    LOG_INFO("Update node %s has no value.\n", ZK_CHANGED_CONFIG_NODE);
  }
  string message_str(buffer, buf_len);

  map<string, string> tmp_options;
  istringstream iss(message_str.c_str());
  string tmp_op, tmp_va;
  while (iss >> tmp_op) {
    iss >> tmp_va;
    tmp_va.erase(0, 1);
    tmp_va.erase(tmp_va.length() - 1, 1);
    tmp_options[tmp_op] = tmp_va;
  }

  tmp_options[option] = value;

  ostringstream ostr;
  map<string, string>::iterator it = tmp_options.begin();
  for (; it != tmp_options.end(); it++) {
    ostr << " " << it->first.c_str() << " '" << it->second.c_str() << "'";
  }
  string new_config(ostr.str());
  try {
    ZOO_ERRORS ret = zk->set_node_message(
        ZK_CHANGED_CONFIG_NODE, new_config.c_str(), new_config.length(), -1);
    if (ret != ZOK) {
      LOG_ERROR("pub new config to zookeeper failed\n");
      throw("pub new config to zookeeper failed");
    }
  } catch (...) {
    LOG_ERROR("pub new config to zookeeper failed.\n");
    throw;
  }
}

/* class MySQLCheckTableNode */
MySQLCheckTableNode::MySQLCheckTableNode(ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLCheckTableNode";
}

string get_check_sql(string schema_name, string table_name) {
  string check_sql = "show create table `";
  check_sql += schema_name;
  check_sql += "`.`";
  check_sql += table_name;
  check_sql += "`";
  return check_sql;
}

string get_server_info(DataSpace *space, string sql) {
  DataServer *server = space->get_data_source()->get_master_server();
  string server_info = server->get_name();
  server_info += ":";
  server_info += server->get_host_ip();
  char port_str[50] = {0};
  sprintf(port_str, ":%d:", server->get_port());
  server_info += port_str;
  server_info += sql;
  return server_info;
}

string ignore_increment_string(string create_sql) {
  std::size_t found = create_sql.find("AUTO_INCREMENT=");
  if (found != string::npos) {
    std::size_t end = create_sql.find(" ", found);
    if (end > found) create_sql.erase(found, end - found + 1);
  }
  return create_sql;
}

string ignore_comment_string(string create_sql) {
  /*
   * CREATE TABLE `test` (
   *   `a` int(11) NOT NULL DEFAULT '0' COMMENT '用户id''',
   *     `b` int(11) DEFAULT NULL COMMENT '用户id111'
   *     ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='\nid'''''
   * / *!50100 PARTITION BY RANGE (a)
   *   (PARTITION P0 VALUES LESS THAN (6) COMMENT = '用户id111' ENGINE = InnoDB,
   *    PARTITION P1 VALUES LESS THAN (11) COMMENT = '用户id111' ENGINE =
   * InnoDB, PARTITION P3 VALUES LESS THAN (21) COMMENT = '用户id111' ENGINE =
   * InnoDB) * /
   */
  std::size_t found = create_sql.find(" COMMENT");
  while (found != string::npos) {
    std::size_t end1 = create_sql.find("',\n", found);
    std::size_t end2 = create_sql.find("'\n", found);
    std::size_t end3 = create_sql.find("' ", found);
    std::size_t end = end1 > end2 ? end2 : end1;
    end = end > end3 ? end3 : end;
    if (end > found) create_sql.erase(found, end - found + 1);
    found = create_sql.find(" COMMENT", found);
  }
  return create_sql;
}

map<string, string> get_check_result(map<DataSpace *, string> check_sql_map,
                                     Session *session) {
  map<string, string> check_result_map;
  map<DataSpace *, string>::iterator it = check_sql_map.begin();
  bool check_get_error = false;
  for (; it != check_sql_map.end(); it++) {
    Connection *conn = NULL;
    string server_info = get_server_info(it->first, it->second);
    try {
      conn = it->first->get_connection(session);
      if (conn) {
        if (conn->get_session_var_map_md5() !=
            session->get_session_var_map_md5()) {
          conn->set_session_var(session->get_session_var_map());
          LOG_DEBUG("conn %@ set session var with session var map size %d.\n",
                    conn, session->get_session_var_map()->size());
          conn->reset();
        }
        string check_result;
        conn->query_for_one_value(it->second.c_str(), check_result, 1);
        conn->get_pool()->add_back_to_free(conn);
        conn = NULL;
        check_result = ignore_increment_string(check_result);
        check_result = ignore_comment_string(check_result);
        if (check_result_map.count(check_result)) {
          server_info = check_result_map[check_result] + "\n" + server_info;
          check_result_map[check_result] = server_info;
        } else {
          check_result_map[check_result] = server_info;
        }
      } else {
        LOG_ERROR("DBScale get check table result fail to get connection.\n");
        throw Error("DBScale get check table result fail to get connection.");
      }
    } catch (...) {
      if (conn) {
        conn->get_pool()->add_back_to_dead(conn);
        conn = NULL;
      }
      check_result_map[CHECK_ERROR_INFO] = server_info;
      check_get_error = true;
    }
  }
  if (!check_get_error && check_result_map.size() == 1) {
    check_result_map.clear();
  }
  return check_result_map;
}

void MySQLCheckTableNode::init_data() {
  Backend *backend = Backend::instance();
  join_node *node =
      plan->statement->get_stmt_node()->sql->check_table_oper->table;
  string schema_name =
      node->schema_name ? node->schema_name : plan->session->get_schema();
  string table_name = node->table_name;

  // generate check map, key is DataSpace, value is check_sql
  DataSpace *ds = backend->get_data_space_for_table(schema_name.c_str(),
                                                    table_name.c_str());
  map<DataSpace *, string> check_sql_map;
  string check_sql = get_check_sql(schema_name, table_name);
  DataSpace *auth_space = backend->get_auth_data_space();
  check_sql_map[auth_space] = check_sql;

  if (ds->get_dataspace_type() == TABLE_TYPE &&
      ((Table *)ds)->is_partitioned()) {
    PartitionedTable *part_table = (PartitionedTable *)ds;
    for (unsigned int i = 0; i < part_table->get_real_partition_num(); i++) {
      if (part_table->get_partition(i)->get_virtual_machine_id() > 0) {
        string new_schema;
        adjust_shard_schema(
            schema_name.c_str(), schema_name.c_str(), new_schema,
            part_table->get_partition(i)->get_virtual_machine_id(),
            part_table->get_partition(i)->get_partition_id());
        string check_sql_tmp =
            get_check_sql(new_schema.c_str(), table_name.c_str());
        check_sql_map[part_table->get_partition(i)] = check_sql_tmp;
      } else {
        check_sql_map[part_table->get_partition(i)] = check_sql;
      }
    }
  } else {
    check_sql_map[ds] = check_sql;
  }

  // generate check result map
  map<string, string> check_result_map =
      get_check_result(check_sql_map, plan->session);

  // generate check result packet
  if (check_result_map.empty()) {
    list<const char *> columns;
    columns.push_back("TABLE_NAME");
    columns.push_back("CHECK_RESULT");
    set_head_packet(2);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "check_table";
    const char *org_table = "check_table";
    list<const char *>::iterator it = columns.begin();
    for (; it != columns.end(); it++) {
      add_column_packet(catalog, schema, table, org_table, *it, *it, 8,
                        NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
    }
    list<const char *> row_data;
    splice_full_table_name(schema_name, table_name, full_table_name);
    row_data.push_back(full_table_name.c_str());
    row_data.push_back("OK");
    add_row_packet(row_data);
  } else {
    list<const char *> columns;
    columns.push_back("TABLE_INFO");
    columns.push_back("SERVER_INFO");
    set_head_packet(2);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "check_table";
    const char *org_table = "check_table";
    list<const char *>::iterator it = columns.begin();
    for (; it != columns.end(); it++) {
      add_column_packet(catalog, schema, table, org_table, *it, *it, 8,
                        NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
    }
    map<string, string>::iterator it_c = check_result_map.begin();
    for (; it_c != check_result_map.end(); it_c++) {
      list<const char *> row_data;
      row_data.push_back(it_c->first.c_str());
      row_data.push_back(it_c->second.c_str());
      add_row_packet(row_data);
    }
  }
}
/* class MySQLDBScaleShowDataSourceNode */
MySQLDBScaleShowDataSourceNode::MySQLDBScaleShowDataSourceNode(
    ExecutePlan *plan, list<const char *> &names, bool need_show_weight)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowDataSourceNode";
  this->names = names;
  this->need_show_weight = need_show_weight;
}

void MySQLDBScaleShowDataSourceNode::init_data() {
  Backend *backend = Backend::instance();
  DataSource *data_source = NULL;
  list<const char *>::iterator names_iter = names.begin();
  unsigned int unexpected_gtid_threshold_value_local =
      unexpected_gtid_threshold_value;
  if (names_iter != names.end()) {
    data_source = backend->find_data_source(*names_iter);
    if (data_source) {
      data_source->set_unexpected_gtid_threshold_value_tmp(
          unexpected_gtid_threshold_value_local);
      data_source->get_show_column_names(data_source->get_data_source_type(),
                                         need_show_weight, columns);
    }
  }
  bool get_rows_from_master = false;
#ifndef CLOSE_MULTIPLE
  bool is_master = true;
  if (multiple_mode) {
    MultipleManager *mul = MultipleManager::instance();
    is_master = mul->get_is_cluster_master();
  }
  if (!is_master && plan->statement->get_stmt_node()->forward_to_master != 2) {
    backend->forward_query_to_master_role_node(plan->statement->get_sql(),
                                               session->get_schema(), row_list);
    get_rows_from_master = true;
  }
#endif
  if (!get_rows_from_master) {
    list<list<string> > row_datas_tmp;
    for (; names_iter != names.end(); names_iter++) {
      row_datas_tmp.clear();
      data_source = backend->find_data_source(*names_iter);
      if (data_source) {
        try {
          data_source->set_unexpected_gtid_threshold_value_tmp(
              unexpected_gtid_threshold_value_local);
          data_source->set_need_show_weight(need_show_weight);
          data_source->get_row_data(row_datas_tmp);
        } catch (exception &e) {
          set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                           "Connection get error!", "");
          throw ErrorPacketException();
        }
        list<list<string> >::iterator row_datas_iter = row_datas_tmp.begin();
        for (; row_datas_iter != row_datas_tmp.end(); row_datas_iter++) {
          add_row_packet(*row_datas_iter);
        }
      } else {
        set_error_packet(9004, "Unknown Data Source", "42S02");
        throw ErrorPacketException();
      }
    }
  }
  if (!is_row_list_empty()) {
    set_head_packet(columns.size());
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";
    list<string>::iterator it = columns.begin();
    for (; it != columns.end(); it++) {
      add_column_packet(catalog, schema, table, org_table, it->c_str(),
                        it->c_str(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0,
                        0, 0);
    }
  } else {
    set_error_packet(9004, "Unknown Data Source", "42S02");
    throw ErrorPacketException();
  }
}

/* class MySQLShowWarningNode */
MySQLShowWarningNode::MySQLShowWarningNode(ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLShowWarningNode";
}

void MySQLShowWarningNode::init_data() {
  list<const char *> columns;
  columns.push_back("Level");
  columns.push_back("Code");
  columns.push_back("Message");
  set_head_packet(3);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "datasources";
  const char *org_table = "datasources";
  list<const char *>::iterator it = columns.begin();
  for (; it != columns.end(); it++) {
    add_column_packet(catalog, schema, table, org_table, *it, *it, 8,
                      NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  }
  list<WarnInfo *> warnings = session->get_warning_info();
  if (!warnings.empty()) {
    list<WarnInfo *>::iterator it = warnings.begin();
    for (; it != warnings.end(); it++) {
      list<const char *> row_data;
      row_data.push_back((*it)->level.c_str());
      row_data.push_back((*it)->code.c_str());
      row_data.push_back((*it)->message.c_str());
      add_row_packet(row_data);
    }
  }
}

/* class MySQLShowLoadWarningNode */
MySQLShowLoadWarningNode::MySQLShowLoadWarningNode(ExecutePlan *plan)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLShowLoadWarningNode";
}
void MySQLShowLoadWarningNode::execute() {
  set<vector<Packet *> *> *warning_packets =
      session->get_load_warning_packets();
  set<vector<Packet *> *>::iterator it = warning_packets->begin();
  bool has_sent_header = false;
  for (; it != warning_packets->end(); it++) {
    size_t len = (*it)->size();
    size_t i = 0;
    if (!has_sent_header)
      has_sent_header = true;
    else {
      while (!driver->is_eof_packet((*it)->at(i))) i++;
      i++;
    }
    for (; i < len - 1; i++) handler->send_to_client((*it)->at(i), true);
  }
  size_t len = (*(warning_packets->begin()))->size();
  handler->send_to_client((*(warning_packets->begin()))->at(len - 1), true);
  status = EXECUTE_STATUS_COMPLETE;
}

/* class MySQLDBScaleMigrateCleanNode */
MySQLDBScaleMigrateCleanNode::MySQLDBScaleMigrateCleanNode(
    ExecutePlan *plan, const char *migrate_id)
    : MySQLReturnOKNode(plan), migrate_id(migrate_id) {
  this->name = "MySQLDBScaleMigrateCleanNode";
  error_packet = NULL;
}

void MySQLDBScaleMigrateCleanNode::do_execute() {
  Connection *conn = NULL;
  try {
    string new_name, source_name;
    // get source ane new name
    bool ret =
        Driver::get_driver()->get_config_helper()->get_migrate_clean_new_name(
            migrate_id, new_name, source_name);
    if (!ret) {
      LOG_ERROR(
          "MySQLDBScaleMigrateCleanNode faild to get migrate info with id "
          "[%s]\n",
          migrate_id);
      throw Error(
          "MySQLDBScaleMigrateCleanNode faild to get migrate info with id");
    }
    // get source
    DataSource *source =
        Backend::instance()->find_data_source(source_name.c_str());
    if (!source) {
      LOG_ERROR(
          "MySQLDBScaleMigrateCleanNode get Unknown source [%s], failed to "
          "clean migrate table\n",
          source_name.c_str());
      throw Error(
          "MySQLDBScaleMigrateCleanNode get Unknown source, failed to clean "
          "migrate table");
    }
    // get Connection
    conn = source->get_connection(NULL, false);
    if (!conn) {
      LOG_ERROR(
          "MySQLDBScaleMigrateCleanNode fail to get conn from source [%s] to "
          "clean migrate table\n",
          source_name.c_str());
      throw Error(
          "MySQLDBScaleMigrateCleanNode fail to get conn from source to clean "
          "migrate table");
    }
    // build drop sql
    string sql = "DROP TABLE IF EXISTS ";
    sql += new_name;
    sql += ";";

    Packet exec_packet;
    MySQLQueryRequest query(sql.c_str());
    query.set_sql_replace_char(
        plan->session->get_query_sql_replace_null_char());
    query.pack(&exec_packet);

    // drop table
    handler->send_to_server(conn, &exec_packet);
    handler->receive_from_server(conn, &exec_packet);
    if (driver->is_error_packet(&exec_packet)) {
      LOG_ERROR(
          "MySQLDBScaleMigrateCleanNode get error packet when drop migrate "
          "table.");
      throw ErrorPacketException();
    }

    // clean migrate record
    ret = Driver::get_driver()->get_config_helper()->clean_migrate_item_for_id(
        migrate_id);
    if (!ret) {
      LOG_ERROR(
          "MySQLDBScaleMigrateCleanNode faild to clean migrate info with id "
          "[%s]\n",
          migrate_id);
      throw Error("MySQLDBScaleMigrateCleanNode faild to clean migrate info.");
    }
  } catch (exception &e) {
    LOG_ERROR("MySQLDBScaleMigrateCleanNode got error [%s].\n", e.what());
    if (conn) {
      conn->get_pool()->add_back_to_dead(conn);
      conn = NULL;
    }
    error_packet = Backend::instance()->get_new_packet();
    MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE, e.what(), "42S02",
                             0);
    error.pack(error_packet);
    throw ErrorPacketException();
  }
  if (conn) {
    conn->get_pool()->add_back_to_free(conn);
    conn = NULL;
  }
}
/* class MySQLRenamePartitionedTableNode */
MySQLRenameTableNode::MySQLRenameTableNode(
    ExecutePlan *plan, PartitionedTable *old_table, PartitionedTable *new_table,
    const char *old_schema_name, const char *old_table_name,
    const char *new_schema_name, const char *new_table_name)
    : MySQLInnerNode(plan),
      old_table(old_table),
      new_table(new_table),
      old_schema_name(old_schema_name),
      old_table_name(old_table_name),
      new_schema_name(new_schema_name),
      new_table_name(new_table_name) {
  this->name = "MySQLRenameTableNode";
}

void MySQLRenameTableNode::execute() {
  ACE_Time_Value tv(1000);
  int ret_old = old_table->acquire_rename_table_lock(tv);
  if (ret_old == -1) {
    LOG_ERROR("Rename table failed cause table [%s] is locked.",
              old_table->get_name());
    throw ExecuteNodeError("Rename table failed cause table is locked.");
    return;
  }
  if (old_table != new_table) {
    int ret_new = new_table->acquire_rename_table_lock(tv);
    if (ret_new == -1) {
      LOG_ERROR("Rename table failed cause table [%s] is locked.",
                new_table->get_name());
      old_table->release_rename_table_lock();
      throw ExecuteNodeError("Rename table failed cause table is locked.");
      return;
    }
  }
  try {
    init_row_map();
    MySQLExecuteNode *node = children.front();
    node->execute();
    node->notify_parent();
    Packet *packet = row_map[node]->front();
    string old_full_table_name;
    string new_full_table_name;
    splice_full_table_name(old_schema_name, old_table_name,
                           old_full_table_name);
    splice_full_table_name(new_schema_name, new_table_name,
                           new_full_table_name);
    // before send to client, transfer the auto data
    if (old_table != new_table) {
      new_table->transfer_data(old_table, old_full_table_name,
                               new_full_table_name);
    }
    handler->deal_with_metadata_execute(
        plan->statement->get_stmt_node()->type, plan->statement->get_sql(),
        session->get_schema(), plan->statement->get_stmt_node());
    if (Backend::instance()->table_is_partitioned(new_schema_name,
                                                  new_table_name)) {
      Backend::instance()->alter_table_flag_on(new_full_table_name);
    }
    handler->send_to_client(packet);
    LOG_DEBUG("Rename Table Node send packet %@ to client.\n", packet);
    status = EXECUTE_STATUS_COMPLETE;
    if (old_table != new_table) new_table->release_rename_table_lock();
    old_table->release_rename_table_lock();
  } catch (exception &e) {
    LOG_DEBUG("Rename Table got error.\n");
    status = EXECUTE_STATUS_COMPLETE;
    if (old_table != new_table) new_table->release_rename_table_lock();
    old_table->release_rename_table_lock();
    throw;
  }
}

/* Class MySQLDropMulTableNode */

MySQLDropMulTableNode::MySQLDropMulTableNode(ExecutePlan *plan)
    : MySQLInnerNode(plan),
      warnings(0),
      error_packet(NULL),
      ok_packet(NULL),
      affect_rows(0),
      error_flag(false) {
  this->name = "MySQLDropMulTableNode";
}
void MySQLDropMulTableNode::handle_children() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if (!row_map[*it]->empty()) handle_child(*it);
  }
  status = EXECUTE_STATUS_FETCH_DATA;
}

void MySQLDropMulTableNode::handle_child(MySQLExecuteNode *child) {
  Packet *packet = NULL;
  while (!row_map[child]->empty()) {
    packet = row_map[child]->front();
    row_map[child]->pop_front();
    MySQLOKResponse ok(packet);
    ok.unpack();
    affect_rows += ok.get_affected_rows();
    warnings += ok.get_warnings();
    LOG_DEBUG("After handle modify node %@, affected_row[%d], warnings[%d].\n",
              child, affect_rows, warnings);
    if (!ok_packet) {
      ok_packet = packet;
      LOG_DEBUG("Keep packet %@ for ok packet rebuild.\n", packet);
    } else {
      LOG_DEBUG("Delete packet %@ after handle modify node %@.\n", packet,
                this);
      delete packet;
    }
  }
  LOG_DEBUG("Handle modify node %@ finish.\n", child);
}

void MySQLDropMulTableNode::rebuild_ok_packet() {
  LOG_DEBUG("Rebuild ok packet with affected_rows [%d] warnings [%d].\n",
            affect_rows, warnings);
  if (ok_packet) {
    MySQLOKResponse ok(ok_packet);
    ok.set_affected_rows(affect_rows);
    ok.set_warnings(warnings);
    Packet *new_packet = Backend::instance()->get_new_packet(row_packet_size);
    ok.pack(new_packet);
    delete ok_packet;
    ok_packet = new_packet;
  } else {
    MySQLOKResponse ok(affect_rows, warnings);
    ok_packet = Backend::instance()->get_new_packet(row_packet_size);
    ok.pack(ok_packet);
  }
  ready_rows->push_back(ok_packet);
  ok_packet = NULL;
}

void MySQLDropMulTableNode::execute() {
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START:
        init_row_map();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;
      case EXECUTE_STATUS_FETCH_DATA:
        try {
          children_execute();
        } catch (...) {
          status = EXECUTE_STATUS_COMPLETE;
          throw;
        }
        status = EXECUTE_STATUS_WAIT;
      case EXECUTE_STATUS_WAIT:
        try {
          wait_children();
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw e;
        }
        break;
      case EXECUTE_STATUS_HANDLE:
#ifdef DEBUG
        node_start_timing();
#endif
        handle_children();
#ifdef DEBUG
        node_end_timing();
#endif
        status = EXECUTE_STATUS_FETCH_DATA;
        break;
      case EXECUTE_STATUS_BEFORE_COMPLETE:
        rebuild_ok_packet();
        status = EXECUTE_STATUS_COMPLETE;
        break;
      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        LOG_DEBUG("MySQLDropMulTableNode %@ cost %d ms\n", this,
                  node_cost_time);
#endif
        break;
      default:
        ACE_ASSERT(0);
        break;
    }
  }
}

void MySQLDropMulTableNode::wait_children() {
  LOG_DEBUG("Node %@ start to wait.\n", this);
  if (all_children_finished) {
    status = EXECUTE_STATUS_BEFORE_COMPLETE;
  } else {
    list<MySQLExecuteNode *>::iterator it;
    unsigned int finished_child = 0;
    for (it = children.begin(); it != children.end(); ++it) {
      try {
        if (!(*it)->notify_parent() && (*it)->is_finished()) {
          finished_child++;
        }
      } catch (ErrorPacketException &e) {
        Packet *packet = NULL;
        packet = (*it)->get_error_packet();
        if (get_error_table(packet)) {
          error_flag = true;
          finished_child++;
        } else {
          MySQLErrorResponse error(packet);
          error.unpack();
          error_packet = Backend::instance()->get_new_packet(row_packet_size);
          error.pack(error_packet);
          throw;
        }
      }
    }
    if (finished_child == children.size()) {
      if (error_flag) {
        build_ERROR_packet(&error_packet);
        throw ErrorPacketException();
      } else {
        all_children_finished = true;
        status = EXECUTE_STATUS_BEFORE_COMPLETE;
      }
    } else {
      status = EXECUTE_STATUS_HANDLE;
    }
  }
  LOG_DEBUG("Node %@ wait finished.\n", this);
}

bool MySQLDropMulTableNode::get_error_table(Packet *packet) {
  MySQLErrorResponse error(packet);
  error.unpack();
  string *tmp_table;
  if (error.get_error_code() == 1051) {
    const char *err_msg_start = error.get_error_message();
    char table[100];
    const char *p = err_msg_start;
    bool over_flag = false;
    while (*p != '\0') {
      if (*p != '\'' && *p != ',') {
        p++;
        continue;
      }
      p++;
      int i = 0;
      while (*p != ',' && *p != '\'') {
        if (*p == '\0') {
          over_flag = true;
          break;
        }
        table[i] = *p;
        i++;
        p++;
      }
      if (over_flag) {
        break;
      }
      table[i] = '\0';
      tmp_table = new string(table);
      error_table_list.push_back(tmp_table);
    }
    return true;
  }
  return false;
}

void MySQLDropMulTableNode::build_ERROR_packet(Packet **packet) {
  string error_message("Unknown table '");
  list<string *>::iterator it;
  vector<string *> tmp_table_name;
  for (it = error_table_list.begin(); it != error_table_list.end(); ++it) {
    bool store_flag = true;
    for (unsigned int j = 0; j < tmp_table_name.size(); ++j) {
      if ((*it)->compare(*(tmp_table_name[j]))) {
        continue;
      } else {
        store_flag = false;
        break;
      }
    }
    if (store_flag) {
      tmp_table_name.push_back(*it);
      store_flag = true;
    }
  }
  unsigned int i = 0;
  for (; i < tmp_table_name.size() - 1; ++i) {
    error_message.append(*(tmp_table_name[i]));
    error_message.append(",");
  }
  error_message.append(*(tmp_table_name[i]));
  error_message.append("'");
  LOG_DEBUG("Rebuild error packet with [%s].\n", error_message.c_str());
  if (*packet) {
    MySQLErrorResponse error(*packet);
    error.set_error_code(1051);
    error.set_sqlstate("42S02");
    error.set_error_message(error_message.c_str());
    Packet *new_packet = Backend::instance()->get_new_packet(row_packet_size);
    error.pack(new_packet);
    delete *packet;
    *packet = new_packet;
  } else {
    MySQLErrorResponse error(1051, error_message.c_str(), "42S02");
    *packet = Backend::instance()->get_new_packet(row_packet_size);
    error.pack(*packet);
  }
}

void MySQLDropMulTableNode::do_clean() {
  if (!error_table_list.empty()) {
    list<string *>::iterator it;
    for (it = error_table_list.begin(); it != error_table_list.end(); ++it) {
      delete *it;
    }
    error_table_list.clear();
  }
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
  if (ok_packet) {
    delete ok_packet;
    ok_packet = NULL;
  }
}
// begin MySQLDBScaleShowExecutionProfileNode
MySQLDBScaleShowExecutionProfileNode::MySQLDBScaleShowExecutionProfileNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowExecutionProfileNode";
}

void MySQLDBScaleShowExecutionProfileNode::init_data() {
  try {
    set_head_packet(8);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "profile";
    const char *org_table = "profile";
    list<const char *> columns;
    columns.push_back("ProfileId");
    columns.push_back("ExecuteNode");
    columns.push_back("MonitorPoint");
    columns.push_back("StartTime");
    columns.push_back("EndTime");
    columns.push_back("CostTime(ms)");
    columns.push_back("SerialInfo");
    columns.push_back("ExtraInfo");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
    try {
      ExecuteProfileHandler *execute_profile = session->get_profile_handler();
      vector<struct execution_profile_node *> profile_nodes =
          execute_profile->get_profile_nodes();
      for (unsigned int i = 0; i < profile_nodes.size(); i++) {
        struct execution_profile_node *it = profile_nodes[i];
        list<const char *> row_data;
        char profile_id[20];
        snprintf(profile_id, sizeof(profile_id), "%d", it->profile_id);
        row_data.push_back(profile_id);
        row_data.push_back(it->execute_node_name.c_str());
        row_data.push_back(it->monitor_point.c_str());
        ACE_Date_Time start_time(it->start_time);
        char start_time_str[100];
        snprintf(start_time_str, sizeof(start_time_str), "%02ld:%02ld:%02ld",
                 start_time.hour(), start_time.minute(), start_time.second());
        row_data.push_back(start_time_str);
        ACE_Date_Time end_time(it->end_time);
        char end_time_str[100];
        snprintf(end_time_str, sizeof(end_time_str), "%02ld:%02ld:%02ld",
                 end_time.hour(), end_time.minute(), end_time.second());
        row_data.push_back(end_time_str);
        char cost_time_str[20];
        snprintf(cost_time_str, sizeof(cost_time_str), "%lu", it->cost_time);
        row_data.push_back(cost_time_str);
        char serial_str[20];
        snprintf(serial_str, sizeof(serial_str), "%s",
                 it->is_serial ? "serial" : "parallel");
        row_data.push_back(serial_str);
        char iterator_count[10];
        string extra_info = "";
        if (it->iterator_count > 1) {
          snprintf(iterator_count, sizeof(iterator_count), "%d",
                   it->iterator_count);
          extra_info += "iterator ";
          extra_info += iterator_count;
        }
        extra_info += it->extra_info;
        row_data.push_back(extra_info.c_str());
        add_row_packet(row_data);
      }

    } catch (...) {
      throw;
    }
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Connection get error!",
                     "42S02");
    throw ErrorPacketException();
  }
}

// begin MySQLShowEngineLockWaitingNode
MySQLShowEngineLockWaitingNode::MySQLShowEngineLockWaitingNode(
    ExecutePlan *plan, int engine_type)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLShowEngineLockWaitingNode";
  this->engine_type = engine_type;
}
void MySQLShowEngineLockWaitingNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
  }
}
bool MySQLShowEngineLockWaitingNode::is_have_send(
    vector<DataServer *> &has_send, DataServer *tmp) {
  vector<DataServer *>::iterator s_it = has_send.begin();
  for (; s_it != has_send.end(); s_it++) {
    if (tmp->equal(*s_it)) return true;
  }
  return false;
}
void MySQLShowEngineLockWaitingNode::execute() {
  packet = Backend::instance()->get_new_packet();
  bool send_header_packet = false;
  bool send_field_packet = false;
  const char *sql = NULL;
  const char *check_sql = NULL;
  if (engine_type == INNODB) {
    if (Backend::instance()->get_backend_server_version() == MYSQL_VERSION_8) {
      sql =
          "SELECT 'SERVER_NAME_DBSCALE' server_name, \
                  r.trx_mysql_thread_id waiting_thread,\
                  b.trx_mysql_thread_id blocking_thread,\
                  r.trx_query waiting_query,\
                  b.trx_query blocking_query,\
                  t.lock_data,\
                  (UNIX_TIMESTAMP()- UNIX_TIMESTAMP(r.trx_started)) as blocking_seconds \
          FROM  performance_schema.data_lock_waits w \
          INNER JOIN information_schema.innodb_trx b \
            ON b.trx_id = w.blocking_engine_transaction_id \
          INNER JOIN information_schema.innodb_trx r \
            ON r.trx_id = w.requesting_engine_transaction_id \
          INNER JOIN performance_schema.data_locks t \
            ON b.trx_id = t.engine_transaction_id";
    } else {
      sql =
          "SELECT 'SERVER_NAME_DBSCALE' server_name, \
                  r.trx_mysql_thread_id waiting_thread,\
                  b.trx_mysql_thread_id blocking_thread, \
                  r.trx_query waiting_query,\
                  b.trx_query blocking_query,\
                  t.lock_table,\
                  (UNIX_TIMESTAMP()- UNIX_TIMESTAMP(r.trx_started)) as blocking_seconds \
          FROM  information_schema.innodb_lock_waits w \
          INNER JOIN information_schema.innodb_trx b \
            ON b.trx_id = w.blocking_trx_id \
          INNER JOIN information_schema.innodb_trx r \
            ON r.trx_id = w.requesting_trx_id \
          INNER JOIN information_schema.innodb_locks t \
            ON b.trx_id = t.lock_trx_id";
    }
    check_sql =
        "SELECT COUNT(*) FROM INFORMATION_SCHEMA.ENGINES WHERE ENGINE='InnoDB'";
  } else if (engine_type == TOKUDB) {
    sql =
        "SELECT 'SERVER_NAME_DBSCALE' server_name,\
                r_p.id waiting_thread,\
                b_p.id blocking_thread,\
                r_p.info waiting_query,\
                b_p.info blocking_query,\
                concat(t.lock_waits_table_schema, t.lock_waits_table_name) lock_table \
        FROM  information_schema.TokuDB_lock_waits t \
        INNER JOIN information_schema.TokuDB_trx r \
          ON r.trx_id = t.requesting_trx_id \
        INNER JOIN information_schema.processlist r_p \
          ON r.trx_mysql_thread_id = r_p.id \
        INNER JOIN information_schema.TokuDB_trx b \
          ON b.trx_id = t.blocking_trx_id \
        INNER JOIN information_schema.processlist b_p \
          ON b.trx_mysql_thread_id = b_p.id";
    check_sql =
        "SELECT COUNT(*) FROM INFORMATION_SCHEMA.ENGINES WHERE ENGINE='TokuDB'";
  } else {
    throw ExecuteNodeError("unknown engine type");
  }

  Packet exec_packet;
  Packet header_packet;
  Backend *backend = Backend::instance();
  map<string, DataServer *> servers;
  backend->get_data_servers(servers);
  map<string, DataServer *>::iterator server_it = servers.begin();
  vector<DataServer *> has_send;
  Connection *conn = NULL;

  try {
    for (; server_it != servers.end(); server_it++) {
      DataServer *tmp = server_it->second;
      if ((tmp->get_cur_status() & SERVER_STATUS_MASK) == SERVER_STATUS_DOWN)
        continue;
      if (is_have_send(has_send, tmp)) continue;
      has_send.push_back(tmp);
      try {
        conn = NULL;
        conn = tmp->create_connection(tmp->get_user(), tmp->get_password(),
                                      "information_schema");
        if (!conn) {
          LOG_ERROR("Fail to create connection from server.\n");
          throw ExecuteNodeError("Fail to create connection.");
        }
        string value;
        conn->query_for_one_value(check_sql, value, 0);
        int has_related_engine = atoi(value.c_str());
        if (!has_related_engine) {
          conn->close();
          delete conn;
          conn = NULL;
          continue;
        }
      } catch (...) {
        if (conn) {
          conn->close();
          delete conn;
          conn = NULL;
        }
        throw ExecuteNodeError(
            "get error when MySQLShowEngineLockWaitingNode execute");
      }
      string tmp_sql(sql);
      tmp_sql.replace(8, 19, tmp->get_name());
      MySQLQueryRequest query(tmp_sql.c_str());
      query.set_sql_replace_char(
          plan->session->get_query_sql_replace_null_char());
      query.pack(&exec_packet);

      LOG_DEBUG("Server [%s] get engine lock info .\n", tmp->get_name());
      LOG_DEBUG("Sql is [%s].\n", tmp_sql.c_str());
      handler->send_to_server(conn, &exec_packet);
      // receive header
      handler->receive_from_server(conn, packet);
      if (driver->is_error_packet(packet)) {
        throw ErrorPacketException();
      }
      if (!send_header_packet) {
        handler->send_to_client(packet);
        send_header_packet = true;
      }

      // receive column packets
      handler->receive_from_server(conn, packet);
      while (!driver->is_eof_packet(packet)) {
        if (driver->is_error_packet(packet)) {
          handler->send_to_client(packet);
          throw ErrorPacketException();
        }
        if (!send_field_packet) {
          handler->send_to_client(packet);
        }
        handler->receive_from_server(conn, packet);
      }
      if (!send_field_packet) {
        handler->send_to_client(packet);
        send_field_packet = true;
      }
      // receive row packets
      handler->receive_from_server(conn, packet);
      while (!driver->is_eof_packet(packet)) {
        handler->send_to_client(packet);
        handler->receive_from_server(conn, packet);
      }

      conn->close();
      delete conn;
      conn = NULL;
    }
    if (!send_header_packet) {
      list<const char *> columns;
      columns.push_back("server_id");
      columns.push_back("waiting_query");
      columns.push_back("blocking_query");
      columns.push_back("lock_table");
      MySQLResultSetHeaderResponse result_set_header(columns.size(), 0);
      result_set_header.pack(packet);
      handler->send_to_client(packet);

      while (!columns.empty()) {
        MySQLColumnResponse field("def", "information_schema", "table", "table",
                                  columns.front(), columns.front(), 8,
                                  NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
        field.pack(packet);
        handler->send_to_client(packet);
        columns.pop_front();
      }
      MySQLEOFResponse eof;
      eof.pack(packet);
      handler->deal_autocommit_with_ok_eof_packet(packet);
      handler->send_to_client(packet);
    }
    MySQLEOFResponse eof;
    eof.pack(packet);
    handler->deal_autocommit_with_ok_eof_packet(packet);
    handler->send_to_client(packet);
  } catch (...) {
    status = EXECUTE_STATUS_COMPLETE;
    MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE,
                             "Connection get error!");
    error.pack(packet);
    error_packet = packet;
    if (conn) {
      conn->close();
      delete conn;
    }
    throw ErrorPacketException();
  }
  has_send.clear();
  status = EXECUTE_STATUS_COMPLETE;
}
// class MySQLDBScaleShowLockUsageNode
MySQLDBScaleShowLockUsageNode::MySQLDBScaleShowLockUsageNode(ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowLockUsageNode";
}

void MySQLDBScaleShowLockUsageNode::send_lock_tables_to_client(
    lock_table_node *head, lock_operation operation,
    set<Session *, compare_session>::iterator it) {
  char thread_id[20];
  lock_table_node *tmp = head;
  while (tmp != NULL) {
    if (tmp->item.operation != operation) {
      tmp = tmp->next;
      continue;
    }
    list<const char *> row_data;
    if (tmp == head) {
      snprintf(thread_id, sizeof(thread_id), "%d", (*it)->get_thread_id());
      row_data.push_back(thread_id);
      row_data.push_back((*it)->get_user_addr());
    } else {
      row_data.push_back("");
      row_data.push_back("");
    }
    row_data.push_back(tmp->item.schema_name);
    string str = "";
    str += tmp->item.table_name;
    if (tmp->item.type == LOCK_TYPE_READ) {
      str += " read";
    } else {
      str += " write";
    }
    row_data.push_back(str.c_str());
    if (tmp == head) {
      row_data.push_back("lock tables");
    } else {
      row_data.push_back("");
    }
    tmp = tmp->next;
    add_row_packet(row_data);
  }
}

void MySQLDBScaleShowLockUsageNode::init_data() {
  try {
    set_head_packet(5);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";
    list<const char *> columns;
    columns.push_back("User_id");
    columns.push_back("User_info");
    columns.push_back("Database");
    columns.push_back("Lock_tables");
    columns.push_back("Lock_operation");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
    driver->acquire_session_mutex();
    try {
      set<Session *, compare_session> *session_set = driver->get_session_set();
      set<Session *, compare_session>::iterator it = session_set->begin();
      // traverse every use session
      for (; it != session_set->end(); it++) {
        lock_table_node *lock_node = (*it)->get_lock_head();

        // traverse tables of LOCK_TABLE
        send_lock_tables_to_client(lock_node, LOCK_TABLES, it);
      }
    } catch (...) {
      driver->release_session_mutex();
      throw;
    }
    driver->release_session_mutex();
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Connection get error!",
                     "42S02");
    throw ErrorPacketException();
  }
}

MySQLEstimateSelectNode::MySQLEstimateSelectNode(ExecutePlan *plan,
                                                 DataSpace *dataspace,
                                                 const char *sql,
                                                 Statement *statement)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLEstimateSelectNode";
  this->sql = sql;
  this->dataspace = dataspace;
  this->par_ids = NULL;
  this->par_table = NULL;
  got_error = false;
  packet = NULL;
  error_packet = NULL;
  this->new_stmt = statement->get_stmt_node();
  schema = statement->get_schema();
  this->statement = statement;
}

MySQLEstimateSelectNode::MySQLEstimateSelectNode(ExecutePlan *plan,
                                                 vector<unsigned int> *par_ids,
                                                 PartitionedTable *par_table,
                                                 const char *sql,
                                                 Statement *statement)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLEstimateSelectNode";
  this->sql = sql;
  this->dataspace = NULL;
  this->par_ids = par_ids;
  this->par_table = par_table;
  got_error = false;
  packet = NULL;
  error_packet = NULL;
  this->new_stmt = statement->get_stmt_node();
  this->schema = statement->get_schema();
  this->statement = statement;
}

void MySQLEstimateSelectNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
  if (par_ids) {
    delete par_ids;
    par_ids = NULL;
  }
  status = EXECUTE_STATUS_COMPLETE;
}

void MySQLEstimateSelectNode::generate_select_sql() {
  const char *table_name = new_stmt->table_list_head->join->table_name;
  const char *schema_name = new_stmt->table_list_head->join->schema_name;
  schema_name = schema_name ? schema_name : schema;

  string tmp_sql;
  tmp_sql += "SELECT * ";
  record_scan *rs = new_stmt->scanner;
  unsigned int pos = plan->statement->get_select_part_start_pos(rs);
  tmp_sql += " FROM ";
  tmp_sql += schema_name;
  tmp_sql += ".";
  tmp_sql += table_name;
  tmp_sql += " ";
  if (pos != 0) tmp_sql.append(sql + pos - 1, strlen(sql) - pos + 1);
  exec_sql = tmp_sql;
}

void MySQLEstimateSelectNode::get_estimate_sql() {
  stmt_type type = new_stmt->estimate_type;
  switch (type) {
    case STMT_SELECT: {
      string tmp_sql;
      int select_position = new_stmt->cur_rec_scan->start_pos - 1;
      tmp_sql.append(sql + select_position);
      exec_sql = tmp_sql;
      LOG_DEBUG("MySQLEstimateSelectNode exec_sql = [%s].\n", exec_sql.c_str());
    } break;
    case STMT_INSERT_SELECT: {
      string tmp_sql;
      record_scan *rs = new_stmt->sql->insert_oper->select_values;
      tmp_sql.append(sql + rs->start_pos - 1, strlen(sql) - rs->start_pos + 1);
      exec_sql = tmp_sql;
    } break;
    case STMT_DELETE:
    case STMT_UPDATE: {
      generate_select_sql();
    } break;
    default:
      break;
  }
}

void MySQLEstimateSelectNode::send_estimate_result_to_client(int rows) {
  try {
    MySQLResultSetHeaderResponse result_set_header(1, 0);
    result_set_header.pack(packet);
    handler->send_to_client(packet);

    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "dataservers";
    const char *org_table = "dataservers";
    list<const char *> columns;
    columns.push_back("Rows");
    while (!columns.empty()) {
      MySQLColumnResponse field(catalog, schema, table, org_table,
                                columns.front(), columns.front(), 8,
                                NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
      field.pack(packet);
      handler->send_to_client(packet);
      columns.pop_front();
    }

    MySQLEOFResponse eof;
    eof.pack(packet);
    handler->deal_autocommit_with_ok_eof_packet(packet);
    handler->send_to_client(packet);

    list<const char *> row_data;
    stringstream ss;
    ss << rows;
    string tmp = ss.str();
    row_data.push_back(tmp.c_str());
    MySQLRowResponse row(row_data);
    row.pack(packet);
    handler->send_to_client(packet);
  } catch (exception &e) {
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    LOG_DEBUG("MySQLEstimateSelectNode fail due to exception [%s].\n",
              e.what());
    string error_message("MySQLEstimateSelectNode fail due to exception:");
    error_message.append(e.what());
    throw ExecuteNodeError(error_message.c_str());
    ;
  }

  MySQLEOFResponse end;
  end.pack(packet);
  handler->deal_autocommit_with_ok_eof_packet(packet);
  handler->send_to_client(packet);
}

int MySQLEstimateSelectNode::fetch_affacted_rows(Connection *conn) {
  bool is_row_packet = false;
  list<int> row_values;
  int eof_num = 0;
  Backend *backend = Backend::instance();
  bool is_server_explain_stmt_always_extended =
      backend->get_is_server_explain_stmt_always_extended();
  do {
    handler->receive_from_server(conn, packet);
    if (driver->is_error_packet(packet)) {
      handle_error_packet(packet);
    }
    if (is_row_packet && !(driver->is_eof_packet(packet))) {
      MySQLRowResponse row_response(packet);
      int pos = is_server_explain_stmt_always_extended ? 9 : 8;
      unsigned rows = row_response.get_uint(pos);
      row_values.push_back(rows);
    }
    if (driver->is_eof_packet(packet)) {
      is_row_packet = true;
      eof_num++;
    }
  } while (eof_num != 2);
  int rows_num = 1;
  list<int>::iterator row_values_iter;
  for (row_values_iter = row_values.begin();
       row_values_iter != row_values.end(); row_values_iter++) {
    rows_num = rows_num * (*row_values_iter);
  }
  return rows_num;
}

int MySQLEstimateSelectNode::send_to_server_estimate() {
  unsigned int total_rows = 0;
  unsigned int loop_num = par_ids ? par_ids->size() : 1;
  vector<Connection *> conn_list;
  Connection *conn = NULL;
  try {
    packet = Backend::instance()->get_new_packet();
    unsigned int i = 0, id;
    for (; i < loop_num; i++) {
      if (par_ids) {
        id = par_ids->at(i);
        dataspace = par_table->get_partition(id);
      }

      Packet exec_packet;
      string explain_sql("EXPLAIN ");
      if (dataspace->get_virtual_machine_id() > 0) {
        string new_sql_tmp;
        Parser *parser = MySQLParser::instance();
        Statement *new_stmt_for_shard = NULL;
        try {
          new_stmt_for_shard = parser->parse(
              exec_sql.c_str(), plan->statement->get_allow_dot_in_ident(), true,
              NULL, NULL, NULL,
              handler->get_session()->get_client_charset_type());

          adjust_virtual_machine_schema(
              dataspace->get_virtual_machine_id(),
              dataspace->get_partition_id(), exec_sql.c_str(),
              plan->statement->get_schema(),
              new_stmt_for_shard->get_stmt_node(),
              plan->statement->get_record_scan_all_table_spaces_map(),
              new_sql_tmp);
#ifndef DBSCALE_TEST_DISABLE
          Backend *bk = Backend::instance();
          dbscale_test_info *test_info = bk->get_dbscale_test_info();
          if (!strcasecmp(test_info->test_case_name.c_str(),
                          "test_func_call_fail") &&
              !strcasecmp(test_info->test_case_operation.c_str(),
                          "adjust_virtual_machine_schema")) {
            throw Error("got error when adjust_virtual_machine_schema");
          }
#endif
          new_stmt_for_shard->free_resource();
          delete new_stmt_for_shard;
          new_stmt_for_shard = NULL;
        } catch (...) {
          if (new_stmt_for_shard) {
            LOG_ERROR(
                "Got error when adjust_virtual_machine_schema, the sql is "
                "[%s]\n",
                exec_sql.c_str());
            new_stmt_for_shard->free_resource();
            delete new_stmt_for_shard;
            new_stmt_for_shard = NULL;
          }
          throw;
        }

        explain_sql.append(new_sql_tmp.c_str());
      } else {
        explain_sql.append(exec_sql.c_str());
      }
      MySQLQueryRequest query(explain_sql.c_str());
      query.set_sql_replace_char(
          plan->session->get_query_sql_replace_null_char());
      query.pack(&exec_packet);

      conn = handler->send_to_server_retry(dataspace, &exec_packet, schema,
                                           session->is_read_only());
      conn_list.push_back(conn);
      conn = NULL;
    }
    for (i = 0; i < loop_num; i++) {
      total_rows += fetch_affacted_rows(conn_list[i]);
    }

    for (i = 0; i < loop_num; i++) {
      if (par_ids) {
        id = par_ids->at(i);
        dataspace = par_table->get_partition(id);
      }
      conn_list[i]->reset();
      handler->put_back_connection(dataspace, conn_list[i]);
    }
  } catch (ErrorPacketException &e) {
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    LOG_DEBUG("MySQLEstimateSelectNode fail due to exception [%s].\n",
              e.what());

    for (unsigned int i = 0; i < conn_list.size(); i++) {
      if (par_ids) {
        int id = par_ids->at(i);
        dataspace = par_table->get_partition(id);
      }
      if (session->is_server_shutdown())
        handler->clean_dead_conn(&(conn_list[i]), dataspace);
      else {
        conn_list[i]->reset();
        handler->put_back_connection(dataspace, conn_list[i]);
      }
    }
    throw ErrorPacketException();
    ;

  } catch (exception &e) {
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    LOG_DEBUG("MySQLEstimateSelectNode fail due to exception [%s].\n",
              e.what());
    string error_message("MySQLEstimateSelectNode fail due to exception:");
    error_message.append(e.what());
    for (unsigned int i = 0; i < conn_list.size(); i++) {
      if (par_ids) {
        int id = par_ids->at(i);
        dataspace = par_table->get_partition(id);
      }
      handler->clean_dead_conn(&(conn_list[i]), dataspace);
    }
    throw ExecuteNodeError(error_message.c_str());
    ;
  }

  return total_rows;
}

void MySQLEstimateSelectNode::execute() {
#ifdef DEBUG
  node_start_timing();
#endif

  get_estimate_sql();
  int rows = send_to_server_estimate();
  send_estimate_result_to_client(rows);
#ifdef DEBUG
  node_end_timing();
#endif
#ifdef DEBUG
  LOG_DEBUG("MySQLEstimateSelectNode %@ cost %d ms\n", this, node_cost_time);
#endif
  status = EXECUTE_STATUS_COMPLETE;
}

void MySQLEstimateSelectNode::handle_error_packet(Packet *packet) {
  status = EXECUTE_STATUS_COMPLETE;
  error_packet = packet;
  MySQLErrorResponse response(error_packet);
  if (response.is_shutdown()) {
    session->set_server_shutdown(true);
  }
  throw ErrorPacketException();
}

/* class MySQLSetPriorityNode */
MySQLSetPriorityNode::MySQLSetPriorityNode(ExecutePlan *plan, string user_name,
                                           int tmp_priority_value)
    : MySQLReturnOKNode(plan), error_packet(NULL), user_name(user_name) {
  if (tmp_priority_value > USER_PRIORITY_LOW)
    tmp_priority_value = USER_PRIORITY_LOW;
  else if (tmp_priority_value < USER_PRIORITY_HIGH)
    tmp_priority_value = USER_PRIORITY_HIGH;
  priority_value = (UserPriorityValue)tmp_priority_value;
}

void MySQLSetPriorityNode::do_execute() {
  Driver *driver = Driver::get_driver();
  Backend *backend = Backend::instance();
  backend->acquire_user_priority_mutex();
  backend->set_user_priority(user_name, priority_value);
  driver->acquire_session_mutex();
  set<Session *, compare_session> *session_set = driver->get_session_set();
  set<Session *, compare_session>::iterator it = session_set->begin();
  for (; it != session_set->end(); it++) {
#ifdef DEBUG
    LOG_DEBUG("Set Priority user_name=[%s], session_user_addr=[%s] \n",
              user_name.c_str(), (*it)->get_user_name().c_str());
#endif
    if (!strcasecmp((*it)->get_user_name().c_str(), user_name.c_str()))
      (*it)->set_user_priority(priority_value);
  }
  driver->release_session_mutex();
  backend->release_user_priority_mutex();
}

/* class MySQLReloadConfigNode */
MySQLReloadConfigNode::MySQLReloadConfigNode(ExecutePlan *plan, const char *sql)
    : MySQLReturnOKNode(plan), error_packet(NULL), exec_sql(sql) {
  error_message = "";
}

void MySQLReloadConfigNode::execute_on_slave_dbscale() {
  if (multiple_mode && MultipleManager::instance()->get_is_cluster_master()) {
    MultipleManager *mul = MultipleManager::instance();
    try {
      mul->execute_master_query("dbscale reload slave config");
    } catch (...) {
      LOG_ERROR("Fail to reload config on slave.\n");
    }
  }
}

void MySQLReloadConfigNode::do_execute() {
  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *dynamic_add_mutex =
      backend->get_dynamic_modify_rep_mutex();
  ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_add_mutex);
  Driver::get_driver()->get_config_helper()->init_partitioned_table_config(
      error_message, true);
  Driver::get_driver()->get_config_helper()->init_table_config(error_message,
                                                               true);
  execute_on_slave_dbscale();

  if (!multiple_mode || MultipleManager::instance()->get_is_cluster_master()) {
    if (error_message.length()) {
      set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, error_message.c_str(),
                       "42S02");
      LOG_ERROR("%s\n", error_message.c_str());
      throw ErrorPacketException();
    }
  }
}

void MySQLReloadConfigNode::set_error_packet(uint16_t error_code,
                                             const char *error_message,
                                             const char *sqlstate,
                                             uint8_t number) {
  error_packet = Backend::instance()->get_new_packet();
  MySQLErrorResponse error(error_code, error_message, sqlstate, number);
  error.pack(error_packet);
}

void MySQLReloadConfigNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}

/* class MySQLFlushNode */
MySQLFlushNode::MySQLFlushNode(ExecutePlan *plan, dbscale_flush_type type)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLFlushNode";
  this->type = type;
}

void MySQLFlushNode::do_execute() {
  switch (type) {
    case FLUSH_VIEW: {
      bool reset_view_ok = Driver::get_driver()
                               ->get_config_helper()
                               ->reset_view_config_in_dbscale();
      if (!reset_view_ok) {
        LOG_ERROR("DBScale reset view error due to backend server error.\n");
        throw Error("DBScale reset view error due to backend server error.");
      }
      if (!Backend::instance()->flush_view_map()) {
        LOG_ERROR("Fail to flush backend view map.\n");
        throw Error("Fail to flush backend view map.");
      }
    } break;
    case FLUSH_EVENT: {
      Backend::instance()->flush_events_on_dbscale();
    } break;
    default:
      break;
  }
}

/* class MySQLFlushWeekPwdFileNode */
MySQLFlushWeekPwdFileNode::MySQLFlushWeekPwdFileNode(ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLFlushWeekPwdFileNode";
}

void MySQLFlushWeekPwdFileNode::do_execute() {
  int is_ok = Backend::instance()->load_weak_password_dict();
  if (is_ok != 0) {
    LOG_ERROR("DBScale reload weak password file error.\n");
    throw Error("DBScale reload weak password fileerror.");
  }
}

MySQLCheckDiskIONode::MySQLCheckDiskIONode(ExecutePlan *plan)
    : MySQLReturnOKNode(plan), log_content(), log_file_addr(NULL) {
  this->name = "MySQLCheckDiskIONode";
}

const char *MySQLCheckDiskIONode::log_file_name = "dbscale.io_check";

void MySQLCheckDiskIONode::do_execute() {
  prepare_log_file_addr();
  ACE_FILE_Connector file_connector;
  ACE_FILE_IO log_file;
  prepare_log_content();
  file_connector.connect(log_file, *log_file_addr);
  log_file.truncate(0);
  log_file.send(log_content.c_str(), log_content.size());
  log_file.close();
}

void MySQLCheckDiskIONode::prepare_log_content() {
  log_content.clear();
  boost::posix_time::ptime now = boost::posix_time::second_clock::local_time();
  log_content.append(boost::posix_time::to_iso_extended_string(now));
  log_content.append(" Disk Checking OK\n");
}

size_t dirname_length(const char *name) {
  const char *pos = name - 1;
  const char *gpos = pos++;
  for (; *pos; pos++) {
    if (*pos == DIR_SEPERATOR) gpos = pos;
  }
  return gpos + 1 - name;
}

void convert_dirname(string &to, const char *from, const char *from_end) {
  to.assign(from, from_end - from);
  if (!to.empty() && to.at(to.length() - 1) != DIR_SEPERATOR)
    to.push_back(DIR_SEPERATOR);
}

void MySQLCheckDiskIONode::prepare_log_file_addr() {
  OptionParser *config = OptionParser::instance();
  const char *dbscale_log_file_path =
      config->get_option_str_value("main", "log-file");
  size_t dirname_len = dirname_length(dbscale_log_file_path);
  string full_name;
  convert_dirname(full_name, dbscale_log_file_path,
                  dbscale_log_file_path + dirname_len);
  log_file_addr = new ACE_FILE_Addr(full_name.append(log_file_name).c_str());
}

/* class MySQLPurgePoolInfoNode */
MySQLPurgePoolInfoNode::MySQLPurgePoolInfoNode(ExecutePlan *plan,
                                               pool_node *pool_info)
    : MySQLReturnOKNode(plan), error_packet(NULL) {
  pool_name = pool_info->pool_name;
}

void MySQLPurgePoolInfoNode::purge_connection_pool_for_source(
    DataSource *source) {
  // check source and conn
  if (!source) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "purge pool info failed due to not find this pool",
                     "42S02");
    LOG_ERROR("purge pool info failed due to not find pool\n");
    throw ErrorPacketException();
  }

  if (source->get_data_source_type() != DATASOURCE_TYPE_SERVER) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "purge pool info failed due to not a server source",
                     "42S02");
    LOG_ERROR(
        "purge pool info failed due to pool_name [%s] is not a server source\n",
        source->get_name());
    throw ErrorPacketException();
  }
  Connection *conn = source->get_connection(NULL, false);
  if (!conn) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "purge pool info failed due fail to get connection",
                     "42S02");
    LOG_ERROR(
        "purge pool info failed due to pool_name [%s] fail to get connection\n",
        source->get_name());
    throw ErrorPacketException();
  }

  int ret = Backend::instance()->purge_source_conn_pool(source, conn);
  conn = NULL;
  switch (ret) {
    case -1: {
      set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                       "pool is busy, plz try this command later.", "42S02");
      throw ErrorPacketException();
    }
    case -2: {
      set_error_packet(
          ERROR_EXECUTE_NODE_FAILED_CODE,
          "purge pool info failed due fail to kill connection get exception",
          "42S02");
      throw ErrorPacketException();
    }
    default:
      return;
  }
}

void MySQLPurgePoolInfoNode::do_execute() {
  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *dynamic_add_mutex =
      backend->get_dynamic_modify_rep_mutex();
  ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_add_mutex);
  if (pool_name) {
    DataSource *source = backend->find_data_source(pool_name);
    purge_connection_pool_for_source(source);
  } else {
    list<DataSource *> sources;
    backend->find_data_source_by_type(DATASOURCE_TYPE_SERVER, sources);
    list<DataSource *>::iterator it = sources.begin();
    for (; it != sources.end(); it++) {
      purge_connection_pool_for_source(*it);
    }
  }
}

void MySQLPurgePoolInfoNode::set_error_packet(uint16_t error_code,
                                              const char *error_message,
                                              const char *sqlstate,
                                              uint8_t number) {
  error_packet = Backend::instance()->get_new_packet();
  MySQLErrorResponse error(error_code, error_message, sqlstate, number);
  error.pack(error_packet);
}

void MySQLPurgePoolInfoNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}
/* class MySQLFlushPoolInfoNode */
MySQLFlushPoolInfoNode::MySQLFlushPoolInfoNode(ExecutePlan *plan,
                                               pool_node *pool_info)
    : MySQLReturnOKNode(plan), error_packet(NULL) {
  pool_name = pool_info->pool_name;
  is_force = pool_info->is_force;
  error_message = "";
}
void MySQLFlushPoolInfoNode::flush_connection_pool_for_source(
    DataSource *source) {
  if (!source) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "flush pool info failed due to not find this pool",
                     "42S02");
    LOG_ERROR("flush pool info failed due to not find pool name [%s]\n",
              pool_name);
    throw ErrorPacketException();
  }

  if (source->get_data_source_type() != DATASOURCE_TYPE_SERVER) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "flush pool info failed due to not find this pool",
                     "42S02");
    LOG_ERROR(
        "flush pool info failed due to pool_name [%s] is not a server source\n",
        pool_name);
    throw ErrorPacketException();
  }

  ServerDataSource *server_data_source =
      static_cast<ServerDataSource *>(source);
  vector<ServerConnectionPool *> *conn_pools =
      server_data_source->get_conn_pools();
  vector<ServerConnectionPool *>::iterator it_pool;
  it_pool = conn_pools->begin();
  for (it_pool = conn_pools->begin(); it_pool != conn_pools->end(); it_pool++) {
    if (is_force) {
      if (!(*it_pool)->increase_version_and_flush()) {
        error_message += " flush source ";
        error_message += source->get_name();
        error_message += " failed;";
        return;
      }
    } else {
      (*it_pool)->increase_version();
    }
  }
}
void MySQLFlushPoolInfoNode::do_execute() {
  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *dynamic_add_mutex =
      backend->get_dynamic_modify_rep_mutex();
  ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_add_mutex);
  if (pool_name) {
    DataSource *source = backend->find_data_source(pool_name);
    flush_connection_pool_for_source(source);
  } else {
    list<DataSource *> sources;
    backend->find_data_source_by_type(DATASOURCE_TYPE_SERVER, sources);
    list<DataSource *>::iterator it = sources.begin();
    for (; it != sources.end(); it++) {
      flush_connection_pool_for_source(*it);
    }
  }

  if (error_message.length()) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, error_message.c_str(),
                     "42S02");
    LOG_ERROR("%s\n", error_message.c_str());
    throw ErrorPacketException();
  }
}
void MySQLFlushPoolInfoNode::set_error_packet(uint16_t error_code,
                                              const char *error_message,
                                              const char *sqlstate,
                                              uint8_t number) {
  error_packet = Backend::instance()->get_new_packet();
  MySQLErrorResponse error(error_code, error_message, sqlstate, number);
  error.pack(error_packet);
}

void MySQLFlushPoolInfoNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}

/* class MySQLXARecoverSlaveDBScaleNode */
MySQLXARecoverSlaveDBScaleNode::MySQLXARecoverSlaveDBScaleNode(
    ExecutePlan *plan, const char *xa_source, const char *top_source,
    const char *ka_update_v)
    : MySQLReturnOKNode(plan),
      xa_source_name(xa_source),
      top_source_name(top_source) {
  this->name = "MySQLXARecoverSlaveDBScaleNode";
  istringstream iss(ka_update_v);
  iss >> ka_update_v_cond;
}

void MySQLXARecoverSlaveDBScaleNode::do_execute() {
  LOG_INFO(
      "Start to do handle_xa_recover_on_slave_dbscale with xa_source [%s] "
      "top_source [%s].\n",
      xa_source_name, top_source_name);
  unsigned int retry = 5;
  MultipleManager *mul = MultipleManager::instance();
  do {
    mul->acquire_slave_dbscale_update_ka_version_mutex();
    unsigned long applyed_ka_update_version = mul->get_update_message_version();
    mul->release_slave_dbscale_update_ka_version_mutex();
    if (applyed_ka_update_version >= ka_update_v_cond) {
      break;
    }
    LOG_INFO(
        "MySQLXARecoverSlaveDBScaleNode with condition_ka_update_version %lu "
        "while got_ka_update_version %lu, re-check after 1s.\n",
        ka_update_v_cond, applyed_ka_update_version);
    ACE_OS::sleep(1);
  } while (--retry);
  if (!retry) {
    LOG_ERROR(
        "Fail to wait for got_ka_update_version to be "
        "condition_ka_update_version %lu.\n",
        ka_update_v_cond);
    throw Error(
        "Fail to wait for got_ka_update_version to be "
        "condition_ka_update_version");
  }
  Backend *bk = Backend::instance();
  DataSource *xa_ds = bk->find_data_source(xa_source_name);
  if (xa_ds == NULL) {
    LOG_ERROR("Cannot find xa_source %s for MySQLXARecoverSlaveDBScaleNode.\n",
              xa_source_name);
    throw Error("Cannot find xa_source.");
  }
  if (xa_ds->handle_xa_recover_on_slave_dbscale(top_source_name)) {
    LOG_ERROR(
        "Fail to do handle_xa_recover_on_slave_dbscale with xa_source [%s] "
        "top_source [%s].\n",
        xa_source_name, top_source_name);
    throw Error("Fail to do handle_xa_recover_on_slave_dbscale.");
  }
}

/* class MySQLForceFlashbackOnlineNode */
MySQLForceFlashbackOnlineNode::MySQLForceFlashbackOnlineNode(
    ExecutePlan *plan, const char *server_name)
    : MySQLReturnOKNode(plan), server_name(server_name) {
  this->name = "MySQLForceFlashbackOnlineNode";
}

void MySQLForceFlashbackOnlineNode::do_execute() {
  Backend *bk = Backend::instance();
  DataServer *server = bk->find_data_server(server_name);
  if (server == NULL) {
    LOG_ERROR("Cannot find data-server %s for MySQLForceFlashbackOnlineNode.\n",
              server_name);
    throw Error("Cannot find data-server.");
  }

  MasterOnlineStatus status = server->get_online_status();
  switch (status) {
    case MASTER_FLASHBACK_PURGE_ERROR:
    case MASTER_FLASHBACK_ERROR:
    case MASTER_OFFLINE:
      server->set_online_status(MASTER_FLASHBACK_FINISH);
      server->reset_retry_flashback_count();
      server->send_server_start_message();
      break;
    case MASTER_FLASHBACK:
      LOG_ERROR(
          "Cannot force online for flashback, cause data-server doing "
          "flashback.\n");
      throw Error("Data-server doing flashback");
    default:
      LOG_ERROR(
          "No need do force online for flashback, cause data-server in normal "
          "status.\n");
      throw Error(
          "Data-server in normal status, no need force online flashback");
  }
}

/*class MySQLSetInfoSchemaMirrorTbStatusNode*/
MySQLSetInfoSchemaMirrorTbStatusNode::MySQLSetInfoSchemaMirrorTbStatusNode(
    ExecutePlan *plan, info_mirror_tb_status_node *tb_status)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLSetInfoSchemaMirrorTbStatusNode";
  this->tb_status = tb_status;
}
void MySQLSetInfoSchemaMirrorTbStatusNode::do_execute() {
  if (tb_status->is_internal && multiple_mode &&
      MultipleManager::instance()->get_is_cluster_master()) {
    return;
  }
  while (tb_status) {
    string tb_name =
        string(tb_status->tb_name);  // table_schema.table_name or
                                     // mirror_tb_name.table_schema.table_name
    vector<string> strs;
    boost::split(strs, tb_name, boost::is_any_of("."),
                 boost::token_compress_on);
    size_t n = strs.size();
    if (n != 2 && n != 3) {
      LOG_INFO(
          "MySQLSetInfoSchemaMirrorTbStatusNode got error because because item "
          "[%s] is not"
          " like table_schema.table_name or "
          "mirror_tb_name.table_schema.table_name\n",
          tb_name.c_str());
      throw Error(
          "item name should be like table_schema.table_name or "
          "mirror_tb_name.table_schema.table_name");
    }
    InfoSchemaMirrorTableStatus status;
    if (tb_status->status == 0)
      status = MIRROR_TABLE_STATUS_OUT_OF_DATE;
    else if (tb_status->status == 2)
      status = MIRROR_TABLE_STATUS_UP_TO_DATE;
    else {
      LOG_INFO(
          "MySQLSetInfoSchemaMirrorTbStatusNode got error because status [%d] "
          "of item [%s] is invalid.\n",
          tb_status->status, tb_name.c_str());
      throw Error(
          "MySQLSetInfoSchemaMirrorTbStatusNode got error because status of "
          "table is invalid (0=out of date, 2=up to date)");
    }
    bool is_all_schema = false;
    bool is_all_table = false;
    if (n == 2) {
      if (strs[0] == DBSCALE_RESERVED_STR) {
        is_all_schema = true;
      }
      if (strs[1] == DBSCALE_RESERVED_STR) {
        is_all_table = true;
      }
      Backend::instance()->set_informationschema_all_mirror_tb_status(
          strs[0].c_str(), strs[1].c_str(), is_all_schema, is_all_table,
          status);
    } else {
      Backend::instance()->set_informationschema_mirror_tb_status(
          strs[0], strs[1], strs[2], status);
    }
    tb_status = tb_status->next;
  }
}

MySQLCreateOracleSequenceNode::MySQLCreateOracleSequenceNode(
    ExecutePlan *plan, create_oracle_seq_op_node *oper)
    : MySQLReturnOKNode(plan), error_packet(NULL) {
  this->oper = oper;
}
void MySQLCreateOracleSequenceNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}
void MySQLCreateOracleSequenceNode::set_error_packet(uint16_t error_code,
                                                     const char *error_message,
                                                     const char *sqlstate,
                                                     uint8_t number) {
  error_packet = Backend::instance()->get_new_packet();
  MySQLErrorResponse error(error_code, error_message, sqlstate, number);
  error.pack(error_packet);
}

void MySQLCreateOracleSequenceNode::do_execute() {
  Backend *backend = Backend::instance();
  char sql_seq[1024];
  const char *schema_name =
      oper->schemaname ? oper->schemaname : session->get_schema();
  if (strlen(schema_name) > 128) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "sequence schema name too long", "42S02");
    throw ErrorPacketException();
  }
  if (strlen(oper->seqname) > 128) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "sequence seq name too long", "42S02");
    throw ErrorPacketException();
  }
  if (oper->cache < 0 || oper->cache > 10000000) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "value of 'cache' should not larger than 10000000",
                     "42S02");
    throw ErrorPacketException();
  }
  if (oper->step < 0 || oper->step > 10000000) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "value of 'increment by' should not larger than 10000000",
                     "42S02");
    throw ErrorPacketException();
  }
  if (!oper->is_alter_sequence && !oper->is_drop_sequence) {
    if (oper->step > (oper->max - oper->min)) {
      set_error_packet(
          ERROR_EXECUTE_NODE_FAILED_CODE,
          "value of 'increment' must be less than 'maxvalue' minus 'minvalue'",
          "42S02");
      throw ErrorPacketException();
    }
    if (oper->cycle && (oper->cache > (oper->max - oper->min))) {
      set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                       "value of 'cache' must be less than one cycle", "42S02");
      throw ErrorPacketException();
    }
    if (oper->min > oper->max) {
      set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                       "value of 'minvalue' must be less than 'maxvalue'",
                       "42S02");
      throw ErrorPacketException();
    }
  }
  int64_t valid_max = (int64_t)(9223372036854775807L - 10000 -
                                (oper->cache + 1) * (oper->step));
  if (oper->max < 0 || valid_max < oper->max) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "value of 'MAXVALUE' too large", "42S02");
    throw ErrorPacketException();
  }
  if ((!oper->is_drop_sequence && !oper->is_alter_sequence) &&
      backend->oracle_sequence_exists(schema_name, oper->seqname)) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "sequence already exists",
                     "42S02");
    throw ErrorPacketException();
  }
  if ((oper->is_drop_sequence || oper->is_alter_sequence) &&
      !backend->oracle_sequence_exists(schema_name, oper->seqname)) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "sequence not exists",
                     "42S02");
    throw ErrorPacketException();
  }
  if (oper->is_alter_sequence && oper->alter_what & ALTER_RESET_CACHE) {
    backend->reset_oracle_seq_cache(schema_name, oper->seqname);
    return;
  }
  if (oper->has_start_with && oper->currval < oper->min) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "START WITH cannot be less than MINVALUE", "42S02");
    throw ErrorPacketException();
  }
  if (oper->is_alter_sequence &&
      oper->alter_what & (ALTER_SEQUENCE_MIN | ALTER_SEQUENCE_MAX)) {
    if (!backend->oracle_sequence_exists(schema_name, oper->seqname)) {
      set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "sequence not exists",
                       "42S02");
      throw ErrorPacketException();
    }
    if (!backend->check_alter_seq_min_or_max_value(schema_name, oper)) {
      set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "alter value invalid",
                       "42S02");
      throw ErrorPacketException();
    }
    backend->check_oracle_sequence_version_ok(schema_name, oper->seqname);
    generate_update_seq_sql(schema_name, sql_seq);
    backend->mul_alter_oracle_seq_min_or_max(schema_name, oper->seqname,
                                             sql_seq, oper->alter_what,
                                             oper->min, oper->max);
    return;
  }
  if (!multiple_mode || oper->is_local) {
    if (!multiple_mode ||
        (multiple_mode &&
         MultipleManager::instance()->get_is_cluster_master())) {
      Connection *conn = NULL;
      try {
        conn = backend->get_auth_data_space()->get_connection(session);
        if (!conn) {
          throw Error(
              "Failed to add new oracle sequence because fail to get "
              "connection from auth dataspace");
        }
        if (oper->is_drop_sequence) {
          sprintf(sql_seq,
                  "delete from oracle_sequence_db.oracle_sequence_meta_table "
                  "where schemaname=lower('%s') and seqname=lower('%s')",
                  schema_name, oper->seqname);
        } else if (oper->is_alter_sequence) {
          backend->check_oracle_sequence_version_ok(schema_name, oper->seqname);
          generate_update_seq_sql(schema_name, sql_seq);
        } else {
          sprintf(sql_seq,
                  "insert into "
                  "oracle_sequence_db.oracle_sequence_meta_table(schemaname, "
                  "seqname, val, step, min, max, cache, cache_random_offset, "
                  "cycle, version) "
                  "values (lower('%s'),lower('%s'), %ld, %ld, %ld, %ld, %ld, "
                  "%ld, %u, 1)",
                  schema_name, oper->seqname,
                  oper->has_start_with ? oper->currval : oper->min, oper->step,
                  oper->min, oper->max, oper->cache, oper->cache_random_offset,
                  oper->cycle);
        }
        conn->execute_one_modify_sql(sql_seq);
        conn->get_pool()->add_back_to_free(conn);
        conn = NULL;
      } catch (...) {
        if (conn) {
          conn->get_pool()->add_back_to_dead(conn);
          conn = NULL;
        }
        status = EXECUTE_STATUS_COMPLETE;
        throw;
      }
    }
    try {
      if (oper->is_drop_sequence) {
        backend->remove_oracle_sequence(schema_name, oper->seqname);
      } else if (oper->is_alter_sequence) {
        backend->update_oracle_sequence(schema_name, oper);
      } else {
        backend->add_new_oracle_sequence(oper, schema_name);
      }
    } catch (exception &e) {
      set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, e.what(), "42S02");
      throw ErrorPacketException();
    }
  } else {
    string sql = plan->statement->get_sql();
    string tmp;
    string postfix;
    int sql_end_pos = oper->sql_end_pos;
    if (sql.size() == string::size_type(sql_end_pos)) {
      tmp = sql;
    } else {
      tmp = string(sql.begin(), sql.begin() + sql_end_pos);
      postfix = string(sql.begin() + sql_end_pos, sql.end());
    }
    tmp.append(" LOCAL");
    if (!oper->schemaname) {
      tmp.append(" SCHEMA ");
      tmp.append(schema_name);
    }
    sql = tmp.append(postfix);
    int failed_count =
        MultipleManager::instance()->execute_modify_sql_on_all_dbscale_nodes(
            sql, false, false);
    if (failed_count > 0) {
      set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                       "got error when create oracle sequence, maybe it is "
                       "partial created, plz drop it forcely",
                       "42S02");
      throw ErrorPacketException();
    }
  }
}

void MySQLCreateOracleSequenceNode::generate_update_seq_sql(
    const char *schema_name, char *sql) {
  sprintf(sql, "UPDATE oracle_sequence_db.oracle_sequence_meta_table SET ");
  if (oper->alter_what & ALTER_STEP) {
    sprintf(sql + strlen(sql), " step = %ld,", oper->step);
  }
  if (oper->alter_what & ALTER_CACHE_VAL) {
    sprintf(sql + strlen(sql), " cache = %ld,", oper->cache);
  }
  if (oper->alter_what & ALTER_CACHE_RANDOM_OFFSET) {
    sprintf(sql + strlen(sql), " cache_random_offset = %ld,",
            oper->cache_random_offset);
  }
  if (oper->alter_what & ALTER_SEQUENCE_MIN)
    sprintf(sql + strlen(sql), " min = %ld,", oper->min);
  if (oper->alter_what & ALTER_SEQUENCE_MAX)
    sprintf(sql + strlen(sql), " max = %ld,", oper->max);
  sprintf(
      sql + strlen(sql),
      " version=version+1 WHERE schemaname=lower('%s') and seqname=lower('%s')",
      schema_name, oper->seqname);
}

/* class MySQLResetZooInfoNode */

MySQLResetZooInfoNode::MySQLResetZooInfoNode(ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {}

void MySQLResetZooInfoNode::do_execute() {
  ZooKeeperTool *zk = ZooKeeperTool::instance();
  zk->set_node_message(ZK_CONFIG_FILE_NODE, "", 0, -1);
}

/* class MySQLResetPoolInfoNode */
MySQLResetPoolInfoNode::MySQLResetPoolInfoNode(ExecutePlan *plan,
                                               pool_node *pool_info)
    : MySQLReturnOKNode(plan), error_packet(NULL) {
  type = pool_info->type;
  pool_name = pool_info->pool_name;
  max_value = pool_info->max_value;
  is_internal = pool_info->is_internal;
}

void MySQLResetPoolInfoNode::set_connection_pool_for_source(
    DataSource *source) {
  if (!source) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "reset pool info failed due to not find this pool",
                     "42S02");
    LOG_ERROR("reset pool info failed due to not find pool name [%s]\n",
              pool_name);
    throw ErrorPacketException();
  }

  if (source->get_data_source_type() != DATASOURCE_TYPE_SERVER) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "reset pool info failed due to not find this pool",
                     "42S02");
    LOG_ERROR(
        "reset pool info failed due to pool_name [%s] is not a server source\n",
        pool_name);
    throw ErrorPacketException();
  }

  ServerDataSource *server_data_source =
      static_cast<ServerDataSource *>(source);
  vector<ServerConnectionPool *> *conn_pools =
      server_data_source->get_conn_pools();

  // to keep one source's conn_pool all share same max_water_mark
  // reset all pool for one source
  vector<ServerConnectionPool *>::iterator it_pool;
  it_pool = conn_pools->begin();
  int max_water_mark = (int)((*it_pool)->get_max_water_mark());
  int change_num = (max_value - max_water_mark) * conn_pool_num;

  OptionParser *config = OptionParser::instance();
  if (!config->dynamic_check_max_open_file(change_num)) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "pool max too high, overflowed limit file num", "42S02");
    LOG_ERROR("pool max too high, overflowed limit file num\n");
    throw ErrorPacketException();
  }
  // loop the connection pool
  for (it_pool = conn_pools->begin(); it_pool != conn_pools->end(); it_pool++) {
    if (!(*it_pool)->set_max_water_mark(max_value)) {
      set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                       "set pool max lower than high water mark or total",
                       "42S02");
      LOG_ERROR("set pool max lower than high water mark\n");
      throw ErrorPacketException();
    }
    config->dynamic_reset_max_open_file(change_num);
  }
  server_data_source->set_max_water_mark(max_value);
}

void MySQLResetPoolInfoNode::set_connection_pool() {
  Backend *backend = Backend::instance();
  if (pool_name) {
    DataSource *source = backend->find_data_source(pool_name);
    set_connection_pool_for_source(source);
  } else {
    list<DataSource *> sources;
    backend->find_data_source_by_type(DATASOURCE_TYPE_SERVER, sources);
    list<DataSource *>::iterator it = sources.begin();
    for (; it != sources.end(); it++) {
      // reset connection pool for each source.
      // may be partial execute.
      // it means some source has been reset. others not cause overflowed ulimit
      set_connection_pool_for_source(*it);
    }
  }
}

template <typename T>
void MySQLResetPoolInfoNode::reset_pool_max_water_mark(Pool<T> *pool) {
  int max_water_mark = (int)(pool->get_max_water_mark());
  int change_num = max_value - max_water_mark;

  OptionParser *config = OptionParser::instance();
  if (!config->dynamic_check_max_open_file(change_num)) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "pool max too high, overflowed limit file num", "42S02");
    LOG_ERROR("pool max too high, overflowed limit file num\n");
    throw ErrorPacketException();
  }
  if (!pool->set_max_water_mark(max_value)) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "set pool max lower than high water mark or total",
                     "42S02");
    LOG_ERROR("set pool max lower than high water mark\n");
    throw ErrorPacketException();
  }
  config->dynamic_reset_max_open_file(change_num);
}

void MySQLResetPoolInfoNode::set_login_pool() {
  Frontend *frontend = Frontend::instance();
  Pool<Handler> *handler_pool = frontend->get_acceptor()->get_pool();
  reset_pool_max_water_mark(handler_pool);
  thread_pool_max = max_value;
}
void MySQLResetPoolInfoNode::set_backend_pool() {
  Backend *backend = Backend::instance();
  vector<Pool<BackendThread> *> *pool_list =
      backend->get_all_backend_thread_pool();
  vector<Pool<BackendThread> *>::iterator it = pool_list->begin();
  for (; it != pool_list->end(); it++) {
    reset_pool_max_water_mark(*it);
  }
  backend_thread_pool_max = max_value;
}
void MySQLResetPoolInfoNode::set_handler_pool() {
  if (!SocketEventBaseManager::instance()->reset_pool_max_water_mark(
          max_value)) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "pool max water mark is invalid", "42S02");
    LOG_ERROR(
        "pool max too high, overflowed limit file num or set pool max lower "
        "than high water mark\n");
    throw ErrorPacketException();
  }
  handler_thread_pool_max = max_value;
}
void MySQLResetPoolInfoNode::do_execute() {
  if (max_value <= 0) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "pool max must larger than 0 ", "42S02");
    throw ErrorPacketException();
  }
  {
    Backend *backend = Backend::instance();
    ACE_RW_Thread_Mutex *dynamic_add_mutex =
        backend->get_dynamic_modify_rep_mutex();
    ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_add_mutex);
    switch (type) {
      case CONNECTION_POOL: {
        set_connection_pool();
      } break;
      case LOGIN_HANDLER_THREAD_POOL: {
        set_login_pool();
      } break;
      case BACKEND_THREAD_POOL: {
        set_backend_pool();
      } break;
      case HANDLER_THREAD_POOL: {
        set_handler_pool();
      } break;
      default: {
        set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Unknown POOL TYPE",
                         "42S02");
        throw ErrorPacketException();
      } break;
    }
  }

  if (multiple_mode && MultipleManager::instance()->get_is_cluster_master() &&
      !is_internal) {
    string sql = plan->statement->get_sql();
    string new_sql = sql + " internal;";
    MultipleManager *mul = MultipleManager::instance();
    try {
      mul->execute_master_query(new_sql);
    } catch (...) {
      LOG_ERROR("Fail to reset pool on slave.\n");
      set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                       "Fail to reset pool on slave", "42S02");
      throw ErrorPacketException();
    }
    acquire_start_config_lock(session->get_zk_start_config_lock_extra_info());
    Backend::instance()->flush_config_to_zoo();
    release_start_config_lock();
    return;
  }
}
void MySQLResetPoolInfoNode::set_error_packet(uint16_t error_code,
                                              const char *error_message,
                                              const char *sqlstate,
                                              uint8_t number) {
  error_packet = Backend::instance()->get_new_packet();
  MySQLErrorResponse error(error_code, error_message, sqlstate, number);
  error.pack(error_packet);
}

void MySQLResetPoolInfoNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}
/*MySQLResetInfoPlanNode*/
MySQLResetInfoPlanNode::MySQLResetInfoPlanNode(ExecutePlan *plan,
                                               dbscale_reset_info_op_node *oper)
    : MySQLReturnOKNode(plan), error_packet(NULL) {
  reset_type = oper->reset_type;
  is_internal = oper->is_internal;
}

void MySQLResetInfoPlanNode::do_execute() {
  switch (reset_type) {
    case TPS_INFO:
      reset_tps_info();
      break;
    default: {
      error_packet = Backend::instance()->get_new_packet();
      MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE,
                               "Unknown reset info TYPE", "42S02", 0);
      error.pack(error_packet);
      throw ErrorPacketException();
    }
  }

  if (multiple_mode && MultipleManager::instance()->get_is_cluster_master() &&
      !is_internal) {
    string sql = plan->statement->get_sql();
    string new_sql = sql + " internal;";
    MultipleManager *mul = MultipleManager::instance();
    try {
      mul->execute_master_query(new_sql);
    } catch (...) {
      LOG_ERROR("Fail to reset info on slave.\n");
      error_packet = Backend::instance()->get_new_packet();
      MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE,
                               "Fail to reset info on slave", "42S02", 0);
      error.pack(error_packet);
      throw ErrorPacketException();
    }
    return;
  }
}

void MySQLResetInfoPlanNode::reset_tps_info() {
  Backend::instance()->dbscale_tps_counting_start_time = ACE_OS::gettimeofday();
  Driver *driver = Driver::get_driver();
  driver->acquire_session_mutex();
  set<Session *, compare_session> *session_set = driver->get_session_set();
  set<Session *, compare_session>::iterator it = session_set->begin();
  for (; it != session_set->end(); ++it) {
    ((MySQLSession *)(*it))->reset_dbscale_status_item(TIMES_TXN);
  }
  driver->release_session_mutex();
  DBScaleStatus::instance()->reset_status_item(TIMES_TXN, 0);
}

void MySQLResetInfoPlanNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}

/* MySQLDBScaleShowPoolVersionNode */
MySQLDBScaleShowPoolVersionNode::MySQLDBScaleShowPoolVersionNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {}
void MySQLDBScaleShowPoolVersionNode::get_connection_pool_for_source(
    DataSource *source) {
  if (!source) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "flush pool info failed due to not find this pool",
                     "42S02");
    LOG_ERROR("flush pool info failed due to not find pool\n");
    throw ErrorPacketException();
  }

  if (source->get_data_source_type() != DATASOURCE_TYPE_SERVER) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "flush pool info failed due to not find this pool",
                     "42S02");
    LOG_ERROR(
        "flush pool info failed due to pool_name [%s] is not a server source\n",
        source->get_name());
    throw ErrorPacketException();
  }

  ServerDataSource *server_data_source =
      static_cast<ServerDataSource *>(source);
  Connection *conn = server_data_source->get_connection(NULL, false);
  if (conn) {
    list<const char *> row_data;
    char pool_version[50];
    char connection[50];
    char connection_version[50];
    sprintf(pool_version, "%d", conn->get_pool()->get_version());
    sprintf(connection, "%p", conn);
    sprintf(connection_version, "%d", conn->version);
    row_data.push_back(source->get_name());
    row_data.push_back(pool_version);
    row_data.push_back(connection);
    row_data.push_back(connection_version);
    add_row_packet(row_data);
    conn->get_pool()->add_back_to_free(conn);
  } else {
    LOG_ERROR("get pool info failed due to pool [%s] can not get connection\n",
              server_data_source->get_name());
    list<const char *> row_data;
    row_data.push_back(source->get_name());
    row_data.push_back("Error");
    row_data.push_back("Error");
    row_data.push_back("Error");
    add_row_packet(row_data);
  }
}
void MySQLDBScaleShowPoolVersionNode::init_data() {
  // set the header
  set_head_packet(4);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "dataservers";
  const char *org_table = "dataservers";
  list<const char *> columns;
  columns.push_back("pool name");
  columns.push_back("pool version");
  columns.push_back("connection");
  columns.push_back("connection version");
  Backend *backend = Backend::instance();
  while (!columns.empty()) {
    add_column_packet(catalog, schema, table, org_table, columns.front(),
                      columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    columns.pop_front();
  }
  ACE_RW_Thread_Mutex *dynamic_mutex = backend->get_dynamic_modify_rep_mutex();
  ACE_Read_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_mutex);
  const char *pool_name =
      plan->statement->get_stmt_node()->sql->show_pool_info_oper->server_name;
  if (pool_name) {
    DataSource *source = backend->find_data_source(pool_name);
    get_connection_pool_for_source(source);
  } else {
    list<DataSource *> sources;
    backend->find_data_source_by_type(DATASOURCE_TYPE_SERVER, sources);
    list<DataSource *>::iterator it = sources.begin();
    for (; it != sources.end(); it++) {
      get_connection_pool_for_source(*it);
    }
  }
}

/* class MySQLDisableServer */
MySQLDisableServer::MySQLDisableServer(ExecutePlan *plan)
    : MySQLReturnOKNode(plan), error_packet(NULL) {}
void MySQLDisableServer::do_execute() {
  const char *server_name =
      plan->statement->get_stmt_node()->sql->disable_server_oper->server_name;
  bool is_disable =
      plan->statement->get_stmt_node()->sql->disable_server_oper->is_disable;

  Backend *backend = Backend::instance();
  DataServer *server = backend->find_data_server(server_name);
  if (!server) {
    LOG_ERROR("execute disable_server [%s] fail to find server\n", server_name);
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "the server is not exist",
                     "42S02");
    throw ErrorPacketException();
  }
  {
    ACE_RW_Thread_Mutex *dynamic_mutex =
        backend->get_dynamic_modify_rep_mutex();
    ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_mutex);
    bool ret = backend->disable_server(server, is_disable);
    if (!ret) {
      LOG_ERROR("execute disable_server [%s] get exception\n", server_name);
      set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                       "disable server get exception", "42S02");
      throw ErrorPacketException();
    }
  }
}

void MySQLDisableServer::set_error_packet(uint16_t error_code,
                                          const char *error_message,
                                          const char *sqlstate,
                                          uint8_t number) {
  error_packet = Backend::instance()->get_new_packet();
  MySQLErrorResponse error(error_code, error_message, sqlstate, number);
  error.pack(error_packet);
}

void MySQLDisableServer::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}

/* class MySQLBlockNode */
MySQLBlockNode::MySQLBlockNode(ExecutePlan *plan)
    : MySQLReturnOKNode(plan), error_packet(NULL) {}
void MySQLBlockNode::do_execute() {
  MigrateBlockType block_type =
      plan->statement->get_stmt_node()->sql->block_oper->block_type;
  join_node *node = plan->statement->get_stmt_node()->sql->block_oper->table;
  const char *schema_name =
      node->schema_name ? node->schema_name : plan->session->get_schema();
  const char *table_name = node->table_name;
  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *dynamic_mutex = backend->get_dynamic_modify_rep_mutex();
  ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_mutex);
  switch (block_type) {
    case BLOCK_ALL_TYPE: {
      unsigned int block_time =
          plan->statement->get_stmt_node()->sql->block_oper->block_time;
      if (block_time == 0) {
        block_time = check_using_table_times;
      }

      LOG_DEBUG("block_all_request for table [%s.%s]\n", schema_name,
                table_name);
      MigrateBlockType old_block_type =
          backend->get_table_block_type(schema_name, table_name);
      if (!backend->block_table(schema_name, table_name, BLOCK_ALL_TYPE,
                                false)) {
        LOG_ERROR(
            "block_all_request for table [%s.%s] failed, maybe table is "
            "doing migrate\n",
            schema_name, table_name);
        set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                         "block all request failed", "42S02");
        throw ErrorPacketException();
      }

      unsigned int check_times = 0;
      while (backend->is_table_in_using(schema_name, table_name)) {
        if (check_times > block_time) {
          // recover block type to old block type
          if (old_block_type == NON_BLOCK_TYPE)
            backend->stop_block_table(schema_name, table_name, false);
          else if (old_block_type == BLOCK_PARTITION_TYPE)
            backend->block_table(schema_name, table_name, BLOCK_PARTITION_TYPE,
                                 false);
          LOG_ERROR(
              "block_all_request for table [%s.%s] failed due to overtime\n",
              schema_name, table_name);
          set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                           "block table overtime", "42S02");
          throw ErrorPacketException();
          break;
        }
        check_times++;
        ACE_OS::sleep(1);
      }
    } break;
    case NON_BLOCK_TYPE: {
      LOG_DEBUG("stop block table [%s.%s]\n", schema_name, table_name);
      backend->stop_block_table(schema_name, table_name, false);
    } break;
    case BLOCK_PARTITION_TYPE: {
      LOG_DEBUG("block table partition for table [%s.%s]\n", schema_name,
                table_name);
      if (!backend->block_table(schema_name, table_name, BLOCK_PARTITION_TYPE,
                                false)) {
        LOG_ERROR(
            "block table partition for table [%s.%s] failed, maybe table is "
            "doing migrate\n",
            schema_name, table_name);
        set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                         "block table partition failed", "42S02");
        throw ErrorPacketException();
      }
    } break;
    default: {
      set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Unknown Block TYPE",
                       "42S02");
      throw ErrorPacketException();
    } break;
  }
}
void MySQLBlockNode::set_error_packet(uint16_t error_code,
                                      const char *error_message,
                                      const char *sqlstate, uint8_t number) {
  error_packet = Backend::instance()->get_new_packet();
  MySQLErrorResponse error(error_code, error_message, sqlstate, number);
  error.pack(error_packet);
}

void MySQLBlockNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}

MySQLDBScaleShowPoolInfoNode::MySQLDBScaleShowPoolInfoNode(ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowPoolInfoNode";
}

void MySQLDBScaleShowPoolInfoNode::init_data() {
  // set the header
  set_head_packet(10);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "dataservers";
  const char *org_table = "dataservers";
  int category =
      plan->statement->get_stmt_node()->sql->show_pool_info_oper->pool_category;
  desc_or_null desc =
      plan->statement->get_stmt_node()->sql->show_pool_info_oper->desc_type;
  list<const char *> columns;
  if (category == SHOW_CONNECTION_POOL)
    columns.push_back("Server name");
  else if (category == SHOW_THREAD_POOL)
    columns.push_back("Thread_Pool");
  else if (category == SHOW_TMP_TABLE_POOL)
    columns.push_back("Real table name");
  else {
    // TODO backend_thread_pool
    throw NotSupportedError("Not supported this pool currently.");
  }
  columns.push_back("Pool name");
  columns.push_back("Used");
  columns.push_back("Free");
  columns.push_back("Dead");
  columns.push_back("Total");
  columns.push_back("Max");
  columns.push_back("Times_s");
  columns.push_back("Times_tens");
  columns.push_back("Times_m");
  while (!columns.empty()) {
    add_column_packet(catalog, schema, table, org_table, columns.front(),
                      columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    columns.pop_front();
  }
  try {
    list<PoolInfo *> *pool_info;
    if (category == SHOW_CONNECTION_POOL) {
      pool_info = get_connection_pool_info();
    } else if (category == SHOW_THREAD_POOL) {
      pool_info = get_thread_pool_info();
    } else if (category == SHOW_TMP_TABLE_POOL) {
      pool_info = get_tmp_table_pool_info(desc);
    } else {
      /* We will support backend_thread_pool in the future. */
      throw NotSupportedError("Not supported this pool currently.");
    }

    if (!pool_info || 0 == pool_info->size()) {
      if (pool_info) {
        delete pool_info;
        pool_info = NULL;
      }
      if (category == SHOW_TMP_TABLE_POOL)
        throw Error("No table pool.");
      else
        throw Error("No Such Server.");
      return;
    }
    list<PoolInfo *>::iterator it;
    for (it = pool_info->begin(); it != pool_info->end(); it++) {
      list<const char *> row_data;
      string tmp_server_name((*it)->server_name);
      string tmp_pool_name((*it)->pool_name);
      row_data.push_back(tmp_server_name.c_str());
      row_data.push_back(tmp_pool_name.c_str());
      char used[MAX_LENGTH_NUM];
      char free[MAX_LENGTH_NUM];
      char dead[MAX_LENGTH_NUM];
      char total[MAX_LENGTH_NUM];
      char max[MAX_LENGTH_NUM];
      char times_s[MAX_LENGTH_NUM];
      char times_tens[MAX_LENGTH_NUM];
      char times_m[MAX_LENGTH_NUM];
      sprintf(used, "%d", (*it)->used);
      sprintf(free, "%d", (*it)->free);
      sprintf(dead, "%d", (*it)->dead);
      sprintf(total, "%d", (*it)->total);
      sprintf(max, "%d", (*it)->max);
      sprintf(times_s, "%d", (*it)->times_s);
      sprintf(times_tens, "%d", (*it)->times_tens);
      sprintf(times_m, "%d", (*it)->times_m);
      row_data.push_back(used);
      row_data.push_back(free);
      row_data.push_back(dead);
      row_data.push_back(total);
      row_data.push_back(max);
      row_data.push_back(times_s);
      row_data.push_back(times_tens);
      row_data.push_back(times_m);
      add_row_packet(row_data);
      delete *it;
    }
    if (pool_info) {
      delete pool_info;
      pool_info = NULL;
    }
  } catch (...) {
    if (category == SHOW_TMP_TABLE_POOL)
      set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "No table pool.",
                       "42S02");
    else
      set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "No such server.",
                       "42S02");
    throw ErrorPacketException();
  }
}

list<PoolInfo *> *MySQLDBScaleShowPoolInfoNode::get_connection_pool_info() {
  const char *server_name =
      plan->statement->get_stmt_node()->sql->show_pool_info_oper->server_name;
  return Backend::instance()->get_backend_conn_pool_info(server_name);
}

list<PoolInfo *> *MySQLDBScaleShowPoolInfoNode::get_tmp_table_pool_info(
    desc_or_null desc_type) {
  return Backend::instance()->get_backend_tmp_table_pool_info(desc_type);
}

list<PoolInfo *> *MySQLDBScaleShowPoolInfoNode::get_thread_pool_info() {
  Frontend *frontend = Frontend::instance();
  Backend *backend = Backend::instance();
  Pool<Handler> *handler_pool = frontend->get_acceptor()->get_pool();
  Pool<BackendThread> *backend_pool = backend->get_backend_thread_pool();

  list<PoolInfo *> *pool_infos = new list<PoolInfo *>();
  PoolInfo *pool_info = NULL;
  if (handler_pool) {
    pool_info = handler_pool->get_pool_info();
    pool_info->server_name = string("LOGIN HANDLER THREAD");
    pool_infos->push_back(pool_info);
  }
  if (backend_pool) {
    vector<Pool<BackendThread> *> *pool_list =
        backend->get_all_backend_thread_pool();
    vector<Pool<BackendThread> *>::iterator it = pool_list->begin();
    for (; it != pool_list->end(); it++) {
      pool_info = (*it)->get_pool_info();
      pool_info->server_name = string("BACKEND THREAD");
      pool_infos->push_back(pool_info);
    }
  }
  SocketEventBaseManager::instance()->fullfil_thread_pool_list(pool_infos);
  return pool_infos;
}

/* class MySQLReturnOKNode */
MySQLReturnOKNode::MySQLReturnOKNode(ExecutePlan *plan)
    : MySQLExecuteNode(plan), ok_packet(NULL), need_set_packet_number(true) {
  this->name = "MySQLReturnOKNode";
}

void MySQLReturnOKNode::set_ok_packet(Packet *p, bool need_set_packet_number) {
  ok_packet = p;
  this->need_set_packet_number = need_set_packet_number;
}

void MySQLReturnOKNode::execute() {
  try {
    do_execute();
    Packet tmp_packet;
    if (!ok_packet) {
      ok_packet = &tmp_packet;
      MySQLOKResponse ok(0, 0);
      ok.pack(&tmp_packet);
    }
    if (!session->is_call_store_procedure()) {
      handler->deal_autocommit_with_ok_eof_packet(ok_packet);
      handler->send_to_client(ok_packet, need_set_packet_number);
    }
  } catch (exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    LOG_ERROR("Execute Node fail in MySQLReturnOKNode due to [%s].\n",
              e.what());
    throw;
  }

  status = EXECUTE_STATUS_COMPLETE;
}

/* class MySQLDBScaleSetAutoIncrementOffsetNode */
MySQLDBScaleSetAutoIncrementOffsetNode::MySQLDBScaleSetAutoIncrementOffsetNode(
    ExecutePlan *plan, PartitionedTable *tab)
    : MySQLReturnOKNode(plan), tab(tab) {
  this->name = "MySQLDBScaleSetAutoIncrementOffsetNode";
}

void MySQLDBScaleSetAutoIncrementOffsetNode::do_execute() {
  LOG_DEBUG("MySQLDBScaleSetAutoIncrementOffsetNode\n");
  join_node *node =
      plan->statement->get_stmt_node()->sql->auto_increment_info_oper->table;
  const char *table_name = node->table_name;
  const char *schema_name =
      node->schema_name ? node->schema_name : plan->session->get_schema();
  string full_table_name;
  splice_full_table_name(schema_name, table_name, full_table_name);

  Backend *backend = Backend::instance();
  if (Backend::instance()->has_auto_increment_field(full_table_name)) {
    ACE_Thread_Mutex *stmt_lock = tab->get_stmt_autoinc_lock(full_table_name);
    if (stmt_lock) stmt_lock->acquire();
    ACE_Thread_Mutex *row_lock = tab->get_row_autoinc_lock(full_table_name);
    if (row_lock) row_lock->acquire();
    try {
      int64_t offset = plan->statement->get_stmt_node()
                           ->sql->auto_increment_info_oper->offset;
      if (multiple_mode) {
        int64_t auto_increment_base_value =
            tab->get_auto_increment_base_value(full_table_name);
        int cluster_id = backend->get_cluster_id();
        int server_num = backend->get_cluster_server_num();
        if (offset < auto_increment_base_value + cluster_id) {
          LOG_ERROR(
              "In multiple_mode, dynamic set offset [%d] should larger than "
              "base value[%d]\n",
              offset, auto_increment_base_value + cluster_id);
          throw Error(
              "In multiple_mode, dynamic set offset should larger than base "
              "value");
        }
        int adjust =
            (offset - auto_increment_base_value - cluster_id) % server_num;
        if (adjust) adjust = server_num - adjust;
        LOG_DEBUG("adjust Auto_increment offset [%d] increase [%d]\n", offset,
                  adjust);
        offset += adjust;
      }
      tab->set_auto_increment_value(full_table_name, offset);
    } catch (...) {
      if (row_lock) row_lock->release();
      if (stmt_lock) stmt_lock->release();
      LOG_ERROR("MySQLDBScaleSetAutoIncrementOffsetNode failed\n");
      throw Error("MySQLDBScaleSetAutoIncrementOffsetNode failed.");
    }
    if (row_lock) row_lock->release();
    if (stmt_lock) stmt_lock->release();
  }
}

/* class MySQLShutDownNode */
MySQLShutDownNode::MySQLShutDownNode(ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLShutDownNode";
}

void MySQLShutDownNode::do_execute() {
  LOG_INFO("DBScale shutdown normally.\n");
  LOG_INFO("before dbscale exit, flush config to file\n");
  OptionParser *parser = OptionParser::instance();
  Backend::instance()->flush_config_to_file(parser->get_config_file().c_str(),
                                            false);
  ACE_Reactor::instance()->end_event_loop();
}
/* class MySQLResetTmpTableNode */
MySQLResetTmpTableNode::MySQLResetTmpTableNode(ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLResetTmpTableNode";
}

void MySQLResetTmpTableNode::do_execute() {
  dbscale_reset_tmp_table_op_node *reset_oper =
      plan->statement->get_stmt_node()->sql->dbscale_reset_tmp_table_oper;
  int type = reset_oper->type;
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    MultipleManager *mul = MultipleManager::instance();
    if (type == RESET_REINIT) {
      try {
        // type == 2 is used just for dbscale internal, the command type that
        // master sent to slave is 2.
        mul->execute_master_query("DBSCALE RESET TMP_TABLE TYPE 2;");
      } catch (...) {
        LOG_ERROR("Fail to reset tmp table on slave.\n");
        throw;
      }
    }
  }
#endif
  Backend *backend = Backend::instance();
  if (type == RESET_DROP_DBSCALE_REPLICATION) {
#ifndef DBSCALE_TEST_DISABLE
    backend->drop_dbscale_replication();
#endif
  } else if (type == RESET_DROP) {
    // backend->drop_tmp_table_database();
    backend->reset_tmp_table_pool(reset_oper->table_pool_name);
  } else
    backend->reset_tmp_table_pool(reset_oper->table_pool_name);
}

/* class MySQLLoadFuncTypeNode */
MySQLLoadFuncTypeNode::MySQLLoadFuncTypeNode(ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLLoadFuncTypeNode";
}

void MySQLLoadFuncTypeNode::do_execute() {
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    MultipleManager *mul = MultipleManager::instance();
    try {
      mul->execute_master_query("DBSCALE DYNAMIC LOAD FUNCTION TYPE INTERNAL;");
    } catch (...) {
      LOG_ERROR("Fail to reload function type on slave.\n");
      throw;
    }
  }
#endif
  Backend *backend = Backend::instance();
  backend->init_function_type();
  backend->reset_tmp_table_pool();
}

/* class MySQLDBScaleCleanTempTableCacheNode */
MySQLDBScaleCleanTempTableCacheNode::MySQLDBScaleCleanTempTableCacheNode(
    ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLDBScaleCleanTempTableCacheNode";
}

void MySQLDBScaleCleanTempTableCacheNode::do_execute() {
#ifndef CLOSE_MULTIPLE
  if (multiple_mode && (!plan->statement->get_stmt_node()
                             ->sql->clean_temp_table_cache_oper->is_internal)) {
    MultipleManager *mul = MultipleManager::instance();
    try {
      mul->execute_master_query(
          "DBSCALE CLEAN TEMPORARY TABLE CACHE INTERNAL;");
    } catch (...) {
      LOG_ERROR("Fail to temp table cache on slave.\n");
      throw;
    }
  }
#endif
  Backend *backend = Backend::instance();
  backend->clear_temp_table_cache();
}

/* class MySQLSelectPlainValueNode */
MySQLSelectPlainValueNode::MySQLSelectPlainValueNode(ExecutePlan *plan,
                                                     const char *str_value,
                                                     const char *alias_name)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLSelectPlainValueNode";
  this->col_name = alias_name ? alias_name : str_value;
  this->col_value = str_value;
}
void MySQLSelectPlainValueNode::execute() {
  try {
    Packet packet;
    MySQLResultSetHeaderResponse result_set_header(1, 0);
    result_set_header.pack(&packet);
    handler->send_to_client(&packet);
    MySQLColumnResponse field("def", "information_schema", "table", "table",
                              col_name, col_name, 8, NAME_LEN + 1,
                              MYSQL_TYPE_VAR_STRING, 0, 0, 0);
    field.pack(&packet);
    handler->send_to_client(&packet);
    MySQLEOFResponse eof;
    if (method_to_handle_autocommit_flag ==
            HANDLE_AUTOCOMMIT_FLAG_BEFORE_SEND &&
        !session->get_stmt_auto_commit_int_value()) {
      eof.set_auto_commit_is_off();
    }
    eof.pack(&packet);
    handler->deal_autocommit_with_ok_eof_packet(&packet);
    handler->send_to_client(&packet);

    list<const char *> row_data;
    row_data.push_back(col_value);
    MySQLRowResponse row(row_data);
    row.pack(&packet);
    handler->send_to_client(&packet);
    MySQLEOFResponse end;
    if (method_to_handle_autocommit_flag ==
            HANDLE_AUTOCOMMIT_FLAG_BEFORE_SEND &&
        !session->get_stmt_auto_commit_int_value()) {
      end.set_auto_commit_is_off();
    }
    end.pack(&packet);
    if (plan->session->is_call_store_procedure() ||
        plan->session->get_has_more_result()) {
      rebuild_eof_with_has_more_flag(&packet, driver);
      LOG_DEBUG(
          "For the call store procedure or multiple stmt, the last eof should "
          "be with flag has_more_result in show node.\n");
    }
    handler->deal_autocommit_with_ok_eof_packet(&packet);
    handler->send_to_client(&packet);
  } catch (exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  }
  status = EXECUTE_STATUS_COMPLETE;
}

/* class MySQLShowUserNotAllowOperationTimeNode*/
MySQLShowUserNotAllowOperationTimeNode::MySQLShowUserNotAllowOperationTimeNode(
    ExecutePlan *plan, string user_name)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLShowUserNotAllowOperationTimeNode";
  this->user_name = user_name;
}

void MySQLShowUserNotAllowOperationTimeNode::init_data() {
  set_head_packet(2);
  add_column_packet("def", "information_schema", "table", "table", "user_name",
                    "user_name", 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0,
                    0);
  add_column_packet("def", "information_schema", "table", "table",
                    "not_allow_operation_time", "not_allow_operation_time", 8,
                    NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  map<string, map<int, int> > val;
  Backend::instance()->get_user_not_allow_operation_time(val, user_name);
  if (val.empty()) {
    return;
  }
  for (map<string, map<int, int> >::iterator it = val.begin(); it != val.end();
       it++) {
    list<const char *> row_data;
    string name = it->first;
    row_data.push_back(name.c_str());
    map<int, int> times = it->second;
    string col_val;
    for (map<int, int>::iterator it2 = times.begin(); it2 != times.end();
         it2++) {
      string from = get_hms_str(it2->first);
      string to = get_hms_str(it2->second);
      col_val = col_val + "[" + from + " - " + to + "]  ";
    }
    row_data.push_back(col_val.c_str());
    add_row_packet(row_data);
  }
}

/* class MySQLReloadUserAllowOperationTimeNode*/
MySQLReloadUserAllowOperationTimeNode::MySQLReloadUserAllowOperationTimeNode(
    ExecutePlan *plan, string user_name)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLReloadUserAllowOperationTimeNode";
  this->user_name = user_name;
}

void MySQLReloadUserAllowOperationTimeNode::do_execute() {
  bool is_reload_ok = Driver::get_driver()
                          ->get_config_helper()
                          ->reload_user_not_allow_operation_time(user_name);
  if (!is_reload_ok) {
    LOG_ERROR("ConfigHelper reload_user_not_allow_operation_time failed\n");
    throw ExecuteNodeError("fail to reload user allow operation time");
  }
}

/* class MySQLSetUserAllowOperationTimeNode*/
MySQLSetUserAllowOperationTimeNode::MySQLSetUserAllowOperationTimeNode(
    ExecutePlan *plan, set_user_not_allow_operation_time_node *oper)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLSetUserAllowOperationTimeNode";
  this->is_allow = oper->is_allow;
  this->user_name = oper->user_name;
  this->from_hour = oper->from_hour;
  this->from_minute = oper->from_minute;
  this->from_second = oper->from_second;
  this->to_hour = oper->to_hour;
  this->to_minute = oper->to_minute;
  this->to_second = oper->to_second;
}

void MySQLSetUserAllowOperationTimeNode::do_execute() {
  if ((from_hour < 0 || from_hour > 23) ||
      (from_minute < 0 || from_minute > 59) ||
      (from_second < 0 || from_second > 59) || (to_hour < 0 || to_hour > 23) ||
      (to_minute < 0 || to_minute > 59) || (to_second < 0 || to_second > 59)) {
    throw ExecuteNodeError("invalid time value, should be Hour:Minute:Second");
  }
  int from = from_hour * 3600 + from_minute * 60 + from_second;
  int to = to_hour * 3600 + to_minute * 60 + to_second;
  bool is_update_ok =
      Driver::get_driver()
          ->get_config_helper()
          ->set_user_not_allow_operation_time(is_allow, user_name, from, to);
  if (!is_update_ok) {
    LOG_ERROR("ConfigHelper set_user_not_allow_operation_time failed\n");
  } else {
    Backend::instance()
        ->notify_all_dbscale_reload_user_not_allow_operation_time(
            is_allow, user_name, from, to);
  }
}

/* class MySQLSetSchemaACLNode */
MySQLSetSchemaACLNode::MySQLSetSchemaACLNode(
    ExecutePlan *plan, set_schema_acl_op_node *set_schema_acl_oper)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLSetSchemaACLNode";
  this->username = set_schema_acl_oper->username;
  this->schema = set_schema_acl_oper->schema;
  this->type = set_schema_acl_oper->type;
  this->is_reset = set_schema_acl_oper->is_reset;
}

void MySQLSetSchemaACLNode::do_execute() {
  if (!is_reset && strcmp(schema, DBSCALE_RESERVED_STR)) {
    if (type == ACL_TYPE_PROCESS || type == ACL_TYPE_RELOAD ||
        type == ACL_TYPE_REPLICATION_CLIENT ||
        type == ACL_TYPE_SHOW_DATABASES || type == ACL_TYPE_SHUTDOWN ||
        type == ACL_TYPE_SUPER) {
      /*
       * below types are also global privileges but do not handle right now
       * CREATE TABLESPACE, FILE, REPLICATION SLAVE: these types are acl types
       * MySQL defined but not sure what kind of DBScale stmt can match CREATE
       * USER: this type is handled by auth source
       * */
      throw Error("Incorrect usage of GLOBAL PRIVILEGES");
    }
  }
  Backend *backend = Backend::instance();
  if (!is_reset)
    backend->set_schema_acl(username, schema, type);
  else
    backend->remove_schema_acl(username, schema, type);
}

/* class MySQLSetTableACLNode */
MySQLSetTableACLNode::MySQLSetTableACLNode(
    ExecutePlan *plan, set_table_acl_op_node *set_table_acl_oper)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLSetTableACLNode";
  this->user_name = set_table_acl_oper->user_name;
  this->schema_name = set_table_acl_oper->schema_name;
  this->table_name = set_table_acl_oper->table_name;
  this->type = set_table_acl_oper->type;
  this->is_reset = set_table_acl_oper->is_reset;
}

void MySQLSetTableACLNode::do_execute() {
  Backend *backend = Backend::instance();
  if (!is_reset) {
    string full_table_name;
    splice_full_table_name(schema_name, table_name, full_table_name);
    backend->set_table_acl(user_name, full_table_name, type);
  } else {
    string full_table_name;
    if (schema_name && table_name) {
      splice_full_table_name(schema_name, table_name, full_table_name);
    }
    backend->remove_table_acl(user_name, full_table_name, type);
  }
}

/* class MySQLMultipleMasterForwardNode */
MySQLMultipleMasterForwardNode::MySQLMultipleMasterForwardNode(
    ExecutePlan *plan, const char *sql, bool is_slow_query)
    : MySQLExecuteNode(plan),
      error_packet(NULL),
      exec_sql(sql),
      is_slow_query(is_slow_query) {}

void MySQLMultipleMasterForwardNode::execute() {
  vector<pair<string, bool> > query_list;
  query_list.push_back(
      make_pair<string, bool>("DBSCALE FLUSH ACL SILENTLY", false));
  const char *schema_name = plan->session->get_schema();
  /*
    the forward db is only for that current session db is mismatch with sql db
    example:
      session : use db1
      forward sql : alter table db2.tb
  */
  string forward_db_name = plan->get_forward_sql_target_db();
  if (!forward_db_name.empty()) schema_name = forward_db_name.c_str();
  string change_schema = "USE `";
  change_schema += schema_name;
  change_schema += "`";
  query_list.push_back(make_pair<string, bool>(string(change_schema), false));
  try {
    string session_option_sql = get_session_option_sql();
    string session_var_sql = get_session_var_sql();
    query_list.push_back(
        make_pair<string, bool>(string(session_option_sql), false));
    query_list.push_back(
        make_pair<string, bool>(string(session_var_sql), false));
    query_list.push_back(make_pair<string, bool>(string(exec_sql), true));
    if (is_slow_query) {
      forward_slow_query_to_master_role_node(query_list);
    } else {
      forward_query_to_master_role_node(query_list);
    }
  } catch (dbscale::sql::SQLError &e) {
    error_packet = Backend::instance()->get_new_packet();
    MySQLErrorResponse error(e.get_error_code(), e.get_message());
    error.pack(error_packet);

    status = EXECUTE_STATUS_COMPLETE;
    throw e;
  } catch (Exception &e) {
    string error_message(
        "Slave DBScale fail forward sql to master dbscale due to exception:");
    error_message.append(e.what());
    status = EXECUTE_STATUS_COMPLETE;
    throw ExecuteNodeError(error_message.c_str(), e.get_errno());
  }
  status = EXECUTE_STATUS_COMPLETE;
}

/*
  map<string, bool>& : sql - result_is_send_to_client
  schema_name
*/
void MySQLMultipleMasterForwardNode::forward_slow_query_to_master_role_node(
    vector<pair<string, bool> > &query_list) {
  Driver *driver = Driver::get_driver();
  Backend *bk = Backend::instance();
  int cluster_master_port = bk->get_cluster_master_port();
  string cluster_master_host = bk->get_cluster_master_host();
  DataServer *tmp_cluster_master_server = NULL;
  tmp_cluster_master_server =
      new DataServer("cluster_master", driver, cluster_master_port,
                     cluster_master_host.c_str(), cluster_master_host.c_str(),
                     cluster_user, cluster_password, false);
  Connection *conn = NULL;
  try {
    if (!tmp_cluster_master_server) {
      throw NoMemoryError("alloc memory failed for master forward node");
    }
    tmp_cluster_master_server->acquire_monitor_conn_mutex();
    conn = tmp_cluster_master_server->get_monitor_conn_for_monitor_check(
        DBSCALE_CLUSTER_ADMIN_SCHEMA);
    if (conn) {
      do_forward_query_to_master(conn, query_list);
    } else {
      throw Error("get connection failed, connection is null");
    }
  } catch (Exception &e) {
    conn = NULL;
    tmp_cluster_master_server->release_monitor_conn_mutex();
    if (tmp_cluster_master_server) {
      delete tmp_cluster_master_server;
      tmp_cluster_master_server = NULL;
    }
    throw e;
  }
  conn = NULL;
  tmp_cluster_master_server->release_monitor_conn_mutex();
  if (tmp_cluster_master_server) {
    delete tmp_cluster_master_server;
    tmp_cluster_master_server = NULL;
  }
}
void MySQLMultipleMasterForwardNode::forward_query_to_master_role_node(
    vector<pair<string, bool> > &query_list) {
  Backend *bk = Backend::instance();
  DataServer *server = bk->get_cluster_master_server();
  Connection *conn = NULL;
  try {
    server->acquire_monitor_conn_mutex();
    conn = server->get_monitor_conn_for_monitor_check();
    if (conn) {
      do_forward_query_to_master(conn, query_list);
    } else {
      throw Error("get connection failed, connection is null");
    }
  } catch (Exception &e) {
    conn = NULL;
    server->release_monitor_conn_mutex();
    throw e;
  }
  conn = NULL;
  server->release_monitor_conn_mutex();
}
void MySQLMultipleMasterForwardNode::do_forward_query_to_master(
    Connection *conn, vector<pair<string, bool> > &query_list) {
  for (unsigned int i = 0; i < query_list.size(); i++) {
    auto &p = query_list[i];
    conn->execute(p.first.c_str());
    list<Packet *, StaticAllocator<Packet *> > recv_list;
    int eof_count = 0;
    status = EXECUTE_STATUS_START;
    while (!is_finished()) {
      get_result_packets(conn, recv_list, eof_count);
      handle_result_packets(recv_list, p.second);
    }
  }
}
void MySQLMultipleMasterForwardNode::clear_recv_packets(
    list<Packet *, StaticAllocator<Packet *> > &recv_list) {
  while (!recv_list.empty()) {
    Packet *tmp_packet = recv_list.front();
    recv_list.pop_front();
    delete tmp_packet;
    tmp_packet = NULL;
  }
}
void MySQLMultipleMasterForwardNode::get_result_packets(
    Connection *conn, list<Packet *, StaticAllocator<Packet *> > &recv_list,
    int &eof_count, unsigned int timeout) {
  ReceivedPacketStatus ret = RECEIVED_ALL;
  try {
    ret = conn->recv_all_packets(recv_list, timeout);
    if (ret == RECEIVED_EOF) {
      ++eof_count;
      // The result set of normal sql contains two EOF to divite into different
      // parts.
      if (eof_count == 2) {
        status = EXECUTE_STATUS_COMPLETE;
      }
    } else if (ret == RECEIVED_ALL) {
      status = EXECUTE_STATUS_COMPLETE;
    } else {
      // HAS_MORE_PACKETS
      status = EXECUTE_STATUS_FETCH_DATA;
    }
  } catch (dbscale::sql::SQLError &e) {
    conn = NULL;
    status = EXECUTE_STATUS_COMPLETE;
    clear_recv_packets(recv_list);
    LOG_ERROR("Got sql error due to: %s\n", e.what());
    throw e;
  } catch (Exception &e) {
    conn = NULL;
    status = EXECUTE_STATUS_COMPLETE;
    clear_recv_packets(recv_list);
    string error_message(
        "Slave DBScale fail forward sql to master dbscale due to exception:");
    error_message.append(e.what());
    LOG_ERROR("Got sql error due to: %s\n", error_message.c_str());
    throw e;
  }
}
void MySQLMultipleMasterForwardNode::handle_result_packets(
    list<Packet *, StaticAllocator<Packet *> > &recv_list,
    bool send_to_client) {
  if (send_to_client) {
    Handler *ha = plan->session->get_handler();
    while (!recv_list.empty()) {
      Packet *tmp_packet = recv_list.front();
      recv_list.pop_front();
      ha->send_to_client(tmp_packet);
      delete (tmp_packet);
      tmp_packet = NULL;
    }
  } else {
    while (!recv_list.empty()) {
      Packet *tmp_packet = recv_list.front();
      recv_list.pop_front();
      delete (tmp_packet);
      tmp_packet = NULL;
    }
  }
}
string MySQLMultipleMasterForwardNode::get_session_var_sql() {
  bool first_append_sql = true;
  string session_sql;
  Session *s = plan->session;
  map<string, string> *session_var_map = s->get_session_var_map();
  if (!session_var_map->empty()) {
    session_sql = "SET ";
    map<string, string>::iterator it = session_var_map->begin();
    for (; it != session_var_map->end(); it++) {
      if (!is_forbidden_session_var(
              it->first, Backend::instance()->get_backend_server_version() ==
                             MYSQL_VERSION_MARIADB)) {
        if (!first_append_sql) {
          session_sql.append(", ");
        }
        first_append_sql = false;
        session_sql.append("SESSION ");
        session_sql.append(it->first);
        session_sql.append("=");
        session_sql.append(it->second);
      }
    }
  }

  return session_sql;
}

string MySQLMultipleMasterForwardNode::get_session_option_sql() {
  bool first_append_sql = true;
  string session_option_sql;
  Session *s = plan->session;
  map<string, OptionValue> session_option_value = s->get_session_option_map();
  map<string, string> option_val_map;
  OptionParser *config = OptionParser::instance();
  config->get_option_map(&option_val_map, &session_option_value);
  if (!option_val_map.empty()) {
    session_option_sql = "DBSCALE SET SESSION ";
    map<string, OptionValue>::iterator it_value = session_option_value.begin();
    map<string, string>::iterator it = option_val_map.begin();
    for (; it != option_val_map.end(); it++) {
      if (!first_append_sql) {
        session_option_sql.append(", ");
      }
      first_append_sql = false;
      session_option_sql.append("'");
      session_option_sql.append(it->first);
      session_option_sql.append("'=");
      if (it_value->second.type > VALUE_TYPE_ULONG) {
        session_option_sql.append("'");
      }
      string opt_val = it->second;
      boost::to_lower(opt_val);
      session_option_sql.append(opt_val);
      if (it_value->second.type > VALUE_TYPE_ULONG) {
        session_option_sql.append("'");
      }
      it_value++;
    }
  }

  return session_option_sql;
}

/*class MySQLDBScaleMulSyncNode */
MySQLDBScaleMulSyncNode::MySQLDBScaleMulSyncNode(
    ExecutePlan *plan, const char *sync_topic, const char *sync_state,
    const char *sync_param, const char *sync_cond, unsigned long version_id)
    : MySQLReturnOKNode(plan),
      sync_topic(sync_topic),
      sync_state(sync_state),
      sync_param(sync_param),
      sync_cond(sync_cond),
      error_packet(NULL),
      version_id(version_id) {}
void MySQLDBScaleMulSyncNode::generate_mul_sync_error_packet() {
  error_packet = Backend::instance()->get_new_packet();
  MySQLErrorResponse error(ERROR_DBSCALE_MUL_SYNC_FAIL_CODE,
                           dbscale_err_msg[ERROR_DBSCALE_MUL_SYNC_FAIL_CODE]);
  error.pack(error_packet);
}

void MySQLDBScaleMulSyncNode::do_execute() {
  SyncTopicState state = SyncTopic::get_state_by_name(sync_state);
  MultipleManager *mul = MultipleManager::instance();
  MultipleSyncTool *sync_tool = mul->get_sync_tool();
  SyncTopic *topic = NULL;

  if (state == SYNC_TOPIC_STATE_START) {
    sync_tool->acquire_mul_sync_mutex();
    if (sync_tool->get_sync_topic()) {
      // Currently, only one sync topic can be run at a time
      LOG_ERROR("There is sync topic running, so refuse this sync request.\n");
      sync_tool->release_mul_sync_mutex();
      generate_mul_sync_error_packet();
      status = EXECUTE_STATUS_COMPLETE;
      throw ErrorPacketException();
    }
    topic = SyncTopic::get_sync_topic_by_name(sync_topic);
    topic->set_version_id(version_id);
    sync_tool->prepare_sync_tool(topic);
    sync_tool->release_mul_sync_mutex();
  }

  if (!sync_tool->get_sync_topic()) {
    LOG_ERROR("The sync topic should be init with sync topic start.\n");
    generate_mul_sync_error_packet();
    status = EXECUTE_STATUS_COMPLETE;
    throw ErrorPacketException();
  }

  SyncCondition *cond = NULL;
  if (sync_cond) {
    cond = SyncCondition::get_cond_by_str(sync_cond);
  }

  bool ret = false;
  ret = sync_tool->handle_sync_request(state, sync_param, cond);
  if (state == SYNC_TOPIC_STATE_CANCEL ||
      (state == SYNC_TOPIC_STATE_FIN && ret)) {
    /*If it is a cancel command or a succefully fin command, reset the sync
     * tool.*/
    sync_tool->reset_tool();
    // TODO: if the conn between master role dbscale and slave are broken, the
    // reset_tool should be invoked during the destructor of session.
  }

  if (cond) {
    delete cond;
    cond = NULL;
  }

  if (!ret) {
    generate_mul_sync_error_packet();
    status = EXECUTE_STATUS_COMPLETE;
    throw ErrorPacketException();
  }
}

/* class MySQLDynamicConfigurationNode */
MySQLDynamicConfigurationNode::MySQLDynamicConfigurationNode(ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLDynamicConfigurationNode";
  packet = NULL;
  got_error = false;
}

void MySQLDynamicConfigurationNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
  }
}

void MySQLDynamicConfigurationNode::set_option(
    dynamic_config_op_node *config_oper, list<set_option_err_msg> &val_err_list,
    list<string> &name_err_list, map<string, OptionValue> *option_map) {
  var_scope_type vst = config_oper->vst;
  LOG_DEBUG("MySQLDynamicConfigurationNode::set_global_or_instance_var.\n");
#ifndef CLOSE_MULTIPLE
  map<string, pair<string, bool> > instance_val;
  if (vst == VAR_SCOPE_GLOBAL && multiple_mode) {
    acquire_start_config_lock(session->get_zk_start_config_lock_extra_info());
  }
#endif
  try {
    OptionParser *config = OptionParser::instance();
    while (config_oper) {
      string option_name(config_oper->option_name);
      boost::to_lower(option_name);
      boost::replace_all(option_name, "-", "_");
      if (vst == VAR_SCOPE_SESSION && option_name == "execute_profile") {
        if (config_oper->int_val) {
          int i = atoi(config_oper->int_val);
          bool b = ((i == 0) ? false : true);
          session->set_in_profile(b);
          config_oper = config_oper->next;
        }
        continue;
      }

      if (option_name == "session_select_once_alloc_size") {
        if (config_oper->int_val) {
          unsigned long config_val = stoul(config_oper->int_val);
          if (config_val > max_global_select_usable_memory_size / 100) {
            if (config_val > max_global_select_usable_memory_size) {
              LOG_ERROR(
                  "Option 'session-select-once-alloc-size' is invalid. "
                  "'session-select-once-alloc-size' should less than "
                  "'max-global-select-usable-memory-size'\n");
              throw OptionError(
                  "Option 'session-select-once-alloc-size' is too big.");
            }
            LOG_WARN(
                "Option 'session-select-once-alloc-size' is too big. "
                "'session-select-once-alloc-size' is recommended to be set "
                "less than 'max-global-select-usable-memory-size' / 100.\n");
          }
        }
      }
      if (option_name == "select_node_once_alloc_size") {
        if (config_oper->int_val) {
          unsigned long config_val = stoul(config_oper->int_val);
          if (config_val / 1024 > session_select_once_alloc_size / 100) {
            if (config_val / 1024 > session_select_once_alloc_size) {
              LOG_ERROR(
                  "Option 'select-node-once-alloc-size' is invalid. "
                  "'select-node-once-alloc-size' / 1024 "
                  "should less than 'session-select-once-alloc-size'\n");
              throw OptionError(
                  "Option 'select-node-once-alloc-size' is too big.");
            }
            LOG_WARN(
                "Option 'select-node-once-alloc-size' is too big. "
                "'select-node-once-alloc-size'/1024 is recommended to be set "
                "less than "
                "'session-select-once-alloc-size'/100\n");
          }
        }
      }
      if (option_map->count(option_name)) {
        try {
          if ((*option_map)[option_name].type == VALUE_TYPE_INT ||
              (*option_map)[option_name].type == VALUE_TYPE_UINT ||
              (*option_map)[option_name].type == VALUE_TYPE_ULONG ||
              ((*option_map)[option_name].type == VALUE_TYPE_BOOL &&
               config_oper->int_val)) {
            if (!config_oper->int_val) {
              got_error = true;
              set_option_err_msg err_msg;
              err_msg.option_name = option_name;
              err_msg.err_msg = "set value for option with wrong data type.";
              val_err_list.push_back(err_msg);
              config_oper = config_oper->next;
              continue;
            }
            Backend *backend = Backend::instance();
            backend->pre_work_for_dynamic_option(
                option_name.c_str(), config_oper->int_val,
                plan->session->get_username(), vst);
            config->dynamic_set_option(config_oper->int_val,
                                       (*option_map)[option_name], vst);
            LOG_INFO("Dynamic configuration set %s = [%s].\n",
                     option_name.c_str(), config_oper->int_val);
            if (vst == VAR_SCOPE_SESSION &&
                option_name == "enable_last_insert_id" &&
                !strcasecmp(config_oper->int_val, "1")) {
              plan->session->set_last_insert_id(0);
            }
            config->handle_special_config(option_name, config_oper->int_val);
            if (vst == VAR_SCOPE_GLOBAL &&
                instance_option_value.count(option_name)) {
              config->dynamic_set_option(config_oper->int_val,
                                         instance_option_value[option_name],
                                         VAR_SCOPE_INSTANCE);
#ifndef CLOSE_MULTIPLE
              string tmp(config_oper->int_val);
              find_and_insert_str(&tmp, "\\", "\\");
              find_and_insert_str(&tmp, "'", "\\");
              instance_val[option_name] = make_pair(tmp, true);
#endif
            }
          } else {
            if (!config_oper->str_val) {
              got_error = true;
              set_option_err_msg err_msg;
              err_msg.option_name = option_name;
              err_msg.err_msg = "set value for option with wrong data type.";
              val_err_list.push_back(err_msg);
              config_oper = config_oper->next;
              continue;
            }
            Backend *backend = Backend::instance();
            backend->pre_work_for_dynamic_option(
                option_name.c_str(), config_oper->str_val,
                plan->session->get_username(), vst);
            config->dynamic_set_option(config_oper->str_val,
                                       (*option_map)[option_name], vst);
            LOG_INFO("Dynamic configuration set %s = [%s].\n",
                     option_name.c_str(), config_oper->str_val);
            if (vst == VAR_SCOPE_INSTANCE &&
                option_name == "dbscale_expected_master_addr") {
              dbscale_expected_master_addr_set.clear();
              string addr_str(config_oper->str_val);
              vector<string> dbscale_expected_master_addr_vec;
              if (addr_str != "")
                boost::split(dbscale_expected_master_addr_vec, addr_str,
                             boost::is_any_of(","), boost::token_compress_on);
              for (unsigned int i = 0;
                   i < dbscale_expected_master_addr_vec.size(); i++) {
                dbscale_expected_master_addr_set.insert(
                    dbscale_expected_master_addr_vec[i]);
              }
            }
            if (vst == VAR_SCOPE_GLOBAL &&
                instance_option_value.count(option_name)) {
              config->dynamic_set_option(config_oper->str_val,
                                         instance_option_value[option_name],
                                         VAR_SCOPE_INSTANCE);
#ifndef CLOSE_MULTIPLE
              string tmp(config_oper->str_val);
              find_and_insert_str(&tmp, "\\", "\\");
              find_and_insert_str(&tmp, "\"", "\\");
              instance_val[option_name] = make_pair(tmp, false);
#endif
            }
          }
        } catch (exception &e) {
          got_error = true;
          set_option_err_msg err_msg;
          err_msg.option_name = option_name;
          err_msg.err_msg = e.what();
          val_err_list.push_back(err_msg);
          LOG_ERROR("Invalid dynamic set, due to %s.\n", e.what());
        }
      } else {
        got_error = true;
        name_err_list.push_back(option_name);
      }
      config_oper = config_oper->next;
    }
#ifndef CLOSE_MULTIPLE
    if (vst == VAR_SCOPE_GLOBAL && multiple_mode) {
      map<string, OptionValue> tmp;
      map<string, OptionValue>::iterator it = option_map->begin();
      for (; it != option_map->end(); it++) {
        if (it->second.dynamic_set) tmp[it->first] = it->second;
      }
      Backend::instance()->flush_config_to_zoo();
      map<string, string> options;
      config->get_option_map(&options, &tmp);
      MultipleManager *mul = MultipleManager::instance();
      mul->pub_ka_dynamic_set_option_info(&options);

      if (!instance_val.empty()) {
        bool first_value = true;
        string instance_str("DBSCALE SET INSTANCE ");
        map<string, pair<string, bool> >::iterator it_ins =
            instance_val.begin();
        for (; it_ins != instance_val.end(); ++it_ins) {
          if (first_value)
            first_value = false;
          else
            instance_str.append(", ");
          instance_str.append(it_ins->first);
          instance_str.append(" = ");
          if (it_ins->second.second) {
            instance_str.append(it_ins->second.first);
          } else {
            instance_str.append("\"");
            instance_str.append(it_ins->second.first);
            instance_str.append("\"");
          }
        }
        LOG_DEBUG("Execute set instance options on slave [%s].\n",
                  instance_str.c_str());
        mul->execute_master_query(instance_str.c_str());
      }
    }
#endif
  } catch (...) {
#ifndef CLOSE_MULTIPLE
    if (vst == VAR_SCOPE_GLOBAL && multiple_mode) release_start_config_lock();
#endif
    throw;
  }
#ifndef CLOSE_MULTIPLE
  if (vst == VAR_SCOPE_GLOBAL && multiple_mode) release_start_config_lock();
#endif
}

void MySQLDynamicConfigurationNode::do_execute() {
  packet = Backend::instance()->get_new_packet(row_packet_size);
  list<set_option_err_msg> val_err_list;
  list<string> name_err_list;
  string err_msg;

  dynamic_config_op_node *config_oper =
      plan->statement->get_stmt_node()->sql->dynamic_config_oper;
  var_scope_type vst = config_oper->vst;
  switch (vst) {
    case VAR_SCOPE_GLOBAL: {
      set_option(config_oper, val_err_list, name_err_list, &all_option_value);
      break;
    }
    case VAR_SCOPE_INSTANCE: {
      set_option(config_oper, val_err_list, name_err_list,
                 &instance_option_value);
      break;
    }
    case VAR_SCOPE_SESSION: {
      set_option(config_oper, val_err_list, name_err_list,
                 &session->get_session_option_map());
      break;
    }
    default:
      break;
  }
  Driver *driver = Driver::get_driver();
  set<Session *, compare_session> *session_set = driver->get_session_set();
  set<Session *, compare_session>::iterator it = session_set->begin();
  for (; it != session_set->end(); it++) {
    (*it)->set_update_option(true);
  }

  if (got_error) {
    if (!name_err_list.empty()) {
      err_msg.append("Not support dynamic configuration for UNKnown option: ");
      list<string>::iterator it = name_err_list.begin();
      for (; it != name_err_list.end(); it++) {
        err_msg.append("'");
        err_msg.append(*it);
        err_msg.append("' ");
      }
      err_msg.append(". ");
    }
    if (!val_err_list.empty()) {
      err_msg.append("Not support the value for ");

      list<set_option_err_msg>::iterator iter = val_err_list.begin();
      for (; iter != val_err_list.end(); iter++) {
        err_msg.append("'");
        err_msg.append(iter->option_name);
        err_msg.append("', ");
        err_msg.append(iter->err_msg);
      }
    }
    LOG_ERROR("%s\n", err_msg.c_str());
    throw Error(err_msg.c_str());
  }
  if (vst == VAR_SCOPE_INSTANCE)
    Backend::instance()->flush_config_to_file(
        OptionParser::instance()->get_config_file().c_str(), false);
  OptionParser::instance()->check_dynamic_switch();
}

/* class MySQLRepStrategyNode */
MySQLRepStrategyNode::MySQLRepStrategyNode(ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLRepStrategyNode";
}

void MySQLRepStrategyNode::do_execute() {
  LOG_DEBUG("MySQLRepStrategyNode::do_execute.\n");
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    acquire_start_config_lock(session->get_zk_start_config_lock_extra_info());
  }
#endif
  try {
    rep_strategy_op_node *rep_strategy_oper =
        plan->statement->get_stmt_node()->sql->rep_strategy_oper;
    string strategy_name(rep_strategy_oper->strategy_name);
    const char *source_name = rep_strategy_oper->source_name;
    boost::to_lower(strategy_name);

    LoadBalanceStrategy strategy;
    if (!strategy_name.compare("master")) {
      strategy = LOAD_BALANCE_STRATEGY_MASTER;
    } else if (!strategy_name.compare("slaves")) {
      strategy = LOAD_BALANCE_STRATEGY_SLAVES;
    } else if (!strategy_name.compare("master-slaves")) {
      strategy = LOAD_BALANCE_STRATEGY_MASTER_SLAVES;
    } else {
      LOG_ERROR(
          "Unknown load_balance_strategy type for replication data_source.\n");
      throw Error(
          "Unknown load_balance_strategy type for replication data_source.");
    }

    Backend *backend = Backend::instance();
    DataSource *data_source = backend->find_data_source(source_name);
    if (data_source == NULL) {
      LOG_ERROR("data-source not found.\n");
      throw UnknownDataSourceException("data-source not found.");
    }

    DataSourceType source_type = data_source->get_data_source_type();
    if (source_type == DATASOURCE_TYPE_REPLICATION) {
      ReplicationDataSource *rep_source = (ReplicationDataSource *)data_source;
      rep_source->set_load_balance_strategy(strategy);
    } else if (source_type == DATASOURCE_TYPE_RWSPLIT) {
      RWSplitDataSource *source = (RWSplitDataSource *)data_source;
      source->set_load_balance_strategy(strategy);
    } else if (source_type == DATASOURCE_TYPE_MGR) {
      MGRDataSource *source = (MGRDataSource *)data_source;
      source->set_load_balance_strategy(strategy);
    } else {
      LOG_ERROR(
          "do not support change load_balance_strategy for datasource of type "
          "%d.\n",
          (int)source_type);
      throw Error(
          "do not support change load_balance_strategy for datasource of "
          "target type");
    }

#ifndef CLOSE_MULTIPLE
    if (multiple_mode) {
      Backend::instance()->flush_config_to_zoo();
      MultipleManager *mul = MultipleManager::instance();
      mul->pub_ka_set_rep_strategy_info(string(source_name), strategy_name);
    }
#endif
  } catch (...) {
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) release_start_config_lock();
#endif
    throw;
  }
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) release_start_config_lock();
#endif
}

/* class MySQLDynamicSetServerWeightNode*/
MySQLDynamicSetServerWeightNode::MySQLDynamicSetServerWeightNode(
    ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLDynamicSetServerWeightNode";
}

void MySQLDynamicSetServerWeightNode::do_execute() {
  LOG_DEBUG("MySQLDynamicSetServerWeightNode::do_execute.\n");
#ifndef CLOSE_MULTIPLE
  if (multiple_mode) {
    acquire_start_config_lock(session->get_zk_start_config_lock_extra_info());
  }
#endif
  dynamic_set_server_weight_node *server_weight_oper =
      plan->statement->get_stmt_node()->sql->dynamic_set_server_weight_oper;
  int weight_value = server_weight_oper->weight_value;
  if (weight_value < 0) {
    LOG_ERROR("server weight value should >= 0.\n");
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) release_start_config_lock();
#endif
    throw DynamicOpFail("server weight value should >= 0");
  }
  const char *data_source_name = server_weight_oper->data_source_name;
  const char *server_name = server_weight_oper->server_name;

  ACE_RW_Thread_Mutex *dynamic_modify_mutex =
      Backend::instance()->get_dynamic_modify_rep_mutex();
  dynamic_modify_mutex->acquire_write();
  bool dynamic_modify_mutex_locked = true;
  try {
    DataSource *source =
        Backend::instance()->find_data_source(data_source_name);
    if (!source) {
      LOG_ERROR("no datasource named [%s]\n", data_source_name);
      throw DynamicOpFail("datasource not found");
    }
    LoadBalanceDataSource *lb_source = NULL;
    DataSourceType source_type = source->get_data_source_type();
    if (source_type == DATASOURCE_TYPE_RWSPLIT)
      lb_source = (LoadBalanceDataSource *)(((RWSplitDataSource *)source)
                                                ->get_read_source());
    else if (source_type == DATASOURCE_TYPE_REPLICATION)
      lb_source = (LoadBalanceDataSource *)(((ReplicationDataSource *)source)
                                                ->get_read_source());
    else if (source_type == DATASOURCE_TYPE_LOAD_BALANCE)
      lb_source = (LoadBalanceDataSource *)source;
    else {
      LOG_ERROR(
          "datasource type can only be RWSplit or Replication or "
          "Load_Balance.\n");
      throw DynamicOpFail(
          "datasource type can only be RWSplit or Replication or Load_Balance");
    }
    SchedulePolicy schedule_type = lb_source->get_scheduler_type();
    if (schedule_type != SCHEDULE_POLICY_WEIGHT) {
      LOG_ERROR(
          "datasource schedule type is not WEIGHT, this operation is "
          "meaningless.\n");
      throw DynamicOpFail(
          "datasource schedule type is not WEIGHT, this operation is "
          "meaningless");
    }
    bool job_done = false;
    vector<DataSource *>::iterator lb_source_it;
    for (lb_source_it = lb_source->get_data_sources().begin();
         lb_source_it != lb_source->get_data_sources().end(); lb_source_it++) {
      if ((*lb_source_it)->get_data_source_type() != DATASOURCE_TYPE_SERVER)
        continue;
      ServerDataSource *server_source = (ServerDataSource *)(*lb_source_it);
      if (!strcasecmp(server_name, server_source->get_server()->get_name())) {
        LOG_DEBUG(
            "set weight of server [%s] in datasource [%s] from [%d] to [%d]\n",
            server_name, data_source_name, server_source->get_weight(),
            weight_value);
        server_source->set_weight(weight_value);
        job_done = true;
        break;
      }
    }
    if (!job_done) {
      if (source_type == DATASOURCE_TYPE_REPLICATION) {
        DataSource *rep_write_source =
            ((ReplicationDataSource *)source)->get_write_source();
        if (rep_write_source->get_data_source_type() ==
            DATASOURCE_TYPE_SERVER) {
          ServerDataSource *server_source =
              (ServerDataSource *)(rep_write_source);
          if (!strcasecmp(server_name,
                          server_source->get_server()->get_name())) {
            LOG_DEBUG(
                "set weight of server [%s] in datasource [%s] from [%d] to "
                "[%d]\n",
                server_name, data_source_name, server_source->get_weight(),
                weight_value);
            server_source->set_weight(weight_value);
            job_done = true;
          }
        }
      }
      if (!job_done) {
        LOG_ERROR("do not find dataserver named [%s] in datasource [%s].\n",
                  server_name, data_source_name);
        throw DynamicOpFail("dataserver not found");
      }
    }
    dynamic_modify_mutex->release();
    dynamic_modify_mutex_locked = false;
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) {
      Backend::instance()->flush_config_to_zoo();
      MultipleManager *mul = MultipleManager::instance();
      char weight_value_tmp[21];
      sprintf(weight_value_tmp, "%d", weight_value);
      mul->pub_ka_set_source_weight_info(string(data_source_name),
                                         string(server_name),
                                         string(weight_value_tmp));
    }
#endif
  } catch (...) {
    if (dynamic_modify_mutex_locked) dynamic_modify_mutex->release();
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) release_start_config_lock();
#endif
    throw;
  }

#ifndef CLOSE_MULTIPLE
  if (multiple_mode) release_start_config_lock();
#endif
}

/* class MySQLKeepmasterNode */
MySQLKeepmasterNode::MySQLKeepmasterNode(ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLKeepmasterNode";
}

void MySQLKeepmasterNode::do_execute() {
  bool is_keepmaster =
      plan->statement->get_stmt_node()->sql->keepmaster_oper->is_keepmaster;
  Session *s = plan->session;
  s->set_is_keepmaster(is_keepmaster);
}

/* class MySQLAsyncTaskControlNode */
MySQLAsyncTaskControlNode::MySQLAsyncTaskControlNode(ExecutePlan *plan,
                                                     unsigned long long id)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLAsyncTaskControlNode";
  this->id = id;
}

void MySQLAsyncTaskControlNode::do_execute() {
  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *exec_mutex = backend->get_dynamic_modify_rep_mutex();
  exec_mutex->acquire_write();
  bool locked = true;
#ifndef CLOSE_MULTIPLE
  bool is_master = true;
  if (multiple_mode) {
    MultipleManager *mul = MultipleManager::instance();
    is_master = mul->get_is_cluster_master();
  }
  if (!is_master && plan->statement->get_stmt_node()->forward_to_master != 2) {
    exec_mutex->release();
    locked = false;
    backend->forward_to_master_role_node(plan->statement->get_sql(),
                                         session->get_schema());
    return;
  }
#endif
  try {
    stmt_type type = plan->statement->get_stmt_node()->type;
    AsyncTaskManager *atm = AsyncTaskManager::instance();
    switch (type) {
      case STMT_DBSCALE_SUSPEND_ASYNC_TASK: {
        atm->set_cmd_to_task(id, ASYNC_COMMAND_SUSPEND);
      } break;
      case STMT_DBSCALE_CONTINUE_ASYNC_TASK: {
        atm->set_cmd_to_task(id, ASYNC_COMMAND_CONTINUE);
      } break;
      case STMT_DBSCALE_CANCEL_ASYNC_TASK: {
        atm->set_cmd_to_task(id, ASYNC_COMMAND_CANCEL);
      } break;
      case STMT_DBSCALE_DELETE_ASYNC_TASK: {
        if (!atm->delete_async_task(id)) {
          exec_mutex->release();
          locked = false;
          throw Error(
              "Not find task or the task state is not fin/cancel/error");
        }
      } break;
      default:
        break;
    }
    exec_mutex->release();
    locked = false;
  } catch (...) {
    if (locked) exec_mutex->release();
    throw;
  }
}

/* class MySQLLoadDataSpaceFileNode */
MySQLLoadDataSpaceFileNode::MySQLLoadDataSpaceFileNode(ExecutePlan *plan,
                                                       const char *filename)
    : MySQLReturnOKNode(plan), filename(filename) {
  this->name = "MySQLLoadDataSpaceFileNode";
}

void MySQLLoadDataSpaceFileNode::do_execute() {
  Backend::instance()->load_dataspace_config_file(filename);
}

/* class MySQLDBScaleTestNode*/

MySQLDBScaleTestNode::MySQLDBScaleTestNode(ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLDBScaleTestNode";
}

void MySQLDBScaleTestNode::do_execute() {
#ifndef DBSCALE_TEST_DISABLE
  Session *s = plan->session;
  dbscale_test_op_node *test_oper =
      plan->statement->get_stmt_node()->sql->dbscale_test_oper;
  if (test_oper) {
    if (!strcasecmp(test_oper->test_case_name, "stop") &&
        !strcasecmp(test_oper->test_case_operation, "monitor")) {
      Backend::instance()->set_stop_monitor();
      return;
    }

    if (!strcasecmp(test_oper->test_case_name, "authentication") &&
        !strcasecmp(test_oper->test_case_operation, "fetch_all_connection")) {
      Backend *backend = Backend::instance();
      backend->test_auth_connection = true;
    }
    if (!strcasecmp(test_oper->test_case_name, "xa_recovered") &&
        !strcasecmp(test_oper->test_case_operation, "false")) {
      Backend *backend = Backend::instance();
      backend->clear_data_server_has_recovered();
      XA_manager *mx = XA_manager::instance();
      mx->set_init_recovered_servers(2);
    }
    if (!strcasecmp(test_oper->test_case_name, "tmp_shard") &&
        !strcasecmp(test_oper->test_case_operation, "init_tmp_shard_space")) {
      Backend *backend = Backend::instance();
      DataSpace *data_space =
          backend->get_data_space_for_table("test", "partition1");
      Schema *join_schema = backend->find_schema(TMP_TABLE_SCHEMA);
      bool independence = true;
      const char *par_key = NULL;
      char *tmp = new char[4];
      strncpy(tmp, "c1", 3);
      tmp[2] = '\0';
      par_key = tmp;
      add_dynamic_str(tmp);

      PartitionedTable *par_ds = (PartitionedTable *)data_space;
      PartitionScheme *part_sch = par_ds->get_partition_scheme();

      PartitionType part_type = part_sch->get_type();
      DataSpace *tab =
          new PartitionedTable("tmp_table", par_key, part_sch, join_schema,
                               part_type, independence, NULL);
      ((PartitionedTable *)tab)->set_virtual_map(par_ds->get_virtual_map());
      join_schema->add_table((PartitionedTable *)tab);
      tab->set_is_tmp_table_space(true);
    }
    if (!strcasecmp(test_oper->test_case_name, "duplicated_table") &&
        !strcasecmp(test_oper->test_case_operation, "init_space")) {
      Backend *backend = Backend::instance();
      DataSpace *dspace =
          backend->get_data_space_for_table("test", "test_dup_table");

      PartitionedTable *par_tb = (PartitionedTable *)dspace;
      DuplicatedTable *dup_tab = new DuplicatedTable(par_tb, "dup_table", NULL);
      Schema *schema = backend->find_schema("test");
      schema->add_table(dup_tab);

      dspace = backend->get_data_space_for_table("test", "dup_table");
      ACE_ASSERT(!strcmp("dup_table", ((DuplicatedTable *)dspace)->get_name()));
    }
    if (test_oper->is_backend) {
      Backend *backend = Backend::instance();
      backend->set_dbscale_test_info(test_oper->test_case_name,
                                     test_oper->test_case_operation,
                                     test_oper->test_case_param);
    } else
      s->set_dbscale_test_info(test_oper->test_case_name,
                               test_oper->test_case_operation,
                               test_oper->test_case_param);
    LOG_DEBUG("Store the test info with name %s, operation %s, param %s.\n",
              test_oper->test_case_name, test_oper->test_case_operation,
              test_oper->test_case_param);
  }
#elif defined(RELEASE_WITH_STOP_MONITOR)
  dbscale_test_op_node *test_oper =
      plan->statement->get_stmt_node()->sql->dbscale_test_oper;
  if (test_oper) {
    if (!strcasecmp(test_oper->test_case_name, "stop") &&
        !strcasecmp(test_oper->test_case_operation, "monitor")) {
      Backend::instance()->set_stop_monitor();
      return;
    }
  }
#endif
}

/* class MySQLDBScalePurgeMonitorPointNode */
MySQLDBScalePurgeMonitorPointNode::MySQLDBScalePurgeMonitorPointNode(
    ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLDBScalePurgeMonitorPointNode";
}

void MySQLDBScalePurgeMonitorPointNode::do_execute() {
  Backend *backend = Backend::instance();
  backend->collect_monitor_point_value();

  backend->reset_monitor_point_map();
}

/* class MySQLDBScaleCleanMonitorPointNode */
MySQLDBScaleCleanMonitorPointNode::MySQLDBScaleCleanMonitorPointNode(
    ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLDBScaleCleanMonitorPointNode";
}

void MySQLDBScaleCleanMonitorPointNode::do_execute() {
  Backend *backend = Backend::instance();

  backend->reset_monitor_point_map();
}

/* class MySQLCallSPReturnOKNode */
MySQLCallSPReturnOKNode::MySQLCallSPReturnOKNode(ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {}

void MySQLCallSPReturnOKNode::execute() {
  if (plan->session->get_call_sp_nest() > 1) {
    LOG_DEBUG(
        "current call_sp_nest=%d, decrease one, no need send OK packet to "
        "client.\n",
        plan->session->get_call_sp_nest());
    session->decrease_call_sp_nest();
    session->pop_cur_sp_worker_one_recurse();
    status = EXECUTE_STATUS_COMPLETE;
    return;
  }
  try {
    do_execute();
    Packet ok_packet;
    MySQLOKResponse ok(0, 0);
    ok.pack(&ok_packet);
    handler->deal_autocommit_with_ok_eof_packet(&ok_packet);
    handler->send_to_client(&ok_packet);
    session->set_cur_sp_worker(NULL);
  } catch (...) {
    status = EXECUTE_STATUS_COMPLETE;
    LOG_ERROR("Execute Node fail in MySQLReturnOKNode.\n");
    throw;
  }

  status = EXECUTE_STATUS_COMPLETE;
}

/* class MySQLDBScaleShowTransactionSqlsNode*/
MySQLDBScaleShowTransactionSqlsNode::MySQLDBScaleShowTransactionSqlsNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {}

void MySQLDBScaleShowTransactionSqlsNode::init_data() {
  try {
    set_head_packet(2);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";
    const char *columns = "Sequence-id";
    add_column_packet(catalog, schema, table, org_table, columns, columns, 8,
                      NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
    columns = "Sqls";
    add_column_packet(catalog, schema, table, org_table, columns, columns, 8,
                      NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
    driver->acquire_session_mutex();
    try {
      user_id = plan->statement->get_stmt_node()
                    ->sql->show_transaction_sqls_oper->user_id;
      Session *s = driver->get_session_by_thread_id(user_id);
      if (!s) {
        LOG_ERROR("Thread id %d not found!\n", user_id);
        throw ThreadIdNotFound();
      }
      list<string> *transaction_sqls = s->get_transaction_sqls();

      if (transaction_sqls->size()) {
        list<string>::iterator it = transaction_sqls->begin();
        uint32_t i = 0;
        for (; it != transaction_sqls->end(); it++, i++) {
          list<const char *> row_data;
          char seq[11];
          snprintf(seq, sizeof(seq), "%d", i);
          row_data.push_back(seq);
          row_data.push_back((*it).c_str());

          add_row_packet(row_data);
        }
      }
    } catch (...) {
      driver->release_session_mutex();
      throw;
    }
    driver->release_session_mutex();
  } catch (ThreadIdNotFound &e) {
    throw;
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Connection get error!",
                     "42S02");
    throw ErrorPacketException();
  }
}

void wait_one_second() {
  ACE_Time_Value sleep_tv(1, 0);
  ACE_OS::sleep(sleep_tv);
}

MySQLMigrateNode::MySQLMigrateNode(ExecutePlan *plan) : MySQLExecuteNode(plan) {
  this->name = "MySQLMigrateNode";
  m_status = MIGRATE_STATUS_START;
  status = EXECUTE_STATUS_START;
  need_roll_back_topo = true;
  migrate_command_id = 0;
}
void MySQLMigrateNode::rollback_migrate_topo() {
  LOG_DEBUG("rollback_migrate_topo\n");
  // add mutex for dynamic change backend struct include space, source, server
  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *dynamic_mutex = backend->get_dynamic_modify_rep_mutex();
  ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_mutex);

  map<DataSpace *, migrate_type>::iterator it = migrate_space_map.begin();
  for (; it != migrate_space_map.end(); it++) {
    if (it->second == MIGRATE_PARTITION || it->second == MIGRATE_SPLIT) {
      ((PartitionedTable *)it->first)->rollback_next_scheme(handler);
    } else if (it->second == MIGRATE_SHARD) {
      PartitionScheme *scheme = NULL;
      if (migrate_shard_scheme_map.count(it->first))
        scheme = migrate_shard_scheme_map[it->first];
      ((PartitionedTable *)it->first)->rollback_next_shard_scheme(scheme);
    } else {
      ((Table *)it->first)->stop_migrate(handler);
    }
  }
}

void MySQLMigrateNode::generate_partition_migrate_task(
    const char *schema_name, const char *table_name,
    map<unsigned int, DataSource *> &migrate_partition_map, DataSpace *space,
    migrate_type type) {
  // generate MigrateThreadTask
  LOG_DEBUG("generate_partition_migrate_task\n");
  bool can_merge = false;
  map<unsigned int, DataSource *>::iterator it = migrate_partition_map.begin();
  for (; it != migrate_partition_map.end(); it++) {
    list<MigrateThreadTask *>::iterator it_t = migrate_task_list.begin();
    can_merge = false;
    for (; it_t != migrate_task_list.end(); it_t++) {
      if ((*it_t)->merge_migrate(schema_name, table_name, it->first,
                                 it->second)) {
        LOG_DEBUG(
            "merge migrate schema_name[%s] table[%s] vid [%u] source [%s]\n",
            schema_name, table_name, it->first, it->second->get_name());
        can_merge = true;
        break;
      }
    }
    if (!can_merge) {
      MigrateThreadTask *task = new MigrateThreadTask(
          plan->handler, space, it->second, schema_name, schema_name,
          table_name, schema_name, table_name, type, it->first);
      migrate_task_list.push_back(task);
    }
  }
}

PartitionedTable *sync_dynamic_add_migrate_part_table(
    PartitionedTable *part_table, const char *schema_name,
    const char *table_name) {
  char param[1024] = {0};
  sprintf(param, "%d %s %s", TYPE_DYNAMIC_ADD_MIGRATE_PART_TABLE, schema_name,
          table_name);
  Schema *schema = NULL;
  try {
    schema = get_migrate_schema(schema_name);
  } catch (...) {
    LOG_ERROR(
        "sync_dynamic_add_migrate_part_table get schema[%s] failed, plz "
        "dynamic add it\n",
        schema_name);
    return NULL;
  }

  Backend *backend = Backend::instance();
  bool is_start_sync = false;
  MultipleManager *mul = MultipleManager::instance();
  MultipleSyncTool *sync_tool = mul->get_sync_tool();
  // start sync, if success, slaves have add space, if failed, slaves have
  // cancaled.
  is_start_sync = backend->start_dynamic_operation_topic(
      DYNAMIC_ADD_MIGRATE_PARTTABLE_TOPIC_NAME, param);
  PartitionedTable *space = new PartitionedTable(part_table, table_name);
  space->set_from_config_source();

  if (is_start_sync) {
    string full_table_name;
    splice_full_table_name(schema_name, table_name, full_table_name);

    ACE_RW_Thread_Mutex *dynamic_add_mutex =
        backend->get_dynamic_modify_rep_mutex();
    ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_add_mutex);

    Backend::instance()->alter_table_flag_on(full_table_name);
    space->clear_name_pattern();
    try {
      schema->add_table(space);
    } catch (...) {
      LOG_ERROR(
          "sync_dynamic_add_migrate_part_table schema [%s] add part_table[%s] "
          "failed\n",
          schema_name, table_name);
      delete space;
      return NULL;
    }
  } else {
    LOG_ERROR("sync_dynamic_add_migrate_part_table start failed\n");
    delete space;
    return NULL;
  }

  DynamicOperationSyncCondition *cond = NULL;
  try {
    Backend::instance()->flush_config_to_zoo(false);
    /*Then fin the sync topic*/
    mul->acquire_dynamic_info_lock();
    unsigned long after_pub_version = mul->get_cur_config_info_version();
    mul->release_dynamic_info_lock();
    cond = new DynamicOperationSyncCondition();
    cond->prepare_condition(
        after_pub_version);  // The fin condition should ensure the slave
                             // dbscale has get the new config version.

    sync_tool->get_sync_topic()->set_sync_info_op_param(param);
    sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param, cond);
    sync_tool->publish_sync_message();
    sync_tool->wait_all_children_sync();
    delete cond;
    cond = NULL;
  } catch (Exception &e) {
    LOG_ERROR("dynamic migrate add parttable failed due to %s.\n", e.what());
    if (cond) {
      delete cond;
      cond = NULL;
    }
  }
  return space;
}

PartitionedTable *add_migrate_part_table(PartitionedTable *part_table,
                                         const char *schema_name,
                                         const char *table_name) {
  if (multiple_mode) {
    return sync_dynamic_add_migrate_part_table(part_table, schema_name,
                                               table_name);
  }
  Schema *schema = NULL;
  try {
    schema = get_migrate_schema(schema_name);
  } catch (...) {
    LOG_ERROR(
        "sync_dynamic_add_migrate_part_table get_migrate_schema failed\n");
    return NULL;
  }
  PartitionedTable *space = new PartitionedTable(part_table, table_name);
  space->set_from_config_source();
  Backend *backend = Backend::instance();
  string full_table_name;
  splice_full_table_name(schema_name, table_name, full_table_name);

  ACE_RW_Thread_Mutex *dynamic_add_mutex =
      backend->get_dynamic_modify_rep_mutex();
  ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_add_mutex);

  Backend::instance()->alter_table_flag_on(full_table_name);
  space->clear_name_pattern();
  try {
    schema->add_table(space);
  } catch (...) {
    LOG_ERROR(
        "sync_dynamic_add_migrate_part_table schema [%s] add part_table[%s] "
        "failed\n",
        schema_name, table_name);
    delete space;
    return NULL;
  }
  return space;
}

Table *sync_dynamic_add_norm_table(const char *schema_name,
                                   const char *table_name, DataSource *source) {
  Table *tab = NULL;
  char param[1024] = {0};
  sprintf(param, "%d %s %s %s %s 0", TYPE_DYNAMIC_ADD_NORM_TABLE, table_name,
          schema_name, source->get_name(), "null");
  Schema *schema = NULL;
  try {
    schema = get_migrate_schema(schema_name);
  } catch (...) {
    LOG_ERROR("sync_dynamic_add_norm_table get_migrate_schema failed\n");
    return NULL;
  }

  Backend *backend = Backend::instance();
  bool is_start_sync = false;
  MultipleManager *mul = MultipleManager::instance();
  MultipleSyncTool *sync_tool = mul->get_sync_tool();
  is_start_sync = backend->start_dynamic_operation_topic(
      DYNAMIC_ADD_NORMTABLE_TOPIC_NAME, param);

  if (is_start_sync) {
    tab = new Table(table_name, source, schema, true, NULL);
    tab->set_from_config_source();
    string full_table_name;
    splice_full_table_name(schema_name, table_name, full_table_name);

    ACE_RW_Thread_Mutex *dynamic_add_mutex =
        backend->get_dynamic_modify_rep_mutex();
    ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_add_mutex);

    Backend::instance()->alter_table_flag_on(full_table_name);
    try {
      schema->add_table(tab);
    } catch (...) {
      LOG_ERROR(
          "sync_dynamic_add_norm_table schema [%s] add norm_table[%s] failed\n",
          schema_name, table_name);
      delete tab;
      tab = NULL;
      return NULL;
    }
  } else {
    LOG_ERROR("sync_dynamic_add_norm_table start failed\n");
    return NULL;
  }

  DynamicOperationSyncCondition *cond = NULL;
  try {
    Backend::instance()->flush_config_to_zoo(false);
    /*Then fin the sync topic*/
    mul->acquire_dynamic_info_lock();
    unsigned long after_pub_version = mul->get_cur_config_info_version();
    mul->release_dynamic_info_lock();
    cond = new DynamicOperationSyncCondition();
    cond->prepare_condition(
        after_pub_version);  // The fin condition should ensure the slave
                             // dbscale has get the new config version.

    sync_tool->get_sync_topic()->set_sync_info_op_param(param);
    sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param, cond);
    sync_tool->publish_sync_message();
    sync_tool->wait_all_children_sync();
    delete cond;
    cond = NULL;
  } catch (Exception &e) {
    LOG_ERROR("dynamic migrate add parttable failed due to %s.\n", e.what());
    if (cond) {
      delete cond;
      cond = NULL;
    }
  }
  return tab;
}

Table *add_migrate_norm_table(const char *schema_name, const char *table_name,
                              DataSource *source) {
  if (multiple_mode) {
    return sync_dynamic_add_norm_table(schema_name, table_name, source);
  }
  Schema *schema = NULL;
  try {
    schema = get_migrate_schema(schema_name);
  } catch (...) {
    LOG_ERROR("sync_dynamic_add_norm_table get_migrate_schema failed\n");
    return NULL;
  }

  Table *tab = new Table(table_name, source, schema, true, NULL);
  tab->set_from_config_source();
  string full_table_name;
  splice_full_table_name(schema_name, table_name, full_table_name);

  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *dynamic_add_mutex =
      backend->get_dynamic_modify_rep_mutex();
  ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_add_mutex);

  Backend::instance()->alter_table_flag_on(full_table_name);
  try {
    schema->add_table(tab);
  } catch (...) {
    LOG_ERROR(
        "sync_dynamic_add_norm_table schema [%s] add norm_table[%s] failed\n",
        schema_name, table_name);
    delete tab;
    tab = NULL;
    return NULL;
  }
  return tab;
}

void MySQLMigrateNode::prepare_space_for_migrate(migrate_op_node *node) {
  const char *table_name = node->table->table_name;
  const char *schema_name = node->table->schema_name == NULL
                                ? session->get_schema()
                                : node->table->schema_name;
  Backend *backend = Backend::instance();
  DataSpace *space = backend->get_data_space_for_table(schema_name, table_name);
  migrate_type type = node->type;
  if (type != MIGRATE_SHARD) {
    if (Driver::get_driver()->get_config_helper()->check_migrate_table(
            schema_name, table_name)) {
      char msg[1000];
      sprintf(msg,
              "the table %s.%s is already in migration, dbscale refuse this "
              "request",
              schema_name, table_name);
      set_error_message(msg);
      throw Error(
          "the table is already in migration, dbscale refuse this request");
    }
  }

  if (type == MIGRATE_PARTITION && space->get_dataspace_type() == TABLE_TYPE &&
      ((Table *)space)->is_partitioned()) {
    PartitionedTable *part_table = (PartitionedTable *)space;
    if (part_table->get_partition_scheme()->is_shard()) {
      set_error_message(
          "DBScale do not support migrate shard table virtual partition now.");
      throw Error(
          "DBScale do not support migrate shard table virtual partition now.");
    }
    if (part_table->get_name_pattern() != NULL) {
      // if the table is pattern_table, create new DataSpace for this migrate
      // table.
      part_table = add_migrate_part_table(part_table, schema_name, table_name);
      space = part_table;
      if (!space) {
        set_error_message("MySQLMigrateNode add part table failed");
        throw Error("MySQLMigrateNode add part table failed");
      }
    }

    migrate_partition_node *partition_node = node->partition;
    // generate migrate_partition_map, migrate_partition_map will record vid map
    // to target_source
    map<unsigned int, DataSource *> migrate_partition_map;
    while (partition_node) {
      unsigned int vid = partition_node->vid;
      const char *source_name = partition_node->source_name;
      DataSource *source = backend->find_data_source(source_name);
      if (source == NULL) {
        set_error_message("migrate table failed cause not find this source");
        throw Error("Not find this source.");
      }
      if (!source->get_is_conf_source()) {
        LOG_ERROR("data-source [%s] should be config source for migrate.\n",
                  source->get_name());
        throw Error("the datasource should be config source for migrate.");
      }

      if (migrate_partition_map.count(vid)) {
        set_error_message(
            "DBScale does not support migrate same virtual partition mul "
            "times");
        throw Error(
            "DBScale does not support migrate same virtual partition mul "
            "times");
      }
      migrate_partition_map[vid] = source;
      partition_node = partition_node->next;
    }
    migrate_space_map[space] = type;
    part_table->generate_next_version_scheme(handler, migrate_partition_map);

    // generate MigrateThreadTask
    generate_partition_migrate_task(schema_name, table_name,
                                    migrate_partition_map, space, type);
  } else if (type == MIGRATE_SPLIT &&
             space->get_dataspace_type() == TABLE_TYPE &&
             ((Table *)space)->is_partitioned()) {
    PartitionedTable *part_table = (PartitionedTable *)space;
    if (part_table->get_partition_scheme()->is_shard()) {
      set_error_message(
          "DBScale do not support migrate shard table virtual partition now.");
      throw Error(
          "DBScale do not support migrate shard table virtual partition now.");
    }

    if (part_table->get_name_pattern() != NULL) {
      // if the table is pattern_table, create new DataSpace for this migrate
      // table.
      part_table = add_migrate_part_table(part_table, schema_name, table_name);
      space = part_table;
      if (!space) {
        set_error_message("MySQLMigrateNode add part table failed");
        throw Error("MySQLMigrateNode add part table failed");
      }
    }

    // get split source DataSource
    const char *source_name = node->source_name;
    DataSource *source = backend->find_data_source(source_name);
    if (source == NULL) {
      set_error_message("migrate table failed cause not find this source");
      throw Error("Not find this source.");
    }
    if (!source->get_is_conf_source()) {
      LOG_ERROR("data-source [%s] should be config source for migrate.\n",
                source->get_name());
      throw Error("the datasource should be config source for migrate.");
    }

    // get split target source size
    name_item *split_source_item = node->split_source_list->next;
    unsigned int source_size = 1;
    while (split_source_item != node->split_source_list) {
      source_size++;
      split_source_item = split_source_item->next;
    }
    // get split source DataSource contains vid_set
    set<unsigned int> vid_set = part_table->get_source_contains_vid_set(source);

    // check vid_set.size larger than source_size
    if (vid_set.size() < source_size) {
      LOG_ERROR(
          "source [%s] contains [%d] virtual partitions, can't split to [%d] "
          "sources\n",
          source_name, vid_set.size(), source_size);
      throw Error("fail to split source into so many sources.");
    }

    // generate migrate_partition_map, migrate_partition_map will record vid map
    // to target_source
    map<unsigned int, DataSource *> migrate_partition_map;
    set<unsigned int>::iterator it_v = vid_set.begin();
    for (; it_v != vid_set.end(); it_v++) {
      const char *target_source_name = split_source_item->name;
      DataSource *target_source = backend->find_data_source(target_source_name);
      if (target_source == NULL) {
        char msg[1000];
        sprintf(msg, "migrate table failed cause not find target source [%s]",
                target_source_name);
        set_error_message(msg);
        throw Error("Not find this target source.");
      }
      if (!target_source->get_is_conf_source()) {
        LOG_ERROR("data-source [%s] should be config source for migrate.\n",
                  target_source->get_name());
        set_error_message("migrate table failed cause not config source.");
        throw Error("the datasource should be config source for migrate.");
      }

      set<unsigned int> vid_set_tmp =
          part_table->get_source_contains_vid_set(target_source);
      if (vid_set_tmp.size() > 0) {
        char msg[1000];
        sprintf(msg,
                "migrate table failed due to target_source [%s] contains "
                "virtual partition",
                target_source_name);
        set_error_message(msg);
        throw Error(
            "migrate table failed due to target_source contains virtual "
            "partition.");
      }
      migrate_partition_map[*it_v] = target_source;
      LOG_INFO("migrate split table [%s.%s] vid [%d] to source [%s]\n",
               schema_name, table_name, *it_v, target_source_name);
      split_source_item = split_source_item->next;
    }

    migrate_space_map[space] = type;
    part_table->generate_next_version_scheme(handler, migrate_partition_map);

    // generate MigrateThreadTask
    generate_partition_migrate_task(schema_name, table_name,
                                    migrate_partition_map, space, type);
  } else if (type == MIGRATE_SHARD &&
             space->get_dataspace_type() == TABLE_TYPE &&
             ((Table *)space)->is_partitioned()) {
    PartitionedTable *part_table = (PartitionedTable *)space;
    if (!part_table->get_partition_scheme()->is_shard()) {
      char msg[1000];
      sprintf(msg, "plz check table[%s.%s], it is not shard table.",
              schema_name, table_name);
      set_error_message(msg);
      throw Error("plz check whether table is shard table or not.");
    }

    const char *target_source_name = node->target_source_name;
    const char *source_name = node->source_name;

    DataSource *source = backend->find_data_source(source_name);
    if (source == NULL) {
      set_error_message("migrate table failed cause not find this source");
      throw Error("Not find this source.");
    }
    if (!source->get_is_conf_source()) {
      LOG_ERROR("data-source [%s] should be config source for migrate.\n",
                source->get_name());
      throw Error("the datasource should be config source for migrate.");
    }

    DataSource *target_source = backend->find_data_source(target_source_name);
    if (target_source == NULL) {
      set_error_message("migrate table failed cause not find target_source");
      throw Error("Not find target_source.");
    }
    if (!target_source->get_is_conf_source()) {
      LOG_ERROR("data-source [%s] should be config source for migrate.\n",
                target_source->get_name());
      set_error_message("migrate table failed cause not config source.");
      throw Error("the datasource should be config source for migrate.");
    }

    if (part_table->get_name_pattern() != NULL) {
      // if the table is pattern_table, create new DataSpace for this migrate
      // table.
      part_table = add_migrate_part_table(part_table, schema_name, table_name);
      space = part_table;
      if (!space) {
        set_error_message("MySQLMigrateNode add part table failed");
        throw Error("MySQLMigrateNode add part table failed");
      }
    }

    unsigned int shard_id = part_table->get_max_shard_id(source);
    migrate_space_map[space] = type;
    PartitionScheme *next_scheme =
        part_table->generate_next_version_shard_scheme(handler, shard_id,
                                                       source, target_source);
    migrate_shard_scheme_map[space] = next_scheme;
    unsigned int source_machine_id =
        part_table->get_partition(shard_id)->get_virtual_machine_id();
    unsigned int target_machine_id =
        next_scheme->get_partition(shard_id)->get_virtual_machine_id();
    string source_schema = schema_name;
    string target_schema = schema_name;
    if (source_machine_id > 0)
      adjust_shard_schema(schema_name, schema_name, source_schema,
                          source_machine_id, shard_id);
    if (target_machine_id > 0)
      adjust_shard_schema(schema_name, schema_name, target_schema,
                          target_machine_id, shard_id);

    MigrateThreadTask *task = new MigrateThreadTask(
        plan->handler, part_table, target_source, schema_name,
        source_schema.c_str(), table_name, target_schema.c_str(), table_name,
        type, shard_id);
    migrate_task_list.push_back(task);
  } else if (type == MIGRATE_NO_PARTITION) {
    if (space->get_dataspace_type() == TABLE_TYPE &&
        ((Table *)space)->is_partitioned()) {
      set_error_message("migrate table failed, migrate a partitioned table.");
      throw Error("Migrate table is a partitioned table.");
    }
    DataSource *source = backend->find_data_source(node->target_source_name);
    if (source == NULL) {
      set_error_message("migrate table failed cause not find this source");
      throw Error("Not find this source.");
    }
    if (!source->get_is_conf_source()) {
      LOG_ERROR("data-source [%s] should be config source for migrate.\n",
                source->get_name());
      throw Error("the datasource should be config source for migrate.");
    }
    DataSource *from_source = space->get_data_source();
    if (session->is_datasource_cover_session_level(from_source, source) ||
        session->is_datasource_cover_session_level(source, from_source)) {
      LOG_ERROR(
          "Should not migrate norm table between sources [%s] and [%s] which "
          "have replication relation.\n",
          from_source->get_name(), source->get_name());
      throw Error(
          "Should not migrate norm table between sources which has replication "
          "relation.");
    }

    if (!(space->get_dataspace_type() == TABLE_TYPE &&
          ((Table *)space)->get_name_pattern() == NULL)) {
      space = add_migrate_norm_table(schema_name, table_name,
                                     space->get_data_source());
      if (!space) {
        set_error_message("MySQLMigrateNode add_migrate_norm_table failed");
        throw Error("MySQLMigrateNode add_migrate_norm_table failed");
      }
    }
    migrate_space_map[space] = type;
    migrate_norm_table_map[space] = source;
    ((Table *)space)->start_migrate(handler);

    MigrateThreadTask *task = new MigrateThreadTask(
        plan->handler, space, source, schema_name, schema_name, table_name,
        schema_name, table_name, type, 0);
    migrate_task_list.push_back(task);
  } else {
    set_error_message("migrate failed, unknown migrate type.");
    throw Error("Unknown migrate type.");
  }
}

string generate_mul_migrate_node_message(migrate_op_node *node,
                                         const char *default_schema_name,
                                         int &node_item_num) {
  // migrate node messge format:
  // node_item_num schema_name table_name migrate_type type_message
  ostringstream ostr;
  node_item_num = 1;
  const char *schema_name = node->table->schema_name == NULL
                                ? default_schema_name
                                : node->table->schema_name;
  const char *table_name = node->table->table_name;
  migrate_type type = node->type;

  ostr << schema_name << MESSAGE_ATTR_DELIMITER << table_name
       << MESSAGE_ATTR_DELIMITER << type << MESSAGE_ATTR_DELIMITER;
  node_item_num += 3;
  if (type == MIGRATE_PARTITION) {
    // MIGRATE_PARTITION message format: vid source_name [vid source_name]
    migrate_partition_node *partition_node = node->partition;
    map<unsigned int, DataSource *> migrate_partition_map;
    while (partition_node) {
      unsigned int vid = partition_node->vid;
      const char *source_name = partition_node->source_name;
      ostr << vid << MESSAGE_ATTR_DELIMITER << source_name
           << MESSAGE_ATTR_DELIMITER;
      node_item_num += 2;
      partition_node = partition_node->next;
    }
  } else if (type == MIGRATE_SPLIT) {
    // MIGRATE_SPLIT message format: source_name target_source_name
    // [target_source_name] get split source DataSource
    const char *source_name = node->source_name;
    ostr << source_name << MESSAGE_ATTR_DELIMITER;
    node_item_num++;
    name_item *split_source_item = node->split_source_list;
    do {
      const char *target_source_name = split_source_item->name;
      ostr << target_source_name << MESSAGE_ATTR_DELIMITER;
      node_item_num++;
      split_source_item = split_source_item->next;
    } while (split_source_item != node->split_source_list);
  } else if (type == MIGRATE_SHARD) {
    // MIGRATE_SHARD message format: source_name target_source_name
    const char *target_source_name = node->target_source_name;
    const char *source_name = node->source_name;
    ostr << source_name << MESSAGE_ATTR_DELIMITER << target_source_name
         << MESSAGE_ATTR_DELIMITER;
    node_item_num += 2;
  } else if (type == MIGRATE_NO_PARTITION) {
    // MIGRATE_NO_PARTITION message format: target_source_name
    ostr << node->target_source_name << MESSAGE_ATTR_DELIMITER;
    node_item_num++;
  }
  char message[10 + ostr.str().length()];
  int len = sprintf(message, "%d %s", node_item_num, ostr.str().c_str());
  message[len] = '\0';
  LOG_DEBUG("generate_mul_migrate_node_message [%s]\n", message);
  return message;
}

string generate_mul_migrate_sync_message(DynamicAddSpaceType type,
                                         migrate_op_node *node,
                                         const char *default_schema_name,
                                         long migrate_command_id) {
  // migrate message format: migrate_status migrate_command_id all_item_num
  // migrate_node_num node_message [node_message]
  string message;
  int node_num = 0;
  int all_item_num = 4;
  int node_item_num = 0;
  while (node) {
    message += generate_mul_migrate_node_message(node, default_schema_name,
                                                 node_item_num);
    all_item_num += node_item_num;
    node_num++;
    node = node->next;
  }

  boost::erase_tail(message, 1);
  char migrate_message[message.length() + 20];
  int len =
      sprintf(migrate_message, "%d %ld %d %d %s", type, migrate_command_id,
              all_item_num, node_num, message.c_str());
  migrate_message[len] = '\0';
  LOG_DEBUG("generate_mul_migrate_sync_message [%s]\n", migrate_message);
  return migrate_message;
}

bool MySQLMigrateNode::migrate_sync(DynamicAddSpaceType type) {
  if (multiple_mode && MultipleManager::instance()->get_is_cluster_master()) {
    Backend *backend = Backend::instance();
    bool is_start_sync = false;
    MultipleManager *mul = MultipleManager::instance();
    MultipleSyncTool *sync_tool = mul->get_sync_tool();

    migrate_op_node *node = plan->statement->get_stmt_node()->sql->migrate_oper;
    string param = generate_mul_migrate_sync_message(
        type, node, session->get_schema(), migrate_command_id);

    string topic_name;
    if (type == TYPE_MIGRATE_PREAPRE)
      topic_name = MIGRATE_PREPARE_TOPIC_NAME;
    else if (type == TYPE_MIGRATE_ROLLBACK)
      topic_name = MIGRATE_ROLLBACK_TOPIC_NAME;
    else if (type == TYPE_MIGRATE_END)
      topic_name = MIGRATE_END_TOPIC_NAME;
    else {
      char msg[1000];
      sprintf(msg, "migrate_sync get Unknown type [%d]", type);
      set_error_message(msg);
      return false;
    }

    is_start_sync = backend->start_dynamic_operation_topic(topic_name.c_str(),
                                                           param.c_str());

    if (!is_start_sync) {
      set_error_message(
          "Fail to start_dynamic_operation_topic for migrate_prepare_sync.");
      throw Error(
          "Fail to start_dynamic_operation_topic for migrate_prepare_sync.");
    }

    if (is_start_sync) {
      DynamicOperationSyncCondition *cond = NULL;
      try {
        /*Then fin the sync topic*/
        mul->acquire_dynamic_info_lock();
        unsigned long after_pub_version = mul->get_cur_config_info_version();
        mul->release_dynamic_info_lock();
        cond = new DynamicOperationSyncCondition();
        cond->prepare_condition(
            after_pub_version);  // The fin condition should ensure the slave
                                 // dbscale has get the new config version.

        sync_tool->get_sync_topic()->set_sync_info_op_param(param);
        sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param.c_str(),
                                           cond);
        sync_tool->publish_sync_message();
        sync_tool->wait_all_children_sync();
        delete cond;
        cond = NULL;

      } catch (Exception &e) {
        char msg[1000];
        sprintf(msg, "error happen in fin migrate_prepare_sync, due to %s.\n",
                e.what());
        set_error_message(msg);
        if (cond) {
          delete cond;
          cond = NULL;
        }
      }
    }
  }
  return true;
}

int MySQLMigrateNode::init_migrate_task() {
  LOG_DEBUG("MySQLMigrateNode init_migrate_task\n");
  try {
    migrate_command_id = Backend::instance()->get_next_migrate_command_id();
    if (!migrate_sync(TYPE_MIGRATE_PREAPRE)) return -1;
    migrate_op_node *node = plan->statement->get_stmt_node()->sql->migrate_oper;
    set<string> migrate_table_set;
    while (node) {
      const char *table_name = node->table->table_name;
      const char *schema_name = node->table->schema_name == NULL
                                    ? session->get_schema()
                                    : node->table->schema_name;

      string full_table_name;
      splice_full_table_name(schema_name, table_name, full_table_name);
      if (migrate_table_set.count(full_table_name)) {
        set_error_message(
            "dbscale do not support migrate same table in different migrate "
            "field");
        throw Error(
            "dbscale do not support migrate same table in different migrate "
            "field.");
      }
      migrate_table_set.insert(full_table_name);
      // do not migrate dbscale_tmp
      if (lower_case_compare(schema_name, "dbscale_tmp"))
        prepare_space_for_migrate(node);
      node = node->next;
    }
  } catch (exception &e) {
    char msg[1000];
    sprintf(msg, "migrate failed, [%s]", e.what());
    set_error_message(msg);
    return -1;
  }
  return 0;
}

void MySQLMigrateNode::start_tasks() {
  LOG_DEBUG("MySQLMigrateNode start_tasks\n");
  list<MigrateThreadTask *>::iterator it = migrate_task_list.begin();
  for (; it != migrate_task_list.end(); it++) {
    (*it)->start();
#ifndef DBSCALE_TEST_DISABLE
    /*just for test coverage*/
    Backend *bk = Backend::instance();
    dbscale_test_info *test_info = bk->get_dbscale_test_info();
    if (!strcasecmp(test_info->test_case_name.c_str(), "migrate_test") &&
        !strcasecmp(test_info->test_case_operation.c_str(), "delay_flush")) {
      // each MigrateThreadTask will delay 1s to start migrate
      wait_one_second();
    }
#endif
  }
}

bool MySQLMigrateNode::unlock_one_server(Connection *conn) {
  try {
    conn->execute_one_modify_sql("unlock tables;");
    string tx_isolation_level =
        handler->get_session()->get_session_var_value("TRANSACTION_ISOLATION");
    if (tx_isolation_level != "") {
      string tx_sql = "SET SESSION TRANSACTION_ISOLATION = ";
      tx_sql += tx_isolation_level;
      tx_sql += ";";
      conn->execute_one_modify_sql(tx_sql.c_str());
    }
  } catch (...) {
    set_error_message("MySQLMigrateNode unlock_tables failed");
    return false;
  }
  return true;
}
bool MySQLMigrateNode::flush_one_server(Connection *conn) {
  try {
    // FLUSH TABLES WITH READ LOCK; require TRANSACTION ISOLATION LEVEL
    // REPEATABLE READ
    conn->execute_one_modify_sql(
        "SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;");
    conn->execute_one_modify_sql("FLUSH TABLES WITH READ LOCK;");
  } catch (...) {
    if (conn) {
      char msg[1000];
      sprintf(msg, "MySQLMigrateNode server [%s] flush_one_table get exception",
              conn->get_server()->get_name());
      set_error_message(msg);
    }
    return false;
  }
  return true;
}

bool MySQLMigrateNode::flush_all_servers_with_consistent_point() {
  bool ret = false;
  ret = flush_all_servers();
  return ret;
}
/* after generate_migrate_flush_group, these task flush flow will like below:
   flush conn;
   this tasks start transaction;
   unlock_tables;
   one server by one server, then one server flush will not affect another
   server.

   if get exception dring flush servers, MySQLMigrateNode will stop all tasks
   and unlock_all_servers.
*/
bool MySQLMigrateNode::flush_all_servers() {
  try {
    list<MigrateThreadTask *>::iterator it = migrate_task_list.begin();
    for (; it != migrate_task_list.end(); it++) {
      if (!(*it)->start_migrate_select_transaction())
        throw Error("MySQLMigrateNode start_migrate_select_transaction failed");
    }
  } catch (...) {
    set_error_message("MySQLMigrateNode flush_all_servers failed");
    return false;
  }
  return true;
}

/* one migrate command may contains multiple migrate tasks
   different task may share same source server.
   so we should first distinguish the tasks */
bool MySQLMigrateNode::generate_migrate_flush_group() {
  list<MigrateThreadTask *>::iterator it = migrate_task_list.begin();
  for (; it != migrate_task_list.end(); it++) {
    Connection *flush_conn = (*it)->generate_flush_conn();
    if (!flush_conn) {
      set_error_message(
          "MySQLMigrateNode generate_migrate_flush_group failed get "
          "connection");
      return false;
    }
    bool find_group = false;

    map<Connection *, list<MigrateThreadTask *> >::iterator it_f =
        flush_conn_map.begin();
    for (; it_f != flush_conn_map.end(); it_f++) {
      if (is_share_same_server(flush_conn->get_server(),
                               it_f->first->get_server())) {
        flush_conn->get_pool()->add_back_to_free(flush_conn);
        flush_conn = NULL;
        it_f->second.push_back(*it);
        find_group = true;
        break;
      }
    }
    if (!find_group) {
      flush_conn_map[flush_conn].push_back(*it);
    }
  }
  return true;
}

void MySQLMigrateNode::unlock_all_servers() {
  map<Connection *, list<MigrateThreadTask *> >::iterator it_f =
      flush_conn_map.begin();
  for (; it_f != flush_conn_map.end();) {
    if (unlock_one_server(it_f->first)) {
      it_f->first->get_pool()->add_back_to_free(it_f->first);
    } else {
      it_f->first->get_pool()->add_back_to_dead(it_f->first);
    }
    flush_conn_map.erase(it_f++);
  }
}

void MySQLMigrateNode::wake_up_tasks() {
  LOG_DEBUG("MySQLMigrateNode wake_up_tasks\n");
  list<MigrateThreadTask *>::iterator it = migrate_task_list.begin();
  for (; it != migrate_task_list.end(); it++) {
    (*it)->wakeup();
  }
}
void MySQLMigrateNode::stop_all_tasks() {
  LOG_DEBUG("MySQLMigrateNode stop_all_tasks\n");
  unlock_all_servers();
  list<MigrateThreadTask *>::iterator it = migrate_task_list.begin();
  for (; it != migrate_task_list.end(); it++) {
    (*it)->stop();
  }
}
bool MySQLMigrateNode::check_all_task_status(MigrateStatus s) {
  list<MigrateThreadTask *>::iterator it = migrate_task_list.begin();
  for (; it != migrate_task_list.end(); it++) {
    MigrateStatus t_status = (*it)->get_migrate_status();
    if (t_status == MIGRATE_STATUS_ERROR) {
      m_status = MIGRATE_STATUS_ERROR;
      return false;
    } else if (t_status != s) {
      return false;
    }
  }
  return true;
}
void MySQLMigrateNode::wait_all_task_stop() {
  LOG_DEBUG("MySQLMigrateNode wait_all_task_stop\n");
  list<MigrateThreadTask *>::iterator it = migrate_task_list.begin();
  for (; it != migrate_task_list.end(); it++) {
    (*it)->wait_for_cond();
  }
}

void MySQLMigrateNode::clean() {
  LOG_DEBUG("MySQLMigrateNode clean\n");
  list<MigrateThreadTask *>::iterator it = migrate_task_list.begin();
  for (; it != migrate_task_list.end(); it++) {
    delete *it;
  }
  migrate_task_list.clear();
}

void MySQLMigrateNode::change_topo() {
  // format: dynamic_type migrate_command_id message_size schema_name table_name
  // table_type [scheme_name virtual_map | source_name]
  string param;
  int message_size = 3;
  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *dynamic_mutex = backend->get_dynamic_modify_rep_mutex();
  dynamic_mutex->acquire_write();
  try {
#ifndef DBSCALE_TEST_DISABLE
    /*just for test fail_change_topo*/
    dbscale_test_info *test_info =
        handler->get_session()->get_dbscale_test_info();
    if (!strcasecmp(test_info->test_case_name.c_str(), "mul_migrate_test") &&
        !strcasecmp(test_info->test_case_operation.c_str(),
                    "fail_change_topo")) {
      LOG_DEBUG("dbscale test migrat change_topo failed.\n");
      ZooKeeperTool::instance()->close_zookeeper();
      ACE_ASSERT(0);
      dynamic_mutex->release();
      return;
    }
#endif

    map<DataSpace *, migrate_type>::iterator it_m = migrate_space_map.begin();
    // change part_table topo
    for (; it_m != migrate_space_map.end(); it_m++) {
      if (it_m->second == MIGRATE_PARTITION || it_m->second == MIGRATE_SPLIT) {
        PartitionedTable *part_table = (PartitionedTable *)it_m->first;
        part_table->finish_migrate_one_virtual_partition();

        param += " ";
        param += part_table->generate_migrate_topo_sync_message();
        message_size += 5;
        backend->clean_join_table_space(it_m->first);
      } else if (it_m->second == MIGRATE_SHARD) {
        PartitionedTable *part_table = (PartitionedTable *)it_m->first;
        PartitionScheme *scheme = migrate_shard_scheme_map[it_m->first];
        part_table->finish_migrate_shard_partition(scheme);

        param += " ";
        param += part_table->generate_migrate_topo_sync_message();
        message_size += 5;
        backend->clean_join_table_space(it_m->first);
      }
    }
    // change norm_table topo
    map<DataSpace *, DataSource *>::iterator it_n =
        migrate_norm_table_map.begin();
    for (; it_n != migrate_norm_table_map.end(); it_n++) {
      DataSpace *source_space = it_n->first;
      source_space->set_data_source(it_n->second);
      ((Table *)source_space)->stop_migrate(handler);
      backend->clean_join_table_space(source_space);

      param += " ";
      param += ((Table *)source_space)->generate_migrate_topo_sync_message();
      message_size += 4;
    }
    dynamic_mutex->release();
  } catch (...) {
    /*the code should never run here, if run here, config source has record
     * the table has channged topo, but in dbscale memory, change_topo failed.
     * we can't clean source data or target data. and the table's topo is
     * inconstance with config_source's record. we just let dbscale down, when
     * dbscale starts, the table topo will be consistence and the migrate data
     * will be clean by MigrateCleanThread*/
    dynamic_mutex->release();
    LOG_ERROR("MigrateThreadTask change_topo get unexpect exception.\n");
    ZooKeeperTool::instance()->close_zookeeper();
    ACE_ASSERT(0);
  }

  if (multiple_mode) {
    char *param_message = new char[1000 + param.length()];
    int p_len = sprintf(param_message, "%d %ld %d%s", TYPE_MIGRATE_CHANGE_TOPO,
                        migrate_command_id, message_size, param.c_str());
    param_message[p_len] = '\0';
    LOG_INFO(
        "Do sync_migrate_change_topo with topic_name [%s] param_message "
        "[%s].\n",
        MIGRATE_CHANGE_TOPO_TOPIC_NAME, param_message);
    sync_migrate_change_topo(MIGRATE_CHANGE_TOPO_TOPIC_NAME, param_message);
    delete[] param_message;
  }

  list<MigrateThreadTask *>::iterator it = migrate_task_list.begin();
  for (; it != migrate_task_list.end(); it++) {
    // all migrate task change block table type
    (*it)->block_after_change_topo();
  }
}
bool MySQLMigrateNode::update_migrate_config() {
  LOG_DEBUG("MySQLMigrateNode update_migrate_config\n");
  list<string> config_list;
  list<MigrateThreadTask *>::iterator it = migrate_task_list.begin();
  for (; it != migrate_task_list.end(); it++) {
    // all migrate task get migrate record, if migrate_type is
    // MIGRATE_NO_PARTITION, get new table config
    list<string> tmp_list = (*it)->get_migrate_config_list();
    config_list.insert(config_list.end(), tmp_list.begin(), tmp_list.end());
  }

  map<DataSpace *, migrate_type>::iterator it_m = migrate_space_map.begin();
  for (; it_m != migrate_space_map.end(); it_m++) {
    if (it_m->second == MIGRATE_PARTITION || it_m->second == MIGRATE_SHARD ||
        it_m->second == MIGRATE_SPLIT) {
      // record next scheme config
      PartitionedTable *part_table = (PartitionedTable *)it_m->first;
      config_list.push_back(Driver::get_driver()
                                ->get_config_helper()
                                ->generate_partition_scheme_config(
                                    part_table->get_next_version_scheme()));

      // record next partition table config
      const char *next_scheme_name =
          part_table->get_next_version_scheme()->get_name();
      config_list.push_back(Driver::get_driver()
                                ->get_config_helper()
                                ->generate_partition_table_config(
                                    part_table, next_scheme_name,
                                    part_table->get_next_virtual_map()));
    }
  }
#ifndef DBSCALE_TEST_DISABLE
  /*just for test fail_change_topo*/
  dbscale_test_info *test_info =
      handler->get_session()->get_dbscale_test_info();
  if (!strcasecmp(test_info->test_case_name.c_str(), "migrate") &&
      !strcasecmp(test_info->test_case_operation.c_str(), "fail_change_topo")) {
    LOG_DEBUG("dbscale test migrat change_topo failed.\n");
    return false;
  }
#endif
  bool ret =
      Driver::get_driver()->get_config_helper()->update_config(config_list);
#ifndef DBSCALE_TEST_DISABLE
  /*just for test coverage*/
  Backend *bk = Backend::instance();
  dbscale_test_info *test_infobk = bk->get_dbscale_test_info();
  if (!strcasecmp(test_infobk->test_case_name.c_str(), "migrate_test") &&
      !strcasecmp(test_infobk->test_case_operation.c_str(),
                  "sync_binlog_update_config_failed")) {
    LOG_DEBUG("sync_binlog reset update_config failed for code coverage\n");
    ret = false;
  }
  /*just for test fail_change_topo*/
  test_info = handler->get_session()->get_dbscale_test_info();
  if (!strcasecmp(test_info->test_case_name.c_str(), "migrate") &&
      !strcasecmp(test_info->test_case_operation.c_str(),
                  "fail_change_topo_update")) {
    LOG_DEBUG("dbscale test migrat change_topo update failed.\n");
    ret = false;
  }
#endif
  return ret;
}

void MySQLMigrateNode::send_error_packet(string msg) {
  if (!plan->handler->get_is_migrate_async_handler()) {
    session->acquire_has_send_client_error_packet_mutex();
    if (session->get_has_send_client_error_packet()) {
      session->release_has_send_client_error_packet_mutex();
      return;
    }
    Packet *packet = handler->get_error_packet(9001, msg.c_str(), NULL);
    try {
      handler->send_to_client(packet);
    } catch (...) {
      LOG_ERROR("migrate send_to_client get error\n");
    }
    delete packet;
    session->release_has_send_client_error_packet_mutex();
  }
}
void MySQLMigrateNode::execute() {
  LOG_DEBUG("MySQLMigrateNode execute\n");
  if (migrate_async && !plan->handler->get_is_migrate_async_handler()) {
    MigrateHandlerTask *task = plan->handler->get_migrate_handler_task(
        plan->statement->get_sql(), plan->session->get_schema());
    task->set_session_var_map(*(plan->session->get_session_var_map()));
    MigrateManager::instance()->add_migrate_task(task);
    send_ok_packet_to_client(handler, 0, 0);
    status = EXECUTE_STATUS_COMPLETE;
    return;
  }
#ifndef CLOSE_MIGRATE
  while (1) {
    if (ACE_Reactor::instance()->reactor_event_loop_done()) {
      LOG_INFO("Stop MySQLMigrateNode::execute due to dbscale stop.\n");
      set_error_message("Stop MySQLMigrateNode::execute due to dbscale stop.");
      m_status = MIGRATE_STATUS_ERROR;
    }
    // TODO: allow to be canceled by other thread by kill command
    switch (m_status) {
      case MIGRATE_STATUS_START: {
        if (init_migrate_task()) {
          try {
            migrate_sync(TYPE_MIGRATE_ROLLBACK);
          } catch (...) {
            set_error_message(
                "MySQLMigrateNode migrate_sync rollback migrate failed");
          }
          set_error_message(
              "migrate failed during status_start, please check log.");
          string msg = generate_error_message();
          send_error_packet(msg);
          rollback_migrate_topo();
          AsyncTaskWorker atw;
          atw.register_async_task(plan->statement->get_sql(),
                                  "Start migrate task");
          atw.report_async_task_state(ASYNC_TASK_ERROR, 100, msg.c_str());
          status = EXECUTE_STATUS_COMPLETE;
          return;
        }
        m_status = MIGRATE_STATUS_PREPARE;

        if (migrate_task_list.empty()) {
          // if migrate command contains "dbscale_tmp", migrate will not
          // generate migrate task for it, then migrate_task_list will be empty,
          // then just return ok packet to client
          AsyncTaskWorker atw;
          atw.register_async_task(plan->statement->get_sql(),
                                  "Start migrate task");
          atw.report_async_task_state(ASYNC_TASK_FIN, 100, "migrate success.");
          send_ok_packet_to_client(handler, 0, 0);
          status = EXECUTE_STATUS_COMPLETE;
          return;
        }
        break;
      }
      case MIGRATE_STATUS_PREPARE: {
        start_tasks();
        m_status = MIGRATE_STATUS_MIGRATE_DATA;
        break;
      }
      case MIGRATE_STATUS_MIGRATE_DATA: {
        wait_one_second();
        if (check_all_task_status(MIGRATE_STATUS_MIGRATE_DATA)) {
          if (!flush_all_servers_with_consistent_point()) {
            m_status = MIGRATE_STATUS_ERROR;
            break;
          }
          wake_up_tasks();
          m_status = MIGRATE_STATUS_SYNC_CLUSTER_BINLOG;
        }
        break;
      }
      case MIGRATE_STATUS_SYNC_CLUSTER_BINLOG: {
        wait_one_second();
        if (check_all_task_status(MIGRATE_STATUS_SYNC_CLUSTER_BINLOG)) {
          map<DataSpace *, PartitionScheme *>::iterator it =
              migrate_shard_scheme_map.begin();
          for (; it != migrate_shard_scheme_map.end(); it++) {
            if (!((PartitionedTable *)it->first)
                     ->check_can_change_topo(it->second)) {
              LOG_ERROR("migrate plan check_can_change_topo failed\n");
              m_status = MIGRATE_STATUS_ERROR;
              break;
            }
          }
          if (m_status != MIGRATE_STATUS_ERROR) {
            LOG_DEBUG("wake_up_tasks to let migrate task do change_topo\n");
            wake_up_tasks();
            m_status = MIGRATE_STATUS_CHANGE_TOPO;
          }
        }
        break;
      }
      case MIGRATE_STATUS_CHANGE_TOPO: {
        wait_one_second();
        if (check_all_task_status(MIGRATE_STATUS_CHANGE_TOPO)) {
          if (update_migrate_config()) {
            change_topo();
            need_roll_back_topo = false;
            wake_up_tasks();
            m_status = MIGRATE_STATUS_END;
          } else {
            m_status = MIGRATE_STATUS_ERROR;
          }
        }
        break;
      }
      case MIGRATE_STATUS_END: {
        wait_one_second();
        if (check_all_task_status(MIGRATE_STATUS_END)) {
          wait_all_task_stop();
          try {
            migrate_sync(TYPE_MIGRATE_END);
          } catch (...) {
            set_error_message(
                "MySQLMigrateNode migrate_sync end migrate failed");
          }
          if (!plan->handler->get_is_migrate_async_handler())
            send_ok_packet_to_client(handler, 0, 0);
          status = EXECUTE_STATUS_COMPLETE;
          return;
        }
        break;
      }
      case MIGRATE_STATUS_ERROR: {
        stop_all_tasks();
        wait_all_task_stop();
        set_error_message("migrate failed during status_error.");
        string msg = generate_error_message();
        if (need_roll_back_topo) {
          rollback_migrate_topo();
          try {
            migrate_sync(TYPE_MIGRATE_ROLLBACK);
          } catch (...) {
            set_error_message(
                "MySQLMigrateNode migrate_sync rollback migrate failed");
          }
        } else {
          // need_roll_back_topo == false means has changed topo, then should
          // notify slave dbscale end migrate. then migrate clean thread will do
          // clean for the migrate task
          try {
            migrate_sync(TYPE_MIGRATE_END);
          } catch (...) {
            set_error_message(
                "MySQLMigrateNode migrate_sync end migrate failed");
          }
        }
        send_error_packet(msg);
        status = EXECUTE_STATUS_COMPLETE;
        return;
      }
      default: {
#ifdef DEBUG
        ACE_ASSERT(0);  // should not be here
#endif
        set_error_message("migrate failed, please check log.");
        string msg = generate_error_message();
        send_error_packet(msg);
        status = EXECUTE_STATUS_COMPLETE;
        return;
      }
    }
  }
#endif
  status = EXECUTE_STATUS_COMPLETE;

  send_error_packet("dbscale don't support migrate data now.");
}

/* Class MySQLDBScaleShowNode */
MySQLDBScaleShowNode::MySQLDBScaleShowNode(ExecutePlan *plan)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLDBScaleShowNode";
  error_packet = NULL;
  head_packet = NULL;
  eof_packet = NULL;
  is_need_record_result_str = false;
  if (plan->statement->get_stmt_node()->filter_info)
    is_need_record_result_str = true;
  filter_index = -1;
  column_num = 0;
}
void MySQLDBScaleShowNode::set_head_packet(uint64_t columns, uint8_t number) {
  head_packet = Backend::instance()->get_new_packet();
  MySQLResultSetHeaderResponse result_set_header(columns, number);
  result_set_header.pack(head_packet);
  column_num = columns;
}
void MySQLDBScaleShowNode::add_column_packet(
    const char *catalog, const char *schema, const char *table,
    const char *org_table, const char *column, const char *org_column,
    uint16_t charset, uint32_t column_length, MySQLColumnType column_type,
    uint16_t flags, char decimals, uint8_t number) {
  Packet *packet = Backend::instance()->get_new_packet();
  MySQLColumnResponse sequence_field(catalog, schema, table, org_table, column,
                                     org_column, charset, column_length,
                                     column_type, flags, decimals, number);
  sequence_field.pack(packet);
  column_list.push_back(packet);
  if (is_need_record_result_str) {
    field_str_list.push_back(column);
  }
}
list<string> *MySQLDBScaleShowNode::get_result_str_list() {
  if (is_need_record_result_str) {
    result_str_list.clear();
    list<Packet *>::iterator it = row_list.begin();
    for (; it != row_list.end(); it++) {
      vector<string> row_data;
      MySQLRowResponse row(*it);
      row.fill_columns_to_vector(&row_data, column_num);
      vector<string>::iterator it_d = row_data.begin();
      string row_data_str;
      for (; it_d != row_data.end(); it_d++) {
        row_data_str += *it_d;
        row_data_str += "\t";
      }
      result_str_list.push_back(row_data_str);
    }
  }
  return &result_str_list;
}
void MySQLDBScaleShowNode::reset_row_list() {
  if (!row_list.empty()) {
    list<Packet *>::iterator it = row_list.begin();
    for (; it != row_list.end(); it++) {
      delete *it;
    }
    row_list.clear();
  }
}
void MySQLDBScaleShowNode::reset_column_list() {
  if (!column_list.empty()) {
    list<Packet *>::iterator it = column_list.begin();
    for (; it != column_list.end(); it++) {
      delete *it;
    }
    column_list.clear();
  }
}
void MySQLDBScaleShowNode::add_row_packet(list<string> &row_data) {
  list<const char *> tmp_row_data;
  list<string>::iterator it;
  for (it = row_data.begin(); it != row_data.end(); it++) {
    tmp_row_data.push_back(it->c_str());
  }
  add_row_packet(tmp_row_data);
}
void MySQLDBScaleShowNode::add_row_packet(list<const char *> &row_data) {
  Packet *packet = Backend::instance()->get_new_packet();
  MySQLRowResponse row(row_data);
  row.pack(packet);
  row_list.push_back(packet);
}
void MySQLDBScaleShowNode::add_row_packet(Packet *packet) {
  row_list.push_back(packet);
}
void MySQLDBScaleShowNode::set_error_packet(uint16_t error_code,
                                            const char *error_message,
                                            const char *sqlstate,
                                            uint8_t number) {
  error_packet = Backend::instance()->get_new_packet();
  MySQLErrorResponse error(error_code, error_message, sqlstate, number);
  error.pack(error_packet);
}

void MySQLDBScaleShowNode::set_filter_info_filter_index(filter_info_item *root,
                                                        const char *column,
                                                        int index) {
  if (!root) return;
  if (root->type == EXPR_STRING) {
    if (strcasecmp(root->column_name, column) == 0) {
      root->filter_index = index;
    }
  } else if ((root->type == EXPR_AND) || (root->type == EXPR_OR)) {
    set_filter_info_filter_index(root->left, column, index);
    set_filter_info_filter_index(root->right, column, index);
  }
}

void MySQLDBScaleShowNode::generate_filter_index() {
  if (!plan->statement->get_stmt_node()->filter_info) return;
  list<string>::iterator it = field_str_list.begin();
  for (int i = 0; it != field_str_list.end(); it++, i++) {
    set_filter_info_filter_index(plan->statement->get_stmt_node()->filter_info,
                                 it->c_str(), i);
  }
}

bool MySQLDBScaleShowNode::filter_packet_item(Packet *packet,
                                              filter_info_item *root) {
  if (!root) return true;
  if (root->type == EXPR_STRING) {
    if (root->filter_index < 0) return false;
    MySQLRowResponse row_resp(packet);
    uint64_t field_len;
    const char *real_value = row_resp.get_str(root->filter_index, &field_len);
    string real_value_str(real_value, field_len);
    string value(root->value);
    boost::to_lower(value);
    boost::to_lower(real_value_str);
    size_t pos = value.find("%");
    while (pos != string::npos) {
      value.replace(pos, 1, ".*");
      pos += 2;
      pos = value.find("%", pos);
    }
    boost::regex pattern(value.c_str());
    if (regex_match(real_value_str, pattern)) {
      return true;
    }
    return false;
  } else if (root->type == EXPR_AND) {
    return filter_packet_item(packet, root->left) &&
           filter_packet_item(packet, root->right);
  } else if (root->type == EXPR_OR) {
    return filter_packet_item(packet, root->left) ||
           filter_packet_item(packet, root->right);
  }
  return true;
}

bool MySQLDBScaleShowNode::filter_packet(Packet *packet) {
  return filter_packet_item(packet,
                            plan->statement->get_stmt_node()->filter_info);
}
void MySQLDBScaleShowNode::execute() {
#ifdef DEBUG
  node_start_timing();
#endif
  Backend *backend = Backend::instance();
  ACE_Thread_Mutex *show_mutex = backend->get_show_command_mutex(name);
  try {
    show_mutex->acquire();
    init_data();
    eof_packet = Backend::instance()->get_new_packet();
    handler->send_to_client(head_packet);
    for (list<Packet *>::iterator it = column_list.begin();
         it != column_list.end(); it++) {
      handler->send_to_client(*it);
    }
    MySQLEOFResponse eof;
    eof.pack(eof_packet);
    handler->deal_autocommit_with_ok_eof_packet(eof_packet);
    handler->send_to_client(eof_packet);
    generate_filter_index();
    for (list<Packet *>::iterator it = row_list.begin(); it != row_list.end();
         it++) {
      if (filter_packet(*it)) {
        handler->send_to_client(*it);
      }
    }
    MySQLEOFResponse end;
    end.pack(eof_packet);
    if (plan->session->is_call_store_procedure() ||
        plan->session->get_has_more_result()) {
      rebuild_eof_with_has_more_flag(eof_packet, driver);
      LOG_DEBUG(
          "For the call store procedure or multiple stmt, the last eof should "
          "be with flag has_more_result in show node.\n");
    }
    handler->deal_autocommit_with_ok_eof_packet(eof_packet);
    handler->send_to_client(eof_packet);
    show_mutex->release();
  } catch (ErrorPacketException &e) {
    show_mutex->release();
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  } catch (ExecuteNodeError &e) {
    show_mutex->release();
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  } catch (exception &e) {
    show_mutex->release();
    status = EXECUTE_STATUS_COMPLETE;
    LOG_ERROR("DBscale show fail due to exception [%s].\n", e.what());
    throw;
  } catch (...) {
    show_mutex->release();
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  }

#ifdef DEBUG
  node_end_timing();
#endif
#ifdef DEBUG
  LOG_DEBUG("MySQLDBScaleShowNode %@ cost %d ms\n", this, node_cost_time);
#endif
  status = EXECUTE_STATUS_COMPLETE;
}
void MySQLDBScaleShowNode::clean() {
  if (head_packet) {
    delete head_packet;
    head_packet = NULL;
  }
  if (eof_packet) {
    delete eof_packet;
    eof_packet = NULL;
  }
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
  if (!column_list.empty()) {
    for (list<Packet *>::iterator it = column_list.begin();
         it != column_list.end(); it++) {
      delete *it;
    }
    column_list.clear();
  }
  if (!row_list.empty()) {
    for (list<Packet *>::iterator it = row_list.begin(); it != row_list.end();
         it++) {
      delete *it;
    }
    row_list.clear();
  }
}

/* Class MySQLDBScaleShowDefaultSessionVarNode */
MySQLDBScaleShowDefaultSessionVarNode::MySQLDBScaleShowDefaultSessionVarNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowDefaultSessionVarNode";
}

void MySQLDBScaleShowDefaultSessionVarNode::init_data() {
  set_head_packet(1);
  const char *schema_name = session->get_schema();
  add_column_packet("def", schema_name, "", "", "Default Session Variables",
                    "Default Session Variables", 8, NAME_LEN + 1,
                    MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  vector<string> session_var;
  Backend *backend = Backend::instance();
  backend->get_default_session_vars(session_var);
  for (vector<string>::iterator it = session_var.begin();
       it != session_var.end(); it++) {
    list<const char *> row_data;
    row_data.push_back(it->c_str());
    add_row_packet(row_data);
  }
}

/* class MySQLDBScaleCleanFailTransactionNode: */
MySQLDBScaleCleanFailTransactionNode::MySQLDBScaleCleanFailTransactionNode(
    ExecutePlan *plan, const char *xid)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLDBScaleCleanFailTransactionNode";
  this->xid = xid;
  error_packet = NULL;
}

void MySQLDBScaleCleanFailTransactionNode::do_execute() {
  Backend *bend = Backend::instance();
  ACE_RW_Thread_Mutex *dynamic_mutex = bend->get_dynamic_modify_rep_mutex();
  ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_mutex);
  try {
    map<DataSource *, list<string> > purge_list;

    bend->acquire_xid_redo_log_item_lock();
    map<string, XA_redo_log_item *> &redo_map =
        bend->get_xid_redo_log_item_map_without_lock();
    if (!xid) {
      map<string, XA_redo_log_item *>::iterator it = redo_map.begin();
      for (; it != redo_map.end(); it++) {
        list<string> source_list = it->second->get_fail_source_list();
        list<string>::iterator it_source = source_list.begin();
        for (; it_source != source_list.end(); it_source++) {
          DataSource *source = bend->find_data_source(it_source->c_str());
          purge_list[source].push_back(it->first);
        }
      }
    } else {
      string xid_str(xid);
      if (redo_map.count(xid_str)) {
        XA_redo_log_item *item = redo_map[xid_str];
        list<string> source_list = item->get_fail_source_list();
        list<string>::iterator it_source = source_list.begin();
        for (; it_source != source_list.end(); it_source++) {
          DataSource *source = bend->find_data_source(it_source->c_str());
          purge_list[source].push_back(xid_str);
        }
      }
    }

    map<DataSource *, list<string> >::iterator it3 = purge_list.begin();
    for (; it3 != purge_list.end(); it3++) {
      MySQLXA_purge_thread::instance()->purge_xids(it3->first, &(it3->second));
    }
    /*Delete the fail transaction info after all purge_xids complete, so it
     * may be partial purged if get exception on some source. In this case,
     * the fail transaction info will not be cleanup at all.*/
    if (!xid) {
      map<string, XA_redo_log_item *>::iterator it = redo_map.begin();
      for (; it != redo_map.end(); it++) {
        delete it->second;
      }
      redo_map.clear();
    } else {
      string xid_str(xid);
      if (redo_map.count(xid_str)) {
        delete redo_map[xid_str];
        redo_map.erase(xid_str);
      }
    }

    bend->release_xid_redo_log_item_lock();
  } catch (exception &e) {
    bend->release_xid_redo_log_item_lock();
    LOG_ERROR("%s\n", e.what());
    error_packet = Backend::instance()->get_new_packet();
    MySQLErrorResponse error(ERROR_EXECUTE_NODE_FAILED_CODE,
                             "Got error when clean fail transaction");
    error.pack(error_packet);
    throw Error("Got error when clean fail transaction");
  }
}

/* Class MySQLDBScaleShowAllFailTransactionNode */
MySQLDBScaleShowAllFailTransactionNode::MySQLDBScaleShowAllFailTransactionNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowAllFailTransactionNode";
}

void MySQLDBScaleShowAllFailTransactionNode::init_data() {
  set_head_packet(3);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "datasources";
  const char *org_table = "datasources";
  list<const char *> columns;
  columns.push_back("Xid");
  columns.push_back("RelatedSource");
  columns.push_back("FailSource");
  while (!columns.empty()) {
    add_column_packet(catalog, schema, table, org_table, columns.front(),
                      columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    columns.pop_front();
  }

  Backend *bend = Backend::instance();
  bend->acquire_xid_redo_log_item_lock();
  map<string, XA_redo_log_item *> &redo_map =
      bend->get_xid_redo_log_item_map_without_lock();
  map<string, XA_redo_log_item *>::iterator it = redo_map.begin();
  for (; it != redo_map.end(); it++) {
    list<const char *> row_data;
    row_data.push_back(it->first.c_str());
    row_data.push_back(it->second->get_related_source().c_str());
    string fail_source_str;
    it->second->get_fail_source_string(fail_source_str);
    row_data.push_back(fail_source_str.c_str());
    add_row_packet(row_data);
  }

  bend->release_xid_redo_log_item_lock();
}

/* Class MySQLDBScaleShowDetailFailTransactionNode*/
MySQLDBScaleShowDetailFailTransactionNode::
    MySQLDBScaleShowDetailFailTransactionNode(ExecutePlan *plan,
                                              const char *xid)
    : MySQLDBScaleShowNode(plan), xid(xid) {
  this->name = "MySQLDBScaleShowDetailFailTransactionNode";
}

void MySQLDBScaleShowDetailFailTransactionNode::init_data() {
  set_head_packet(3);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "datasources";
  const char *org_table = "datasources";
  list<const char *> columns;
  columns.push_back("FailSource");
  columns.push_back("XidMachine");
  columns.push_back("RedoSQL");
  while (!columns.empty()) {
    add_column_packet(catalog, schema, table, org_table, columns.front(),
                      columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    columns.pop_front();
  }

  Backend *bend = Backend::instance();
  bend->acquire_xid_redo_log_item_lock();
  map<string, XA_redo_log_item *> &redo_map =
      bend->get_xid_redo_log_item_map_without_lock();
  if (redo_map.count(xid)) {
    XA_redo_log_item *item = redo_map[xid];
    map<string, map<string, vector<string> > > &detail_log_map =
        item->get_source_redo_log_map();
    map<string, map<string, vector<string> > >::iterator it =
        detail_log_map.begin();
    for (; it != detail_log_map.end(); it++) {
      map<string, vector<string> > &xid_machine_log_map = it->second;
      map<string, vector<string> >::iterator it2 = xid_machine_log_map.begin();
      for (; it2 != xid_machine_log_map.end(); it2++) {
        vector<string> &log_vec = it2->second;
        vector<string>::iterator it3 = log_vec.begin();
        for (; it3 != log_vec.end(); it3++) {
          list<const char *> row_data;
          row_data.push_back(it->first.c_str());
          row_data.push_back(it2->first.c_str());
          row_data.push_back(it3->c_str());
          add_row_packet(row_data);
        }
      }
    }
  }
  bend->release_xid_redo_log_item_lock();
}
MySQLDBScaleShowPartitionTableStatusNode::
    MySQLDBScaleShowPartitionTableStatusNode(ExecutePlan *plan,
                                             const char *table_name)
    : MySQLDBScaleShowNode(plan), table_name(table_name) {
  this->name = "MySQLDBScaleShowPartitionTableStatusNode";
}

void MySQLDBScaleShowPartitionTableStatusNode::init_data() {
  string full_table_name(table_name);
  size_t pos = full_table_name.find(".");
  if (pos == string::npos) {
    tmp_schema_name = session->get_schema();
    tmp_table_name = full_table_name;
    splice_full_table_name(tmp_schema_name, tmp_table_name, full_table_name);
  } else {
    tmp_schema_name = string(table_name, 0, pos);
    tmp_table_name =
        string(table_name, pos + 1, full_table_name.length() - pos - 1);
  }

  set_head_packet(10);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "datasources";
  const char *org_table = "datasources";
  list<const char *> columns;
  columns.push_back("schema");
  columns.push_back("table");
  columns.push_back("partition_key");
  columns.push_back("partition_key_pos");
  columns.push_back("auto_increment_key");
  columns.push_back("auto_increment_key_pos");
  columns.push_back("auto_increment_base_value");
  columns.push_back("virtual_map");
  columns.push_back("original_layout");
  columns.push_back("shard_map");
  while (!columns.empty()) {
    add_column_packet(catalog, schema, table, org_table, columns.front(),
                      columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    columns.pop_front();
  }
  Backend *backend = Backend::instance();
  DataSpace *ds = backend->get_data_space_for_table(tmp_schema_name.c_str(),
                                                    tmp_table_name.c_str());
  if (!ds->is_partitioned()) {
    LOG_ERROR("The table [%s.%s] is not the partition table. \n",
              tmp_schema_name.c_str(), tmp_table_name.c_str());
    throw Error("The related table is not partitioned");
  }

  PartitionedTable *par_table = (PartitionedTable *)ds;
  list<const char *> row_data;
  row_data.push_back(tmp_schema_name.c_str());
  row_data.push_back(tmp_table_name.c_str());
  vector<const char *> *key_names = par_table->get_key_names();
  vector<unsigned int> key_pos_vec;
  par_table->get_key_pos_vec(tmp_schema_name.c_str(), tmp_table_name.c_str(),
                             key_pos_vec, plan->session);
  row_data.push_back(key_names->at(0));
  sprintf(key_pos_str, "%d", key_pos_vec.at(0));
  row_data.push_back(key_pos_str);
  auto_increment_key = par_table->get_auto_increment_key(full_table_name);
  row_data.push_back(auto_increment_key.c_str());

  if (!auto_increment_key.empty()) {
    int auto_increment_key_pos =
        par_table->get_auto_increment_key_pos(full_table_name);
    sprintf(auto_increment_key_pos_str, "%d", auto_increment_key_pos);
    row_data.push_back(auto_increment_key_pos_str);
    int64_t auto_increment_base_value =
        par_table->get_auto_increment_base_value(full_table_name);
    sprintf(auto_increment_base_value_str, "%ld", auto_increment_base_value);
    row_data.push_back(auto_increment_base_value_str);
  } else {
    row_data.push_back("");
    row_data.push_back("");
  }
  map<unsigned int, unsigned int> next_version_virtual_partition_map =
      par_table->get_virtual_map();
  virtual_map = "";
  map<unsigned int, unsigned int>::iterator it =
      next_version_virtual_partition_map.begin();
  for (; it != next_version_virtual_partition_map.end(); it++) {
    char tmp[20];
    int tmp_len = 0;
    tmp_len = sprintf(tmp, "%d", it->second);
    tmp[tmp_len] = '\0';
    if (virtual_map != "") virtual_map += ":";
    virtual_map += tmp;
  }
  row_data.push_back(virtual_map.c_str());
  par_table->check_virtual_map_nature();
  row_data.push_back(par_table->is_virtual_map_nature() ? "true" : "false");

  shard_map = "";
  if (par_table->is_shard()) {
    PartitionScheme *par_scheme = par_table->get_partition_scheme();
    ShardPartitionScheme *shard_par_scheme = (ShardPartitionScheme *)par_scheme;
    shard_map = shard_par_scheme->get_shard_map_string();
  }
  row_data.push_back(shard_map.c_str());
  add_row_packet(row_data);
}

/* class MySQLDBScaleShowSchemaACLInfoNode */
MySQLDBScaleShowSchemaACLInfoNode::MySQLDBScaleShowSchemaACLInfoNode(
    ExecutePlan *plan, bool is_show_all)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowSchemaACLInfoNode";
  this->is_show_all = is_show_all;
}

void MySQLDBScaleShowSchemaACLInfoNode::init_data() {
  if (!is_show_all) {
    set_head_packet(2);
    add_column_packet("def", "information_schema", "", "", "schema_name",
                      "schema_name", 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0,
                      0, 0);
    add_column_packet("def", "information_schema", "", "", "schema_acl_type",
                      "schema_acl_type", 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    map<string, SchemaACLType, strcasecomp> *session_schema_acl_map =
        plan->session->get_schema_acl_map();
    map<string, SchemaACLType, strcasecomp>::iterator it =
        session_schema_acl_map->begin();
    for (; it != session_schema_acl_map->end(); it++) {
      list<const char *> row_data;
      row_data.push_back(it->first.c_str());
      string schema_acl_str;
      get_acl_str(schema_acl_str, (ACLType)it->second);
      row_data.push_back(schema_acl_str.c_str());
      add_row_packet(row_data);
    }
  } else {
    set_head_packet(3);
    add_column_packet("def", "information_schema", "", "", "user_name",
                      "user_name", 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0,
                      0);
    add_column_packet("def", "information_schema", "", "", "schema_name",
                      "schema_name", 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0,
                      0, 0);
    add_column_packet("def", "information_schema", "", "", "schema_acl_type",
                      "schema_acl_type", 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    map<string, map<string, SchemaACLType, strcasecomp> >
        *all_schema_acl_info_map =
            new map<string, map<string, SchemaACLType, strcasecomp> >;
    Backend::instance()->get_schema_acl_info_all(all_schema_acl_info_map);
    if (!(all_schema_acl_info_map->empty())) {
      map<string, map<string, SchemaACLType, strcasecomp> >::iterator it =
          all_schema_acl_info_map->begin();
      for (; it != all_schema_acl_info_map->end(); it++) {
        if (!(it->second).empty()) {
          map<string, SchemaACLType, strcasecomp>::iterator it2 =
              (it->second).begin();
          for (; it2 != (it->second).end(); it2++) {
            list<const char *> row_data;
            row_data.push_back((it->first).c_str());
            row_data.push_back(it2->first.c_str());
            string schema_acl_str;
            get_acl_str(schema_acl_str, (ACLType)it2->second);
            row_data.push_back(schema_acl_str.c_str());
            add_row_packet(row_data);
          }
        }
      }
    }
    delete all_schema_acl_info_map;
  }
}

/* class MySQLDBScaleShowTableACLInfoNode*/
MySQLDBScaleShowTableACLInfoNode::MySQLDBScaleShowTableACLInfoNode(
    ExecutePlan *plan, bool is_show_all)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowTableACLInfoNode";
  this->is_show_all = is_show_all;
}

void MySQLDBScaleShowTableACLInfoNode::init_data() {
  if (!is_show_all) {
    set_head_packet(2);
    add_column_packet("def", "information_schema", "", "", "full_table_name",
                      "full_table_name", 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    add_column_packet("def", "information_schema", "", "", "table_acl_type",
                      "table_acl_type", 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    map<string, TableACLType, strcasecomp> *session_table_acl_map =
        plan->session->get_table_acl_map();
    map<string, TableACLType, strcasecomp>::iterator it =
        session_table_acl_map->begin();
    for (; it != session_table_acl_map->end(); it++) {
      list<const char *> row_data;
      row_data.push_back(it->first.c_str());
      string tb_acl_str;
      get_acl_str(tb_acl_str, (ACLType)it->second);
      row_data.push_back(tb_acl_str.c_str());
      add_row_packet(row_data);
    }
  } else {
    set_head_packet(3);
    add_column_packet("def", "information_schema", "", "", "user_name",
                      "user_name", 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0,
                      0);
    add_column_packet("def", "information_schema", "", "", "full_table_name",
                      "full_table_name", 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    add_column_packet("def", "information_schema", "", "", "table_acl_type",
                      "table_acl_type", 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    map<string, map<string, TableACLType, strcasecomp> >
        *all_table_acl_info_map =
            new map<string, map<string, TableACLType, strcasecomp> >;
    Backend::instance()->get_table_acl_info_all(all_table_acl_info_map);
    if (!(all_table_acl_info_map->empty())) {
      map<string, map<string, TableACLType, strcasecomp> >::iterator it =
          all_table_acl_info_map->begin();
      for (; it != all_table_acl_info_map->end(); it++) {
        if (!(it->second).empty()) {
          map<string, TableACLType, strcasecomp>::iterator it2 =
              (it->second).begin();
          for (; it2 != (it->second).end(); it2++) {
            list<const char *> row_data;
            row_data.push_back((it->first).c_str());
            row_data.push_back(it2->first.c_str());
            string tb_acl_str;
            get_acl_str(tb_acl_str, (ACLType)it2->second);
            row_data.push_back(tb_acl_str.c_str());
            add_row_packet(row_data);
          }
        }
      }
    }
    delete all_table_acl_info_map;
  }
}

/* class MySQLDBScaleShowPathInfoNode */
MySQLDBScaleShowPathInfoNode::MySQLDBScaleShowPathInfoNode(ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowPathInfoNode";
}

void MySQLDBScaleShowPathInfoNode::init_data() {
  set_head_packet(2);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "datasources";
  const char *org_table = "datasources";
  list<const char *> columns;
  columns.push_back("DBScale_basedir");
  columns.push_back("DBScale_log");
  while (!columns.empty()) {
    add_column_packet(catalog, schema, table, org_table, columns.front(),
                      columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    columns.pop_front();
  }

  list<const char *> row_data;
  char link[64], path[1024];
  sprintf(link, "/proc/%d/exe", getpid());
  int len = readlink(link, path, sizeof(path));
  path[len] = '\0';
  row_data.push_back(path);
  OptionParser *config = OptionParser::instance();
  row_data.push_back(config->get_option_str_value("main", "log-file"));
  add_row_packet(row_data);
}

/* class MySQLDBScaleShowCriticalErrorNode */
MySQLDBScaleShowCriticalErrorNode::MySQLDBScaleShowCriticalErrorNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowCriticalErrorNode";
}

void MySQLDBScaleShowCriticalErrorNode::init_data() {
  set_head_packet(2);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "datasources";
  const char *org_table = "datasources";
  list<const char *> columns;
  columns.push_back("Error_Type");
  columns.push_back("Error_SQL");
  while (!columns.empty()) {
    add_column_packet(catalog, schema, table, org_table, columns.front(),
                      columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    columns.pop_front();
  }
  list<string> string_list;
  Backend::instance()->get_metadata_error_sqls(string_list);
  list<string>::iterator it;
  for (it = string_list.begin(); it != string_list.end(); it++) {
    list<const char *> row_data;
    row_data.push_back("Metadata Error");
    row_data.push_back(it->c_str());
    add_row_packet(row_data);
  }
}

/* class MySQLDBScaleShowWarningsNode*/
MySQLDBScaleShowWarningsNode::MySQLDBScaleShowWarningsNode(ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowWarningsNode";
}

void MySQLDBScaleShowWarningsNode::init_data() {
  set_head_packet(2);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "dbscale_warings";
  const char *org_table = "dbscale_warnings";
  add_column_packet(catalog, schema, table, org_table, "time", "time", 8,
                    NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  add_column_packet(catalog, schema, table, org_table,
                    "dbscale_latest_important_warnings",
                    "dbscale_latest_important_warnings", 8, NAME_LEN + 1,
                    MYSQL_TYPE_VAR_STRING, 0, 0, 0);

  Backend::instance()->get_latest_important_dbscale_warning(
      dbscale_warning_list);
  list<pair<string, string> >::iterator it;
  for (it = dbscale_warning_list.begin(); it != dbscale_warning_list.end();
       it++) {
    list<const char *> row_data;
    row_data.push_back(it->first.c_str());
    row_data.push_back(it->second.c_str());
    add_row_packet(row_data);
  }
}

/* Class MySQLDBScaleShowVersionNode*/
MySQLDBScaleShowVersionNode::MySQLDBScaleShowVersionNode(ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowVersionNode";
}

void MySQLDBScaleShowVersionNode::init_data() {
  set_head_packet(2);
  const char *schema_name = session->get_schema();
  add_column_packet("def", schema_name, "", "", "DBScale version",
                    "DBScale version", 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                    0, 0, 0);
  add_column_packet("def", schema_name, "", "", "Comment", "Comment", 8,
                    NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  string dbscale_info = "", comment_info = "";
  Backend::instance()->get_dbscale_version(&dbscale_info, &comment_info);
  list<const char *> row_data;
  row_data.push_back(dbscale_info.c_str());
  row_data.push_back(comment_info.c_str());
  add_row_packet(row_data);
}

/* Class MySQLShowVersionNode */
MySQLShowVersionNode::MySQLShowVersionNode(ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLShowVersionNode";
}

void MySQLShowVersionNode::init_data() {
  unsigned int head_num = 0;

  vector<const char *> columns;
  columns.push_back("version");

  head_num = columns.size();
  set_head_packet(head_num);

  const char *schema_name = session->get_schema();
  for (unsigned int i = 0; i < columns.size(); i++) {
    const char *column = columns[i];
    add_column_packet("def", schema_name, "", "", column, column, 8,
                      NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  }
  string everdb_version = "";
  Backend::instance()->get_everdb_powerby_version_with_prefix(
      everdb_version, EVERDB_VERSION_PREFIX);

  list<const char *> row_data;
  row_data.push_back(everdb_version.c_str());
  add_row_packet(row_data);
}

/* Class ShowComponentsVersionNode */
ShowComponentsVersionNode::ShowComponentsVersionNode(ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "ShowComponentsVersionNode";
}

void ShowComponentsVersionNode::init_data() {
  unsigned int head_num = 0;

  vector<const char *> columns;
  columns.push_back("component");
  columns.push_back("version");

  head_num = columns.size();
  set_head_packet(head_num);
  const char *schema_name = session->get_schema();
  for (unsigned int i = 0; i < columns.size(); i++) {
    const char *column = columns[i];
    add_column_packet("def", schema_name, "", "", column, column, 8,
                      NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  }

  Backend *bk = Backend::instance();
  string db_server_version = "";
  bk->get_everdb_backend_server_version_num_with_prefix(db_server_version);
  string dbscale_version_num_prefix_with_comment = "";
  bk->get_dbscale_version_num_prefix_and_comment(
      dbscale_version_num_prefix_with_comment);

  ZooKeeperTool *zk = ZooKeeperTool::instance();
  string dependent_zk_version;
  bool is_contains_zk = true;
  try {
    if (multiple_mode)
      zk->fetch_zookeeper_version(dependent_zk_version);
    else
      is_contains_zk = false;
  } catch (Exception &e) {
    is_contains_zk = false;
    LOG_ERROR("fetch zookeeper version got error:%s\n", e.what());
  }

  short row_size = is_contains_zk ? 3 : 2;
  list<const char *> row_data[row_size];
  row_data[0].push_back(SHOW_COMPONENTS_VERSION_FIRST_ROW_NAME);
  row_data[0].push_back(dbscale_version_num_prefix_with_comment.c_str());

  row_data[1].push_back("DB");
  row_data[1].push_back(db_server_version.c_str());

  if (is_contains_zk) {
    row_data[2].push_back("ZK");
    row_data[2].push_back(dependent_zk_version.c_str());
  }

  for (short index = 0; index < row_size; index++) {
    add_row_packet(row_data[index]);
  }
}

/* Class MySQLDBScaleShowHostnameNode*/
MySQLDBScaleShowHostnameNode::MySQLDBScaleShowHostnameNode(ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowHostnameNode";
}

void MySQLDBScaleShowHostnameNode::init_data() {
  set_head_packet(1);
  const char *schema_name = session->get_schema();
  add_column_packet("def", schema_name, "", "", "Hostname", "Hostname", 8,
                    NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  const char *hostname = Backend::instance()->get_os_local_host_name();
  list<const char *> row_data;
  row_data.push_back(hostname);
  add_row_packet(row_data);
}

/* Class MySQLDBScaleShowBaseStatusNode*/
MySQLDBScaleShowBaseStatusNode::MySQLDBScaleShowBaseStatusNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowBaseStatusNode";
}

void MySQLDBScaleShowBaseStatusNode::init_data() {
  set_head_packet(16);

  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "socketbase";
  const char *org_table = "socketbase";
  list<const char *> columns;
  columns.push_back("Id");
  columns.push_back("BindClient");
  columns.push_back("BindServer");
  columns.push_back("CountServer");
  columns.push_back("CountClient");
  columns.push_back("CountTimeout");
  columns.push_back("CountListener");
  columns.push_back("WaitSize");
  columns.push_back("WorkSize");
  columns.push_back("ReadySize");
  columns.push_back("LatestEventHandle");
  columns.push_back("LatestEventAdd");
  columns.push_back("CountObtain");
  columns.push_back("CountAssign");
  columns.push_back("CountUnAssign");
  columns.push_back("CountIncMax");
  while (!columns.empty()) {
    add_column_packet(catalog, schema, table, org_table, columns.front(),
                      columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    columns.pop_front();
  }

  vector<SocketEventBase *> &sv =
      SocketEventBaseManager::instance()->get_event_base_vec();

  vector<SocketEventBase *>::iterator it = sv.begin();
  for (; it != sv.end(); it++) {
    list<const char *> row_data;
    char id[20];
    char bind_client[20];
    char bind_server[20];
    char count_server[20];
    char count_client[20];
    char count_timeout[20];
    char count_listener[20];
    char wait_size[20];
    char work_size[20];
    char ready_size[20];
    char latest_event_handle[20];
    char latest_event_add[20];
    char count_obtain[20];
    char count_assign[20];
    char count_unassign[20];
    char count_inc_max[20];
    snprintf(id, sizeof(id), "%u", (*it)->get_id());
    snprintf(bind_client, sizeof(bind_client), "%lu", (*it)->get_bind_client());
    snprintf(bind_server, sizeof(bind_server), "%lu", (*it)->get_bind_server());
    snprintf(count_server, sizeof(count_server), "%lu",
             (*it)->get_count_server_serve());
    snprintf(count_client, sizeof(count_client), "%lu",
             (*it)->get_count_client_serve());
    snprintf(count_timeout, sizeof(count_timeout), "%lu",
             (*it)->get_count_timeout());
    snprintf(count_listener, sizeof(count_listener), "%lu",
             (*it)->get_count_listener());
    snprintf(wait_size, sizeof(wait_size), "%lu", (*it)->get_wait_size());
    snprintf(work_size, sizeof(work_size), "%lu", (*it)->get_work_size());
    snprintf(ready_size, sizeof(ready_size), "%lu", (*it)->get_ready_size());
    snprintf(latest_event_handle, sizeof(latest_event_handle), "%lu",
             (*it)->get_latest_event_handle());
    snprintf(latest_event_add, sizeof(latest_event_add), "%lu",
             (*it)->get_latest_event_add());
    snprintf(count_obtain, sizeof(count_obtain), "%lu",
             (*it)->get_count_obtain());
    snprintf(count_assign, sizeof(count_assign), "%lu",
             (*it)->get_count_assign());
    snprintf(count_unassign, sizeof(count_unassign), "%lu",
             (*it)->get_count_unassign());
    snprintf(count_inc_max, sizeof(count_inc_max), "%lu",
             (*it)->get_count_dynamic_inc_max());
    row_data.push_back(id);
    row_data.push_back(bind_client);
    row_data.push_back(bind_server);
    row_data.push_back(count_server);
    row_data.push_back(count_client);
    row_data.push_back(count_timeout);
    row_data.push_back(count_listener);
    row_data.push_back(wait_size);
    row_data.push_back(work_size);
    row_data.push_back(ready_size);
    row_data.push_back(latest_event_handle);
    row_data.push_back(latest_event_add);
    row_data.push_back(count_obtain);
    row_data.push_back(count_assign);
    row_data.push_back(count_unassign);
    row_data.push_back(count_inc_max);
    add_row_packet(row_data);
  }
}

/* Class MYSQLDBScaleAddDefaultSessionVarNode */
MySQLDBScaleAddDefaultSessionVarNode::MySQLDBScaleAddDefaultSessionVarNode(
    ExecutePlan *plan, DataSpace *dataspace)
    : MySQLExecuteNode(plan), conn(NULL) {
  this->name = "MySQLDBScaleAddDefaultSessionVarNode";
  this->dataspace = dataspace;
  packet = NULL;
  error_packet = NULL;
  got_error = false;
}

void MySQLDBScaleAddDefaultSessionVarNode::execute() {
  Backend *backend = Backend::instance();
  string sql("SET ");
  set_default_session_node *set_list =
      plan->statement->get_stmt_node()->sql->set_default_session_oper;
  while (set_list) {
    const char *session_name = set_list->default_session_name;
    if (backend->is_ignore_session_var(session_name)) {
      char err_msg[128];
      int len = sprintf(err_msg,
                        "can not add session variable %s to default session, "
                        "cause it is a ignore session variable in dbscale.",
                        session_name);
      err_msg[len] = 0;
      LOG_ERROR("%s\n", err_msg);
      throw Error(err_msg);
    }
    sql.append("@@");
    sql.append(session_name);
    sql.append("=@@");
    sql.append(session_name);
    sql.append(",");
    set_list = set_list->next;
  }
  packet = Backend::instance()->get_new_packet();
  Packet exec_packet;
  string tmpstr = sql.substr(0, sql.length() - 1);
  MySQLQueryRequest query(tmpstr.c_str());
  query.set_sql_replace_char(plan->session->get_query_sql_replace_null_char());
  query.pack(&exec_packet);
  try {
    conn = handler->send_to_server_retry(dataspace, &exec_packet,
                                         session->get_schema(),
                                         session->is_read_only());
    handler->receive_from_server(conn, packet);
    if (driver->is_error_packet(packet)) {
      status = EXECUTE_STATUS_COMPLETE;
      error_packet = packet;
      MySQLErrorResponse response(error_packet);
      if (response.is_shutdown()) {
        if (session->defined_lock_need_kept_conn(dataspace))
          session->remove_defined_lock_kept_conn(dataspace);
        if (conn) {
          handler->clean_dead_conn(&conn, dataspace);
        }
        session->set_server_shutdown(true);
      }
      throw ErrorPacketException();
    }
    if (driver->is_ok_packet(packet)) {
      handler->deal_autocommit_with_ok_eof_packet(packet);
      set_list =
          plan->statement->get_stmt_node()->sql->set_default_session_oper;
      vector<string> session_vec;
      while (set_list) {
        string tmp(set_list->default_session_name);
        boost::to_upper(tmp);
        session_vec.push_back(tmp);
        set_list = set_list->next;
      }
      backend->add_default_session_vars(session_vec);
    }
    status = EXECUTE_STATUS_COMPLETE;
    handler->send_to_client(packet);
  } catch (ErrorPacketException &e) {
    LOG_DEBUG("DBscale add default session variables got an error packet.\n");
    throw;
  } catch (ExecuteNodeError &e) {
    if (session->defined_lock_need_kept_conn(dataspace))
      session->remove_defined_lock_kept_conn(dataspace);
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace);
    }
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  } catch (exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    string error_message(
        "MySQLDBScaleAddDefaultSessionVarNode fail due to exception:");
    error_message.append(e.what());
    got_error = true;
    throw ExecuteNodeError(error_message.c_str());
  }
}

void MySQLDBScaleAddDefaultSessionVarNode::clean() {
  if (conn) {
    if (!got_error || error_packet) {
      handler->put_back_connection(dataspace, conn);
    } else {
      handler->clean_dead_conn(&conn, dataspace);
    }
    conn = NULL;
  }
  if (packet) {
    delete packet;
    packet = NULL;
    error_packet = NULL;
  }
}
/* Class MySQLDBScaleRemoveDefaultSessionVarNode */
MySQLDBScaleRemoveDefaultSessionVarNode::
    MySQLDBScaleRemoveDefaultSessionVarNode(ExecutePlan *plan)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLDBScaleRemoveDefaultSessionVarNode";
  error_packet = NULL;
  packet = NULL;
  ok_packet = NULL;
}
void MySQLDBScaleRemoveDefaultSessionVarNode::execute() {
  set_default_session_node *set_list =
      plan->statement->get_stmt_node()->sql->set_default_session_oper;
  Backend *backend = Backend::instance();
  set<string> session_set;
  while (set_list) {
    string tmp(set_list->default_session_name);
    boost::to_upper(tmp);
    if (!session_set.count(tmp)) session_set.insert(tmp);
    set_list = set_list->next;
  }
  string err_msg;
  bool error_flag = backend->rm_default_session_vars(session_set, err_msg);
  if (!error_flag) {
    status = EXECUTE_STATUS_COMPLETE;
    MySQLErrorResponse error(ERROR_DBSCALE_DEFAULT_SESSION_VAR_NOT_EXIST_CODE,
                             err_msg.c_str());
    error_packet = Backend::instance()->get_new_packet();
    error.pack(error_packet);
    throw ErrorPacketException();
  } else {
    MySQLOKResponse ok(0, 0);
    ok_packet = Backend::instance()->get_new_packet();
    ok.pack(ok_packet);
    handler->deal_autocommit_with_ok_eof_packet(ok_packet);
    status = EXECUTE_STATUS_COMPLETE;
    handler->send_to_client(ok_packet);
  }
}
void MySQLDBScaleRemoveDefaultSessionVarNode::clean() {
  if (packet) {
    delete packet;
    packet = NULL;
  }
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
  if (ok_packet) {
    delete ok_packet;
    ok_packet = NULL;
  }
}

// MySQLDispatchPacketNode
MySQLDispatchPacketNode::MySQLDispatchPacketNode(ExecutePlan *plan,
                                                 TransferRule *rule)
    : MySQLSendNode(plan) {
  this->name = "MySQLDispatchPacketNode";
  this->dispatch_strategy = rule->get_dispatch_strategy();
  this->rule = rule;
  node_can_swap = false;
  key_position = 0;
}

void MySQLDispatchPacketNode::do_clean() {
  rule->signal_follower_threads();
  if (session->get_work_session())
    session->get_work_session()->increase_finished_federated_num();

#ifndef DBSCALE_TEST_DISABLE
  Backend *backend = Backend::instance();
  if (rule == backend->dispatch_rule) {
    rule->get_dispatch_strategy()->dispatch_handler.clear();
  }
#endif
}

void MySQLDispatchPacketNode::send_header() {
  Packet *header_packet = get_header_packet();
  send_header_fields_and_eof_packet(header_packet);
  list<Packet *> *field_packets = get_field_packets();
  int column_position = 0;
  for (std::list<Packet *>::iterator it = field_packets->begin();
       it != field_packets->end(); it++) {
    if (!dispatch_strategy->dup_table) {
      MySQLColumnResponse column_response(*it);
      column_response.unpack();
      const char *key_string = dispatch_strategy->local_key_name.c_str();
      if (!strcasecmp(key_string, column_response.get_column())) {
        key_position = column_position;
      }
      column_position++;
    }
    send_header_fields_and_eof_packet(*it);
  }
  Packet *eof_packet = get_eof_packet();
  send_header_fields_and_eof_packet(eof_packet);
}

void MySQLDispatchPacketNode::send_header_fields_and_eof_packet(
    Packet *packet) {
  map<int, Handler *>::iterator iter =
      dispatch_strategy->dispatch_handler.begin();
  for (; iter != dispatch_strategy->dispatch_handler.end(); iter++) {
    ((MySQLHandler *)iter->second)
        ->send_mysql_packet_to_client_by_buffer(packet);
  }
}

void MySQLDispatchPacketNode::flush_net_buffer() {
  map<int, Handler *>::iterator iter =
      dispatch_strategy->dispatch_handler.begin();
  for (; iter != dispatch_strategy->dispatch_handler.end(); iter++) {
    ((MySQLHandler *)iter->second)->flush_net_buffer();
  }
}

void MySQLDispatchPacketNode::send_row_packet(Packet *packet) {
  PartitionMethod *method = dispatch_strategy->method;
  PartitionedTable *part_table = dispatch_strategy->part_table;
  MySQLRowResponse row_resp(packet);
  uint64_t field_len;
  const char *key_string = row_resp.get_str(key_position, &field_len);
  string key_str(key_string, field_len);
  vector<const char *> key_string_vector;
  key_string_vector.push_back(key_str.c_str());
  // TODO Support multiple partition key
  string sql_replace_char = plan->session->get_query_sql_replace_null_char();
  unsigned int partition_id =
      method->get_partition_id(&key_string_vector, sql_replace_char);
  unsigned int real_id =
      part_table->get_real_par_id_from_virtual_id(partition_id);
  if (dispatch_strategy->dispatch_handler.count(real_id)) {
    MySQLHandler *handler_tmp =
        (MySQLHandler *)dispatch_strategy->dispatch_handler[real_id];
    handler_tmp->send_mysql_packet_to_client_by_buffer(packet);
  }
}

void MySQLDispatchPacketNode::send_row(MySQLExecuteNode *ready_child) {
  AllocList<Packet *, Session *, StaticAllocator<Packet *> > *tmp_list =
      row_map[ready_child];
  list<Packet *, StaticAllocator<Packet *> >::iterator it = tmp_list->begin();
  for (; it != tmp_list->end(); it++) {
    row_num++;
    if (row_num > federated_max_rows || row_num > cross_join_max_rows) {
      if (federated_max_rows > cross_join_max_rows)
        LOG_ERROR("Reach the max rows of [%u] for cross join max result set.\n",
                  row_num);
      else
        LOG_ERROR(
            "Reach the max rows of [%u] for federated middle result set.\n",
            row_num);
      status = EXECUTE_STATUS_COMPLETE;
      Packet *packet = handler->get_error_packet(
          ERROR_EXECUTE_NODE_FAILED_CODE,
          "Reach the max row number of federated middle result set.", NULL);
      try {
        map<int, Handler *>::iterator h_it =
            dispatch_strategy->dispatch_handler.begin();
        for (; h_it != dispatch_strategy->dispatch_handler.end(); h_it++) {
          MySQLHandler *handler_tmp = (MySQLHandler *)(h_it->second);
          handler_tmp->send_mysql_packet_to_client_by_buffer(packet);
          handler_tmp->flush_net_buffer();
        }
      } catch (...) {
        LOG_ERROR("Fail to send error packet to federated thread.\n");
        if (packet) delete packet;
        throw;
      }
      delete packet;
      throw CrossNodeJoinFail(
          "Reach the max row number of federated middle result set.");
    }
    if (!dispatch_strategy->dup_table) {
      send_row_packet(*it);
    } else {
      map<int, Handler *>::iterator h_it =
          dispatch_strategy->dispatch_handler.begin();
      for (; h_it != dispatch_strategy->dispatch_handler.end(); h_it++) {
        MySQLHandler *handler_tmp = (MySQLHandler *)(h_it->second);
        handler_tmp->send_mysql_packet_to_client_by_buffer(*it);
      }
    }
  }
  it = tmp_list->begin();
  for (; it != tmp_list->end(); it++) delete *it;
  tmp_list->clear();
}

void MySQLDispatchPacketNode::send_eof() {
  Packet *end_packet = get_end_packet();
  if (!end_packet) {
    build_eof_packet();
    end_packet = &generated_eof;
  }
  handler->deal_autocommit_with_ok_eof_packet(end_packet);
  send_header_fields_and_eof_packet(end_packet);
}  // send_eof

void MySQLDispatchPacketNode::dispatch_error_packet() {
  Packet *packet = get_error_packet();
  try {
    map<int, Handler *>::iterator h_it =
        dispatch_strategy->dispatch_handler.begin();
    for (; h_it != dispatch_strategy->dispatch_handler.end(); h_it++) {
      MySQLHandler *handler_tmp = (MySQLHandler *)(h_it->second);
      handler_tmp->send_mysql_packet_to_client_by_buffer(packet);
      handler_tmp->flush_net_buffer();
    }
  } catch (...) {
    LOG_ERROR("Fail to send error packet to federated thread.\n");
    if (packet) delete packet;
    throw;
  }
}

MySQLFederatedTableNode::MySQLFederatedTableNode(ExecutePlan *plan)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLFederatedTableNode";
  name = "MySQLFederatedTableNode";
}

void MySQLFederatedTableNode::execute() {
  Packet packet;

  try {
    // set the header
    MySQLResultSetHeaderResponse result_set_header(18, 0);
    result_set_header.pack(&packet);
    handler->send_to_client(&packet);

    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "dataservers";
    const char *org_table = "dataservers";

    list<const char *> columns;
    columns.push_back("Name");
    columns.push_back("Engine");
    columns.push_back("Version");
    columns.push_back("Row_format");
    columns.push_back("Rows");
    columns.push_back("Avg_row_length");
    columns.push_back("Data_length");
    columns.push_back("Max_data_length");
    columns.push_back("Index_length");
    columns.push_back("Data_free");
    columns.push_back("Auto_increment");
    columns.push_back("Create_time");
    columns.push_back("Update_time");
    columns.push_back("Check_time");
    columns.push_back("Collation");
    columns.push_back("Checksum");
    columns.push_back("Create_options");
    columns.push_back("Comment");
    while (!columns.empty()) {
      MySQLColumnResponse field(catalog, schema, table, org_table,
                                columns.front(), columns.front(), 8,
                                NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
      field.pack(&packet);
      handler->send_to_client(&packet);
      columns.pop_front();
    }

    MySQLEOFResponse eof;
    eof.pack(&packet);
    handler->deal_autocommit_with_ok_eof_packet(&packet);
    handler->send_to_client(&packet);

    list<const char *> row_data;

    row_data.push_back("test1");
    row_data.push_back("InnoDB");
    row_data.push_back("10");
    row_data.push_back("Compact");
    row_data.push_back("0");
    row_data.push_back("0");
    row_data.push_back("16384");
    row_data.push_back("0");
    row_data.push_back("0");
    row_data.push_back("0");
    row_data.push_back("NULL");
    row_data.push_back("2015-03-30 15:09:01");
    row_data.push_back("NULL");
    row_data.push_back("NULL");
    row_data.push_back("utf8_general_ci");
    row_data.push_back("NULL");
    row_data.push_back("");
    row_data.push_back("");

    MySQLRowResponse row(row_data);
    row.pack(&packet);
    handler->send_to_client(&packet);

    MySQLEOFResponse end;
    end.pack(&packet);
    handler->deal_autocommit_with_ok_eof_packet(&packet);
    handler->send_to_client(&packet);
  } catch (exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  }

  status = EXECUTE_STATUS_COMPLETE;
}

MySQLEmptySetNode::MySQLEmptySetNode(ExecutePlan *plan, int field_num)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLEmptySetNode";
  this->field_num = field_num;
}

void MySQLEmptySetNode::execute() {
  Packet packet;

  try {
    // set the header
    MySQLResultSetHeaderResponse result_set_header(field_num, 0);
    result_set_header.pack(&packet);
    handler->send_to_client(&packet);

    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "dataservers";
    const char *org_table = "dataservers";

    list<const char *> columns;
    for (int i = 0; i < field_num; i++) {
      columns.push_back("Name");
    }
    while (!columns.empty()) {
      MySQLColumnResponse field(catalog, schema, table, org_table,
                                columns.front(), columns.front(), 8,
                                NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
      field.pack(&packet);
      handler->send_to_client(&packet);
      columns.pop_front();
    }

    MySQLEOFResponse eof;
    eof.pack(&packet);
    handler->deal_autocommit_with_ok_eof_packet(&packet);
    handler->send_to_client(&packet);

    MySQLEOFResponse end;
    end.pack(&packet);
    handler->deal_autocommit_with_ok_eof_packet(&packet);
    handler->send_to_client(&packet);

  } catch (exception &e) {
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  }

  status = EXECUTE_STATUS_COMPLETE;
}

/* class  MySQLCursorDirectNode */

MySQLCursorDirectNode::MySQLCursorDirectNode(ExecutePlan *plan,
                                             DataSpace *dataspace,
                                             const char *sql)
    : MySQLExecuteNode(plan, dataspace),
      sql(sql),
      conn(NULL),
      got_error(false),
      packet(NULL),
      error_packet(NULL),
      ces(OPEN_EXECUTE),
      not_found_flag(false),
      field_num(0),
      row_num(0) {
  this->name = "MySQLCursorDirectNode";
  select_uservar_flag = statement->get_select_uservar_flag();
  select_field_num = statement->get_select_field_num();
  if (select_uservar_flag) {
    uservar_vec = *(statement->get_select_uservar_vec());
  }
  server_name = NULL;
  force_use_non_trx_conn = false;
}

void MySQLCursorDirectNode::clean() {
  if (conn) {
    if ((!got_error || error_packet) && not_found_flag) {
      handler->put_back_connection(dataspace, conn, force_use_non_trx_conn);
    } else {
      handler->clean_dead_conn(&conn, dataspace, !force_use_non_trx_conn);
    }
    conn = NULL;
  }
  if (packet) {
    if (packet == error_packet) {
      error_packet = NULL;
    }
    delete packet;
    packet = NULL;
  }
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}
void MySQLCursorDirectNode::handle_error_packet(Packet *packet) {
  status = EXECUTE_STATUS_COMPLETE;
  error_packet = packet;
  MySQLErrorResponse response(error_packet);
  if (response.is_shutdown()) {
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace, !force_use_non_trx_conn);
      conn = NULL;
    }
    session->set_server_shutdown(true);
  }
  throw ErrorPacketException();
}

void MySQLCursorDirectNode::execute() {
  if (plan->session->check_for_transaction()) {
    if (session->get_session_option("cursor_use_free_conn").int_val == 0) {
      LOG_ERROR(
          "can not call stored procedure with cursor when current session is "
          "in transaction.\n");
      throw NotSupportedError(
          "not support call stored procedure with cursor when current session "
          "is in transaction");
    } else {
      force_use_non_trx_conn = plan->is_cursor;
    }
  }
  switch (ces) {
    case OPEN_EXECUTE: {
      open_execute();
      break;
    }
    case FETCH_EXECUTE: {
      fetch_execute();
      break;
    }
    default:
      throw ExecuteNodeError("MySQLCursorDirectNode ces has not init.");
      break;
  }
}

void MySQLCursorDirectNode::open_execute() {
  bool has_more_result = false;
  packet = Backend::instance()->get_new_packet();
  Packet exec_packet;
  MySQLQueryRequest query(sql);
  query.set_sql_replace_char(plan->session->get_query_sql_replace_null_char());
  query.pack(&exec_packet);
  try {
    if (conn) {
      conn->reset();
      handler->send_to_server(conn, &exec_packet);
    } else {
      if (plan->statement->is_cross_node_join()) {
        conn = handler->send_to_server_retry(
            dataspace, &exec_packet, session->get_schema(), false, false, false,
            force_use_non_trx_conn);
      } else {
        conn = handler->send_to_server_retry(
            dataspace, &exec_packet, session->get_schema(),
            session->is_read_only(), false, false, force_use_non_trx_conn);
      }
    }
    server_name = conn->get_server()->get_name();
    do {
      handler->receive_from_server(conn, packet);
      if (driver->is_error_packet(packet)) {
        has_more_result = false;
        handle_error_packet(packet);
      } else if (driver->is_ok_packet(packet)) {
        LOG_DEBUG("Cursor return ok packet in MySQLCursorDirectNode.\n");
        throw ExecuteNodeError("Cursor return ok packet.");
      } else if (driver->is_result_set_header_packet(packet)) {
        handler->receive_from_server(conn, packet);
        while (!driver->is_eof_packet(packet)) {
          if (driver->is_error_packet(packet)) {
            handle_error_packet(packet);
          }
          MySQLColumnResponse column_response(packet);
          column_response.unpack();
          bool is_number = column_response.is_number();
          field_is_number_vec.push_back(is_number);
          field_num++;
          handler->receive_from_server(conn, packet);
        }
        break;
      } else {
#ifdef DEBUG
        ACE_ASSERT(0);  // should not be here
#endif
      }
      LOG_DEBUG("Has more result %d.\n", has_more_result ? 1 : 0);
    } while (has_more_result);
  } catch (ErrorPacketException &e) {
    LOG_DEBUG("CursorDirectNode get an error packet when execute sql [%s].\n",
              sql);
    throw e;
  } catch (ExecuteNodeError &e) {
    if (session->defined_lock_need_kept_conn(dataspace))
      session->remove_defined_lock_kept_conn(dataspace);
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace, !force_use_non_trx_conn);
    }
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    throw e;
  } catch (exception &e) {
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    LOG_DEBUG("CursorDirectNode fail due to exception [%s].\n", e.what());
    string error_message("CursorDirectNode fail due to exception:");
    error_message.append(e.what());
    if (session->defined_lock_need_kept_conn(dataspace))
      session->remove_defined_lock_kept_conn(dataspace);
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace, !force_use_non_trx_conn);
    }
    throw ExecuteNodeError(error_message.c_str());
  }
}
void MySQLCursorDirectNode::fetch_execute() {
  try {
    if (not_found_flag) {
      if (!error_packet) {
        error_packet = Backend::instance()->get_new_packet();
      }
      MySQLErrorResponse error_response(
          ERROR_SP_FETCH_NO_DATA_CODE,
          dbscale_err_msg[ERROR_SP_FETCH_NO_DATA_CODE], "02000");
      error_response.pack(error_packet);
      throw dbscale::sql::SQLError(dbscale_err_msg[ERROR_SP_FETCH_NO_DATA_CODE],
                                   "02000", ERROR_SP_FETCH_NO_DATA_CODE);
    }
    handler->receive_from_server(conn, packet);
    if ((!driver->is_eof_packet(packet)) &&
        (!driver->is_error_packet(packet))) {
      row_num++;
      MySQLRowResponse row_response(packet);
      uint64_t field_len = 0;
      const char *result = NULL;
      for (unsigned int i = 0; i < field_num; i++) {
        if (row_response.field_is_null(i)) {
          row_vector.push_back("NULL");
        } else {
          result = row_response.get_str(i, &field_len);
          string column(result, field_len);
          string full_column;
          if (field_is_number_vec[i]) {
            full_column = column;
          } else {
            CharsetType ctype = session->get_client_charset_type();
            deal_with_str_column_value(&column, ctype);
            full_column.append("'");
            full_column.append(column);
            full_column.append("'");
          }
          row_vector.push_back(full_column);
        }
      }
    } else {
      if (driver->is_error_packet(packet)) {
        handle_error_packet(packet);
      }
      if (conn) {
        if (has_sql_calc_found_rows()) {
          session->set_found_rows(0);
          found_rows(handler, driver, conn, session);
          Packet *packet_tmp = session->get_error_packet();
          if (packet_tmp) {
            handle_error_packet(packet_tmp);
          }
        } else {
          session->set_found_rows(row_num);
        }
      }
      session->set_real_fetched_rows(row_num);
      if (select_uservar_flag) {
        deal_with_var(conn, plan, driver);
      }
      bool non_modified_conn = true;
      if (driver->is_ok_packet(packet)) {
        MySQLOKResponse ok(packet);
        ok.unpack();
        uint64_t affected_rows = ok.get_affected_rows();
        if (affected_rows > 0) {
          non_modified_conn = false;
          if (conn) session->record_xa_modified_conn(conn);
        }
      }
      status = EXECUTE_STATUS_COMPLETE;
      record_xa_modify_sql(plan, session, dataspace, sql, non_modified_conn);
      record_modify_server(plan, session, server_name,
                           dataspace->get_virtual_machine_id(),
                           non_modified_conn);
      not_found_flag = true;
      if (!error_packet) {
        error_packet = Backend::instance()->get_new_packet();
      }
      MySQLErrorResponse error_response(
          ERROR_SP_FETCH_NO_DATA_CODE,
          dbscale_err_msg[ERROR_SP_FETCH_NO_DATA_CODE], "02000");
      error_response.pack(error_packet);
      throw dbscale::sql::SQLError(dbscale_err_msg[ERROR_SP_FETCH_NO_DATA_CODE],
                                   "02000", ERROR_SP_FETCH_NO_DATA_CODE);
    }
  } catch (dbscale::sql::SQLError &e) {
    if (conn && !not_found_flag) {
      handler->clean_dead_conn(&conn, dataspace, force_use_non_trx_conn);
    }
    throw;
  } catch (ErrorPacketException &e) {
    LOG_DEBUG("CursorDirectNode get an error packet when execute sql [%s].\n",
              sql);
    throw e;
  } catch (ExecuteNodeError &e) {
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace, force_use_non_trx_conn);
    }
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    throw e;
  } catch (exception &e) {
    got_error = true;
    status = EXECUTE_STATUS_COMPLETE;
    LOG_DEBUG("CursorDirectNode fail due to exception [%s].\n", e.what());
    string error_message("CursorDirectNode fail due to exception:");
    error_message.append(e.what());
    if (conn) {
      handler->clean_dead_conn(&conn, dataspace, force_use_non_trx_conn);
    }
    throw ExecuteNodeError(error_message.c_str());
    ;
  }
}
/* class MySQLCursorSendNode */

MySQLCursorSendNode::MySQLCursorSendNode(ExecutePlan *plan)
    : MySQLInnerNode(plan),
      field_num(0),
      error_packet(NULL),
      not_found_flag(false) {
  this->name = "MySQLCursorSendNode";
  select_uservar_flag = statement->get_select_uservar_flag();
  select_field_num = statement->get_select_field_num();
  if (select_uservar_flag) {
    uservar_vec = *(statement->get_select_uservar_vec());
  }
}

void MySQLCursorSendNode::dealwith_header() {
  list<Packet *> *field_packets = get_field_packets();
  for (std::list<Packet *>::iterator it = field_packets->begin();
       it != field_packets->end(); it++) {
    field_num++;
    MySQLColumnResponse column_response(*it);
    column_response.unpack();
    bool is_number = column_response.is_number();
    field_is_number_vec.push_back(is_number);
  }
}

void MySQLCursorSendNode::send_row(MySQLExecuteNode *ready_child) {
  Packet *end_row = NULL;
  if (!row_map[ready_child]->empty()) end_row = row_map[ready_child]->back();
  if (statement->get_select_uservar_flag() && end_row != NULL) {
    MySQLRowResponse row_response(end_row);
    set_select_uservar_by_result(end_row, field_is_number_vec, uservar_vec,
                                 select_field_num, session);
  }
  Packet *row;
  if (!row_map[ready_child]->empty()) {
    row = row_map[ready_child]->front();
    MySQLRowResponse row_response(row);
    uint64_t field_len = 0;
    const char *result = NULL;
    for (unsigned int i = 0; i < field_num; i++) {
      if (row_response.field_is_null(i)) {
        row_vector.push_back("NULL");
      } else {
        result = row_response.get_str(i, &field_len);
        string column(result, field_len);
        string full_column;
        if (field_is_number_vec[i]) {
          full_column = column;
        } else {
          CharsetType ctype = session->get_client_charset_type();
          deal_with_str_column_value(&column, ctype);
          full_column.append("'");
          full_column.append(column);
          full_column.append("'");
        }
        row_vector.push_back(full_column);
      }
    }
    row_map[ready_child]->pop_front();
    delete row;
    row_num++;
  }
}

void MySQLCursorSendNode::handle_children() {
  list<MySQLExecuteNode *>::iterator it;
  unsigned finished_child_num = 0;
  for (it = children.begin(); it != children.end(); ++it) {
    if (!row_map[*it]->empty()) {
      handle_child(*it);
      break;
    } else {
      finished_child_num++;
      status = EXECUTE_STATUS_WAIT;
    }
  }
  if (finished_child_num == children.size()) {
    status = EXECUTE_STATUS_COMPLETE;
  }
}

void MySQLCursorSendNode::execute() {
  if (plan->session->check_for_transaction() &&
      (session->get_session_option("cursor_use_free_conn").int_val == 0)) {
    LOG_ERROR(
        "can not call stored procedure with cursor when current session is in "
        "transaction.\n");
    throw NotSupportedError(
        "not support call stored procedure with cursor when current session is "
        "in transaction");
  }
  switch (ces) {
    case OPEN_EXECUTE: {
      open_execute();
      break;
    }
    case FETCH_EXECUTE: {
      fetch_execute();
      break;
    }
    default:
      throw ExecuteNodeError("MySQLCursorSendNode ces has not init.");
      break;
  }
}
void MySQLCursorSendNode::open_execute() {
  row_num = 0;
  init_row_map();
  children_execute();
  try {
    wait_children();
  } catch (ExecuteNodeError &e) {
    status = EXECUTE_STATUS_COMPLETE;
    throw e;
  }
  dealwith_header();
}

void MySQLCursorSendNode::fetch_execute() {
  if (status == EXECUTE_STATUS_WAIT) {
    try {
      wait_children();
    } catch (ExecuteNodeError &e) {
      status = EXECUTE_STATUS_COMPLETE;
      throw e;
    }
  }
  if (status == EXECUTE_STATUS_HANDLE) {
    handle_children();
  }
  if (status == EXECUTE_STATUS_BEFORE_COMPLETE) {
    handle_children();
  }
  if (status == EXECUTE_STATUS_COMPLETE) {
    if (!has_sql_calc_found_rows()) {
      session->set_found_rows(row_num);
    }
    session->set_real_fetched_rows(row_num);
    if (error_packet) {
      delete error_packet;
      error_packet = NULL;
    }
    error_packet = Backend::instance()->get_new_packet();
    MySQLErrorResponse error_response(
        ERROR_SP_FETCH_NO_DATA_CODE,
        dbscale_err_msg[ERROR_SP_FETCH_NO_DATA_CODE], "02000");
    error_response.pack(error_packet);
    not_found_flag = true;
    throw dbscale::sql::SQLError(dbscale_err_msg[ERROR_SP_FETCH_NO_DATA_CODE],
                                 "02000", ERROR_SP_FETCH_NO_DATA_CODE);
  }
}

/*class MySQLDBScaleShowStatusNode*/
MySQLDBScaleShowStatusNode::MySQLDBScaleShowStatusNode(ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowStatusNode";
  const char *_variable_name =
      plan->statement->get_stmt_node()->sql->dbscale_show_oper->variable_name;
  if (_variable_name) {
    variable_name.assign(_variable_name);
  } else {
    variable_name.assign("");
  }
}

void MySQLDBScaleShowStatusNode::init_data() {
  try {
    set_head_packet(2);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";
    list<const char *> columns;
    columns.push_back("Variable_name");
    columns.push_back("Value");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
    DBScaleStatus *showstatus = DBScaleStatus::instance();
    map<int, StatusNode *> *dbscalestatus = showstatus->get_status();
    map<int, StatusNode *>::iterator it = dbscalestatus->begin();
    for (; it != dbscalestatus->end(); it++) {
      if (!variable_name.empty()) {
        if (strcasecmp(variable_name.c_str(), it->second->key.c_str()))
          continue;
      }
      list<const char *> row_data;
      row_data.push_back(it->second->key.c_str());
      char str[100];
      snprintf(str, sizeof(str), "%lu", it->second->value);
      row_data.push_back(str);
      add_row_packet(row_data);
    }
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Connection get error!",
                     "42S02");
    throw ErrorPacketException();
  }
}

/* class MySQLDBScaleShowNode */
MySQLShowCatchupNode::MySQLShowCatchupNode(ExecutePlan *plan,
                                           const char *source_name)
    : MySQLDBScaleShowNode(plan), source_name(source_name) {
  this->name = "MySQLShowCatchupNode";
}

void MySQLShowCatchupNode::init_data() {
  Backend *bk = Backend::instance();
  DataSource *data_source = bk->find_data_source(source_name);
  if (data_source == NULL) {
    LOG_ERROR("Cannot find datasource %s for catchup.\n", source_name);
    throw Error("Cannot find datasource for catchup");
  }
  DataSourceType type = data_source->get_data_source_type();
  if (type != DATASOURCE_TYPE_REPLICATION) {
    LOG_ERROR(
        "Only support show catchup binlog info for replication data-source.\n");
    throw Error(
        "Only support show catchup binlog info for replication data-source");
  }
  ReplicationDataSource *rep_source = (ReplicationDataSource *)data_source;
  DataSourceWorkState state = rep_source->get_work_state();
  if (state != DATASOURCE_STATE_CATCHUP_BINLOG) {
    LOG_ERROR("DataSource does not in catchup binlog state.\n");
    throw Error("DataSource does not in catchup binlog state");
  }
  string task_name = rep_source->get_catchup_task_name();
  string file = rep_source->get_catchup_binlog_file();
  int pos = rep_source->get_catchup_binlog_pos();
  DataServer *catchup_server = rep_source->get_catchup_server();
  if (task_name == "" || file == "" || catchup_server == NULL) {
    LOG_ERROR(
        "Failed to get catchup info, cannot get the task_name or binlog_file "
        "or catchup data-server.\n");
    throw Error(
        "Failed to get catchup info, cannot get the task_name or binlog_file "
        "or catchup data-server");
  }

  Connection *conn = NULL;
  vector<string> task_result;
  int apply_size = 0;
  try {
    const char *username = catchup_server->get_user();
    const char *password = catchup_server->get_password();
    conn = catchup_server->create_connection(username, password, NULL);
    if (!conn) {
      LOG_ERROR("Fail to create connection from server.\n");
      throw ExecuteNodeError("Fail to create connection.");
    }
    char task_sql[256];
    sprintf(task_sql,
            "SELECT new_server, failover_server, "
            "last_sql_gtid, last_execute_trx_gtid, cur_trx_exec_time, "
            "total_size, begin_time, has_error "
            "FROM dbscale_tmp.catchup_binlog WHERE task='%s'",
            task_name.c_str());
    conn->query_for_one_row_send(task_sql);
    conn->query_for_one_row_recv(&task_result);
#ifdef DEBUG
    ACE_ASSERT(task_result.size() == 8);
#endif

    vector<vector<string> > binary_logs;
    conn->query_for_all_column("SHOW BINARY LOGS", &binary_logs);
    vector<vector<string> >::iterator it;
    for (it = binary_logs.begin(); it != binary_logs.end(); it++) {
#ifdef DEBUG
      ACE_ASSERT(it->size() == 2);
#endif
      if (it->size() != 2) {
        LOG_ERROR("Failed to get binary logs for data-server %s.\n",
                  catchup_server->get_name());
        throw ExecuteNodeError("Failed to get binary logs for data-server");
      }
      if (it->at(0) == file) {
        int tmp_pos = atoi(it->at(1).c_str());
        if (tmp_pos < pos) {
          LOG_ERROR("Got wrong binary logs for data-server %s.\n",
                    catchup_server->get_name());
          throw ExecuteNodeError("Got wrong binary logs for data-server");
        }
        apply_size += tmp_pos - pos;
      }
    }
    conn->close();
    delete conn;
  } catch (...) {
    if (conn) {
      conn->close();
      delete conn;
    }
    throw;
  }

  try {
    set_head_packet(10);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";
    list<const char *> columns;
    columns.push_back("data-source");
    columns.push_back("master-server");
    columns.push_back("failover-server");
    columns.push_back("last_sql_gtid");
    columns.push_back("last_catchup_gtid");
    columns.push_back("cur_trx_exec_time(s)");
    columns.push_back("total-size");
    columns.push_back("apply-size");
    columns.push_back("total-execute-time(s)");
    columns.push_back("estimate-left-time(s)");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }

    list<const char *> row_data;
    row_data.push_back(rep_source->get_name());
    // server info
    row_data.push_back(task_result[0].c_str());
    row_data.push_back(task_result[1].c_str());
    // gtid info
    row_data.push_back(task_result[2].c_str());
    if (task_result[3].size() != 0) {
      row_data.push_back(task_result[3].c_str());
    } else {
      row_data.push_back("NULL");
    }
    unsigned long end_time_in_msec = ACE_OS::gettimeofday().msec();
    unsigned long begin_trx_in_msec = strtoll(task_result[4].c_str(), NULL, 10);
    char trx_exec_time_str[64];
    if (begin_trx_in_msec != 0) {
      unsigned long trx_exec_time =
          (end_time_in_msec - begin_trx_in_msec) / 1000;
      sprintf(trx_exec_time_str, "%lus", trx_exec_time);
      row_data.push_back(trx_exec_time_str);
    } else {
      row_data.push_back("NULL");
    }
    // binlog size info
    row_data.push_back(task_result[5].c_str());
    char apply_size_str[32];
    sprintf(apply_size_str, "%d", apply_size);
    row_data.push_back(apply_size_str);
    unsigned long begin_task_in_msec =
        strtoll(task_result[6].c_str(), NULL, 10);
    unsigned long total_exec_time =
        (end_time_in_msec - begin_task_in_msec) / 1000;
    char total_exec_time_str[64];
    sprintf(total_exec_time_str, "%lus", total_exec_time);
    row_data.push_back(total_exec_time_str);
    if (task_result[4] == "TRUE" || apply_size == 0) {
      row_data.push_back("");
    } else {
      unsigned long total_size = strtoll(task_result[5].c_str(), NULL, 10);
      unsigned long left_time =
          total_exec_time * (total_size - apply_size) / apply_size;
      char left_time_str[64];
      sprintf(left_time_str, "%lus", left_time);
      row_data.push_back(left_time_str);
    }
    add_row_packet(row_data);
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Connection get error!",
                     "42S02");
    throw ErrorPacketException();
  }
}

/* class MySQLSkipWaitCatchupNode */
MySQLSkipWaitCatchupNode::MySQLSkipWaitCatchupNode(ExecutePlan *plan,
                                                   const char *source_name)
    : MySQLReturnOKNode(plan), error_packet(NULL), source_name(source_name) {}

void MySQLSkipWaitCatchupNode::do_execute() {
  Backend *bk = Backend::instance();
  DataSource *data_source = bk->find_data_source(source_name);
  if (data_source == NULL) {
    LOG_ERROR("Cannot find data-source %s.\n", source_name);
    throw Error("Cannot find data-source");
  }
  if (data_source->get_data_source_type() != DATASOURCE_TYPE_REPLICATION) {
    LOG_ERROR("DataSource %s is not a replcation source for catchup.\n",
              source_name);
    throw Error("DataSource is not a replication source for catchup");
  }
  ReplicationDataSource *rep_source = (ReplicationDataSource *)data_source;
  if (rep_source->get_work_state() != DATASOURCE_STATE_CATCHUP_BINLOG) {
    LOG_ERROR("Replication data-source %s does not in catchup binlog state.\n",
              source_name);
    throw Error("Replication data-source does not in catchup binlog state");
  }
  rep_source->set_force_exit_catchup(true);
  if (max_catchup_binlog_time != 0 &&
      bk->get_backend_server_version() == MYSQL_VERSION_MARIADB) {
    LOG_ERROR(
        "Cannot skip catchup binlog when backend is MariaDB "
        "related server when max_catchup_binlog_time is not 0.\n");
    throw Error(
        "Cannot skip catchup binlog when backend is MariaDB "
        "related server when max_catchup_binlog_time is not 0");
  }
}

void MySQLSkipWaitCatchupNode::clean() {}

/*class MySQLDBScaleShowAsyncTaskNode*/
MySQLDBScaleShowAsyncTaskNode::MySQLDBScaleShowAsyncTaskNode(ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowAsyncTaskNode";
}

void MySQLDBScaleShowAsyncTaskNode::init_data() {
  Backend *backend = Backend::instance();
  ACE_RW_Thread_Mutex *show_mutex = backend->get_dynamic_modify_rep_mutex();
  show_mutex->acquire_read();
  bool locked = true;
  try {
    set_head_packet(6);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";
    list<const char *> columns;
    columns.push_back("Id");
    columns.push_back("Name");
    columns.push_back("State");
    columns.push_back("Progress");
    columns.push_back("StartTime");
    columns.push_back("Extra");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }

#ifndef CLOSE_MULTIPLE
    bool is_master = true;
    if (multiple_mode) {
      MultipleManager *mul = MultipleManager::instance();
      is_master = mul->get_is_cluster_master();
    }
    if (!is_master &&
        plan->statement->get_stmt_node()->forward_to_master != 2) {
      show_mutex->release();
      locked = false;
      backend->forward_query_to_master_role_node(
          plan->statement->get_sql(), session->get_schema(), row_list);
      return;
    }
#endif
    map<unsigned long long, AsyncTask> task_map =
        AsyncTaskManager::instance()->get_async_task_map();
    map<unsigned long long, AsyncTask>::iterator it = task_map.begin();
    for (; it != task_map.end(); it++) {
      list<const char *> row_data;
      char str[100];
      snprintf(str, sizeof(str), "%llu", it->first);
      row_data.push_back(str);
      row_data.push_back(it->second.name.c_str());
      string state(AsyncTaskManager::instance()
                       ->async_state_str(it->second.state)
                       .c_str());
      row_data.push_back(state.c_str());
      char progress[100];
      snprintf(progress, sizeof(progress), "%d", it->second.progress);
      row_data.push_back(progress);
      ACE_Date_Time start_time(it->second.start_time);
      char start_time_str[100];
      snprintf(start_time_str, sizeof(start_time_str),
               "%04ld-%02ld-%02ld %02ld:%02ld:%02ld", start_time.year(),
               start_time.month(), start_time.day(), start_time.hour(),
               start_time.minute(), start_time.second());
      row_data.push_back(start_time_str);
      string extra_str(it->second.extra_info.c_str());
      row_data.push_back(extra_str.c_str());
      add_row_packet(row_data);
    }
    show_mutex->release();
    locked = false;
  } catch (exception &e) {
    if (locked) show_mutex->release();
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Connection get error!",
                     "42S02");
    throw ErrorPacketException();
  }
}

/*class MySQLDBScaleExplainNode*/
MySQLDBScaleExplainNode::MySQLDBScaleExplainNode(
    ExecutePlan *plan, bool is_server_explain_stmt_always_extended)
    : MySQLDBScaleShowNode(plan),
      is_server_explain_stmt_always_extended(
          is_server_explain_stmt_always_extended) {
  this->name = "MySQLDBScaleExplainNode";
}

void MySQLDBScaleExplainNode::init_data() {
  try {
    if (is_server_explain_stmt_always_extended)
      set_head_packet(14);
    else
      set_head_packet(12);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "explain";
    const char *org_table = "explain";

    list<const char *> columns;
    columns.push_back("exec_node");
    columns.push_back("data_source");
    columns.push_back("id");
    columns.push_back("select_type");
    columns.push_back("table");
    if (is_server_explain_stmt_always_extended) columns.push_back("partitions");
    columns.push_back("type");
    columns.push_back("possible_keys");
    columns.push_back("key");
    columns.push_back("key_len");
    columns.push_back("ref");
    columns.push_back("rows");
    if (is_server_explain_stmt_always_extended) columns.push_back("filtered");
    columns.push_back("extra");

    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
    list<list<string> > *row_data = plan->statement->get_explain_info();
    list<list<string> >::iterator it = row_data->begin();
    for (; it != row_data->end(); it++) {
      list<string>::iterator it2 = (*it).begin();
      list<const char *> row;
      for (; it2 != (*it).end(); it2++) {
        row.push_back((*it2).c_str());
      }
      add_row_packet(row);
    }
  } catch (exception &e) {
    plan->statement->clean_explain_info();
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
    LOG_ERROR("got exception when execute dbscale explain.\n");
    throw;
  }
}

void MySQLDBScaleExplainNode::init_data_for_explain_node_sql() {
  try {
    set_head_packet(2);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";
    reset_column_list();
    reset_row_list();
    list<const char *> columns;
    columns.push_back("node_id");
    columns.push_back("sql");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }

    list<pair<string, string> > *row_data = session->get_explain_sub_sql_list();
    list<pair<string, string> >::iterator it = row_data->begin();
    for (; it != row_data->end(); it++) {
      list<const char *> row;
      row.push_back((*it).first.c_str());
      row.push_back((*it).second.c_str());
      add_row_packet(row);
    }
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
    LOG_ERROR("got exception when execute dbscale explain.\n");
    throw;
  }
}

void MySQLDBScaleExplainNode::handle_data(bool has_more_result) {
  eof_packet = Backend::instance()->get_new_packet();
  handler->send_to_client(head_packet);
  for (list<Packet *>::iterator it = column_list.begin();
       it != column_list.end(); it++) {
    handler->send_to_client(*it);
  }
  MySQLEOFResponse eof;
  eof.pack(eof_packet);
  handler->deal_autocommit_with_ok_eof_packet(eof_packet);
  handler->send_to_client(eof_packet);
  for (list<Packet *>::iterator it = row_list.begin(); it != row_list.end();
       it++) {
    handler->send_to_client(*it);
  }
  MySQLEOFResponse end;
  end.pack(eof_packet);
  if (has_more_result) rebuild_eof_with_has_more_flag(eof_packet, driver);
  handler->deal_autocommit_with_ok_eof_packet(eof_packet);
  handler->send_to_client(eof_packet);
}

void MySQLDBScaleExplainNode::execute() {
#ifdef DEBUG
  node_start_timing();
#endif
  try {
    init_data();
    bool need_show_explain_sub_sql =
        !(session->get_explain_sub_sql_list()->empty());
    if (need_show_explain_sub_sql) {
      handle_data(true);

      delete head_packet;
      head_packet = NULL;
      delete eof_packet;
      eof_packet = NULL;

      init_data_for_explain_node_sql();
      handle_data(false);
    } else {
      handle_data(false);
    }
  } catch (ErrorPacketException &e) {
    plan->statement->clean_explain_info();
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  } catch (ExecuteNodeError &e) {
    plan->statement->clean_explain_info();
    status = EXECUTE_STATUS_COMPLETE;
    throw;
  } catch (exception &e) {
    plan->statement->clean_explain_info();
    status = EXECUTE_STATUS_COMPLETE;
    LOG_ERROR("DBscale show fail due to exception [%s].\n", e.what());
    throw;
  }

  plan->statement->clean_explain_info();
#ifdef DEBUG
  node_end_timing();
#endif
#ifdef DEBUG
  LOG_DEBUG("MySQLDBScaleShowNode %@ cost %d ms\n", this, node_cost_time);
#endif
  status = EXECUTE_STATUS_COMPLETE;
}

/*class MySQLDBScaleShowAutoIncInfoNode*/
MySQLDBScaleShowAutoIncInfoNode::MySQLDBScaleShowAutoIncInfoNode(
    ExecutePlan *plan, PartitionedTable *tab)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowAutoIncInfoNode";
  this->tab = tab;
}

void MySQLDBScaleShowAutoIncInfoNode::init_data() {
  try {
    set_head_packet(2);

    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "auto_inc";
    const char *org_table = "auto_inc";

    list<const char *> columns;
    columns.push_back("has_auto_increment_field");
    columns.push_back("auto_incrment_value");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
    string field1;
    string field2;
    join_node *node =
        plan->statement->get_stmt_node()->sql->auto_increment_info_oper->table;
    const char *table_name = node->table_name;
    const char *schema_name =
        node->schema_name ? node->schema_name : plan->session->get_schema();
    string full_table_name;
    splice_full_table_name(schema_name, table_name, full_table_name);

    if (Backend::instance()->has_auto_increment_field(full_table_name)) {
      ACE_Thread_Mutex *stmt_lock = tab->get_stmt_autoinc_lock(full_table_name);
      if (stmt_lock) stmt_lock->acquire();
      ACE_Thread_Mutex *row_lock = tab->get_row_autoinc_lock(full_table_name);
      if (row_lock) row_lock->acquire();
      int64_t curr_auto_inc_val = -1;
      try {
        curr_auto_inc_val =
            tab->get_current_auto_increment_value(full_table_name);
      } catch (...) {
        if (row_lock) row_lock->release();
        if (stmt_lock) stmt_lock->release();
        LOG_ERROR(
            "MySQLDBScaleShowAutoIncInfoNode failed to get_auto_inc_value\n");
        throw Error(
            "MySQLDBScaleShowAutoIncInfoNode failed to get_auto_inc_value");
      }
      if (row_lock) row_lock->release();
      if (stmt_lock) stmt_lock->release();
      field1 = "true";
      char value[100] = {0};
      sprintf(value, "%ld", curr_auto_inc_val);
      field2 = value;
    } else {
      field1 = "false";
      field2 = "-1";
    }

    list<const char *> row_data;
    row_data.push_back(field1.c_str());
    row_data.push_back(field2.c_str());
    add_row_packet(row_data);
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
    LOG_ERROR("got exception when execute dbscale show virtual_map.\n");
    throw;
  }
}
/*class MySQLDBScaleShowShardMapNode*/
MySQLDBScaleShowShardMapNode::MySQLDBScaleShowShardMapNode(
    ExecutePlan *plan, PartitionedTable *tab)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowShardMapNode";
  this->tab = tab;
}

void MySQLDBScaleShowShardMapNode::init_data() {
  try {
    set_head_packet(1);

    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "shard_map";
    const char *org_table = "shard_map";

    list<const char *> columns;
    columns.push_back("option");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }

    vector<string> shard_vec = tab->get_scheme_shard_str_vector();
    vector<string>::iterator it = shard_vec.begin();
    for (; it != shard_vec.end(); it++) {
      list<const char *> row_data;
      row_data.push_back(it->c_str());
      add_row_packet(row_data);
    }
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
    LOG_ERROR("got exception when execute dbscale show virtual_map.\n");
    throw;
  }
}

/*class MySQLDBScaleShowVirtualMapNode*/
MySQLDBScaleShowVirtualMapNode::MySQLDBScaleShowVirtualMapNode(
    ExecutePlan *plan, PartitionedTable *tab)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowVirtualMapNode";
  this->tab = tab;
}

void MySQLDBScaleShowVirtualMapNode::init_data() {
  try {
    set_head_packet(1);

    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "virtual_map";
    const char *org_table = "virtual_map";

    list<const char *> columns;
    columns.push_back("option");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }

    vector<string> vm_vec = tab->get_vm_str_vector();
    vector<string>::iterator it = vm_vec.begin();
    for (; it != vm_vec.end(); it++) {
      list<const char *> row_data;
      row_data.push_back(it->c_str());
      add_row_packet(row_data);
    }
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
    LOG_ERROR("got exception when execute dbscale show virtual_map.\n");
    throw;
  }
}

/* class MySQLDBScaleRequestAllClusterIncInfoNode */
MySQLDBScaleRequestAllClusterIncInfoNode::
    MySQLDBScaleRequestAllClusterIncInfoNode(ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleRequestAllClusterIncInfoNode";
}

void MySQLDBScaleRequestAllClusterIncInfoNode::init_data() {
  MultipleManager *mul = MultipleManager::instance();
  MultipleSyncTool *sync_tool = mul->get_sync_tool();
  try {
    if (sync_tool->get_sync_topic()) {
      const char *topic_name = sync_tool->get_sync_topic()->get_name();
      if (strcmp(topic_name, DYNAMIC_RESET_CLUSTER_NUM_TOPIC_NAME)) {
        LOG_ERROR(
            "DBScale is not increase cluster server num, but execute get all "
            "cluster inc info\n");
        throw Error(
            "DBScale is not increase cluster server num, but execute get all "
            "cluster inc info");
      }
    } else {
      LOG_ERROR(
          "DBScale is not increase cluster server num, but execute get all "
          "cluster inc info\n");
      throw Error(
          "DBScale is not increase cluster server num, but execute get all "
          "cluster inc info");
    }

    set_head_packet(1);

    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "dbscale_cluster";
    const char *org_table = "dbscale_cluster";

    list<const char *> columns;
    columns.push_back("table_name");
    columns.push_back("inc_value");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }

    Backend *backend = Backend::instance();
    map<string, bool> auto_increment_info =
        backend->get_auto_increment_info(false);
    map<string, bool>::iterator it = auto_increment_info.begin();
    for (; it != auto_increment_info.end(); it++) {
      if (it->second) {
        Table *t = backend->find_table(it->first);
        if (t && t->is_partitioned()) {
          PartitionedTable *part_table = (PartitionedTable *)t;
          string full_table_name = it->first;
          int inc_step;  // not used
          vector<string> strs;
          boost::split(strs, full_table_name, boost::is_any_of("."),
                       boost::token_compress_on);
          int64_t value = part_table->get_auto_increment_value(
              full_table_name, strs[0].c_str(), strs[1].c_str(), 1, inc_step);
          char value_str[22];
          int len = sprintf(value_str, "%ld", value);
          value_str[len] = '\0';

          // pack row data
          list<const char *> row_data;
          row_data.push_back(full_table_name.c_str());
          row_data.push_back(value_str);
          add_row_packet(row_data);
        } else {
          LOG_ERROR(
              "prepare_increase_cluster_num not find PartitionedTable [%s]\n",
              it->first.c_str());
#ifdef DEBUG
          ACE_ASSERT(0);
#endif
        }
      }
    }
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
    LOG_ERROR(
        "got exception when execute DBSCALE REQUEST CLUSTER ID, due to %s.\n",
        e.what());
    throw;
  }
}
/* class MySQLDBScaleRequestClusterInfoNode */
MySQLDBScaleRequestClusterInfoNode::MySQLDBScaleRequestClusterInfoNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleRequestClusterInfoNode";
}

Packet *MySQLDBScaleRequestClusterInfoNode::generate_error_cluster_node_info(
    string host_port) {
  list<const char *> row_data;
  row_data.push_back("node error");
  row_data.push_back("0");
  row_data.push_back(host_port.c_str());
  row_data.push_back("0");
  row_data.push_back("0");
  row_data.push_back("0");
  row_data.push_back("0");
  row_data.push_back("0");
  row_data.push_back("0");
  row_data.push_back("0");
  Packet *packet = Backend::instance()->get_new_packet();
  MySQLRowResponse row(row_data);
  row.pack(packet);
  return packet;
}
void MySQLDBScaleRequestClusterInfoNode::init_data() {
  try {
    MultipleManager *mul = MultipleManager::instance();
    list<string> error_nodes;
    list<Packet *> result_packet = mul->get_cluster_info(&error_nodes);
    set_head_packet(10);

    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "dbscale_cluster";
    const char *org_table = "dbscale_cluster";

    list<const char *> columns;
    columns.push_back("master_dbscale");
    columns.push_back("cluster_server_id");
    columns.push_back("host");
    columns.push_back("join time");
    columns.push_back("ka init Version");
    columns.push_back("ka update Version");
    columns.push_back("dynamic node Version");
    columns.push_back("dynamic space Version");
    columns.push_back("master re-scramble delay");
    columns.push_back("dbscale version");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
    list<Packet *>::iterator it = result_packet.begin();
    for (; it != result_packet.end(); it++) {
      add_row_packet(*it);
    }
    list<string>::iterator it_en = error_nodes.begin();
    for (; it_en != error_nodes.end(); it_en++) {
      Packet *packet = generate_error_cluster_node_info((*it_en).c_str());
      add_row_packet(packet);
    }
  } catch (exception &e) {
    string err_msg = string("get cluster info failed. ") + string(e.what());
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, err_msg.c_str());
    LOG_ERROR("%s\n", err_msg.c_str());
    throw ErrorPacketException();
  }
}

/* class MySQLDBScaleRequestUserStatusNode */
MySQLDBScaleRequestUserStatusNode::MySQLDBScaleRequestUserStatusNode(
    ExecutePlan *plan, const char *id, bool only_show_running, bool show_count)
    : MySQLDBScaleShowNode(plan),
      user_id(id),
      only_show_running(only_show_running),
      show_status_count(show_count) {
  this->name = "MySQLDBScaleRequestUserStatusNode";
}

void MySQLDBScaleRequestUserStatusNode::get_user_status_count() {
  int head_num = 2;
  set_head_packet(head_num);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "datasources";
  const char *org_table = "datasources";
  list<const char *> columns;
  columns.push_back("Status count");
  columns.push_back("Cluster id");
  while (!columns.empty()) {
    add_column_packet(catalog, schema, table, org_table, columns.front(),
                      columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    columns.pop_front();
  }
  get_user_status_running_count(driver, row_list, only_show_running);
}

void MySQLDBScaleRequestUserStatusNode::init_data() {
  if (show_status_count) {
    get_user_status_count();
  } else if (!user_id) {
    get_all_user_status();
  } else {
    get_user_status_by_id();
  }
}

void MySQLDBScaleRequestUserStatusNode::get_all_user_status() {
  set_head_packet(13);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "datasources";
  const char *org_table = "datasources";
  list<const char *> columns;
  columns.push_back("Thread_id_Handler");
  columns.push_back("User_id");
  columns.push_back("Cur_schema");
  columns.push_back("User_info");
  columns.push_back("Executing SQL");
  columns.push_back("Executing time(ms)");
  columns.push_back("Working State");
  columns.push_back("Login time");
  columns.push_back("using_conn_num");
  columns.push_back("net_in(Byte)");
  columns.push_back("net_out(Byte)");
  columns.push_back("Cluster id");
  columns.push_back("Transaction executing time(ms)");
  while (!columns.empty()) {
    add_column_packet(catalog, schema, table, org_table, columns.front(),
                      columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    columns.pop_front();
  }
  get_user_status(driver, row_list, only_show_running);
}

void MySQLDBScaleRequestUserStatusNode::get_user_status_by_id() {
  set_head_packet(6);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "datasources";
  const char *org_table = "datasources";
  list<const char *> columns;
  columns.push_back("User_id");
  columns.push_back("Cur_schema");
  columns.push_back("Working State");
  columns.push_back("Extra Info");
  columns.push_back("Kept Conn List");
  columns.push_back("Cluster id");
  while (!columns.empty()) {
    add_column_packet(catalog, schema, table, org_table, columns.front(),
                      columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);
    columns.pop_front();
  }
  uint32_t thread_id = atoi(this->user_id);
  get_user_status(driver, thread_id, row_list);
}

/* class MySQLDBScaleRequestNodeInfoNode */
MySQLDBScaleRequestNodeInfoNode::MySQLDBScaleRequestNodeInfoNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleRequestNodeInfoNode";
}

void MySQLDBScaleRequestNodeInfoNode::init_data() {
  try {
    set_head_packet(10);

    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "dbscale_cluster";
    const char *org_table = "dbscale_cluster";

    list<const char *> columns;
    columns.push_back("master_dbscale");
    columns.push_back("cluster_server_id");
    columns.push_back("host");
    columns.push_back("join time");
    columns.push_back("ka init Version");
    columns.push_back("ka update Version");
    columns.push_back("dynamic node Version");
    columns.push_back("dynamic space Version");
    columns.push_back("master re-scramble delay");
    columns.push_back("dbscale version");
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }

    // master_dbscale
    list<const char *> row_data;
    string master_dbscale;
    MultipleManager *mul = MultipleManager::instance();
    if (mul->get_is_cluster_master()) {
      master_dbscale = "master";
    } else {
      master_dbscale = "slave";
    }
    row_data.push_back(master_dbscale.c_str());

    // cluster_server_id
    Backend *backend = Backend::instance();
    int cluster_id = backend->get_cluster_id();
    char cluster_id_str[256];
    int len = sprintf(cluster_id_str, "%d", cluster_id);
    cluster_id_str[len] = '\0';
    row_data.push_back(cluster_id_str);
    // host
    row_data.push_back(mul->get_node_value().c_str());

    // join_time
    ACE_Date_Time join_time(mul->get_join_time());
    char join_time_str[100];
    snprintf(join_time_str, sizeof(join_time_str),
             "%04ld-%02ld-%02ld %02ld:%02ld:%02ld", join_time.year(),
             join_time.month(), join_time.day(), join_time.hour(),
             join_time.minute(), join_time.second());
    row_data.push_back(join_time_str);

    // ka init message Version
    unsigned long init_v = mul->get_ka_init_message_version();
    char tmp[256];
    len = sprintf(tmp, "%lu", init_v);
    tmp[len] = '\0';
    row_data.push_back(tmp);
    // ka update message Version
    unsigned long update_v = mul->get_got_update_message_version();
    unsigned long update_apply_v =
        mul->get_cur_ka_update_message_version_for_display();
    char tmp2[256];
    len = sprintf(tmp2, "%lu|%lu", update_v, update_apply_v);
    tmp2[len] = '\0';
    row_data.push_back(tmp2);
    // dynamic node message Version
    unsigned long dynamic_v = mul->get_cur_dynamic_op_message_version();
    char tmp3[256];
    len = sprintf(tmp3, "%lu", dynamic_v);
    tmp3[len] = '\0';
    row_data.push_back(tmp3);
    // dynamic space node message Version
    unsigned long dynamic_sv =
        mul->get_cur_dynamic_cluster_management_info_message_version();
    char tmp4[256];
    len = sprintf(tmp4, "%lu", dynamic_sv);
    tmp4[len] = '\0';
    row_data.push_back(tmp4);

    char tmp5[256];
    len = sprintf(
        tmp5, "%d",
        instance_option_value["dbscale_master_rescramble_delay"].uint_val);
    row_data.push_back(tmp5);

    string dbscale_version;
    backend->get_dbscale_version(&dbscale_version);
    row_data.push_back(dbscale_version.c_str());

    add_row_packet(row_data);
  } catch (...) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "got exception when execute DBSCALE REQUEST NODE INFO.");
    LOG_ERROR("got exception when execute DBSCALE REQUEST NODE INFO.\n");
    throw ErrorPacketException();
  }
}

/* class MySQLDBScaleClusterShutdownNode */
MySQLDBScaleClusterShutdownNode::MySQLDBScaleClusterShutdownNode(
    ExecutePlan *plan, int cluster_id)
    : MySQLReturnOKNode(plan), cluster_id(cluster_id) {
  this->name = "MySQLDBScaleClusterShutdownNode";
}

void MySQLDBScaleClusterShutdownNode::do_execute() {
#ifndef CLOSE_MULTIPLE
  MultipleManager *mul = MultipleManager::instance();
  if (mul->get_is_cluster_master()) {
    mul->execute_master_query("SHUTDOWN", cluster_id);
  }
  status = EXECUTE_STATUS_COMPLETE;
#endif
}

/* class MySQLDBScaleRequestClusterIdNode */
MySQLDBScaleRequestClusterIdNode::MySQLDBScaleRequestClusterIdNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleRequestClusterIdNode";
}

void MySQLDBScaleRequestClusterIdNode::init_data() {
  try {
    /*First check whether the master role dbscale allow the new slave role
     * dbscale join the cluster.*/
    MultipleManager *mul = MultipleManager::instance();
#ifdef DEBUG
    ACE_ASSERT(mul->get_is_cluster_master());
#endif
    if (!mul->allow_new_slave_join()) {
      LOG_ERROR("Master refuse slave join.\n");
      throw ExecuteNodeError("Master refuse join.");
    }

    set_head_packet(1);

    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "dbscale_cluster";
    const char *org_table = "dbscale_cluster";

    list<const char *> columns;
    columns.push_back("cluster_id");
    add_column_packet(catalog, schema, table, org_table, columns.front(),
                      columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);

    Backend *backend = Backend::instance();
    int cluster_id = backend->get_cluster_next_id();
    char tmp[256];
    int len = sprintf(tmp, "%d", cluster_id);
    tmp[len] = '\0';

    list<const char *> row_data;
    row_data.push_back(tmp);
    add_row_packet(row_data);
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
    LOG_ERROR("got exception when execute DBSCALE REQUEST CLUSTER ID.\n");
    throw;
  }
}
/* class MySQLDBScaleRequestClusterIncInfoNode */
MySQLDBScaleRequestClusterIncInfoNode::MySQLDBScaleRequestClusterIncInfoNode(
    ExecutePlan *plan, PartitionedTable *space, const char *schema_name,
    const char *table_name)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleRequestClusterIncInfoNode";
  this->space = space;
  this->schema_name = schema_name;
  this->table_name = table_name;
}

int64_t MySQLDBScaleRequestClusterIncInfoNode::get_auto_inc_value() {
  int64_t ret = -1;
  if (!space || !schema_name || !table_name) return ret;
  string full_table_name;
  splice_full_table_name(schema_name, table_name, full_table_name);
  if (2 == space->set_auto_increment_info(plan->handler, plan->statement,
                                          schema_name, table_name, true,
                                          true)) {
    return ret;
  }
  space->acquire_row_autoinc_lock(full_table_name);
  try {
    if (auto_increment_range_value) {
      ret = Driver::get_driver()->get_config_helper()->get_table_inc_info(
          full_table_name, auto_increment_range_value);
    } else {
      ret = space->get_auto_increment_base_value(full_table_name);
      if (ret < 0 || space->get_need_check_auto_increment_value_from_server(
                         full_table_name)) {
        ret = plan->handler->get_part_table_next_insert_id(schema_name,
                                                           table_name, space);
        space->set_auto_increment_base_value(full_table_name, ret - 1);
        space->set_auto_increment_value(full_table_name, ret - 1);
        space->set_need_check_auto_increment_value_from_server(full_table_name,
                                                               false);
        MultipleManager *mul = MultipleManager::instance();
        mul->pub_auto_increment_info(schema_name, table_name);
      } else {
        // keep same result with mysql, use ret-1 as base value
        ret = ret + 1;
      }
    }
  } catch (...) {
    space->release_row_autoinc_lock(full_table_name);
    throw;
  }
  space->release_row_autoinc_lock(full_table_name);
  return ret;
}

void MySQLDBScaleRequestClusterIncInfoNode::init_data() {
  LOG_DEBUG("DBSCALE REQUEST CLUSTER auto_incrment info %s.%s.\n", schema_name,
            table_name);
  try {
    set_head_packet(1);

    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "dbscale_cluster";
    const char *org_table = "dbscale_cluster";

    list<const char *> columns;
    columns.push_back("auto_incrment_value");
    add_column_packet(catalog, schema, table, org_table, columns.front(),
                      columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                      0, 0, 0);

    int64_t inc_value = get_auto_inc_value();
    if (inc_value < 0) {
      string str_err = "get cluster auto_increment value for table [";
      str_err.append(schema_name);
      str_err.append(".");
      str_err.append(table_name);
      str_err.append("] failed.");
      LOG_ERROR("%s\n", str_err.c_str());
      throw Error(str_err.c_str());
    }
    char tmp[256];
    int len = sprintf(tmp, "%ld", inc_value);
    tmp[len] = '\0';

    LOG_DEBUG(
        "DBSCALE REQUEST CLUSTER auto_incrment info [%s.%s] value [%s].\n",
        schema_name, table_name, tmp);

    list<const char *> row_data;
    row_data.push_back(tmp);
    add_row_packet(row_data);
  } catch (exception &e) {
    LOG_ERROR(
        "got exception when execute DBSCALE REQUEST CLUSTER auto_incrment info "
        "%s.%s.\n",
        schema_name, table_name);
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
    throw;
  }
}

#ifndef CLOSE_ZEROMQ

/* class MySQLMessageServiceNode */
MySQLMessageServiceNode::MySQLMessageServiceNode(ExecutePlan *plan)
    : MySQLExecuteNode(plan), error_packet(NULL) {}

void MySQLMessageServiceNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}

void MySQLMessageServiceNode::execute() {
  if (Backend::instance()->get_service_in_restore_stage()) {
    LOG_ERROR(
        "Service is restore, cannot execute service related operation.\n");
    throw MessageServiceError(
        "Service is restore, cannot execute service related operation.");
  }
  message_service_op_node *service_node =
      plan->statement->get_stmt_node()->sql->message_service_oper;
  bool is_start = service_node->is_start;
  bool is_server = service_node->is_server;

  try {
    if (is_server) {
      if (is_start) {
        start_server_service();
      } else {
        stop_server_service();
      }
    } else {
      if (is_start) {
        start_client_service();
      } else {
        stop_client_service();
      }
    }
  } catch (Exception &e) {
    MySQLErrorResponse error_response(e.get_errno(), e.what());
    error_packet = Backend::instance()->get_new_packet();
    error_response.pack(error_packet);
    status = EXECUTE_STATUS_COMPLETE;
    throw ErrorPacketException();
  }
  send_ok_packet_to_client(handler, 0, 0);
  status = EXECUTE_STATUS_COMPLETE;
}

void MySQLMessageServiceNode::start_server_service() {
  // TODO: broker may be separated from server-service in multiple-dbscale
  // start server broker
  MessageBroker *broker = MessageBroker::instance();
  broker->start_broker();
  // start server service
  MessageServerManager *msg_manager = MessageServerManager::instance();
  msg_manager->start_service();
}

void MySQLMessageServiceNode::stop_server_service() {
  // stop server service
  MessageServerManager *msg_manager = MessageServerManager::instance();
  msg_manager->stop_service();
  // stop server broker
  MessageBroker *broker = MessageBroker::instance();
  broker->stop_broker();
}

void MySQLMessageServiceNode::start_client_service() {
  // start server service
  MessageClientManager *msg_manager = MessageClientManager::instance();
  msg_manager->start_service();
}

void MySQLMessageServiceNode::stop_client_service() {
  MessageClientManager *msg_manager = MessageClientManager::instance();
  msg_manager->stop_service();
}

/* class MySQLBinlogTaskNode */
MySQLBinlogTaskNode::MySQLBinlogTaskNode(ExecutePlan *plan)
    : MySQLExecuteNode(plan), error_packet(NULL) {}

void MySQLBinlogTaskNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}

void MySQLBinlogTaskNode::execute() {
  if (Backend::instance()->get_service_in_restore_stage()) {
    LOG_ERROR(
        "Service is restore, cannot execute service related operation.\n");
    throw MessageServiceError(
        "Service is restore, cannot execute service related operation.");
  }
  if (!Backend::instance()->get_server_service_start()) {
    LOG_ERROR("Server service not start, can not do task operation.\n");
    throw MessageServiceError(
        "Server service not start, can not do task operation.");
  }

  try {
    create_task_service();
  } catch (Exception &e) {
    MySQLErrorResponse error_response(e.get_errno(), e.what());
    error_packet = Backend::instance()->get_new_packet();
    error_response.pack(error_packet);
    status = EXECUTE_STATUS_COMPLETE;
    throw ErrorPacketException();
  }
  send_ok_packet_to_client(handler, 0, 0);
  status = EXECUTE_STATUS_COMPLETE;
}

void MySQLBinlogTaskNode::create_task_service() {
  binlog_task_op_node *binlog_task =
      plan->statement->get_stmt_node()->sql->binlog_task_oper;
  MessageTaskName task_name(binlog_task->server_task_name,
                            binlog_task->client_task_name);
  task_service_type type = binlog_task->task_type;
  bool is_data_source = binlog_task->is_data_source;
  const char *data_source_name = binlog_task->data_source;
  const char *data_server_name = binlog_task->data_server;
  bool dump_use_gtid = binlog_task->dump_use_gtid;
  const char *binlog_file = binlog_task->binlog_file;
  int binlog_pos = binlog_task->binlog_pos;
  const char *binlog_gtid = binlog_task->binlog_gtid;
  bool replay_use_gtid = binlog_task->replay_use_gtid;

  string name;
  MessageManager *msg_manager = NULL;
  switch (type) {
    case TASK_SERVER:
      msg_manager = MessageServerManager::instance();
      name = task_name.server_task_name;
      break;
    case TASK_CLIENT:
      if (!Backend::instance()->get_client_service_start()) {
        LOG_ERROR("Client service not start, can not do task operation.\n");
        throw MessageServiceError(
            "Client service not start, can not do task operation.");
      }
      msg_manager = MessageClientManager::instance();
      name = task_name.client_task_name;
      break;
    default:
      LOG_ERROR("Unsupport binlog task type.\n");
      throw MessageServiceError(ERROR_UNKNOWN_TASK_TYPE_MSG);
      break;
  }

  if (msg_manager->has_task(name)) {
    LOG_ERROR("Binlog task already exist.\n");
    throw Error("Binlog task already exist.");
  }

  Backend *backend = Backend::instance();
  DataServer *data_server = NULL;
  DataSource *data_source = NULL;
  bool is_backend_mariadb =
      backend->get_backend_server_version() == MYSQL_VERSION_MARIADB;
  if (is_data_source) {
    data_source = backend->find_data_source(data_source_name);
    if (!strcmp(data_source_name, auth_data_source_name) ||
        !strcmp(data_source_name, config_data_source_name) ||
        !strcmp(data_source_name, dbscale_metadata_source_name)) {
      LOG_ERROR("Binlog task cannot related to auth/config/metadata source.\n");
      throw MessageServiceError(
          "Binlog task cannot related to auth/config/metadata source.");
    }
    if (data_source == NULL) {
      LOG_ERROR("Binlog task cannot find data_source.\n");
      throw MessageServiceError("Binlog task cannot find data_source.");
    }
    if (data_source->is_internal()) {
      LOG_ERROR("Binlog task cannot releated to internal data-source.\n");
      throw MessageServiceError(
          "Binlog task cannot releated to internal data-source.");
    }
  } else {
    data_server = backend->find_data_server(data_server_name);
    if (data_server == NULL) {
      LOG_ERROR("Binlog task cannot find dataserver.\n");
      throw MessageServiceError("Binlog task cannot find dataserver.");
    }
  }

  MessageTask *binlog_msg_task = NULL;
  switch (type) {
    case TASK_SERVER:
      if (is_data_source) {
        binlog_msg_task = new MySQLBinlogServerDataSourceTask(
            name.c_str(), data_source, binlog_gtid, is_backend_mariadb);
      } else if (!dump_use_gtid) {
        binlog_msg_task =
            new MySQLBinlogServerTask(name.c_str(), data_server, binlog_file,
                                      binlog_pos, is_backend_mariadb);
      } else {
        binlog_msg_task = new MySQLBinlogServerTask(
            name.c_str(), data_server, binlog_gtid, is_backend_mariadb);
      }
      break;
    case TASK_CLIENT:
      if (is_data_source) {
        binlog_msg_task = new MySQLBinlogClientDataSourceTask(
            name.c_str(), data_source, binlog_gtid, is_backend_mariadb);
      } else if (!dump_use_gtid) {
        binlog_msg_task = new MySQLBinlogClientTask(
            name.c_str(), data_server, binlog_file, binlog_pos, replay_use_gtid,
            is_backend_mariadb);
      } else {
        binlog_msg_task =
            new MySQLBinlogClientTask(name.c_str(), data_server, binlog_gtid,
                                      replay_use_gtid, is_backend_mariadb);
      }
      ((ClientMessageTask *)binlog_msg_task)
          ->set_server_task_name(task_name.server_task_name);
      break;
    default:
      LOG_ERROR("Unsupport binlog task type.\n");
      throw Error("Unsupport binlog task type.");
      break;
  }

  try {
    msg_manager->create_task(task_name, binlog_msg_task);
  } catch (...) {
    delete binlog_msg_task;
    throw;
  }
}

/* class MySQLTaskNode */
MySQLTaskNode::MySQLTaskNode(ExecutePlan *plan)
    : MySQLExecuteNode(plan), error_packet(NULL) {}

void MySQLTaskNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}

void MySQLTaskNode::execute() {
  if (Backend::instance()->get_service_in_restore_stage()) {
    LOG_ERROR(
        "Service is restore, cannot execute service related operation.\n");
    throw MessageServiceError(
        "Service is restore, cannot execute service related operation.");
  }
  if (!Backend::instance()->get_server_service_start()) {
    LOG_ERROR("Server service not start, can not do task operation.\n");
    throw MessageServiceError(
        "Server service not start, can not do task operation.");
  }

  task_op_node *task_oper = plan->statement->get_stmt_node()->sql->task_oper;
  string task_name = task_oper->task_name;
  bool is_start = task_oper->is_start;
  bool is_server = task_oper->is_server;
  task_thread_type type = task_oper->thread_type;

  try {
    if (is_start)
      start_task(is_server, task_name, type);
    else
      stop_task(is_server, task_name, type);
  } catch (Exception &e) {
    MySQLErrorResponse error_response(e.get_errno(), e.what());
    error_packet = Backend::instance()->get_new_packet();
    error_response.pack(error_packet);
    status = EXECUTE_STATUS_COMPLETE;
    throw ErrorPacketException();
  }

  send_ok_packet_to_client(handler, 0, 0);
  status = EXECUTE_STATUS_COMPLETE;
}

void MySQLTaskNode::start_task(bool is_server, string task_name,
                               task_thread_type type) {
  if (is_server) {
    MessageServerManager::instance()->start_task(task_name);
  } else {
    if (!Backend::instance()->get_client_service_start()) {
      LOG_ERROR("Client service not start, can not do task operation.\n");
      throw MessageServiceError(
          "Client service not start, can not do task operation.");
    }
    MessageClientManager *msg_manager = MessageClientManager::instance();
    switch (type) {
      case CLIENT_ALL_THREAD:
        msg_manager->start_task(task_name);
        break;
      case CLIENT_SQL_THREAD:
        msg_manager->start_sql_thread(task_name);
        break;
      case CLIENT_IO_THREAD:
        msg_manager->start_io_thread(task_name);
        break;
      default:
        LOG_ERROR("Unsupport thread type for client task.\n");
        throw MessageServiceError("Unsupport thread type for client task.");
    }
  }
}

void MySQLTaskNode::stop_task(bool is_server, string task_name,
                              task_thread_type type) {
  if (is_server) {
    MessageServerManager::instance()->stop_task(task_name);
  } else {
    if (!Backend::instance()->get_client_service_start()) {
      LOG_ERROR("Client service not start, can not do task operation.\n");
      throw MessageServiceError(
          "Client service not start, can not do task operation.");
    }
    MessageClientManager *msg_manager = MessageClientManager::instance();
    switch (type) {
      case CLIENT_ALL_THREAD:
        msg_manager->stop_task(task_name);
        break;
      case CLIENT_SQL_THREAD:
        msg_manager->stop_sql_thread(task_name);
        break;
      case CLIENT_IO_THREAD:
        msg_manager->stop_io_thread(task_name);
        break;
      default:
        LOG_ERROR("Unsupport thread type for client task.\n");
        throw MessageServiceError("Unsupport thread type for client task.");
    }
  }
}

/* class MySQLBinlogTaskAddFilterNode */
MySQLBinlogTaskAddFilterNode::MySQLBinlogTaskAddFilterNode(ExecutePlan *plan)
    : MySQLExecuteNode(plan), error_packet(NULL) {
  binlog_task_filter_op_node *filter_oper =
      plan->statement->get_stmt_node()->sql->binlog_task_filter_oper;
  task_name = filter_oper->task_name;
  is_server = filter_oper->is_server;
  bool first_item = true;
  filter_table_node *filter_item = filter_oper->filter_table;
  while (filter_item != filter_oper->filter_table || first_item) {
    first_item = false;
    string full_name(filter_item->table_name);
    vector<string> strs;
    boost::split(strs, full_name, boost::is_any_of("."),
                 boost::token_compress_on);
    string schema_name = strs[0];
    string table_name;
    for (unsigned int i = 1; i < strs.size(); i++) {
      if (i != 1) {
        table_name.append(".");
      }
      table_name.append(strs[i]);
    }
    set<unsigned int> vid_set;
    vid_item *vid_list = filter_item->vid_list;
    while (vid_list) {
      vid_set.insert(vid_list->vid);
      vid_list = vid_list->next;
    }
    filter_table_struct ft = {schema_name, table_name, vid_set};
    filter_table_vector.push_back(ft);
    filter_item = filter_item->next;
  }
}

void MySQLBinlogTaskAddFilterNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}

void MySQLBinlogTaskAddFilterNode::execute() {
  if (Backend::instance()->get_service_in_restore_stage()) {
    LOG_ERROR(
        "Service is restore, cannot execute service related operation.\n");
    throw MessageServiceError(
        "Service is restore, cannot execute service related operation.");
  }
  if (!Backend::instance()->get_server_service_start()) {
    LOG_ERROR("Server service not start, can not do task operation.\n");
    throw MessageServiceError(
        "Server service not start, can not do task operation.");
  }

  try {
    add_filter();
  } catch (Exception &e) {
    MySQLErrorResponse error_response(e.get_errno(), e.what());
    error_packet = Backend::instance()->get_new_packet();
    error_response.pack(error_packet);
    status = EXECUTE_STATUS_COMPLETE;
    throw ErrorPacketException();
  }

  send_ok_packet_to_client(handler, 0, 0);
  status = EXECUTE_STATUS_COMPLETE;
}

void MySQLBinlogTaskAddFilterNode::add_filter() {
  MessageManager *msg_manager = NULL;
  if (is_server)
    msg_manager = MessageServerManager::instance();
  else {
    if (!Backend::instance()->get_client_service_start()) {
      LOG_ERROR("Client service not start, can not do task operation.\n");
      throw MessageServiceError(
          "Client service not start, can not do task operation.");
    }
    msg_manager = MessageClientManager::instance();
  }

  MySQLBinlogTaskFilter filter(filter_table_vector);
  msg_manager->add_filter(task_name, &filter);
  add_filter_to_database();
}

void MySQLBinlogTaskAddFilterNode::add_filter_to_database() {
  vector<filter_table_struct>::iterator it = filter_table_vector.begin();
  const char *is_server_str = is_server ? "1" : "0";
  for (; it != filter_table_vector.end(); it++) {
    string schema_name = (*it).schema_name;
    string table_name = (*it).table_name;
    set<unsigned int> vid_set = (*it).vid_set;
    string full_table_name;
    splice_full_table_name(schema_name, table_name, full_table_name);
    string vid_string;
    set<unsigned int>::iterator it = vid_set.begin();
    for (; it != vid_set.end(); it++) {
      char vid_c[20];
      sprintf(vid_c, "%d", *it);
      if (it != vid_set.begin()) vid_string.append(",");
      vid_string.append(vid_c);
    }

    string replace_sql("REPLACE INTO dbscale_message_meta.filter VALUES(");
    replace_sql.append(is_server_str);
    replace_sql.append(", '");
    replace_sql.append(task_name);
    replace_sql.append("', '");
    replace_sql.append(schema_name);
    replace_sql.append("', '");
    replace_sql.append(table_name);
    replace_sql.append("', '");
    replace_sql.append(vid_string);
    replace_sql.append("')");

    Backend *backend = Backend::instance();
    backend->execute_sql_in_metadata(replace_sql);
  }
}

void MySQLDropTaskFilterNode::drop_filter_to_database() {
  const char *is_server_str = is_server ? "1" : "0";

  string replace_sql(
      "DELETE FROM dbscale_message_meta.filter WHERE is_server = ");
  replace_sql.append(is_server_str);
  replace_sql.append(" AND task_name = '");
  replace_sql.append(task_name);
  replace_sql.append("'");

  Backend *backend = Backend::instance();
  DataSpace *data_space = backend->get_metadata_data_space();
  if (data_space) {
    Connection *conn = NULL;
    try {
      conn = data_space->get_connection(session);
      if (conn) {
        conn->execute_one_modify_sql(replace_sql.c_str());
        conn->get_pool()->add_back_to_free(conn);
        conn = NULL;
      } else {
        LOG_ERROR(
            "DBScale fail to update tables to filter since fail to get "
            "connection.\n");
        throw Error(
            "DBScale fail to update tables to filter since fail to get "
            "connection.");
      }
    } catch (...) {
      if (conn) {
        conn->get_pool()->add_back_to_dead(conn);
      }
      conn = NULL;
      LOG_ERROR(
          "DBScale fail to update tables in metadata dataspace for message "
          "queue.\n");
      throw Error(
          "DBScale fail to update tables in metadata dataspace for message "
          "queue.");
    }
  }
}

/* class MySQLDropTaskFilterNode */
MySQLDropTaskFilterNode::MySQLDropTaskFilterNode(ExecutePlan *plan)
    : MySQLExecuteNode(plan), error_packet(NULL) {
  drop_task_filter_op_node *filter_oper =
      plan->statement->get_stmt_node()->sql->drop_task_filter_oper;
  task_name = filter_oper->task_name;
  is_server = filter_oper->is_server;
}

void MySQLDropTaskFilterNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}

void MySQLDropTaskFilterNode::execute() {
  if (Backend::instance()->get_service_in_restore_stage()) {
    LOG_ERROR(
        "Service is restore, cannot execute service related operation.\n");
    throw MessageServiceError(
        "Service is restore, cannot execute service related operation.");
  }
  if (!Backend::instance()->get_server_service_start()) {
    LOG_ERROR("Server service not start, can not do task operation.\n");
    throw MessageServiceError(
        "Server service not start, can not do task operation.");
  }

  try {
    MessageManager *msg_manager = NULL;
    if (is_server)
      msg_manager = MessageServerManager::instance();
    else {
      if (!Backend::instance()->get_client_service_start()) {
        LOG_ERROR("Client service not start, can not do task operation.\n");
        throw MessageServiceError(
            "Client service not start, can not do task operation.");
      }
      msg_manager = MessageClientManager::instance();
    }

    msg_manager->drop_filter(task_name);
  } catch (Exception &e) {
    MySQLErrorResponse error_response(e.get_errno(), e.what());
    error_packet = Backend::instance()->get_new_packet();
    error_response.pack(error_packet);
    status = EXECUTE_STATUS_COMPLETE;
    throw ErrorPacketException();
  }
  drop_filter_to_database();
  send_ok_packet_to_client(handler, 0, 0);
  status = EXECUTE_STATUS_COMPLETE;
}

/* class MySQLDropTaskNode */
MySQLDropTaskNode::MySQLDropTaskNode(ExecutePlan *plan)
    : MySQLExecuteNode(plan), error_packet(NULL) {}

void MySQLDropTaskNode::clean() {
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}

void MySQLDropTaskNode::execute() {
  if (Backend::instance()->get_service_in_restore_stage()) {
    LOG_ERROR(
        "Service is restore, cannot execute service related operation.\n");
    throw MessageServiceError(
        "Service is restore, cannot execute service related operation.");
  }
  if (!Backend::instance()->get_server_service_start()) {
    LOG_ERROR("Server service not start, can not do task operation.\n");
    throw MessageServiceError(
        "Server service not start, can not do task operation.");
  }

  task_op_node *task_oper = plan->statement->get_stmt_node()->sql->task_oper;
  string task_name = task_oper->task_name;
  bool is_server = task_oper->is_server;

  try {
    drop_task(is_server, task_name);
  } catch (Exception &e) {
    MySQLErrorResponse error_response(e.get_errno(), e.what());
    error_packet = Backend::instance()->get_new_packet();
    error_response.pack(error_packet);
    status = EXECUTE_STATUS_COMPLETE;
    throw ErrorPacketException();
  }
  delete_task_metadata(is_server, task_name);
  send_ok_packet_to_client(handler, 0, 0);
  status = EXECUTE_STATUS_COMPLETE;
}

void MySQLDropTaskNode::delete_task_metadata(bool is_server, string task) {
  string delete_sql("DELETE FROM ");
  if (is_server)
    delete_sql.append("dbscale_message_meta.server_task");
  else
    delete_sql.append("dbscale_message_meta.client_task");

  delete_sql.append(" WHERE name='");
  delete_sql.append(task);
  delete_sql.append("'");

  Backend *backend = Backend::instance();
  DataSpace *data_space = backend->get_metadata_data_space();
  if (data_space) {
    Connection *conn = NULL;
    try {
      conn = data_space->get_connection(session);
      if (conn) {
        conn->execute_one_modify_sql(delete_sql.c_str());
        conn->get_pool()->add_back_to_free(conn);
        conn = NULL;
      }
    } catch (...) {
      if (conn) {
        conn->get_pool()->add_back_to_dead(conn);
      }
      throw;
    }
  }
}

void MySQLDropTaskNode::drop_task(bool is_server, string task_name) {
  MessageManager *msg_manager = NULL;
  if (is_server)
    msg_manager = MessageServerManager::instance();
  else {
    if (!Backend::instance()->get_client_service_start()) {
      LOG_ERROR("Client service not start, can not do task operation.\n");
      throw MessageServiceError(
          "Client service not start, can not do task operation.");
    }
    msg_manager = MessageClientManager::instance();
  }

  if (!msg_manager->has_task(task_name)) {
    LOG_ERROR("Unknown task name.\n");
    throw MessageServiceError("Unknown task name");
  }

  msg_manager->drop_task(task_name);
}

/* class MySQLShowClientTaskStatusNode */
MySQLShowClientTaskStatusNode::MySQLShowClientTaskStatusNode(
    ExecutePlan *plan, const char *task_name, const char *server_task_name)
    : MySQLDBScaleShowNode(plan),
      task_name(task_name),
      server_task_name(server_task_name) {
  this->name = "MySQLShowClientTaskStatusNode";
}

void MySQLShowClientTaskStatusNode::handle_one_client_task(const char *name) {
  MessageClientManager *cmg = MessageClientManager::instance();
  map<string, ClientMessageTask *> *tasks =
      cmg->get_service()->get_client_tasks_without_lock();
  if (!tasks->count(name)) {
    return;
  }
  ClientMessageTask *c_task = (*tasks)[name];
  vector<string> task_info;
  c_task->get_task_replay_info(&task_info);
#ifdef DEBUG
  ACE_ASSERT(task_info.size() == 9);
#endif
  list<const char *> row_data;
  row_data.push_back(name);
  row_data.push_back(task_info[0].c_str());
  row_data.push_back(task_info[1].c_str());
  row_data.push_back(task_info[2].c_str());
  row_data.push_back(task_info[3].c_str());
  row_data.push_back(task_info[4].c_str());
  row_data.push_back(task_info[5].c_str());
  row_data.push_back(task_info[6].c_str());
  row_data.push_back(task_info[7].c_str());
  row_data.push_back(task_info[8].c_str());
  row_data.push_back(c_task->get_server_task_name().c_str());
  add_row_packet(row_data);
}

void MySQLShowClientTaskStatusNode::init_data() {
  MessageClientManager *cmg = MessageClientManager::instance();

  set_head_packet(11);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "dataservers";
  const char *org_table = "dataservers";
  list<const char *> columns;
  columns.push_back("ClientTaskName");
  columns.push_back("IO_Status");
  columns.push_back("SQL_Status");
  columns.push_back("DataSource");
  columns.push_back("SourceStatus");
  columns.push_back("DataServer");
  columns.push_back("ServerStatus");
  columns.push_back("BinlogFile");
  columns.push_back("BinlogPos");
  columns.push_back("BinlogGtid");
  columns.push_back("RelatedServerTask");
  list<const char *>::iterator it = columns.begin();
  for (; it != columns.end(); it++) {
    add_column_packet(catalog, schema, table, org_table, *it, *it, 8,
                      NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  }

  cmg->get_service()->acquire_client_manager_mutex();

  if (task_name) {
    handle_one_client_task(task_name);
  } else {
    map<string, ClientMessageTask *> *tasks =
        cmg->get_service()->get_client_tasks_without_lock();
    map<string, ClientMessageTask *>::iterator it = tasks->begin();
    for (; it != tasks->end(); it++) {
      ClientMessageTask *client_task = it->second;
      if (!server_task_name ||
          client_task->get_server_task_name() == server_task_name)
        handle_one_client_task(it->first.c_str());
    }
  }
  cmg->get_service()->release_client_manager_mutex();
}

/* class MySQLShowServerTaskStatusNode */
MySQLShowServerTaskStatusNode::MySQLShowServerTaskStatusNode(
    ExecutePlan *plan, const char *task_name)
    : MySQLDBScaleShowNode(plan), task_name(task_name) {
  this->name = "MySQLShowServerTaskStatusNode";
}

void MySQLShowServerTaskStatusNode::handle_one_server_task(const char *name) {
  MessageServerManager *cmg = MessageServerManager::instance();
  map<string, ServerMessageTask *> *tasks =
      cmg->get_service()->get_server_tasks_without_lock();
  if (!tasks->count(name)) {
    return;
  }
  ServerMessageTask *c_task = (*tasks)[name];
  vector<string> task_info;
  c_task->get_task_replay_info(&task_info);
#ifdef DEBUG
  ACE_ASSERT(task_info.size() == 8);
#endif
  list<const char *> row_data;
  row_data.push_back(name);
  row_data.push_back(task_info[0].c_str());
  row_data.push_back(task_info[1].c_str());
  row_data.push_back(task_info[2].c_str());
  row_data.push_back(task_info[3].c_str());
  row_data.push_back(task_info[4].c_str());
  row_data.push_back(task_info[5].c_str());
  row_data.push_back(task_info[6].c_str());
  row_data.push_back(task_info[7].c_str());
  add_row_packet(row_data);
}

void MySQLShowServerTaskStatusNode::init_data() {
  MessageServerManager *cmg = MessageServerManager::instance();

  set_head_packet(9);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "dataservers";
  const char *org_table = "dataservers";
  list<const char *> columns;
  columns.push_back("TaskName");
  columns.push_back("TaskStatus");
  columns.push_back("DataSource");
  columns.push_back("SourceStatus");
  columns.push_back("DataServer");
  columns.push_back("ServerStatus");
  columns.push_back("BinlogFile");
  columns.push_back("BinlogPos");
  columns.push_back("BinlogGtid");
  list<const char *>::iterator it = columns.begin();
  for (; it != columns.end(); it++) {
    add_column_packet(catalog, schema, table, org_table, *it, *it, 8,
                      NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  }

  cmg->get_service()->acquire_server_manager_mutex();

  if (task_name) {
    handle_one_server_task(task_name);
  } else {
    map<string, ServerMessageTask *> *tasks =
        cmg->get_service()->get_server_tasks_without_lock();
    map<string, ServerMessageTask *>::iterator it = tasks->begin();
    for (; it != tasks->end(); it++) {
      handle_one_server_task(it->first.c_str());
    }
  }
  cmg->get_service()->release_server_manager_mutex();
}

#endif

/* class MySQLMulModifyNode */
MySQLMulModifyNode::MySQLMulModifyNode(
    ExecutePlan *plan, map<DataSpace *, const char *> &spaces_map,
    map<DataSpace *, int> *sql_count_map, bool need_handle_more_result)
    : MySQLExecuteNode(plan) {
  this->name = "MySQLMulModifyNode";
  this->spaces_map = spaces_map;
  this->sql_count_map.clear();
  if (need_handle_more_result && sql_count_map != NULL)
    this->sql_count_map = *sql_count_map;
  got_error = false;
  packet = NULL;
  error_packet = NULL;
  conn = NULL;
  this->need_handle_more_result = need_handle_more_result;
  update_via_del_ins_has_reverted = false;
  is_ddl_blocked = false;
  stmt_type st_type = plan->statement->get_stmt_node()->type;
  local_disable_parallel_modify = 0;
  if (st_type > STMT_DDL_START && st_type < STMT_DDL_END) {
    local_disable_parallel_modify =
        session->get_session_option("disable_parallel_modify").int_val;
  }
}

void MySQLMulModifyNode::execute() {
#ifndef DBSCALE_TEST_DISABLE
  /*just for test coverage*/
  Backend *bk = Backend::instance();
  dbscale_test_info *test_info = bk->get_dbscale_test_info();
  if (!strcasecmp(test_info->test_case_name.c_str(), "alter_test") &&
      !strcasecmp(test_info->test_case_operation.c_str(), "execute_fail")) {
    status = EXECUTE_STATUS_COMPLETE;
    throw Error("MySQLMulModifyNode start execute fail");
  }
#endif

  if (local_disable_parallel_modify == 1) {
    map<DataSpace *, const char *>::iterator it = spaces_map.begin();
    session->set_skip_mutex_in_operation(true);
    for (; it != spaces_map.end(); ++it) {
      int send_ret = execute_send_on(it->first);
      if (send_ret == 0) {
        if (!packet) packet = Backend::instance()->get_new_packet();
        session->set_mul_node_timeout(false);
        if (execute_recv_on(it->first)) {
          executed_conn.erase(it->first);
        }
      }
    }
    session->set_skip_mutex_in_operation(false);
  } else if (local_disable_parallel_modify > 1) {
    map<DataSpace *, const char *>::iterator it = spaces_map.begin();
    session->set_skip_mutex_in_operation(true);
    for (; it != spaces_map.end(); ++it) {
      execute_send_on(it->first);
      if (!check_ddl_executing(it->first)) {
        LOG_ERROR("Failed to query DDL execution.");
        throw ExecuteNodeError("ddl execution error");
      }
    }
    session->set_skip_mutex_in_operation(false);
    packet = Backend::instance()->get_new_packet();
    map<DataSpace *, Connection *>::iterator it_conn = executed_conn.begin();
    session->set_mul_node_timeout(false);
    for (; it_conn != executed_conn.end();) {
      bool is_need_erase_conn = execute_recv_on(it_conn->first);
      if (is_need_erase_conn) {
        executed_conn.erase(it_conn++);
      } else {
        it_conn++;
      }
    }
  } else {
    if (session->get_session_state() == SESSION_STATE_WORKING) {
      session->set_skip_mutex_in_operation(true);
      map<DataSpace *, const char *>::iterator it = spaces_map.begin();
      for (; it != spaces_map.end(); it++) {
        execute_send_on(it->first);
      }
      /*  Check if we can swap, the function is used in ExecutionNode.  If it
       *  can swap, the state changes from SESSION_STATE_WORKING =>
       *  SESSION_STATE_WAITING_SERVER. Otherwise, session state cahnge from
       *  SESSION_STATE_WORKING => SESSION_STATE_HANDLING_RESULT */
      if (session->is_may_backend_exec_swap_able()) {
        swap_session();
        conn = NULL;
        session->set_mul_connection_num(executed_conn.size());
        return;
      }
      session->set_skip_mutex_in_operation(false);
    }
    packet = Backend::instance()->get_new_packet();
    map<DataSpace *, Connection *>::iterator it_conn = executed_conn.begin();
    session->set_mul_node_timeout(false);
    for (; it_conn != executed_conn.end();) {
      bool is_need_erase_conn = execute_recv_on(it_conn->first);
      if (is_need_erase_conn) {
        executed_conn.erase(it_conn++);
      } else {
        it_conn++;
      }
    }
  }

  if (plan->statement->get_is_update_via_delete_insert_revert()) {
    DataSpace *space =
        plan->statement->get_update_via_delete_insert_space_insert();
    Connection *conn = executed_conn[space];
    if (!update_via_del_ins_revert_ins_stat.first) {
      executed_conn.erase(space);
    }
    if (!update_via_del_ins_revert_del_stat.first ||
        update_via_del_ins_revert_del_stat.second == 0) {
      if (update_via_del_ins_revert_ins_stat.first &&
          update_via_del_ins_revert_ins_stat.second > 0) {
        // if delete fail or delete hit 0 rows, and insert succeed, then insert
        // should be revert
        LOG_DEBUG(
            "the delete part of update statement via delete and insert (with "
            "try revert) is failed or hit 0 rows,"
            " and the insert part succeed, then the insert part should be "
            "reverted\n");
        try {
          conn->execute_one_modify_sql(
              plan->statement->get_update_sql_insert_part_revert().c_str());
          update_via_del_ins_has_reverted = true;
        } catch (...) {
          string msg =
              "got error when try to revert the insert part of update sql, the "
              "insert part is [";
          msg.append(plan->statement->get_update_sql_insert_part_revert());
          msg.append("]");
          throw ExecuteNodeError(msg.c_str());
        }
      }
    }
  }

  session->set_mul_node_timeout(false);
  bool ret = deal_with_create_part_table_def(commit_conn_result.empty());
#ifndef DBSCALE_TEST_DISABLE
  if (test_info->test_case_name.length() &&
      !strcasecmp(test_info->test_case_name.c_str(), "create_part_table_def") &&
      !strcasecmp(test_info->test_case_operation.c_str(),
                  "deal_with_create_part_table_def_fail")) {
    LOG_DEBUG(
        "dbscale_test estimate deal_with_create_part_table_def() fail.\n");
    ret = false;
  }
#endif
  get_tokudb_consistent_point();
  deal_with_errors(ret);
  deal_connections();
  deal_with_packets();
  status = EXECUTE_STATUS_COMPLETE;
#ifdef DEBUG
  LOG_DEBUG("Finish execute MySQLMulModifyNode.\n");
#endif
}

bool MySQLMulModifyNode::check_ddl_executing(DataSpace *space) {
  int check_retry = 0;
  string query_sql =
      "SELECT PROCESSLIST_STATE FROM PERFORMANCE_SCHEMA.THREADS WHERE "
      "PROCESSLIST_INFO=\"";
  string tmp_sql = find_and_replace_quota(spaces_map[space]);
  query_sql.append(tmp_sql.c_str()).append("\"");
exec:
  Connection *conn = NULL;
  TimeValue tv(backend_sql_net_timeout);
  try {
    conn = space->get_connection(session);
    if (conn) {
      while (true) {
        bool find_meta_lock = false;
        vector<string> result;
        conn->query_for_one_column(query_sql.c_str(), 0, &result, &tv, true);
        vector<string>::iterator it = result.begin();
        for (; it != result.end(); ++it) {
          string value = *it;
          if (value.find("metadata lock") != string::npos) {
            find_meta_lock = true;
            break;
          }
        }
        if (!find_meta_lock) {
          break;
        }
        timespec_t t =
            (timespec_t)ACE_Time_Value(0, local_disable_parallel_modify * 1000);
        ACE_OS::nanosleep(&t);
      }
      conn->get_pool()->add_back_to_free(conn);
      conn = NULL;
    }
  } catch (...) {
    if (conn) {
      conn->get_pool()->add_back_to_dead(conn);
    }
    if (check_retry > 2) {
      return false;
    }
    check_retry += 1;
    goto exec;
  }
  return true;
}

int MySQLMulModifyNode::execute_send_on(DataSpace *space) {
  LOG_DEBUG("MySQLMulModifyNode execute_send_on dataspace [%s%@]\n",
            space->get_name(), space);
  Packet exec_packet;
  MySQLQueryRequest query;
  query.set_sql_replace_char(plan->session->get_query_sql_replace_null_char());
  const char *sql = spaces_map[space];
  query.set_query(sql);
  query.pack(&exec_packet);
  int ret = 0;
  bool is_ddl = Backend::instance()->need_deal_with_metadata(
                    plan->statement->get_stmt_node()->type,
                    plan->statement->get_stmt_node()) &&
                run_ddl_retry;
  try {
    int check_working_time = 0;
    while (is_ddl &&
           space->data_source->get_work_state() != DATASOURCE_STATE_WORKING &&
           check_working_time < 60) {
      check_working_time += 1;
      sleep(1);
    }
    if (check_working_time == 60) {
      conn = NULL;
      LOG_ERROR(
          "Fail to get an usable connection, because space is not working\n");
      throw Error(
          "Fail to get an usable connection, because space is not working");
    }
    // We set the read only to false since we execute modify sql
    conn = handler->send_to_server_retry(
        space, &exec_packet, session->get_schema(), false, true, false);
    executed_conn[space] = conn;
    LOG_DEBUG("MySQLMulModifyNode executed_conn add [%@] : [%@]\n", space,
              conn);
    conn = NULL;
  } catch (Exception &e) {
    LOG_ERROR("MySQLMulModifyNode fail send due to exception due to [%s].\n",
              e.what());
    conn_result con_ret;
    con_ret.is_dead_conn = true;
    con_ret.conn = NULL;
    con_ret.space = space;
    con_ret.packet = NULL;
    commit_conn_result.push_back(con_ret);
    if (executed_conn.empty()) {
      status = EXECUTE_STATUS_COMPLETE;
      throw;
    }
    ret = -1;
  }
  return ret;
}
void MySQLMulModifyNode::handle_error_update_via_delete_insert_revert(
    bool &need_erase_conn, DataSpace *space) {
  if (plan->statement->get_is_update_via_delete_insert_revert()) {
    pair<bool, int> p(false, 0);
    if (plan->statement->get_update_via_delete_insert_space_insert() == space) {
      update_via_del_ins_revert_ins_stat = p;
    } else {
      update_via_del_ins_revert_del_stat = p;
      need_erase_conn = true;
    }
  } else {
    need_erase_conn = true;
  }
}

bool MySQLMulModifyNode::execute_recv_on(DataSpace *space, int retry_count) {
  LOG_DEBUG("MySQLMulModifyNode execute_recv_on dataspace [%s%@]\n",
            space->get_name(), space);
  bool need_erase_conn = false;
  conn = executed_conn[space];
  TimeValue tv(backend_sql_net_timeout, 0);
  bool is_ddl = Backend::instance()->need_deal_with_metadata(
                    plan->statement->get_stmt_node()->type,
                    plan->statement->get_stmt_node()) &&
                run_ddl_retry;
  if (is_ddl_blocked && is_ddl) {
    int ret = check_ddl_need_retry(session, space, spaces_map[space]);
    if (ret == 0) {
      const char *err_msg =
          "run ddl timeout, because stmt is blocked by other stmt";
      handler->kill_timeout_connection(conn, packet, true, &tv);
      conn_result con_ret;
      con_ret.is_dead_conn = true;
      con_ret.conn = conn;
      con_ret.space = space;
      con_ret.packet = NULL;
      con_ret.err_message.assign(err_msg);
      commit_conn_result.push_back(con_ret);
      return true;
    }
  }
  try {
    // For begin packet, we only consider two situation, 1 receive OK
    // packet. 2. throw ERROR, we consider all the errors here as dead
    // connection, since it happens few.
    conn->handle_merge_exec_begin_or_xa_start(session);
    try {
      if (need_handle_more_result) {
        /*need_handle_more_result means one of below situation:
         * 1. the shard table create stmt need to receive the database create
         * stmt or use db's ok packet.
         * 2. the update sql is split into 1 delete sql and more than 1 insert
         * sql
         * */
        if (plan->statement->get_stmt_node()->type == STMT_CREATE_TB) {
#ifdef DEBUG
          ACE_ASSERT(!sql_count_map.empty());
#endif
          conn->handle_client_modify_return();
          if (sql_count_map[space] == 3) {
            conn->handle_client_modify_return();
            /*If the sql_count_map[space] == 3 means the "use schema" has been
             * executed, and the used schema must be the session schema. So
             * adjust the conn's schema to session schema.*/
            conn->set_schema(session->get_schema());
          }
        } else if (plan->statement->get_stmt_node()->type == STMT_UPDATE) {
#ifdef DEBUG
          ACE_ASSERT(!sql_count_map.empty());
#endif
          for (int i = 0; i < sql_count_map[space] - 1; ++i) {
            conn->handle_client_modify_return();
          }
        }
      }
    } catch (dbscale::sql::SQLError &e) {
      LOG_DEBUG("MySQLMulModifyNode catch SQLError and struct error packet\n");
      if (!packet) packet = Backend::instance()->get_new_packet();
      MySQLErrorResponse error(e.get_error_code(), e.get_message(),
                               e.get_sqlstate(), 0);
      error.pack(packet);
      handle_error_packet(space, conn);
      handle_error_update_via_delete_insert_revert(need_erase_conn, space);
      packet = Backend::instance()->get_new_packet();
      return need_erase_conn;
    }
    /* The receive function would only receive two kinds of Packets, the
     * first one is error packet and the second one is OK packet. */
    // ddl statement need check whether be locked, so need set timeout
    if (!session->get_mul_node_timeout() && !is_ddl) {
      handler->receive_from_server(conn, packet);
    } else if (is_ddl) {
      conn->set_need_kill_timeout_conn(false);
      tv = TimeValue(ddl_recv_timeout);
      handler->receive_from_server(conn, packet, &tv);
      conn->set_need_kill_timeout_conn(true);
    } else
      handler->receive_from_server(conn, packet, &tv);
    if (driver->is_error_packet(packet)) {
      MySQLErrorResponse error(packet);
      error.unpack();
      if (retry_count && is_ddl &&
          Backend::instance()->need_deal_with_metadata(
              plan->statement->get_stmt_node()->type,
              plan->statement->get_stmt_node()) &&
          Backend::instance()->check_ddl_return_retry_ok(
              plan->statement->get_stmt_node()->type,
              plan->statement->get_stmt_node(), error.get_error_code())) {
        LOG_DEBUG("after retry, ddl statement run success \n");
        delete packet;
        packet = Backend::instance()->get_new_packet();
        MySQLOKResponse ok(0, 0);
        ok.pack(packet);
        handle_ok_packet(space, conn);
      } else {
        handle_error_packet(space, conn);
        handle_error_update_via_delete_insert_revert(need_erase_conn, space);
      }
    } else {
      handle_ok_packet(space, conn);
    }
    packet = Backend::instance()->get_new_packet();
  } catch (exception &e) {
    LOG_ERROR("MySQLMulModifyNode fail receive due to exception due to [%s].\n",
              e.what());
    const char *err_msg = e.what();
    if (retry_count < 5 && is_ddl) {
      retry_count++;
      int ret = check_ddl_need_retry(session, space, spaces_map[space]);
      bool need_retry = true;
      if (ret == 0) {
        err_msg = "run ddl timeout, because stmt is blocked by other stmt";
        is_ddl_blocked = true;
      }
      if (ret <= 0) {
        try {
          if (conn && conn->get_need_kill_timeout_connection())
            handler->kill_timeout_connection(conn, packet, true, &tv);
        } catch (exception &kill_exception) {
          LOG_ERROR("try to kill timeout connection failed \n");
          need_retry = false;
        }
      }
      if (need_retry && (ret == 1 || ret == -1)) {
        if (conn) {
          conn->get_pool()->add_back_to_dead(conn);
        }
        sleep(monitor_interval + 1);
        if (execute_send_on(space) == 0) {
          return execute_recv_on(space, retry_count);
        } else {
          err_msg = "retry send statement to dataspace failed";
          conn = NULL;
        }
      }
    }
    conn_result con_ret;
    con_ret.is_dead_conn = true;
    con_ret.conn = conn;
    con_ret.space = space;
    con_ret.packet = NULL;
    con_ret.err_message.assign(err_msg);
    commit_conn_result.push_back(con_ret);
    handle_error_update_via_delete_insert_revert(need_erase_conn, space);
  }
  return need_erase_conn;
}

void MySQLMulModifyNode::swap_session() {
  map<DataSpace *, Connection *>::iterator it;
  for (it = executed_conn.begin(); it != executed_conn.end(); it++) {
    conn = it->second;
    if (conn) {
      conn->set_session(session);
      struct event *conn_event = conn->get_conn_socket_event();
      evutil_socket_t conn_socket = conn->get_conn_socket();
      SocketEventBase *conn_base = conn->get_socket_base();
#ifdef DEBUG
      ACE_ASSERT(conn_base);
#endif
      session->add_server_base(conn_socket, conn_base, conn_event);
#ifdef DEBUG
      LOG_DEBUG(
          "Finish to prepare the conn socket session swap for conn %@ and "
          "session %@.\n",
          conn, session);
#endif
    }
  }
}

void MySQLMulModifyNode::deal_connections() {
  map<DataSpace *, Connection *>::iterator it_conn2 = executed_conn.begin();
  for (; it_conn2 != executed_conn.end(); it_conn2++) {
    conn = it_conn2->second;
    DataSpace *space = it_conn2->first;
    handler->put_back_connection(space, conn);
  }
}

void MySQLMulModifyNode::get_tokudb_consistent_point() {
  if (support_tokudb) {
    // if sql is START TRANSACTION WITH CONSISTENT SNAPSHOT
    if (plan->statement->get_stmt_node()->type == STMT_START_TRANSACTION &&
        plan->statement->get_stmt_node()
            ->sql->start_tran_oper->with_consistence) {
      if (!commit_conn_result.empty()) {
        // MySQLMulModifyNode has get error, ignore select tokudb table
        return;
      }
      map<DataSpace *, Connection *>::iterator it_conn2 = executed_conn.begin();
      for (; it_conn2 != executed_conn.end(); it_conn2++) {
        Connection *tmp_conn = it_conn2->second;
        DataSpace *space = it_conn2->first;
        try {
          vector<vector<string> > result;
          string select_sql = "SELECT * FROM dbscale.tokudbdummy;";
          tmp_conn->query_for_all_column(select_sql.c_str(), &result);
        } catch (exception &e) {
          LOG_ERROR(
              "MySQLMulModifyNode fail to get_tokudb_consistent_point due to "
              "[%s].\n",
              e.what());
          conn_result con_ret;
          con_ret.is_dead_conn = true;
          con_ret.conn = tmp_conn;
          con_ret.space = space;
          con_ret.packet = NULL;
          con_ret.err_message.assign(e.what());
          commit_conn_result.push_back(con_ret);
          executed_conn.erase(it_conn2);
          // if get exception, no need do select for other Connection
          return;
        }
      }
    }
  }
}

void MySQLMulModifyNode::handle_error_packet(DataSpace *space,
                                             Connection *conn) {
  conn_result con_ret;
  con_ret.conn = conn;
  con_ret.space = space;
  con_ret.is_dead_conn = false;

  MySQLErrorResponse response(packet);
  if (response.is_shutdown()) {
    if (conn) {
      con_ret.is_dead_conn = true;
    }
    session->set_server_shutdown(true);
  }
  if (handle_tokudb_lock_timeout(conn, packet)) {
    if (conn) {
      con_ret.is_dead_conn = true;
    }
  }
  con_ret.packet = packet;
  commit_conn_result.push_back(con_ret);
  conn = NULL;
}
void MySQLMulModifyNode::handle_ok_packet(DataSpace *space, Connection *conn) {
  bool is_non_modified_conn = true;
  if (support_show_warning)
    handle_warnings_OK_and_eof_packet(plan, packet, handler, space, conn);
  if (driver->is_ok_packet(packet)) {
    MySQLOKResponse ok(packet);
    ok.unpack();
    if (ok.get_affected_rows() > 0) {
      is_non_modified_conn = false;
    }
    if (plan->statement->get_is_update_via_delete_insert_revert()) {
      pair<bool, int> p(true, ok.get_affected_rows());
      if (plan->statement->get_update_via_delete_insert_space_insert() ==
          space) {
        update_via_del_ins_revert_ins_stat = p;
      } else {
        update_via_del_ins_revert_del_stat = p;
      }
    }
  }

  if (enable_xa_transaction &&
      session->get_session_option("close_session_xa").int_val == 0 &&
      session->check_for_transaction() &&
      !session->is_in_transaction_consistent()) {
    if (!is_non_modified_conn) session->record_xa_modified_conn(conn);
  }
  record_xa_modify_sql(plan, session, space, spaces_map[space],
                       is_non_modified_conn);
  record_modify_server(plan, session, conn->get_server()->get_name(),
                       space->get_virtual_machine_id(), is_non_modified_conn);
  ok_packets.push_back(packet);
}

void MySQLMulModifyNode::deal_with_errors(
    bool handle_create_part_table_def_ok) {
  string err_message;
  if (!commit_conn_result.empty()) {
    bool has_dead_conn = false;
    error_packet = NULL;
    vector<conn_result>::iterator it = commit_conn_result.begin();
    for (; it != commit_conn_result.end(); it++) {
      if (it->is_dead_conn) {
        has_dead_conn = true;
        if (it->conn) handler->clean_dead_conn(&(it->conn), it->space);
        if (it->packet) delete it->packet;
        if (err_message.empty()) err_message.assign(it->err_message.c_str());
        continue;
      }
      if (it->packet) {
        if (error_packet) {
          LOG_DEBUG(
              "MySQLMulModifyNode error_packet is not null, ignore set error "
              "packet.\n");
          delete it->packet;
        } else {
          LOG_DEBUG(
              "MySQLMulModifyNode error_packet is null, set first packet to "
              "error packet.\n");
          error_packet = it->packet;
        }

        if (it->conn) handler->put_back_connection(it->space, it->conn);
      }
    }
    deal_connections();
    vector<Packet *>::iterator it_packet = ok_packets.begin();
    for (; it_packet != ok_packets.end(); it_packet++) {
      delete *it_packet;
    }
    ok_packets.clear();
    status = EXECUTE_STATUS_COMPLETE;
    if (has_dead_conn) {
      if (err_message.empty())
        throw ExecuteNodeError(
            "Get exception for MySQLMulModifyNode with dead connection.");
      else {
        string err("Execute fail for MulModifyNode, due to '");
        err.append(err_message.c_str());
        err.append("'");
        throw ExecuteNodeError(err.c_str());
      }
    }
    if (error_packet) throw ErrorPacketException();
    throw ExecuteNodeError("Unexpect behavior.");
  }
  if (!handle_create_part_table_def_ok) {
    deal_connections();
    vector<Packet *>::iterator it_packet = ok_packets.begin();
    for (; it_packet != ok_packets.end(); it_packet++) {
      delete *it_packet;
    }
    ok_packets.clear();
    status = EXECUTE_STATUS_COMPLETE;
    throw ExecuteNodeError("got error when create partitioned table");
  }
}

// return true if handle ok, return false if got error
bool MySQLMulModifyNode::deal_with_create_part_table_def(bool all_ok) {
  if (!plan->statement->get_create_part_tb_def()) return true;

  PartitionedTable *part_table = plan->statement->get_create_part_tb_def();

  plan->statement->reset_create_part_tb_def();

#ifndef CLOSE_MULTIPLE
  if (all_ok && multiple_mode) {
    const char *schema_name = part_table->get_schema()->get_name();
    const char *table_name = part_table->get_name();
    try {
      bool is_sync_finish = false;
      bool is_start_sync = false;
      // TODO: start_sync
      MultipleManager *mul = MultipleManager::instance();
      MultipleSyncTool *sync_tool = mul->get_sync_tool();
      char param[1500] = {0};
      const char *part_table_key = part_table->get_key_names()->at(0);
      const char *part_table_scheme =
          part_table->get_partition_scheme()->get_name();
      int raise_alter_table_flag = 1;
      sprintf(param, "%d %s %s %s %s %s %d 0", TYPE_DYNAMIC_ADD_PART_TABLE,
              table_name, schema_name, part_table_key, part_table_scheme,
              "null", raise_alter_table_flag);
      is_start_sync = Backend::instance()->start_dynamic_operation_topic(
          DYNAMIC_ADD_PARTTABLE_TOPIC_NAME, param);
      if (!is_start_sync)
        throw Error(
            "Fail to sync partition table in deal_with_create_part_table_def.");
      DynamicOperationSyncCondition *cond = NULL;
      try {
        /*first store the modified conf to the zookeeper*/
        Backend::instance()->flush_config_to_zoo(false);

        /*Then fin the sync topic*/
        mul->acquire_dynamic_info_lock();
        unsigned long after_pub_version = mul->get_cur_config_info_version();
        mul->release_dynamic_info_lock();
        cond = new DynamicOperationSyncCondition();
        cond->prepare_condition(
            after_pub_version);  // The fin condition should ensure the slave
                                 // dbscale has get the new config version.

        sync_tool->get_sync_topic()->set_sync_info_op_param(param);
        sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param, cond);
        sync_tool->publish_sync_message();
        sync_tool->wait_all_children_sync();
        delete cond;
        cond = NULL;
        is_sync_finish = true;
      } catch (Exception &e) {
        all_ok = false;
        LOG_ERROR("error happen in dynamic add parttable, due to %s.\n",
                  e.what());
        if (cond) {
          delete cond;
          cond = NULL;
        }
      }
      if (!is_sync_finish) {
        all_ok = false;
        LOG_ERROR(
            "Fail to sync the add dataspace operation in multiple dbscale "
            "mode.");
      }
    } catch (...) {
      all_ok = false;
      LOG_ERROR("got error when start dynamic add parttable [%s.%s] topic.\n",
                schema_name, table_name);
    }
  }
#endif

  // here if not all things are ok, we could not rollback previous operation of
  // schema->add_table(), since those info is needed when execute 'drop table'
  if (!all_ok) return false;

  // record dynamic add part_table into config source
  list<string> config_list;
  const char *scheme_name = part_table->get_partition_scheme()->get_name();
  config_list.push_back(
      Driver::get_driver()
          ->get_config_helper()
          ->generate_partition_table_config(part_table, scheme_name,
                                            part_table->get_virtual_map()));
  Driver::get_driver()->get_config_helper()->update_config(config_list);

  return true;
}

void MySQLMulModifyNode::deal_with_packets() {
  Packet *tmp_packet = NULL;
  int affect_rows = 0;
  int warnings = 0;
  int first_msg = 0;
  int second_msg = 0;
  stmt_type type = plan->statement->get_stmt_node()->type;
  handler->deal_with_metadata_execute(type, plan->statement->get_sql(),
                                      session->get_schema(),
                                      plan->statement->get_stmt_node());
  if (!plan->session->is_call_store_procedure() &&
      !plan->statement->is_cross_node_join() &&
      !plan->statement->is_union_table_sub() &&
      !plan->session->get_is_silence_ok_stmt()) {
    while (!ok_packets.empty()) {
      tmp_packet = ok_packets.back();
      ok_packets.pop_back();
      MySQLOKResponse ok(tmp_packet);
      ok.unpack();
      affect_rows += ok.get_affected_rows();
      warnings += ok.get_warnings();
      if (type == STMT_UPDATE) {
        first_msg += session->get_matched_or_change_message("matched:",
                                                            ok.get_message());
        second_msg += session->get_matched_or_change_message("Changed:",
                                                             ok.get_message());
      }
      LOG_DEBUG("After receive the packet, affected_row[%d], warnings[%d].\n",
                affect_rows, warnings);
      if (!packet) {
        packet = tmp_packet;
        LOG_DEBUG("Keep packet %@ for ok packet rebuild.\n", tmp_packet);
      } else {
        LOG_DEBUG("Delete packet %@ after the packet is useless node %@.\n",
                  tmp_packet, this);
        delete tmp_packet;
        tmp_packet = NULL;
      }
    }
    if (!packet) packet = Backend::instance()->get_new_packet();

    if (affect_rows > 1 &&
        plan->statement->get_stmt_with_limit_using_quick_limit()) {
      throw ExecuteNodeError(
          "update_delete_quick_limit enabled, but update/delete limit 1 hit "
          "more than 1 row");
    }
    if (affect_rows > 1 &&
        (plan->statement->get_is_update_via_delete_insert() ||
         plan->statement->get_is_update_via_delete_insert_revert())) {
      LOG_DEBUG(
          "update statement is split into 1 delete and 1(or more) insert stmt, "
          "affect_rows should be decreased 1\n");
      affect_rows = affect_rows - 1;
    }
    if (update_via_del_ins_has_reverted) {
      LOG_DEBUG(
          "the insert part of update statement via delete and insert (with try "
          "revert) has been reverted, affect_rows should be set 0\n");
      affect_rows = 0;
    }
    MySQLOKResponse ok(affect_rows, warnings);
    if (type == STMT_UPDATE) {
      string ok_msg = "Rows matched: ";
      ok_msg.append(to_string(first_msg));
      ok_msg.append("  Changed: ");
      ok_msg.append(to_string(second_msg));
      ok_msg.append("  Warnings: ");
      ok_msg.append(to_string(warnings));
      ok.set_message(ok_msg);
    }
    if (method_to_handle_autocommit_flag ==
            HANDLE_AUTOCOMMIT_FLAG_BEFORE_SEND &&
        !session->get_stmt_auto_commit_int_value()) {
      ok.set_auto_commit_is_off();
    }
    ok.pack(packet);
    handler->deal_autocommit_with_ok_eof_packet(packet);
    handler->record_affected_rows(packet);
    if (plan->session->get_has_more_result()) {
      rebuild_ok_with_has_more_flag(packet, driver);
      LOG_DEBUG(
          "For multiple stmt, the ok packet of middle stmt should be with"
          "flag has_more_result.\n");
    }
    handler->send_to_client(packet);
  } else {
    vector<Packet *>::iterator it = ok_packets.begin();
    for (; it != ok_packets.end(); it++) {
      delete *it;
    }
    ok_packets.clear();
  }
}
void MySQLMulModifyNode::clean() {
  if (packet && packet != error_packet) {
    delete packet;
    packet = NULL;
  }
  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
    packet = NULL;
  }
  vector<Packet *>::iterator it = ok_packets.begin();
  for (; it != ok_packets.end(); it++) {
    delete *it;
  }
  ok_packets.clear();
}

/* class MySQLFetchExplainResultNode */
MySQLFetchExplainResultNode::MySQLFetchExplainResultNode(
    ExecutePlan *plan, list<list<string> > *result,
    bool is_server_explain_stmt_always_extended)
    : MySQLInnerNode(plan),
      result(result),
      is_server_explain_stmt_always_extended(
          is_server_explain_stmt_always_extended) {
  this->name = "MySQLFetchExplainResultNode";
  this->send_packet_profile_id = -1;
  this->wait_child_profile_id = -1;
}

void MySQLFetchExplainResultNode::handle_child(MySQLExecuteNode *ready_child) {
  send_packet_profile_id = session->get_profile_handler()->start_serial_monitor(
      get_executenode_name(), "FetchExplainResultNode get one row", "",
      send_packet_profile_id);
  Packet *row;
  while (!row_map[ready_child]->empty()) {
    row = row_map[ready_child]->front();

    const char *field_data = NULL;
    uint64_t row_len = 0;
    MySQLRowResponse response(row);
    int64_t id_tmp = response.get_int(0);
    char id[20];
    sprintf(id, "%ld", id_tmp);
    list<string> one_row_result;
    one_row_result.push_back(id);
    // MySQL Explain resultset has totally 12 fields if version >= 5.7.3,
    // otherwise, 10.
    unsigned int size = is_server_explain_stmt_always_extended ? 12 : 10;
    for (unsigned int i = 1; i != size; i++) {
      field_data = response.get_str(i, &row_len);
      string value =
          response.field_is_null(i) ? string("") : string(field_data, row_len);
      one_row_result.push_back(value);
    }
    result->push_back(one_row_result);

    row_map[ready_child]->pop_front();
    delete row;
  }
  session->get_profile_handler()->end_execute_monitor(send_packet_profile_id);
}

void MySQLFetchExplainResultNode::handle_children() {
  list<MySQLExecuteNode *>::iterator it;
  for (it = children.begin(); it != children.end(); ++it) {
    if (!row_map[*it]->empty()) handle_child(*it);
  }
}

void MySQLFetchExplainResultNode::execute() {
  ExecuteProfileHandler *execute_profile = session->get_profile_handler();
  unsigned int tmp_id = execute_profile->start_serial_monitor(
      get_executenode_name(), "FetchExplainResultNode all", "");
  while (status != EXECUTE_STATUS_COMPLETE) {
    switch (status) {
      case EXECUTE_STATUS_START:
        init_row_map();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_FETCH_DATA:
        children_execute();
        status = EXECUTE_STATUS_WAIT;
        session->set_session_state(SESSION_STATE_HANDLING_RESULT);

      case EXECUTE_STATUS_WAIT:
        try {
          wait_child_profile_id =
              session->get_profile_handler()->start_serial_monitor(
                  get_executenode_name(), "FetchExplainResultNode wait", "",
                  wait_child_profile_id);
          wait_children();
          session->get_profile_handler()->end_execute_monitor(
              wait_child_profile_id);
        } catch (ExecuteNodeError &e) {
          status = EXECUTE_STATUS_COMPLETE;
          throw;
        }
        break;

      case EXECUTE_STATUS_HANDLE:
        handle_children();
        status = EXECUTE_STATUS_FETCH_DATA;
        break;

      case EXECUTE_STATUS_BEFORE_COMPLETE:
        status = EXECUTE_STATUS_COMPLETE;

      case EXECUTE_STATUS_COMPLETE:
#ifdef DEBUG
        node_end_timing();
#endif
        execute_profile->end_execute_monitor(tmp_id);
#ifdef DEBUG
        LOG_DEBUG("MySQLFetchExplainResultNode %@ cost %d ms\n", this,
                  node_cost_time);
#endif
        break;

      default:
        break;
    }
  }
}

/* class MySQLLoadSelectNode */
MySQLLoadSelectNode::MySQLLoadSelectNode(ExecutePlan *plan,
                                         const char *schema_name,
                                         const char *table_name)
    : MySQLInnerPipeNode(plan),
      schema_name(schema_name),
      table_name(table_name) {
  this->name = "MySQLLoadSelectNode";
  inited_load = false;
  load_conn = NULL;
  packet = NULL;
  data_len = 0;
  get_error = false;
  max_len = DEFAULT_MAX_LOAD_SELECT_LEN;  // 16k
  timeout = TimeValue(UINT_MAX, 0);
  dataspace =
      Backend::instance()->get_data_space_for_table(schema_name, table_name);
  packet_num = 0;
  end_load_times = 0;
  row_buffer = new char[DEFAULT_BUFFER_SIZE];
  data = new char[DEFAULT_LOAD_PACKET_SIZE + 128];
  data_buffer_size = DEFAULT_LOAD_PACKET_SIZE + 128;
}
MySQLLoadSelectNode::~MySQLLoadSelectNode() {
  delete[] row_buffer;
  delete[] data;
}
void MySQLLoadSelectNode::do_clean() {
  if (packet) {
    delete packet;
    packet = NULL;
  }
  if (load_conn && !plan->get_migrate_tool()) {
    if (get_error)
      handler->clean_dead_conn(&load_conn, dataspace);
    else {
      handler->put_back_connection(dataspace, load_conn);
      session->record_xa_modified_conn(load_conn);
    }
  }
}

// make max packet 16k, if mysql max packet less than 16k, then set it
void MySQLLoadSelectNode::set_max_len(unsigned int len) {
  if (len < max_len) {
    max_len = len;
  }
}

void expend_hex_set_column_sql(const char *schema_name, const char *table_name,
                               string &sql, set<int> &hex_pos,
                               Session *session) {
  string real_schema(schema_name);
  size_t pos_name = real_schema.find("_dbscale_");
  string tmp_schema;
  if (pos_name != std::string::npos) {
    tmp_schema.append(real_schema.c_str(), pos_name);
    real_schema.assign(tmp_schema.c_str());
  }
  TableInfoCollection *tic = TableInfoCollection::instance();
  TableInfo *ti = tic->get_table_info_for_read(real_schema.c_str(), table_name);
  vector<TableColumnInfo> *column_info_vec;
  try {
    column_info_vec =
        ti->element_table_column->get_element_vector_table_column_info(session);
  } catch (exception &e) {
    LOG_ERROR(
        "Error occured when try to get table info column "
        "info(vector_table_column_info) of table [%s.%s]\n",
        real_schema.c_str(), table_name);
    ti->release_table_info_lock();
    throw e;
  }
  int pos = 0;
  string column_list;
  string var_set_list;
  vector<TableColumnInfo>::iterator it = column_info_vec->begin();
  for (; it != column_info_vec->end(); it++) {
    if (hex_pos.count(pos)) {
      column_list.append("@`");
      column_list.append(it->column_name);
      column_list.append("`,");
      var_set_list.append("`");
      var_set_list.append(it->column_name);
      var_set_list.append("`=UNHEX(@`");
      var_set_list.append(it->column_name);
      var_set_list.append("`),");
    } else {
      column_list.append("`");
      column_list.append(it->column_name);
      column_list.append("`,");
    }
    pos++;
  }
  ti->release_table_info_lock();
  boost::erase_tail(column_list, 1);
  boost::erase_tail(var_set_list, 1);
  sql.append("(");
  sql.append(column_list.c_str());
  sql.append(") ");
  sql.append("SET ");
  sql.append(var_set_list.c_str());
}

void MySQLLoadSelectNode::generate_load_sql() {
  if (strlen(session->get_session_option("load_insert_select_fields_term")
                 .char_val) > 1 ||
      strlen(session->get_session_option("load_insert_select_lines_term")
                 .char_val) > 1) {
    LOG_ERROR(
        "DBScale does not support FIELDS/LINES TERMINATED BY with multiple "
        "characters "
        "when use load for insert select.\n");
    throw NotImplementedError(
        "Load insert select with FIELDS or LINES TERMINATED BY has multiple "
        "characters");
  }
  string real_schema(schema_name);
  size_t pos_name = real_schema.find("_dbscale_");
  string tmp_schema;
  if (pos_name != std::string::npos) {
    tmp_schema.append(real_schema.c_str(), pos_name);
    real_schema.assign(tmp_schema.c_str());
  }

  string charset;
  Backend::instance()->get_charset_for_table(real_schema.c_str(), table_name,
                                             session, &charset);
  if (charset.empty()) {
    string err("Fail to find a unique charset for load_select from table:");
    err.append(schema_name);
    err.append(".");
    err.append(table_name);
    LOG_ERROR("Fail do MySQLLoadSelectNode::generate_load_sql due to [%s].\n",
              err.c_str());
    throw Error(err.c_str());
  }
  char fields_term =
      session->get_session_option("load_insert_select_fields_term").char_val[0];
  char lines_term =
      session->get_session_option("load_insert_select_lines_term").char_val[0];
  sql = "LOAD DATA LOCAL INFILE '/tmp/dbscale_load_select_file' INTO TABLE ";
  sql.append(schema_name);
  sql.append(".");
  sql.append(table_name);
  sql.append(" CHARSET ");
  sql.append(charset.c_str());
  sql.append(" FIELDS TERMINATED BY '");
  sql.append(1, fields_term);
  sql.append("' ENCLOSED BY '\"' LINES TERMINATED BY '");
  sql.append(1, lines_term);
  sql.append("' ");

  set<int> &hex_pos = plan->statement->get_hex_pos();
  if (!hex_pos.empty()) {
    expend_hex_set_column_sql(schema_name, table_name, sql, hex_pos, session);
  }

  LOG_DEBUG("MySQLLoadSelectNode generate_load_sql [%s]\n", sql.c_str());

  into_outfile.fields_term[0] = fields_term;
  strcpy(into_outfile.fields_enclosed, "\"");
  strcpy(into_outfile.fields_escaped, "\\");
  into_outfile.lines_term[0] = lines_term;
}
void MySQLLoadSelectNode::init_load_select() {
  if (!inited_load) {
    try {
      inited_load = true;
      generate_load_sql();
      init_header_packet();
      packet = Backend::instance()->get_new_packet();
      MySQLQueryRequest query(sql.c_str());
      query.set_sql_replace_char(
          plan->session->get_query_sql_replace_null_char());
      Packet exec_query;
      query.pack(&exec_query);
      // since this may execuate a long time, use UINT_MAX
      // to avoid max_conn_execute_time
      LOG_INFO("MySQLLoadSelectNode start execute with load sql [%s]\n",
               sql.c_str());
      if (plan->get_migrate_tool()) {
        load_conn = plan->get_migrate_tool()->get_migrate_write_conn(NULL);
        if (load_conn) {
          load_conn->set_load(true);
          load_conn->reset();
          handler->send_to_server(load_conn, &exec_query);
        } else {
          LOG_ERROR(
              "MySQLLoadSelectNode doen not find load conn, node exits\n");
          record_migrate_error_message(
              plan, "MySQLLoadSelectNode doen not find load conn, node exits");
          status = EXECUTE_STATUS_COMPLETE;
          throw Error("MySQLLoadSelectNode init_load_select failed");
        }
      } else {
        load_conn = handler->send_to_server_retry(dataspace, &exec_query,
                                                  schema_name, false);
        load_conn->set_load(true);
      }
      handler->receive_from_server(load_conn, packet, &timeout);
      if (driver->is_error_packet(packet)) {
        record_migrate_error_message(plan, packet,
                                     "MySQLLoadSelectNode get error packet.");
        LOG_INFO("MySQLLoadSelectNode get error packet, for sql [%s]\n",
                 sql.c_str());
        status = EXECUTE_STATUS_COMPLETE;
        throw Error(
            "MySQLLoadSelectNode init_load_select failed, get error packet");
      }
    } catch (...) {
      get_error = true;
      LOG_ERROR("MySQLLoadSelectNode init_load_select get exception\n");
      throw;
    }
  }
}

void MySQLLoadSelectNode::handle_child(MySQLExecuteNode *child) {
  Packet *row = NULL;
  char *tmp_data = NULL;
  init_load_select();
  try {
    while (!row_map[child]->empty()) {
      if (ACE_Reactor::instance()->reactor_event_loop_done()) {
        LOG_ERROR(
            "MySQLLoadSelectNode::handle_child failed due to "
            "reactor_event_loop_done\n");
        throw Error(
            "MySQLLoadSelectNode::handle_child failed due to "
            "reactor_event_loop_done");
      }
      row = row_map[child]->front();
      row_map[child]->pop_front();
      if (migrate_load_packet_num && plan->get_migrate_tool()) {
        if (packet_num >= migrate_load_packet_num) {
          if (data_len != 0) {
            // send data packet
            pack_packet(packet, data, data_len);
            handler->send_to_server(load_conn, packet);
            data_len = 0;
          }

          end_load_times++;
          LOG_INFO("migrate [%d] times end load [%d] rows\n", end_load_times,
                   migrate_load_packet_num);
          // send empty to tell server load end
          pack_packet(packet, data, 0);
          handler->send_to_server(load_conn, packet);
          handler->receive_from_server(load_conn, packet, &timeout);
          if (driver->is_error_packet(packet)) {
            record_migrate_error_message(
                plan, packet, "MySQLLoadSelectNode get error packet.");
            LOG_INFO("MySQLLoadSelectNode end get error packet, node exits\n");
            status = EXECUTE_STATUS_COMPLETE;
            throw Error("MySQLLoadSelectNode failed, get error packet");
          }
          if (driver->is_ok_packet(packet)) {
            MySQLOKResponse ok(packet);
            ok.unpack();
            uint16_t warnings = ok.get_warnings();
            if (warnings) {
              LOG_ERROR(
                  "MySQLLoadSelectNode %@ get OK packet with %u warnings.\n",
                  this, (unsigned int)warnings);
              MySQLErrorResponse err(
                  9002,
                  "Migrate MySQLLoadSelectNode get OK packet with warnings.",
                  NULL);
              err.pack(packet);
              record_migrate_error_message(
                  plan, packet,
                  "MySQLLoadSelectNode get OK packet with warnings.");
              status = EXECUTE_STATUS_COMPLETE;
              print_warning_infos(load_conn);
              throw Error(
                  "MySQLLoadSelectNode failed, get ok packet with warnings.");
            }
          }
          load_conn->reset();

          // do load again
          MySQLQueryRequest query(sql.c_str());
          query.set_sql_replace_char(
              plan->session->get_query_sql_replace_null_char());
          Packet exec_query;
          query.pack(&exec_query);
          handler->send_to_server(load_conn, &exec_query);
          handler->receive_from_server(load_conn, packet, &timeout);
          if (driver->is_error_packet(packet)) {
            record_migrate_error_message(
                plan, packet, "MySQLLoadSelectNode get error packet.");
            LOG_INFO("MySQLLoadSelectNode get error packet, node exits\n");
            status = EXECUTE_STATUS_COMPLETE;
            throw Error(
                "MySQLLoadSelectNode init_load_select failed, get error "
                "packet");
          }
#ifndef DBSCALE_TEST_DISABLE
          /*just for test coverage*/
          Backend *bk = Backend::instance();
          dbscale_test_info *test_info = bk->get_dbscale_test_info();
          if (!strcasecmp(test_info->test_case_name.c_str(), "migrate_test") &&
              !strcasecmp(test_info->test_case_operation.c_str(),
                          "load_select_failed")) {
            LOG_INFO("MySQLLoadSelectNode get error packet, node exits\n");
            throw Error(
                "MySQLLoadSelectNode init_load_select failed, get error "
                "packet");
          }
#endif
          packet_num = 0;
        }
      }
      size_t tmp_len;
      tmp_data = handle_one_row(row, tmp_len);
      if (data_len + tmp_len > max_len) {
        if (data_len != 0) {
          pack_packet(packet, data, data_len);
          handler->send_to_server(load_conn, packet);
        }
        if (tmp_len > data_buffer_size) {
          delete[] data;
          data = new char[2 * tmp_len + 128];
          data_buffer_size = 2 * tmp_len + 128;
        }
        // data = tmp_data;
        memcpy(data + 0, tmp_data, tmp_len);
        data_len = tmp_len;
      } else {
        // data += tmp_data;
        memcpy(data + data_len, tmp_data, tmp_len);
        data_len += tmp_len;
      }
      delete row;
      delete[] tmp_data;
      row = NULL;
      tmp_data = NULL;
      packet_num++;
    }
  } catch (exception &e) {
    if (row) delete row;
    if (tmp_data) delete[] tmp_data;
    get_error = true;
    LOG_ERROR(
        "MySQLLoadSelectNode get exception when send data to server [%s].\n",
        e.what());
    string err("MySQLLoadSelectNode get exception when send data to server:");
    err.append(e.what());
    record_migrate_error_message(plan, err.c_str());
    throw;
  }
}

void MySQLLoadSelectNode::pack_packet(Packet *packet, char *data, size_t len) {
  packet->size(len + PACKET_HEADER_SIZE + 1);

  try {
    memcpy(packet->base() + PACKET_HEADER_SIZE, data, len);
  } catch (exception &e) {
    LOG_ERROR(
        "MySQLLoadSelectNode::pack_packet get exception for "
        "MySQLLoadSelectNode::pack_packet [%s].\n",
        e.what());
    string err(
        "MySQLLoadSelectNode::pack_packet get exception for "
        "MySQLLoadSelectNode::pack_packet:");
    err.append(e.what());
    record_migrate_error_message(plan, err.c_str());
    throw;
  }
  pack_header(packet, len);
}

char *MySQLLoadSelectNode::handle_one_row(Packet *packet, size_t &copy_len) {
  char enclosed = into_outfile.fields_enclosed[0];
  char escaped = into_outfile.fields_escaped[0];

  MySQLRowResponse row(packet);
  unsigned int size = is_column_number.size();
  uint64_t packet_len = row.get_row_length();
  uint64_t extra_len_per_column =
      (2 + 1 + 1 + 1) *
      2;  // 2 for escaped 1 for field_term 1 for field_enclose 1 for line term
  char *line_data =
      new char[packet_len * 2 + extra_len_per_column * size + 128];
#ifdef DEBUG
  LOG_DEBUG(
      "MySQLLoadSelectNode::handle_one_row new line_data %@ with len %d.\n",
      line_data, packet_len + extra_len_per_column * size + 128);
#endif
  copy_len = 0;

  for (unsigned int i = 0; i != size; i++) {
    uint64_t row_len;
    const char *row_data = row.get_str(i, &row_len);
    bool is_null = row.field_is_null(i);
    /**
     * If the FIELDS ESCAPED BY is NULL(""), the NULL value will output as
     * "NULL", else the NULL values will output as the ESCAPED char + "N", like
     * the ESCAPED is "\\", output is "\N". But if the FIELDS TERNINATED BY is
     * NULL, it will output its length sapce.
     */
    if (is_null) {
      /*
      line_data.append(1, into_outfile.fields_escaped[0]);
      line_data.append("N");
      */
      memcpy(line_data + copy_len, (const char *)into_outfile.fields_escaped,
             1);
      copy_len++;
      memcpy(line_data + copy_len, "N", 1);
      copy_len++;
      if (i != size - 1) {
        // line_data.append(1, into_outfile.fields_term[0]);
        memcpy(line_data + copy_len, (const char *)into_outfile.fields_term, 1);
        copy_len++;
      }
      continue;
    }

    // line_data.append(1, into_outfile.fields_enclosed[0]);
    memcpy(line_data + copy_len, (const char *)into_outfile.fields_enclosed, 1);
    copy_len++;

    char *escaped_str = (char *)malloc(row_len * 2 + 1);
    const char *c = row_data;
    int index = 0;
    unsigned int row_index = 0;
    if (!is_column_number[i]) {
      // set the field escaped code, and the field is not a number
      while (row_index++ != row_len) {
        if (*c == escaped || *c == enclosed ||
            *c == into_outfile.fields_term[0] ||
            *c == into_outfile.lines_term[0]) {
          *(escaped_str + index) = escaped;
          index++;
        }
        *(escaped_str + index) = *c++;
        index++;
      }
    } else {
      while (row_index++ != row_len) {
        *(escaped_str + index) = *c++;
        index++;
      }
    }
    *(escaped_str + index) = '\0';
    // line_data.append(escaped_str);
    memcpy(line_data + copy_len, (const char *)(escaped_str), index);
    free(escaped_str);
    copy_len += index;

    // line_data.append(1, into_outfile.fields_enclosed[0]);
    memcpy(line_data + copy_len, (const char *)into_outfile.fields_enclosed, 1);
    copy_len++;

    if (i != size - 1) {
      // line_data.append(1, into_outfile.fields_term[0]);
      memcpy(line_data + copy_len, (const char *)into_outfile.fields_term, 1);
      copy_len++;
    }
  }

  // line_data.append(1, into_outfile.lines_term[0]);
  memcpy(line_data + copy_len, into_outfile.lines_term, 1);
  copy_len++;

  return line_data;
}

void MySQLLoadSelectNode::handle_before_complete() {
  LOG_DEBUG("handle_before_complete\n");
  if (!inited_load) {
    // fetchnode get no data for load, just end load select
    if (!plan->get_migrate_tool()) send_ok_packet_to_client(handler, 0, 0);
    return;
  }
  try {
    // send the last data packet
    if (data_len) {
      LOG_DEBUG("handle_before_complete pack last packet\n");
      pack_packet(packet, data, data_len);
      handler->send_to_server(load_conn, packet);
    }
    // send empty to tell server load end
    pack_packet(packet, data, 0);
    handler->send_to_server(load_conn, packet);
    handler->receive_from_server(load_conn, packet, &timeout);
    if (plan->get_migrate_tool()) {
      if (driver->is_error_packet(packet)) {
        record_migrate_error_message(plan, packet,
                                     "MySQLLoadSelectNode get error packet.");
        LOG_ERROR("handle_before_complete get error packet\n");
        MySQLErrorResponse error(packet);
        error.unpack();
        LOG_ERROR(
            "MySQLLoadSelectNode %@ get an error packet %@, %d (%s) %s.\n",
            this, packet, error.get_error_code(), error.get_sqlstate(),
            error.get_error_message());
        throw Error("MySQLLoadSelectNode get error packet before complete\n");
      }
      if (driver->is_ok_packet(packet)) {
        MySQLOKResponse ok(packet);
        ok.unpack();
        uint16_t warnings = ok.get_warnings();
        if (warnings) {
          LOG_ERROR("MySQLLoadSelectNode %@ get OK packet with %u warnings.\n",
                    this, (unsigned int)warnings);
          MySQLErrorResponse err(
              9002, "Migrate MySQLLoadSelectNode get OK packet with warnings.",
              NULL);
          err.pack(packet);
          record_migrate_error_message(
              plan, packet, "MySQLLoadSelectNode get OK packet with warnings.");
          print_warning_infos(load_conn);
          throw Error(
              "MySQLLoadSelectNode failed, get ok packet with warnings.");
        }
      }
    } else {
      handler->deal_autocommit_with_ok_eof_packet(packet);
      handler->send_to_client(packet);
    }
  } catch (exception &e) {
    get_error = true;
    LOG_ERROR("MySQLLoadSelectNode handle_before_complete get exception [%s]\n",
              e.what());
    string err("MySQLLoadSelectNode handle_before_complete get exception:");
    err.append(e.what());
    record_migrate_error_message(plan, err.c_str());
    status = EXECUTE_STATUS_COMPLETE;
    throw Error(err.c_str());
  }
}

void MySQLLoadSelectNode::init_header_packet() {
  list<Packet *> *field_packets = get_field_packets();
  for (std::list<Packet *>::iterator it = field_packets->begin();
       it != field_packets->end(); it++) {
    MySQLColumnResponse column_response(*it);
    column_response.unpack();
    is_column_number.push_back(column_response.is_number());
  }
}

/* class MySQLLoadSelectNode */
MySQLLoadSelectPartitionNode::MySQLLoadSelectPartitionNode(
    ExecutePlan *plan, const char *schema_name, const char *table_name)
    : MySQLLoadSelectNode(plan, schema_name, table_name),
      cond(lock),
      analysis_sync_cond(analysis_sync_lock) {
  this->name = "MySQLLoadSelectPartitionNode";
  partition_num = ((PartitionedTable *)dataspace)->get_real_partition_num();
  analysis_finish_num = 0;
  analysis_num = load_analysis_num;
  last_added_analysis_node_pos = analysis_num - 1;
#ifdef DEBUG
  ACE_ASSERT(analysis_num >= 1);
#endif
}
void MySQLLoadSelectPartitionNode::do_clean() {
  LOG_INFO("Start to clean MySQLLoadSelectPartitionNode.\n");
  status = EXECUTE_STATUS_COMPLETE;
  LOG_DEBUG("Clean up partition nodes.\n");
  unsigned int part_size = partition_nodes.size();
  for (unsigned int i = 0; i < part_size; i++) {
    if (partition_nodes[i]) {
      if (!partition_nodes[i]->is_finished()) {
        partition_nodes[i]->stop_execute();
      }
      partition_nodes[i]->wait();
      partition_nodes[i]->clean();
      delete partition_nodes[i];
      partition_nodes[i] = NULL;
    }
  }

  LOG_DEBUG("Clean up analysis nodes.\n");
  clean_up_analysis_nodes();
  LOG_DEBUG("Clean up ready_packets.\n");
  vector<list<Packet *> *>::iterator it2 = ready_packets.begin();
  for (; it2 != ready_packets.end(); it2++) {
    list<Packet *>::iterator it3 = (*it2)->begin();
    for (; it3 != (*it2)->end(); it3++) {
      delete (*it3);
    }
    delete (*it2);
  }
  ready_packets.clear();
}

Packet *MySQLLoadSelectPartitionNode::get_error_packet() {
  unsigned int num = partition_nodes.size();
  for (unsigned int i = 0; i < num; i++) {
    Packet *result = partition_nodes[i]->get_result();
    if (driver->is_error_packet(result)) {
      return result;
    }
  }
  return MySQLInnerNode::get_error_packet();
}

void MySQLLoadSelectPartitionNode::init_load_select() {
  if (!inited_load) {
    inited_load = true;
    generate_load_sql();
    init_header_packet();
    vector<unsigned int> key_pos;
    ((PartitionedTable *)dataspace)
        ->get_key_pos_vec(schema_name, table_name, key_pos, plan->session);
    PartitionMethod *method =
        ((PartitionedTable *)dataspace)->get_partition_method();

    // Intialize packet buffer for all partitions
    for (unsigned int i = 0; i < partition_num; i++) {
      DataSpace *part_ds = get_partition(i);
      partition_nodes.push_back(
          new MySQLLoadLocalPartitionNode(plan, this, part_ds, sql.c_str()));
      if (partition_nodes[i] == NULL) {
        LOG_ERROR("cannot allocate memory for LOAD DATA partition node\n");
        throw NoMemoryError("allocate memory for LAOD DATA partition node");
      }
      list<Packet *> *tmp_list = new list<Packet *>();
      ready_packets.push_back(tmp_list);
    }
    size_t n = 0;
    LOG_DEBUG(
        "MySQLLoadSelectPartitionNode Init the analysis with num %d partition "
        "node with num %d.\n",
        analysis_num, partition_num);

    // Start processing all partitions in there own threads
    for (unsigned int i = 0; i < partition_num; i++) {
      partition_nodes[i]->activate();
    }
    wait_partition();

    for (; n < analysis_num; n++) {
      analysis_vec.push_back(new MySQLLoadSelectAnalysisNode(
          this, key_pos, method, partition_num, dataspace, driver));
      analysis_vec[n]->start_thread();
    }
  }
}

void MySQLLoadSelectPartitionNode::clean_up_analysis_nodes() {
  vector<MySQLLoadSelectAnalysisNode *>::iterator it = analysis_vec.begin();
  for (; it != analysis_vec.end(); it++) {
    if (*it) {
      (*it)->set_is_stop();
      delete (*it);
    }
  }
  analysis_vec.clear();
}

void MySQLLoadSelectPartitionNode::handle_child(MySQLExecuteNode *child) {
  Packet *row;
  try {
    init_load_select();

    check_all_analysis_node();
    while (!row_map[child]->empty()) {
      row = row_map[child]->front();
      row_map[child]->pop_front();
      assign_row_to_analysis(row);
    }
    check_all_analysis_node();
    flush_ready_packets_to_child();
    wait_partition();
#ifndef DBSCALE_TEST_DISABLE
    Backend *bk = Backend::instance();
    dbscale_test_info *test_info = bk->get_dbscale_test_info();
    if (test_info->test_case_name.length() &&
        !strcasecmp(test_info->test_case_name.c_str(), "load_select") &&
        !strcasecmp(test_info->test_case_operation.c_str(),
                    "load_select_handle_child_fail")) {
      throw Exception("dbscale test fail.");
    }
#endif

  } catch (Exception &e) {
    LOG_ERROR("MySQLLoadSelectPartitionNode::handle_child fail due to %s.\n",
              e.what());
    throw e;
  }
}

void MySQLLoadSelectPartitionNode::assign_row_to_analysis(Packet *packet) {
  last_added_analysis_node_pos++;
  if (last_added_analysis_node_pos == analysis_num)
    last_added_analysis_node_pos = 0;
  analysis_vec[last_added_analysis_node_pos]->add_packet_to_analysis(packet);
  LOG_DEBUG("Assign packet %@ to analysis node %d.\n", packet,
            last_added_analysis_node_pos);
}

void MySQLLoadSelectPartitionNode::wait_analysis_node_finish() {
  LOG_DEBUG("MySQLLoadSelectPartitionNode::wait_analysis_node_finish start.\n");

  status = EXECUTE_STATUS_HANDLE;  // we should first ensure all analysis node
                                   // packet has been send to partition nodes,
                                   // then adjust the status back to
                                   // before_complete, otherwise the execute
                                   // node will be blocked at the wait of
                                   // partition node finish.

  bool has_finish = true;
  vector<MySQLLoadSelectAnalysisNode *>::iterator it;
  it = analysis_vec.begin();
  for (; it != analysis_vec.end(); it++) {
    (*it)->set_table_node_has_get_empty_packet();
  }

  do {
    has_finish = true;
    if (ACE_Reactor::instance()->reactor_event_loop_done()) {
      LOG_INFO("DBScale is exiting, so stop this LoadSelect task.\n");
      throw Error("DBScale is exiting.");
    }
    check_all_analysis_node();
    flush_ready_packets_to_child();
    LOG_DEBUG(
        "MySQLLoadSelectPartitionNode::wait_analysis_node_finish wait "
        "partition.\n");
    wait_partition();

    LOG_DEBUG(
        "Load select node %@ start to check and wait finish analysis node.\n",
        this);
    analysis_sync_lock.acquire();
    if (analysis_finish_num < analysis_num) {
      LOG_DEBUG(
          "Load select node %@ start to wait due to"
          " analysis_finish_num is %d while analysis_num is %d, so wait.\n",
          this, analysis_finish_num, analysis_num);
      analysis_sync_cond.wait();
    }
    analysis_sync_lock.release();

    if (ACE_Reactor::instance()->reactor_event_loop_done()) {
      LOG_INFO("DBScale is exiting, so stop this LoadSelect task.\n");
      throw Error("DBScale is exiting.");
    }

    it = analysis_vec.begin();
    for (; it != analysis_vec.end(); it++) {
      if (!(*it)->has_finish()) {
        has_finish = false;
        break;
      }
    }
    LOG_DEBUG(
        "Load select node %@ start to check finish analysis node with result "
        "%d.\n",
        this, has_finish ? 1 : 0);
    if (has_finish) {
      /*here all analysis node has finish the packet handling*/
      vector<MySQLLoadSelectAnalysisNode *>::iterator it = analysis_vec.begin();
      check_all_analysis_node();
      for (; it != analysis_vec.end(); it++) {
        (*it)->check_exception();
      }
      flush_ready_packets_to_child();
      wait_partition();
    }
  } while (!has_finish);

  status = EXECUTE_STATUS_BEFORE_COMPLETE;
  LOG_DEBUG("MySQLLoadSelectPartitionNode::wait_analysis_node_finish end.\n");
}

void MySQLLoadSelectPartitionNode::check_all_analysis_node() {
  LOG_DEBUG("MySQLLoadSelectPartitionNode::check_all_analysis_node.\n");
  try {
    vector<MySQLLoadSelectAnalysisNode *>::iterator it = analysis_vec.begin();
    for (; it != analysis_vec.end(); it++) {
      (*it)->check_exception();
    }
  } catch (Exception &e) {
    LOG_ERROR(
        "MySQLLoadSelectPartitionNode::check_all_analysis_node fail due to "
        "%s.\n",
        e.what());
    throw e;
  }
}

void MySQLLoadSelectPartitionNode::handle_before_complete() {
  if (!inited_load) {
    // fetchnode get no data for load, just end load select
    send_ok_packet_to_client(handler, 0, 0);
    return;
  }
  try {
#ifndef DBSCALE_TEST_DISABLE
    Backend *bk = Backend::instance();
    dbscale_test_info *test_info = bk->get_dbscale_test_info();
    if (test_info->test_case_name.length() &&
        !strcasecmp(test_info->test_case_name.c_str(), "load_select") &&
        !strcasecmp(test_info->test_case_operation.c_str(),
                    "load_select_handle_before_fail")) {
      throw Exception("dbscale test fail.");
    }
#endif

    wait_analysis_node_finish();
    flush_all();
    wait_partition();
    report_result_to_client();
  } catch (Exception &e) {
    LOG_ERROR("MySQLLoadSelectNode handle_before_complete get exception [%s]\n",
              e.what());
    string err("MySQLLoadSelectNode handle_before_complete get exception:");
    err.append(e.what());
    status = EXECUTE_STATUS_COMPLETE;
    throw Error(err.c_str());
  } catch (...) {
    LOG_ERROR("MySQLLoadSelectNode handle_before_complete get exception\n");
    status = EXECUTE_STATUS_COMPLETE;
    throw Error("MySQLLoadSelectNode handle_before_complete get exception");
  }
}

void MySQLLoadSelectPartitionNode::report_result_to_client() {
  Packet *result = NULL;
  uint64_t affected_rows = 0;
  uint64_t warnings = 0;
  for (unsigned int i = 0; i < partition_num; i++) {
    result = partition_nodes[i]->get_result();
    ASSERT(!driver->is_error_packet(result));
    if (!driver->is_ok_packet(result)) {
      LOG_ERROR("Got unexpected result packet from server\n");
      throw ExecuteNodeError("Got unexpected packet from server");
    }
    MySQLOKResponse ok(result);
    ok.unpack();
    affected_rows += ok.get_affected_rows();
    warnings += ok.get_warnings();
  }
  // Use the last result packet to store the merged result
  MySQLOKResponse ok(result);
  ok.set_affected_rows(affected_rows);
  ok.set_warnings(warnings);
  Packet *ok_packet = Backend::instance()->get_new_packet();
  ok.pack(ok_packet);
  handler->deal_autocommit_with_ok_eof_packet(ok_packet);
  handler->record_affected_rows(ok_packet);
  handler->send_to_client(ok_packet);
  delete ok_packet;
}

void MySQLLoadSelectPartitionNode::wait_partition() {
  ACE_GUARD_REACTION(ACE_Thread_Mutex, guard, lock,
                     LOG_ERROR("%p\n", "acquire LOAD DATA partitions lock");
                     throw HandlerError("acquire LOAD DATA partitions failed"));
  switch (status) {
    case EXECUTE_STATUS_START:
      for (unsigned int i = 0; i < partition_num; i++) {
        while (!partition_nodes[i]->requesting_file()) cond.wait();
      }
      break;
    case EXECUTE_STATUS_BEFORE_COMPLETE:
      for (unsigned int i = 0; i < partition_num; i++) {
        while (!partition_nodes[i]->fetched_result()) cond.wait();
      }
      break;
    default:
      for (unsigned int i = 0; i < partition_num; i++) {
        partition_nodes[i]->check_exception();
      }
      break;
  }
}

void MySQLLoadSelectPartitionNode::flush_all() {
  /*before we send last empty packet to partition_node, we should first ensure
   * that all partition node has already finish the data packet handling,
   * otherwise there may be partial execution of the load, which means some
   * node has data,some not.*/
  LOG_INFO("Start to flush all for MySQLLoadSelectPartitionNode %@.\n", this);
  for (unsigned int i = 0; i < partition_num; i++) {
    partition_nodes[i]->set_table_node_has_get_empty_packet(true);
  }
  wait_children();
  for (unsigned int i = 0; i < partition_num; i++) {
    // prepare the last empty packet
    partition_nodes[i]->wait_ready_to_finish();
    LOG_DEBUG("Partition node %@ done for wait ready to finish.\n",
              partition_nodes[i]);
    wait_children();
  }
  /*Before send the last empty packet, we should re-ensure that all partition
   * load node work without exception.*/
  for (unsigned int i = 0; i < partition_num; i++) {
    partition_nodes[i]->check_exception();
  }

  for (unsigned int i = 0; i < partition_num; i++) {
    Packet *tmp = Backend::instance()->get_new_packet();
    tmp->rewind();
    Packet::pack3int(tmp->base(), 0);
    driver->set_packet_number(tmp, 0);
    tmp->wr_ptr(PACKET_HEADER_SIZE);
    partition_nodes[i]->add_packet_to_ready_list(tmp);
  }
  LOG_INFO("Finish to flush all for MySQLLoadSelectPartitionNode %@.\n", this);
}

void MySQLLoadSelectPartitionNode::flush_ready_packets_to_child() {
  LOG_DEBUG("MySQLLoadSelectPartitionNode::flush_ready_packets_to_child.\n");
  vector<list<Packet *> *> tmp_vec;
  ready_packet_lock.acquire();
  size_t ready_packets_size = ready_packets.size();
  for (size_t i = 0; i < ready_packets_size; i++) {
    tmp_vec.push_back(ready_packets[i]);
    ready_packets[i] = new list<Packet *>();
  }
  ready_packet_lock.release();
  try {
    for (size_t i = 0; i < ready_packets_size; i++) {
      list<Packet *>::iterator it = tmp_vec[i]->begin();
      for (; it != tmp_vec[i]->end(); it++) {
#ifdef DEBUG
        ASSERT(!driver->is_empty_packet(*it));
#endif
        try {
          partition_nodes[i]->add_packet_to_ready_list(*it);
          LOG_DEBUG(
              "MySQLLoadSelectPartitionNode::flush_ready_packets_to_child "
              "flush packet %@ to partition node %d %@.\n",
              *it, i, partition_nodes[i]);
          *it = NULL;
        } catch (...) {
          *it = NULL;
          throw;
        }
      }
      tmp_vec[i]->clear();
      delete tmp_vec[i];
      tmp_vec[i] = NULL;
    }
  } catch (...) {
    for (size_t i = 0; i < ready_packets_size; i++) {
      if (!tmp_vec[i]) {
        continue;
      }
      list<Packet *>::iterator it = tmp_vec[i]->begin();
      for (; it != tmp_vec[i]->end(); it++) {
        if (*it) {
          delete *it;
          *it = NULL;
        }
      }
      tmp_vec[i]->clear();
      delete tmp_vec[i];
      tmp_vec[i] = NULL;
    }
    throw;
  }
}

/* class MySQLLoadSelectAnalysisNode */

MySQLLoadSelectAnalysisNode::~MySQLLoadSelectAnalysisNode() {
  if (thread_status == THREAD_STARTED) {
    LOG_DEBUG("MySQLLoadSelectAnalysisNode %@ cleaning wait.\n", this);
    this->wait_for_cond();
    LOG_DEBUG("MySQLLoadSelectAnalysisNode %@ cleaning wait finish.\n", this);
  } else if (thread_status == THREAD_CREATED) {
    bthread->re_init_backend_thread();
    bthread->get_pool()->add_back_to_free(bthread);
  }

  LOG_DEBUG("MySQLLoadSelectAnalysisNode %@ has stopped.\n", this);

  list<Packet *>::iterator it = waiting_packets.begin();
  for (; it != waiting_packets.end(); it++) {
    delete (*it);
  }
  waiting_packets.clear();
  int i = 0;
  for (; i < partition_num; i++) {
    if (datas[i]) {
      delete[] datas[i];
      datas[i] = NULL;
    }
  }
}

string MySQLLoadSelectAnalysisNode::get_field_replace_null_str(
    string &field, const char *buffer, size_t len) {
  set<char> dbscale_replace_null_char_set_session =
      Backend::instance()->get_dbscale_replace_null_char_set();
  field.clear();
  field.assign(buffer, len);
  size_t null_count = 0;
  string sql_replace_null_char;
  vector<size_t> query_sql_null_pos;
  for (size_t i = 0; i < len; i++) {
    if (buffer[i] == 0) {
      query_sql_null_pos.push_back(i);
      null_count++;
    }
    if (dbscale_replace_null_char_set_session.count(buffer[i]))
      dbscale_replace_null_char_set_session.erase(buffer[i]);
  }
  if (!dbscale_replace_null_char_set_session.empty()) {
    char replace_null_char = *(dbscale_replace_null_char_set_session.begin());
    sql_replace_null_char = string(1, replace_null_char);
    for (size_t i = 0; i < null_count; i++) {
      field[query_sql_null_pos[i]] = replace_null_char;
    }
    LOG_DEBUG(
        "NUL replace char is [%c], query sql with NUL character is modified to "
        "[%s]\n",
        replace_null_char, field.c_str());
  } else {
    // single-char is not enough, try multi-char
    const char *dbscale_replace_null_char_candidate = DBSCALE_REPLACE_NULL_CHAR;
    size_t len_null = strlen(dbscale_replace_null_char_candidate);
    size_t max_candiate_len = 10;
    bool found_candidate = false;
    for (size_t m = 2; m <= max_candiate_len; m++) {
      for (size_t i = 0; i < len_null; i++) {
        string candidate = string(
            m, dbscale_replace_null_char_candidate[i]);  // TODO build
                                                         // more candiates
        if (field.find(candidate) == std::string::npos) {
          found_candidate = true;
          sql_replace_null_char = candidate;
          for (size_t j = 0; j < null_count; j++) {
            field.replace(query_sql_null_pos[j] + j * (m - 1), 1, candidate);
          }
          LOG_DEBUG(
              "NUL replace char is multi-char [%s], query sql with NUL "
              "character is modified to [%s]\n",
              candidate.c_str(), field.c_str());
          break;
        }
      }
      if (found_candidate) break;
    }
    if (!found_candidate) {
      LOG_ERROR("the sql which contains NUL character can not be handled\n");
      throw Error("the sql which contains NUL character can not be handled");
    }
  }
  return sql_replace_null_char;
}

void MySQLLoadSelectAnalysisNode::handle_one_row() {
  Packet *row = get_packet_from_waiting_packets();
  LOG_DEBUG(
      "MySQLLoadPacketAnalysisNode %@ get packet %@ from waiting packets.\n",
      this, row);
  if (!row) {
    LOG_DEBUG(
        "MySQLLoadPacketAnalysisNode %@ has finish all packet handling.\n",
        this);
    return;
  }
  size_t tmp_len = 0;
  char *tmp_data = table_node->handle_one_row(row, tmp_len);

  MySQLRowResponse row_resp(row);
  uint64_t field_len;
  const char *key_string = row_resp.get_str(key_pos[0], &field_len);
  string *key_str = new string(key_string, field_len);
  string sql_replace_char;
  if (field_len > key_str->length()) {
    sql_replace_char =
        get_field_replace_null_str(*key_str, key_string, field_len);
  }
  vector<const char *> key_string_vector;
  key_string_vector.push_back(key_str->c_str());
  unsigned int partition_id =
      method->get_partition_id(&key_string_vector, sql_replace_char);
  unsigned int real_id = ((PartitionedTable *)dataspace)
                             ->get_real_par_id_from_virtual_id(partition_id);
#ifdef DEBUG
  if (data_lens[real_id] + tmp_len > 2048) {
#else
  if (data_lens[real_id] + tmp_len + 2048 > DEFAULT_LOAD_PACKET_SIZE) {
#endif
    if (data_lens[real_id] != 0) {
      flush_rows(real_id);
    }
    if (tmp_len + 2048 > data_buffer_size[real_id]) {
      delete[] datas[real_id];
      datas[real_id] = new char[tmp_len + 2048];
      data_buffer_size[real_id] = tmp_len + 2048;
    }
    // datas[real_id] = tmp_data;
    memcpy(datas[real_id] + 0, tmp_data, tmp_len);
    data_lens[real_id] = tmp_len;
  } else {
    // datas[real_id] += tmp_data;
    memcpy(datas[real_id] + data_lens[real_id], tmp_data, tmp_len);
    data_lens[real_id] += tmp_len;
  }
  delete[] tmp_data;
  delete row;
  delete key_str;
  LOG_DEBUG("After handle row %@ real_id %d with datas len %d.\n", row, real_id,
            data_lens[real_id]);
}

int MySQLLoadSelectAnalysisNode::svc() {
  LOG_INFO("MySQLLoadSelectAnalysisNode %@ start to run.\n", this);
#ifndef DBSCALE_TEST_DISABLE
  int loop_count = 0;
#endif
  try {
    while (1) {
      if (is_stop || ACE_Reactor::instance()->reactor_event_loop_done()) break;

      try {
        handle_one_row();
#ifndef DBSCALE_TEST_DISABLE
        loop_count++;
        if (loop_count == 100) {
          Backend *bk = Backend::instance();
          dbscale_test_info *test_info = bk->get_dbscale_test_info();
          if (test_info->test_case_name.length() &&
              !strcasecmp(test_info->test_case_name.c_str(), "load_select") &&
              !strcasecmp(test_info->test_case_operation.c_str(),
                          "analysis_fail")) {
            throw Exception("dbscale test fail.");
          }
        }
#endif
      } catch (Exception &e) {
        LOG_ERROR("Analysis %@ get Exception %s.\n", this, e.what());
        throw enable_current_exception(e);
      }

      if (is_stop || ACE_Reactor::instance()->reactor_event_loop_done()) {
        break;
      }

      if (table_node_has_get_empty_packet && waiting_size == 0) {
        LOG_INFO("MySQLLoadPacketAnalysisNode %@ has finished.\n", this);
        break;
      }
    }
    flush_all();
  } catch (...) {
    LOG_ERROR(
        "Get exception for MySQLLoadSelectAnalysisNode::svc of node %@.\n",
        this);
    exception = boost::current_exception();
    table_node->wakeup_analysis_sync_cond();
  }
  has_finished = true;
  wakeup_sync_parent_waiting_before_fin();
  table_node->wakeup_analysis_sync_for_fin();
  LOG_INFO("MySQLLoadSelectAnalysisNode %@ finish running.\n", this);
  return FINISHED;
}

void MySQLLoadSelectAnalysisNode::start_thread() {
  // start the new thread
  if (thread_status == THREAD_STOP) {
    Backend *backend = Backend::instance();
    bthread = backend->get_backend_thread_pool()->get_one_from_free();
    if (!bthread) {
      LOG_ERROR(
          "Fail to get thread for "
          "MySQLLoadSelectAnalysisNode::start_thread.\n");
      throw Error(
          "Fail to get a backend thread from pool, so stop execute the sql");
    }
    thread_status = THREAD_CREATED;
    bthread->set_task(this);
    thread_status = THREAD_STARTED;
    bthread->wakeup_handler_thread();
  }
}

void MySQLLoadSelectAnalysisNode::add_packet_to_analysis(Packet *row) {
  acquire_sync_lock();

  /*If the analysis node has finish, the table node should not do the
   * wait_sync_cond*/
  while (waiting_size >= max_analysis_wait_size_local && !has_finished) {
#ifdef DEBUG
    ACE_ASSERT(!analysis_is_waiting);
#endif
    table_node_is_waiting = true;
    LOG_DEBUG("Table node is waiting for add_packet_to_analysis %@.\n", this);
    if (is_stop || ACE_Reactor::instance()->reactor_event_loop_done()) {
      if (analysis_is_waiting) wakeup_sync_cond();
      release_sync_lock();
      break;
    }
    wait_sync_cond();
    if (is_stop || ACE_Reactor::instance()->reactor_event_loop_done()) {
      if (analysis_is_waiting) wakeup_sync_cond();
      release_sync_lock();
      break;
    }
  }
  table_node_is_waiting = false;

  waiting_packets.push_back(row);
  waiting_size++;
  if (analysis_is_waiting) {
    wakeup_sync_cond();
    analysis_is_waiting = false;
  }
  release_sync_lock();
}

Packet *MySQLLoadSelectAnalysisNode::get_packet_from_waiting_packets() {
  acquire_sync_lock();
  if (waiting_size == 0) {
#ifdef DEBUG
    ACE_ASSERT(waiting_packets.empty());
#endif
    if (table_node_has_get_empty_packet) {
      release_sync_lock();
      return NULL;
    }
#ifdef DEBUG
    ACE_ASSERT(!table_node_is_waiting);
#endif
    analysis_is_waiting = true;
    LOG_DEBUG(
        "MySQLLoadSelectAnalysisNode %@ wait for packets with "
        "table_node_has_get_empty_packet %d waiting_size %d.\n",
        this, table_node_has_get_empty_packet ? 1 : 0, waiting_size);
    wait_sync_cond();
    if (waiting_size == 0 && table_node_has_get_empty_packet) {
      release_sync_lock();  // there is no waiting packets and the load select
                            // node
                            // has finish the select part, so there is no more
                            // packet need to be handled.
      return NULL;
    }
    analysis_is_waiting = false;
  }
  if (is_stop || ACE_Reactor::instance()->reactor_event_loop_done()) {
    if (table_node_is_waiting) wakeup_sync_cond();
    release_sync_lock();
    return NULL;
  }
#ifdef DEBUG
  ACE_ASSERT(!waiting_packets.empty());
#endif
  Packet *ret = waiting_packets.front();
  waiting_packets.pop_front();
#ifdef DEBUG
  ACE_ASSERT(waiting_size >= 1);
#endif
  waiting_size--;

  if (table_node_is_waiting &&
      waiting_size < max_analysis_wait_size_local / 2) {
    wakeup_sync_cond();
    table_node_is_waiting = false;
  }

  release_sync_lock();
  return ret;
}

void MySQLLoadSelectAnalysisNode::flush_rows(unsigned int par_id) {
  Packet *packet =
      Backend::instance()->get_new_packet(DEFAULT_LOAD_PACKET_SIZE);

  try {
    table_node->pack_packet(packet, datas[par_id], data_lens[par_id]);
  } catch (...) {
    delete packet;
    packet = NULL;
    throw;
  }
  data_lens[par_id] = 0;
  if (!driver->is_empty_packet(packet)) {
    char *p = packet->base();
    size_t len = packet->length();
    ASSERT(len >= PACKET_HEADER_SIZE);
    Packet::pack3int(p, len - PACKET_HEADER_SIZE);
    LOG_DEBUG(
        "MySQLLoadSelectAnalysisNode %@ flush_rows packet %@ with partition "
        "%d.\n",
        this, packet, par_id);
    table_node->add_to_ready_packets_from_Analysis(par_id, packet);
  } else {
    delete packet;
  }
}

void MySQLLoadSelectAnalysisNode::flush_all() {
  LOG_DEBUG("MySQLLoadSelectAnalysisNode::flush_all start.\n");
  for (int j = 0; j < partition_num; j++) {
    flush_rows(j);
  }
  LOG_DEBUG("MySQLLoadSelectAnalysisNode::flush_all fin.\n");
}

/*class MySQLDBScaleUpdateAuditUserNode*/
MySQLDBScaleUpdateAuditUserNode::MySQLDBScaleUpdateAuditUserNode(
    ExecutePlan *plan, const char *username, bool is_add)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLDBScaleUpdateAuditUserNode";
  this->username = username;
  this->is_add = is_add;
}
void MySQLDBScaleUpdateAuditUserNode::do_execute() {
  if (username.empty()) {
    LOG_ERROR("invalid user name\n");
    throw Error("invalid user name");
  }
  Backend::instance()->update_audit_user_set(username, is_add);
}

void MySQLLoadLocalPartTableNode::eat_all_packet_from_client() {
  Packet packet;
  // If we got error during handle client packet, we have to discard all
  // packets from client before sending error to client.
  packet.rewind();
  do {
    if (load_data_quick_error >
        0) {  // to let user cancel the progress at any time he wants.
      LOG_INFO(
          "during load data is eating the subsequent packet from client, "
          "load_data_quick_error is set to 1, so cancel the eating progress\n");
      break;
    }
    if (session->get_is_killed()) {
      LOG_DEBUG(
          "load data local is killed, so cancel "
          "request_file_and_send_data_to_server\n");
      break;
    }
    handler->receive_from_client(&packet);
  } while (!driver->is_empty_packet(&packet));
}

void MySQLLoadLocalPartTableNode::add_empty_pkt_to_analysis_node() {
  analysis_vec[last_added_analysis_node_pos]->set_is_analysis_last_packet(true);
  has_got_empty_packet = true;
  vector<MySQLLoadPacketAnalysisNode *>::iterator it = analysis_vec.begin();
  for (; it != analysis_vec.end(); it++) {
    Packet *packet = Backend::instance()->get_new_packet();
    packet->rewind();
    Packet::pack3int(packet->base(), 0);
    driver->set_packet_number(packet, 0);
    packet->wr_ptr(PACKET_HEADER_SIZE);
    LOG_DEBUG("Add empty packet to analysis node %@.\n", (*it));
    (*it)->add_packet_to_analysis(packet, true);
  }
}

void MySQLLoadPacketAnalysisNode::flush_tmp_buffer() {
  if (tmp_buffer->wr_ptr() != tmp_buffer->base()) {
    char *pos = tmp_buffer->wr_ptr();
    size_t line_term_len = line_terminate.length();
    if (is_analysis_last_packet) {
      /*The last row of the last data packet may not contain the line
       * terminate char. In multiple analysis node mode, the last row of the
       * last data packet may not the last row be loaded into the table.  If
       * there is another row followed this last row, this row will be
       * losted. So here we ensure that the last row of last data packet is
       * ended with line_terminate.*/
      size_t tmp_buffer_len = tmp_buffer->size();
      if (tmp_buffer_len < line_term_len) {
        tmp_buffer_len = tmp_buffer_len + (line_term_len - tmp_buffer->space());
        tmp_buffer->size(tmp_buffer_len);
        tmp_buffer->packdata(line_terminate.c_str(), line_terminate.length());
      } else if (string(pos - line_term_len, line_term_len) != line_terminate) {
        if (tmp_buffer->space() < line_term_len) {
          tmp_buffer_len =
              tmp_buffer_len + (line_term_len - tmp_buffer->space());
          tmp_buffer->size(tmp_buffer_len);
        }
        tmp_buffer->packdata(line_terminate.c_str(), line_terminate.length());
      }
    }
    vector<int> *escaped_pos_vec = new vector<int>();
    add_column_value(&tmp_st, tmp_buffer->base(), tmp_buffer->wr_ptr(),
                     *escaped_pos_vec, false);
    try {
      handle_row(tmp_buffer->base(), tmp_buffer->wr_ptr());
    } catch (...) {
      delete escaped_pos_vec;
      throw;
    }
    delete escaped_pos_vec;
  }
  LOG_DEBUG("Flush rows for last empty packet.\n");
  flush_rows();
}

void MySQLLoadPacketAnalysisNode::save_to_previous_buff(
    const char *end, const char *line_start, const char *field_start,
    unsigned int fields_count, bool need_enclose, bool enclose_field_end,
    bool is_char_escaped, vector<int> *escaped_pos_vec) {
  // line not terminated, copy to tmp buffer
  size_t len = end - line_start;
  tmp_buffer->size(len);
  tmp_buffer->rewind();
  tmp_buffer->packdata(line_start, len);
  tmp_buffer->rd_ptr(field_start - line_start);
  previous_packet_fields_count = fields_count;
  previous_packet_need_enclose = need_enclose;
  previous_packet_enclose_field_end = enclose_field_end;
  previous_packet_unbroken_field_found = unbroken_field_found;
  previous_packet_is_char_escaped = is_char_escaped;
  previous_packet_just_found_field_before_end = just_found_field_before_end;
  previous_packet_escaped_pos_vec->clear();
  if (!escaped_pos_vec->empty()) {
    *previous_packet_escaped_pos_vec = *escaped_pos_vec;
  }
}

// check if tail of tmp_buffer contains a prefix of line-terminate-string or
// field-terminate-string
void MySQLLoadPacketAnalysisNode::
    check_tmp_buffer_tail_contain_line_term_prefix(
        handle_load_local_params &param, Packet *packet, size_t line_len) {
  const char *pos = param.pos;
  const char *packet_data_start_pos = param.packet_data_start_pos;
  int line_term_len = param.line_term_len;
  int field_term_len = param.field_term_len;
  string::size_type sub_str_pos_line_term = param.sub_str_pos_line_term;
  string::size_type sub_str_pos_field_term = param.sub_str_pos_field_term;
  const char *field_end_pos = param.field_end_pos;
  bool enclose_field_end_spec = param.enclose_field_end_spec;
  bool need_handle_line_term = param.need_handle_line_term;
  bool need_handle_field_term = param.need_handle_field_term;
  bool is_first_part_enough = param.is_first_part_enough;

  char tmp_str[line_term_len * 2 - 1];
  size_t tmp_len = 0;
  if ((size_t)(line_term_len - 1) > line_len) {
    tmp_len = line_len;
    is_first_part_enough = false;
  } else {
    tmp_len = (size_t)(line_term_len - 1);
    is_first_part_enough = true;
  }
  memcpy(tmp_str, tmp_buffer->wr_ptr() - tmp_len, tmp_len);
  size_t tmp_len2 =
      packet->length() - PACKET_HEADER_SIZE > (size_t)(line_term_len - 1)
          ? line_term_len - 1
          : packet->length() - PACKET_HEADER_SIZE;
  memcpy(tmp_str + tmp_len, pos, tmp_len2);
  tmp_str[tmp_len + tmp_len2] = '\0';
  int i = 0;
  while (!(*(tmp_str + i))) {
    i++;
    if (i == int(tmp_len + tmp_len2)) break;
  }
  if (i == int(tmp_len + tmp_len2) ||
      (tmp_len + tmp_len2 - (size_t)i) < size_t(line_term_len)) {
    sub_str_pos_line_term = string::npos;
  } else {
    string tmp_string(tmp_str + i, tmp_len + tmp_len2 - (size_t)i);
    sub_str_pos_line_term = tmp_string.find(line_terminate);
    if (sub_str_pos_line_term != string::npos)
      sub_str_pos_line_term = sub_str_pos_line_term + i;
  }

  if (sub_str_pos_line_term != string::npos) {
    if (is_first_part_enough)
      field_end_pos = tmp_buffer->wr_ptr() -
                      (line_term_len - 1 - sub_str_pos_line_term) - 1;
    else
      field_end_pos = tmp_buffer->base() + sub_str_pos_line_term - 1;
    if (*field_end_pos != field_escape) {
      if (!previous_packet_need_enclose ||
          (previous_packet_need_enclose && (*field_end_pos == field_enclose) &&
           (*(field_end_pos - 1) != field_escape))) {
        need_handle_line_term = true;
        enclose_field_end_spec = previous_packet_need_enclose;
      } else {
        need_handle_line_term = false;
      }
    }
  }

  param.pos = pos;
  param.packet_data_start_pos = packet_data_start_pos;
  param.line_term_len = line_term_len;
  param.field_term_len = field_term_len;
  param.sub_str_pos_line_term = sub_str_pos_line_term;
  param.sub_str_pos_field_term = sub_str_pos_field_term;
  param.field_end_pos = field_end_pos;
  param.enclose_field_end_spec = enclose_field_end_spec;
  param.need_handle_line_term = need_handle_line_term;
  param.need_handle_field_term = need_handle_field_term;
  param.is_first_part_enough = is_first_part_enough;
}

// no split line-terminate field found, check field-terminate string
void MySQLLoadPacketAnalysisNode::
    check_tmp_buffer_tail_contain_field_term_prefix(
        handle_load_local_params &param, Packet *packet, size_t line_len) {
  const char *pos = param.pos;
  const char *packet_data_start_pos = param.packet_data_start_pos;
  int line_term_len = param.line_term_len;
  int field_term_len = param.field_term_len;
  string::size_type sub_str_pos_line_term = param.sub_str_pos_line_term;
  string::size_type sub_str_pos_field_term = param.sub_str_pos_field_term;
  const char *field_end_pos = param.field_end_pos;
  bool enclose_field_end_spec = param.enclose_field_end_spec;
  bool need_handle_line_term = param.need_handle_line_term;
  bool need_handle_field_term = param.need_handle_field_term;
  bool is_first_part_enough = param.is_first_part_enough;

  char tmp_str[field_term_len * 2 - 1];
  size_t tmp_len = 0;
  if ((size_t)(field_term_len - 1) > line_len) {
    tmp_len = line_len;
    is_first_part_enough = false;
  } else {
    tmp_len = (size_t)(field_term_len - 1);
    is_first_part_enough = true;
  }
  memcpy(tmp_str, tmp_buffer->wr_ptr() - tmp_len, tmp_len);
  size_t tmp_len3 =
      packet->length() - PACKET_HEADER_SIZE > (size_t)(field_term_len - 1)
          ? field_term_len - 1
          : packet->length() - PACKET_HEADER_SIZE;
  memcpy(tmp_str + tmp_len, pos, tmp_len3);
  tmp_str[tmp_len + tmp_len3] = '\0';
  int i = 0;
  while (!(*(tmp_str + i))) {
    i++;
    if (i == int(tmp_len + tmp_len3)) break;
  }
  if (i == int(tmp_len + tmp_len3) ||
      (tmp_len + tmp_len3 - (size_t)i) < size_t(field_term_len)) {
    sub_str_pos_field_term = string::npos;
  } else {
    string tmp_string(tmp_str + i, tmp_len + tmp_len3 - (size_t)i);
    sub_str_pos_field_term = tmp_string.find(field_terminate);
    if (sub_str_pos_field_term != string::npos)
      sub_str_pos_field_term = sub_str_pos_field_term + i;
  }
  if (sub_str_pos_field_term != string::npos) {
    if (is_first_part_enough)
      field_end_pos = tmp_buffer->wr_ptr() -
                      (field_term_len - 1 - sub_str_pos_field_term) -
                      1;  // field_end_pos is the last char of field
    else
      field_end_pos = tmp_buffer->base() + sub_str_pos_field_term - 1;
    if (*field_end_pos != field_escape) {
      if (!previous_packet_need_enclose ||
          (previous_packet_need_enclose && (*field_end_pos == field_enclose) &&
           (*(field_end_pos - 1) != field_escape))) {
        need_handle_field_term = true;
        enclose_field_end_spec = previous_packet_need_enclose;
      } else {
        need_handle_field_term = false;
      }
    }
  }

  param.pos = pos;
  param.packet_data_start_pos = packet_data_start_pos;
  param.line_term_len = line_term_len;
  param.field_term_len = field_term_len;
  param.sub_str_pos_line_term = sub_str_pos_line_term;
  param.sub_str_pos_field_term = sub_str_pos_field_term;
  param.field_end_pos = field_end_pos;
  param.enclose_field_end_spec = enclose_field_end_spec;
  param.need_handle_line_term = need_handle_line_term;
  param.need_handle_field_term = need_handle_field_term;
  param.is_first_part_enough = is_first_part_enough;
}

void MySQLLoadPacketAnalysisNode::add_column_value_shell_1(
    stmt_node *st, const char *start, const char *end,
    vector<int> &escaped_pos_vec, bool need_enclose) {
  add_column_value(st, start, end, escaped_pos_vec, need_enclose);
}
void MySQLLoadPacketAnalysisNode::add_column_value_shell_2(
    stmt_node *st, const char *start, const char *end,
    vector<int> &escaped_pos_vec, bool need_enclose) {
  add_column_value(st, start, end, escaped_pos_vec, need_enclose);
}
void MySQLLoadPacketAnalysisNode::handle_line_term_first_part_enough(
    int line_len, string::size_type sub_str_pos_line_term, const char *pos) {
  tmp_buffer->size(line_len + sub_str_pos_line_term +
                   1);  // sub_str_pos_line_term + 1 = line_term_len -
                        // (line_term_len - (sub_str_pos_line_term + 1))
  memcpy(tmp_buffer->wr_ptr(), pos, sub_str_pos_line_term + 1);
  tmp_buffer->wr_ptr(sub_str_pos_line_term + 1);
  tmp_buffer->rd_ptr(tmp_buffer->wr_ptr() - tmp_buffer->rd_ptr());
  handle_row(tmp_buffer->base(), tmp_buffer->wr_ptr());
}

void MySQLLoadPacketAnalysisNode::handle_line_term_first_part_not_enough(
    int line_len, int line_term_len, string::size_type sub_str_pos_line_term,
    const char *pos) {
  tmp_buffer->size(line_term_len + sub_str_pos_line_term);
  memcpy(tmp_buffer->wr_ptr(), pos,
         line_term_len + sub_str_pos_line_term - line_len);
  tmp_buffer->wr_ptr(line_term_len + sub_str_pos_line_term - line_len);
  tmp_buffer->rd_ptr(tmp_buffer->wr_ptr() - tmp_buffer->rd_ptr());
  handle_row(tmp_buffer->base(), tmp_buffer->wr_ptr());
}

void MySQLLoadPacketAnalysisNode::handle_field_term_first_part_enough(
    int line_len, string::size_type sub_str_pos_field_term, const char *pos) {
  tmp_buffer->size(line_len + sub_str_pos_field_term +
                   1);  // sub_str_pos_field_term + 1 = field_term_len -
                        // (field_term_len - (sub_str_pos_field_term + 1))
  memcpy(tmp_buffer->wr_ptr(), pos, sub_str_pos_field_term + 1);
  tmp_buffer->wr_ptr(sub_str_pos_field_term + 1);
  tmp_buffer->rd_ptr(tmp_buffer->wr_ptr() - tmp_buffer->rd_ptr());
}

void MySQLLoadPacketAnalysisNode::handle_field_term_first_part_not_enough(
    int line_len, int field_term_len, string::size_type sub_str_pos_field_term,
    const char *pos) {
  tmp_buffer->size(field_term_len + sub_str_pos_field_term);
  memcpy(tmp_buffer->wr_ptr(), pos,
         field_term_len + sub_str_pos_field_term - line_len);
  tmp_buffer->wr_ptr(field_term_len + sub_str_pos_field_term - line_len);
  tmp_buffer->rd_ptr(tmp_buffer->wr_ptr() - tmp_buffer->rd_ptr());
}

void MySQLLoadPacketAnalysisNode::handle_previous_line_char_one_by_one(
    handle_char_one_by_one_params &param, vector<int> *escaped_pos_vec) {
  const char *start = param.start;
  const char *pos = param.pos;
  const char *end = param.end;
  const char *packet_data_start_pos = param.packet_data_start_pos;
  bool is_char_escaped = param.is_char_escaped;
  bool need_enclose = param.need_enclose;
  bool enclose_field_end = param.enclose_field_end;
  unsigned int fields_count = param.fields_count;
  size_t line_len = param.line_len;
  int line_term_len = param.line_term_len;
  int field_term_len = param.field_term_len;

  while (pos != end) {
    unbroken_field_found = false;
    just_found_field_before_end = false;
    if (is_char_escaped) {
      is_char_escaped = false;
      pos++;
      continue;
    }

    if (*pos == line_terminate[0]) {
      if (line_term_len == 1 ||
          (end - pos >= line_term_len &&
           line_terminate == string(pos, line_term_len))) {
        fields_count++;
        break;
      }
    }

    if (*pos == field_escape) {
      is_char_escaped = true;
      if (start == packet_data_start_pos)
        escaped_pos_vec->push_back(
            int(pos - start + tmp_buffer->wr_ptr() - tmp_buffer->rd_ptr()));
      else
        escaped_pos_vec->push_back(int(pos - start));
      pos++;
      continue;
    }

    if (need_enclose && !enclose_field_end && *pos == field_enclose &&
        (pos != start || (pos == packet_data_start_pos &&
                          !previous_packet_unbroken_field_found))) {
      enclose_field_end = true;
      pos++;
      continue;
    }

    if (enclose_field_end && *pos == field_enclose) {
      LOG_DEBUG(
          "program hit mysql_plan.cc: 'enclose_field_end && *pos == "
          "field_enclose'\n");
      if (start == packet_data_start_pos)
        escaped_pos_vec->push_back(
            int(pos - 1 - start + tmp_buffer->wr_ptr() - tmp_buffer->rd_ptr()));
      else
        escaped_pos_vec->push_back(int(pos - 1 - start));
    } else if (*pos == field_terminate[0]) {
      if (need_enclose && !enclose_field_end) {
        pos++;
        continue;
      }
      if (field_term_len != 1 &&
          (end - pos < field_term_len ||
           field_terminate != string(pos, field_term_len))) {
        pos++;
        continue;
      }
      fields_count++;
      pos += field_term_len - 1;
      size_t len = pos - start + 1;
      line_len += len;
      tmp_buffer->size(line_len);
      memcpy(tmp_buffer->wr_ptr(), start, len);
      tmp_buffer->wr_ptr(len);
      add_column_value(&tmp_st, tmp_buffer->rd_ptr(),
                       tmp_buffer->wr_ptr() - field_term_len, *escaped_pos_vec,
                       enclose_field_end);
      // new field start here
      tmp_buffer->rd_ptr(tmp_buffer->wr_ptr());
      start = pos + 1;
      if (start == end) {
        just_found_field_before_end = true;
      } else if (has_field_enclose && *start == field_enclose) {
        need_enclose = true;
        enclose_field_end = false;
      } else {
        need_enclose = false;
      }
      unbroken_field_found = true;
    }
    pos++;
    enclose_field_end = false;
  }
  param.start = start;
  param.pos = pos;
  param.end = end;
  param.packet_data_start_pos = packet_data_start_pos;
  param.is_char_escaped = is_char_escaped;
  param.need_enclose = need_enclose;
  param.enclose_field_end = enclose_field_end;
  param.fields_count = fields_count;
  param.line_len = line_len;
}

void MySQLLoadPacketAnalysisNode::save_current_line_status(
    handle_char_one_by_one_params &param, vector<int> *escaped_pos_vec) {
  const char *start = param.start;
  const char *pos = param.pos;
  bool is_char_escaped = param.is_char_escaped;
  bool need_enclose = param.need_enclose;
  bool enclose_field_end = param.enclose_field_end;
  unsigned int fields_count = param.fields_count;
  size_t line_len = param.line_len;

  // line not terminated, continue with the next packet
  size_t len = pos - start;
  line_len += len;
  tmp_buffer->size(line_len);
  memcpy(tmp_buffer->wr_ptr(), start, len);
  tmp_buffer->wr_ptr(len);
  previous_packet_fields_count = fields_count;
  previous_packet_need_enclose = need_enclose;
  previous_packet_enclose_field_end = enclose_field_end;
  previous_packet_unbroken_field_found = unbroken_field_found;
  previous_packet_just_found_field_before_end = just_found_field_before_end;
  previous_packet_is_char_escaped = is_char_escaped;
  previous_packet_escaped_pos_vec->clear();
  if (!escaped_pos_vec->empty()) {
    *previous_packet_escaped_pos_vec = *escaped_pos_vec;
  }
}

void MySQLLoadPacketAnalysisNode::
    concatenate_one_row_from_previous_pkt_and_cur_pkt_to_handle(
        handle_char_one_by_one_params &param, vector<int> *escaped_pos_vec) {
  const char *start = param.start;
  const char *pos = param.pos;
  unsigned int fields_count = param.fields_count;
  size_t line_len = param.line_len;
  bool enclose_field_end = param.enclose_field_end;
  int line_term_len = param.line_term_len;

  if ((!sql_specify_column_list && auto_inc_at_last_field && fields_count > 0 &&
       fields_count == table_fields_num - 1) ||
      column_list_has_append_auto_inc) {
    need_append_auto_inc_column = true;
  }
  unsigned int expected_field_count = 0;
  unsigned int actually_got_field_count = 0;
  if (sql_specify_column_list) {
    expected_field_count = table_node->get_specified_column_count();
    actually_got_field_count = fields_count;
  } else {
    expected_field_count = table_fields_num;
    actually_got_field_count =
        fields_count + (need_append_auto_inc_column ? 1 : 0);
  }
  if ((actually_got_field_count != expected_field_count) &&
      load_data_strict_mode_this_stmt) {
    string line_content = string(tmp_buffer->base(), line_len);
    char err_msg[256];
    sprintf(err_msg,
            "load_data_strict_mode is on, but file column count does not match"
            " sql or target table, expected [%d], actually got [%d]",
            expected_field_count, actually_got_field_count);
    LOG_INFO("%s\n", err_msg);
    LOG_INFO(
        "file column count does not match sql or target table, load table name "
        "[%s.%s], load file [%s]\n",
        table_node->get_schema_name(), table_node->get_table_name(),
        plan->statement->get_stmt_node()->sql->load_oper->filename);
    string related_line_info = "related line is : [" + line_content + "]";
    LOG_INFO("%s\n", related_line_info.c_str());
    string err_info = err_msg;
    err_info.append(". ");
    err_info.append(related_line_info);
    throw Error(err_info.c_str());
  }
  pos += line_term_len;
  size_t len = pos - start;
  line_len += len;
  tmp_buffer->size(line_len);
  memcpy(tmp_buffer->wr_ptr(), start, len);
  tmp_buffer->wr_ptr(len);
  add_column_value(&tmp_st, tmp_buffer->rd_ptr(),
                   tmp_buffer->wr_ptr() - line_term_len, *escaped_pos_vec,
                   enclose_field_end);
  tmp_buffer->rd_ptr(tmp_buffer->wr_ptr());
  handle_row(tmp_buffer->base(), tmp_buffer->wr_ptr());

  param.pos = pos;
}

void MySQLLoadPacketAnalysisNode::handle_char_in_packet_one_by_one(
    handle_char_one_by_one_params &param, vector<int> *escaped_pos_vec) {
  const char *start = param.start;
  const char *pos = param.pos;
  const char *end = param.end;
  bool is_char_escaped = param.is_char_escaped;
  bool need_enclose = param.need_enclose;
  bool enclose_field_end = param.enclose_field_end;
  unsigned int fields_count = param.fields_count;
  size_t line_len = param.line_len;
  const char *field_start = param.field_start;
  const char *line_start = param.line_start;
  while (pos != end) {
    unbroken_field_found = false;
    just_found_field_before_end = false;
    if (is_char_escaped) {
      is_char_escaped = false;
      pos++;
      continue;
    }
    if (*pos == field_escape) {
      is_char_escaped = true;
      escaped_pos_vec->push_back(int(pos - field_start));
      pos++;
      continue;
    }

    if (need_enclose && !enclose_field_end && *pos == field_enclose &&
        pos != field_start) {
      enclose_field_end = true;
      // this maybe not means a field has been found, when the followed char
      // from here is not field_terminated_char
      pos++;
      continue;
    }

    // enclose_field_end=true means we just found a enclosed-char at the
    // previous pos
    if (enclose_field_end && *pos == field_enclose) {
      LOG_DEBUG(
          "program hit mysql_plan.cc: 'enclose_field_end && *pos == "
          "field_enclose'\n");
      // mysql behavior: if one enclosed-char followed by another enclosed-char,
      // one of them is discarded, just like semantic of escaped-by
      escaped_pos_vec->push_back(int(pos - 1 - field_start));
    } else if (*pos == field_terminate[0]) {
      if (need_enclose && !enclose_field_end) {
        pos++;
        enclose_field_end = false;
        continue;
      }
      int len = field_terminate.length();
      if (len == 1) {
        fields_count++;
      } else if (end - pos < len) {
        pos++;
        continue;
      } else if (field_terminate != string(pos, len)) {
        pos++;
        continue;
      } else {
        fields_count++;
      }
      add_column_value_shell_1(&tmp_st, field_start, pos, *escaped_pos_vec,
                               enclose_field_end);
      pos += len - 1;  // here we need keep this assignment, because 'pos' is
                       // used for next loop's check
      field_start = pos + 1;
      if (field_start == end) {
        // here we just found a field and 'field_start' now point at 'end' pos,
        // then '*field_start' is meaningless so we add below special flag to
        // mark such situation, it will be used when next packet come
        just_found_field_before_end = true;
      } else if (has_field_enclose && *field_start == field_enclose) {
        need_enclose = true;
        enclose_field_end = false;
      } else {
        need_enclose = false;
      }
      unbroken_field_found = true;
    } else if (*pos == line_terminate[0]) {
      int len = line_terminate.length();
      if (len != 1 && (end - pos < len || line_terminate != string(pos, len))) {
        pos++;
        continue;
      }
      fields_count++;
      if ((!sql_specify_column_list && auto_inc_at_last_field &&
           fields_count > 0 && fields_count == table_fields_num - 1) ||
          column_list_has_append_auto_inc) {
        need_append_auto_inc_column = true;
      }
      unsigned int expected_field_count = 0;
      unsigned int actually_got_field_count = 0;
      if (sql_specify_column_list) {
        expected_field_count = table_node->get_specified_column_count();
        actually_got_field_count = fields_count;
      } else {
        expected_field_count = table_fields_num;
        actually_got_field_count =
            fields_count + (need_append_auto_inc_column ? 1 : 0);
      }
      if ((actually_got_field_count != expected_field_count) &&
          load_data_strict_mode_this_stmt) {
        string line_content = string(line_start, pos - line_start + len);
        char err_msg[256];
        sprintf(err_msg,
                "load_data_strict_mode=1, but file column count does not match"
                " sql or target table, expected [%d], actually got [%d]",
                expected_field_count, actually_got_field_count);
        LOG_INFO("%s\n", err_msg);
        LOG_INFO(
            "file column count does not match sql or target table, load table "
            "name [%s.%s], load file [%s]\n",
            table_node->get_schema_name(), table_node->get_table_name(),
            plan->statement->get_stmt_node()->sql->load_oper->filename);
        string related_line_info = "related line is : [" + line_content + "]";
        LOG_INFO("%s\n", related_line_info.c_str());
        string err_info = err_msg;
        err_info.append(". ");
        err_info.append(related_line_info);
        throw Error(err_info.c_str());
      }
      fields_count = 0;
      add_column_value_shell_2(&tmp_st, field_start, pos, *escaped_pos_vec,
                               enclose_field_end);

      pos += len - 1;
      handle_row(line_start, pos + 1);
      field_start = pos + 1;
      line_start = pos + 1;
      if (has_field_enclose && *field_start == field_enclose) {
        need_enclose = true;
        enclose_field_end = false;
      } else {
        need_enclose = false;
      }
      unbroken_field_found = true;
    }
    pos++;
    enclose_field_end = false;
  }
  param.start = start;
  param.pos = pos;
  param.end = end;
  param.is_char_escaped = is_char_escaped;
  param.need_enclose = need_enclose;
  param.enclose_field_end = enclose_field_end;
  param.fields_count = fields_count;
  param.line_len = line_len;
  param.field_start = field_start;
  param.line_start = line_start;
}

int64_t MySQLLoadPacketAnalysisNode::get_auto_inc_value(const char *line_start,
                                                        const char *line_end) {
  int64_t auto_inc_val = 0;
  if (auto_inc_value_local_remain_count > 0) {
    auto_inc_value_local += auto_inc_step;
    auto_inc_val = auto_inc_value_local;
    auto_inc_value_local_remain_count--;
#ifdef DEBUG
    LOG_DEBUG(
        "MySQLLoadPacketAnalysisNode[%@] get auto_inc value [%Q] internally.\n",
        this, auto_inc_val);
#endif
  } else {
    ACE_Thread_Mutex *row_lock = NULL;
    try {
      row_lock = part_space->get_row_autoinc_lock(full_table_name);
      if (row_lock)
        row_lock->acquire();
      else
        table_node->acquire_analysis_lock();
      auto_inc_value_local = part_space->get_auto_inc_value(
          handler, plan->statement, schema_name, table_name,
          auto_inc_value_local_capacity, auto_inc_step);
      if (row_lock)
        row_lock->release();
      else
        table_node->release_analysis_lock();
      auto_inc_val = auto_inc_value_local;
      auto_inc_value_local_remain_count = auto_inc_value_local_capacity - 1;
    } catch (Exception &e) {
      if (row_lock)
        row_lock->release();
      else
        table_node->release_analysis_lock();
      size_t line_len = line_end - line_start;
      line_len = line_len + 1 > DEFAULT_BUFFER_SIZE ? DEFAULT_BUFFER_SIZE - 1
                                                    : line_len;
      char *line_tmp = line_buffer;
      memcpy(line_tmp, line_start, line_len);
      line_tmp[line_len] = '\0';
      LOG_ERROR("Error occured when LOAD DATA, at line [%s] with error %s\n",
                line_tmp, e.what());
      string tmp("Got unsupported row data for LOAD DATA, due to ");
      tmp += e.what();
      throw ExecuteNodeError(tmp.c_str());
    }
  }
  return auto_inc_val;
}

int64_t MySQLLoadPacketAnalysisNode::check_auto_inc_value(
    const char *line_start, const char *line_end) {
  int64_t auto_inc_val = 0;
  expr_list_item *auto_inc_expr_item = column_values.expr_list_head;
  int i = 0;
  for (; i < auto_inc_key_pos; i++) {
    auto_inc_expr_item = auto_inc_expr_item->next;
  }
  try {
    table_node->acquire_analysis_lock();
    auto_inc_val = stmt->get_auto_increment_value(
        part_space, plan, schema_name, table_name, auto_inc_expr_item->expr, 1,
        true);
    table_node->release_analysis_lock();
  } catch (Exception &e) {
    size_t line_len = line_end - line_start;
    line_len =
        line_len + 1 > DEFAULT_BUFFER_SIZE ? DEFAULT_BUFFER_SIZE - 1 : line_len;
    char *line_tmp = line_buffer;
    memcpy(line_tmp, line_start, line_len);
    line_tmp[line_len] = '\0';
    LOG_ERROR("Error occured when LOAD DATA, at line [%s] with error %s\n",
              line_tmp, e.what());
    table_node->release_analysis_lock();
    string tmp("Got unsupported row data for LOAD DATA, due to ");
    tmp += e.what();
    throw ExecuteNodeError(tmp.c_str());
  }
  return auto_inc_val;
}

/*class MySQLDBScaleShowCreateOracleSeqPlan*/
MySQLDBScaleShowCreateOracleSeqPlan::MySQLDBScaleShowCreateOracleSeqPlan(
    ExecutePlan *plan, const char *seq_schema, const char *seq_name)
    : MySQLDBScaleShowNode(plan) {
  this->seq_schema = seq_schema ? seq_schema : session->get_schema();
  this->seq_name = seq_name;
}

void MySQLDBScaleShowCreateOracleSeqPlan::init_data() {
  Backend *backend = Backend::instance();
  OracleSequence seq;
  if (!backend->retrive_oracle_seq_meta_data(seq_schema.c_str(),
                                             seq_name.c_str(), seq)) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "sequence not exists",
                     "42S02");
    throw ErrorPacketException();
  }
  set_head_packet(1);
  add_column_packet("def", "information_schema", "Sequence", "Sequence",
                    "Sequence_Definition", "Sequence_Definition", 8,
                    NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);

  list<string> row;
  const char *seq_def_format_str =
      "CREATE SEQUENCE %s.%s START WITH %lu INCREMENT BY %lu "
      "%s %s%s%s";
  boost::format seq_def_format = boost::format(seq_def_format_str) %
                                 seq_schema % seq_name % seq.currval % seq.step;
  if (seq.cycle == 0)
    seq_def_format % "NOCYCLE";
  else
    seq_def_format % "CYCLE";
  if (seq.cache == 0) {
    seq_def_format % "NOCACHE";
  } else {
    if (seq.cache_random_offset != 0) {
      seq_def_format % (boost::format("CACHE %lu RANDOM %lu") % seq.cache %
                        seq.cache_random_offset)
                           .str();
    } else {
      seq_def_format % (boost::format("CACHE %lu") % seq.cache).str();
    }
  }
  if (seq.min != 0)
    seq_def_format % (boost::format(" MINVALUE %lu") % seq.min);
  else
    seq_def_format % "";
  if (seq.max != 9223372036850000000)
    seq_def_format % (boost::format(" MAXVALUE %lu") % seq.max);
  else
    seq_def_format % "";
  row.push_back(seq_def_format.str());
  add_row_packet(row);
}
/*MySQLDBScaleShowSeqStatusPlan*/
MySQLDBScaleShowSeqStatusPlan::MySQLDBScaleShowSeqStatusPlan(
    ExecutePlan *plan, const char *seq_schema, const char *seq_name)
    : MySQLDBScaleShowNode(plan) {
  this->seq_schema = seq_schema ? seq_schema : session->get_schema();
  this->seq_name = seq_name;
}

void get_trx_block_info_rows(MySQLDriver *driver, list<Packet *> &row_list) {
  driver->acquire_session_mutex();
  set<Session *, compare_session> *session_set = driver->get_session_set();
  set<Session *, compare_session>::iterator it;

  list<const char *> rows;
  for (it = session_set->begin(); it != session_set->end(); ++it) {
    rows.clear();
    string extra_info = "";
    string query_sql;
    char thread_id[20];
    char sql_sum[10];
    char query_time[20];
    stringstream trx_sqls;
    stringstream kept_conn_list_str;

    (*it)->get_query_sql(query_sql);

#ifndef DBSCALE_TEST_DISABLE
    dbscale_test_info *test_info = Backend::instance()->get_dbscale_test_info();
    if (!strcasecmp(test_info->test_case_name.c_str(),
                    "mul_transaction_block_info") &&
        !strcasecmp(test_info->test_case_operation.c_str(),
                    "show_transaction_block_info")) {
      if (!strcasecmp(query_sql.c_str(),
                      "dbscale show transaction block info") ||
          !strcasecmp(query_sql.c_str(),
                      "dbscale show transaction local block info"))
        continue;
    }
#endif

    if ((*it)->is_in_transaction() || (*it)->get_query_time() > 0) {
      snprintf(thread_id, sizeof(thread_id), "%d", (*it)->get_thread_id());
      rows.push_back(thread_id);
      rows.push_back((*it)->get_schema());
      get_user_work_state((*it), rows, extra_info);
      rows.push_back(query_sql.c_str());
      snprintf(query_time, sizeof(query_time), "%lu", (*it)->get_query_time());
      rows.push_back(query_time);
      rows.push_back(extra_info.c_str());
      (*it)->get_conn_list_str(kept_conn_list_str);
      string kept_conn_tmp_str = kept_conn_list_str.str();
      if (kept_conn_tmp_str.size() == 0)
        rows.push_back("");
      else
        rows.push_back(kept_conn_tmp_str.c_str());
      list<string> *transaction_sqls = (*it)->get_transaction_sqls();
      unsigned int sqls_size = transaction_sqls->size();
      snprintf(sql_sum, sizeof(sql_sum), "%u", sqls_size);
      rows.push_back(sql_sum);
      list<string>::iterator it_l = transaction_sqls->begin();
      for (; it_l != transaction_sqls->end(); ++it_l) {
        trx_sqls << it_l->c_str() << ";";
      }
      string trx_sqls_tmp_str = trx_sqls.str();
      if (trx_sqls_tmp_str.size() == 0)
        rows.push_back("");
      else
        rows.push_back(trx_sqls_tmp_str.c_str());
      // session_id is same to user id
      rows.push_back(thread_id);
#ifndef CLOSE_MULTIPLE
      char cluster_id_str[20];
      int cluster_id = Backend::instance()->get_cluster_id();
      snprintf(cluster_id_str, sizeof(cluster_id_str), "%d", cluster_id);
      if (multiple_mode) {
        rows.push_back(cluster_id_str);
      }
#endif
      Packet *packet = Backend::instance()->get_new_packet();
      MySQLRowResponse row(rows);
      row.pack(packet);
      row_list.push_back(packet);
    }
  }
  driver->release_session_mutex();
}

void MySQLDBScaleShowTrxBlockInfoNode::init_data() {
  try {
    uint64_t head_num = 10;
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) {
      head_num = head_num + 1;
    }
#endif
    set_head_packet(head_num);
    const char *catalog = "def";
    const char *schema = "information_schema";
    const char *table = "datasources";
    const char *org_table = "datasources";
    list<const char *> columns;
    columns.push_back("User Id");
    columns.push_back("Cur Schema");
    columns.push_back("Working State");
    columns.push_back("Executing SQL");
    columns.push_back("Executing time(ms)");
    columns.push_back("Extra Info");
    columns.push_back("Kept Conn List");
    columns.push_back("Sqls Count");
    columns.push_back("Sqls");
    columns.push_back("Session Id");
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) {
      columns.push_back("Cluster Id");
    }
#endif
    while (!columns.empty()) {
      add_column_packet(catalog, schema, table, org_table, columns.front(),
                        columns.front(), 8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING,
                        0, 0, 0);
      columns.pop_front();
    }
#ifndef CLOSE_MULTIPLE
    if (multiple_mode && !is_local) {
      const char *sql = "dbscale show transaction local block info";
      MultipleManager *mul_manager = MultipleManager::instance();
      mul_manager->get_cluster_packet(sql, row_list, NULL, 0, true);
      get_trx_block_info_rows(driver, row_list);
    } else {
#endif
      get_trx_block_info_rows(driver, row_list);
#ifndef CLOSE_MULTIPLE
    }
#endif
  } catch (exception &e) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "Connection get error!",
                     "42S02");
    throw ErrorPacketException();
  }
}

void MySQLDBScaleShowSeqStatusPlan::init_data() {
  Backend *backend = Backend::instance();
  OracleSequence seq;
  char min[20] = {0};
  char max[20] = {0};
  char version[20] = {0};
  if (!backend->retrive_oracle_seq_meta_data(seq_schema.c_str(),
                                             seq_name.c_str(), seq)) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, "sequence not exists",
                     "42S02");
    throw ErrorPacketException();
  }
  set_head_packet(9);
  add_column_packet("def", "information_schema", "Sequence", "Sequence",
                    "Sequence_Schema", "Sequence_Schema", 8, NAME_LEN + 1,
                    MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  add_column_packet("def", "information_schema", "Sequence", "Sequence",
                    "Sequence_Name", "Sequence_Name", 8, NAME_LEN + 1,
                    MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  add_column_packet("def", "information_schema", "Sequence", "Sequence",
                    "Current_Value", "Current_Value", 8, NAME_LEN + 1,
                    MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  add_column_packet("def", "information_schema", "Sequence", "Sequence",
                    "Cached_Count", "Cached_Count", 8, NAME_LEN + 1,
                    MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  add_column_packet("def", "information_schema", "Sequence", "Sequence",
                    "Remain_Count", "Remain_Count", 8, NAME_LEN + 1,
                    MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  add_column_packet("def", "information_schema", "Sequence", "Sequence",
                    "Min_Value", "Min_Value", 8, NAME_LEN + 1,
                    MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  add_column_packet("def", "information_schema", "Sequence", "Sequence",
                    "Max_Value", "Max_Value", 8, NAME_LEN + 1,
                    MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  add_column_packet("def", "information_schema", "Sequence", "Sequence",
                    "Has_Retrieved_Next_Val", "Has_Retrieved_Next_Val", 8,
                    NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  add_column_packet("def", "information_schema", "Sequence", "Sequence",
                    "Version", "Version", 8, NAME_LEN + 1,
                    MYSQL_TYPE_VAR_STRING, 0, 0, 0);

  sprintf(min, "%ld", seq.min);
  sprintf(max, "%ld", seq.max);
  sprintf(version, "%d", seq.version);

  list<string> rows;
  rows.push_back(seq.schemaname);
  rows.push_back(seq.seqname);
  rows.push_back(boost::to_string(seq.currval));
  rows.push_back(boost::to_string(seq.get_generated_random_cache()));
  rows.push_back(boost::to_string(seq.remain_count));
  rows.push_back(min);
  rows.push_back(max);
  if (seq.has_retrieved_next_val)
    rows.push_back("True");
  else
    rows.push_back("False");
  rows.push_back(version);
  add_row_packet(rows);
}

/*MySQLDBScaleShowFetchNodeBufferNode*/
void MySQLDBScaleShowFetchNodeBufferNode::init_data() {
  try {
    set_head_packet(4);
    const char *schema_name = "information_schema";
    const char *catalog = "def";
    const char *table_name = "Fetch_Node_Buffer";

    list<const char *> columns;
    columns.push_back("Session");
    columns.push_back("Used");
    columns.push_back("Left");
    columns.push_back("Total");
    list<const char *>::iterator it = columns.begin();
    for (; it != columns.end(); it++) {
      add_column_packet(catalog, schema_name, table_name, table_name, *it, *it,
                        8, NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
    }
    Driver *driver = Driver::get_driver();
    driver->acquire_handler_mutex();
    set<Handler *> *handler_set = driver->get_handler_set();
    set<Handler *>::iterator it_h = handler_set->begin();
    unsigned long long session_total_used = 0;
    unsigned long long session_total_left = 0;
    unsigned long long session_total_alloc = 0;
    for (; it_h != handler_set->end(); ++it_h) {
      list<string> row_data;
      Session *session = (*it_h)->get_session();
      if (!session) continue;
      if (!session->get_thread_id()) continue;
      row_data.push_back(boost::to_string(session->get_thread_id()));
      row_data.push_back(boost::to_string(session->get_max_alloc() -
                                          session->get_left_alloc()));
      session_total_used +=
          session->get_max_alloc() - session->get_left_alloc();
      row_data.push_back(boost::to_string(session->get_left_alloc()));
      session_total_left += session->get_left_alloc();
      row_data.push_back(boost::to_string(session->get_max_alloc()));
      session_total_alloc += session->get_max_alloc();
      add_row_packet(row_data);
    }
    driver->release_handler_mutex();
    list<string> row_data;
    row_data.push_back("Summary");
    row_data.push_back(boost::to_string(session_total_used));
    row_data.push_back(boost::to_string(session_total_left));
    row_data.push_back(boost::to_string(session_total_alloc));
    add_row_packet(row_data);
  } catch (exception &e) {
    LOG_ERROR("got exception when execute DBSCALE SHOW FETCHNODE BUFFER\n");
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, e.what());
    throw ErrorPacketException();
  }
}

/* class MySQLDBScaleShowTrxBlockInfoNode*/
MySQLDBScaleShowTrxBlockInfoNode::MySQLDBScaleShowTrxBlockInfoNode(
    ExecutePlan *plan, bool is_local)
    : MySQLDBScaleShowNode(plan) {
  this->name = "MySQLDBScaleShowTrxBlockInfoNode";
  this->is_local = is_local;
}

/* class MySQLDBScaleInternalSetNode*/
MySQLDBScaleInternalSetNode::MySQLDBScaleInternalSetNode(ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {
  this->name = "MySQLDBScaleInternalSetNode";
}

void MySQLDBScaleInternalSetNode::do_execute() {
  LOG_DEBUG("MySQLDBScaleInternalSetNode::do_execute.\n");
  internal_set_op_node *set_oper =
      plan->statement->get_stmt_node()->sql->dbscale_internal_set_oper;
  string err_msg;
  if (!strcasecmp(set_oper->internal_set_name,
                  "should_exclude_from_working_servers")) {
    const char *server_name = set_oper->internal_option_name;
    if (!server_name || !set_oper->int_val) {
      err_msg.append(
          "should_exclude_from_working_servers must be with dataserver and "
          "value");
      LOG_ERROR("MySQLDBScaleInternalSetNode:%s\n", err_msg.c_str());
      throw Error(err_msg.c_str());
    }
    int tmp = atoi(set_oper->int_val);
    bool exclude = ((tmp == 0) ? false : true);
    DataServer *data_server =
        Backend::instance()->find_data_server(server_name);
    if (data_server == NULL) {
      LOG_ERROR("MySQLDBScaleInternalSetNode cannot find dataserver %s.\n",
                server_name);
      err_msg.append("Can not find dataserver: ").append(server_name);
      throw Error(err_msg.c_str());
    }
    data_server->set_should_exclude_from_working_servers(exclude);
  } else if (!strcasecmp(set_oper->internal_set_name, "is_in_failover")) {
    const char *source_name = set_oper->internal_option_name;
    if (!source_name || !set_oper->int_val) {
      err_msg.append(
          "is_in_failover must be with datasource and "
          "value");
      LOG_ERROR("MySQLDBScaleInternalSetNode:%s\n", err_msg.c_str());
      throw Error(err_msg.c_str());
    }
    int tmp = atoi(set_oper->int_val);
    bool failover = ((tmp == 0) ? false : true);
    DataSource *data_source =
        Backend::instance()->find_data_source(source_name);
    if (data_source == NULL) {
      LOG_ERROR("MySQLDBScaleInternalSetNode cannot find datasource %s.\n",
                source_name);
      err_msg.append("Can not find datasource: ").append(source_name);
      throw Error(err_msg.c_str());
    }
    data_source->set_is_in_failover(failover);
  } else {
    err_msg.append("Unknown internal set case name ")
        .append(set_oper->internal_set_name);
    LOG_ERROR("MySQLDBScaleInternalSetNode:%s\n", err_msg.c_str());
    throw Error(err_msg.c_str());
  }
}

MySQLRestoreRecycleTablePrecheckNode::MySQLRestoreRecycleTablePrecheckNode(
    ExecutePlan *plan, const char *from_schema, const char *from_table,
    const char *to_table, const char *recycle_type, DataSpace *dspace)
    : MySQLExecuteNode(plan),
      from_schema(from_schema),
      from_table(from_table),
      to_table(to_table),
      recycle_type(recycle_type),
      table_dataspace(dspace),
      error_packet(NULL) {
  LOG_DEBUG("get a restore recycle table precheck node: [%s.%s]\n", from_schema,
            to_table);
}

void MySQLRestoreRecycleTablePrecheckNode::execute() {
  string count_str;
  Connection *conn = NULL;
  try {
    if (recycle_type == RECYCLE_TYPE_TRUNCATE) {  // RECYCLE_TYPE_TRUNCATE
      string select_sql = "SELECT 1 FROM `%s`.`";
      select_sql.append(to_table.c_str()).append("` LIMIT 1");
      if (table_dataspace->is_partitioned()) {  // partitioned table
        PartitionedTable *pt = (PartitionedTable *)table_dataspace;
        for (unsigned int id = 0; id < pt->get_real_partition_num(); id++) {
          Partition *par = pt->get_partition(id);
          string new_schema;
          adjust_shard_schema(from_schema.c_str(), NULL, new_schema,
                              par->get_virtual_machine_id(), id);
          conn = par->get_connection(session, new_schema.c_str());
          if (!conn) throw Error("can not get connection");
          boost::format formatted_sql = boost::format(select_sql) % new_schema;
          count_str.clear();
          conn->query_for_one_value(formatted_sql.str().c_str(), count_str, 0);
          conn->get_pool()->add_back_to_free(conn);
          conn = NULL;
          if (count_str == "1") {
            throw Error("Truncated table is not empty now, can't do restore");
          }
        }
      } else {  // normal table
        conn = table_dataspace->get_connection(session, from_schema.c_str());
        if (!conn) throw Error("can not get connection");
        boost::format formatted_sql =
            boost::format(select_sql) % from_schema.c_str();
        count_str.clear();
        conn->query_for_one_value(formatted_sql.str().c_str(), count_str, 0);
        conn->get_pool()->add_back_to_free(conn);
        conn = NULL;
        if (count_str == "1") {
          throw Error("Truncated table is not empty now, can't do restore");
        }
      }
    } else {  // RECYCLE_TYPE_DROP
      boost::format check_table_sql =
          boost::format(
              "SELECT 1 FROM information_schema.tables WHERE table_schema = "
              "'%s' and table_name = '%s'") %
          from_schema.c_str() % to_table.c_str();
      DataSpace *auth_dspace = Backend::instance()->get_auth_data_space();
      conn = auth_dspace->get_connection(session, "information_schema");
      count_str.clear();
      conn->query_for_one_value(check_table_sql.str().c_str(), count_str, 0);
      conn->get_pool()->add_back_to_free(conn);
      conn = NULL;
      if (count_str == "1") {
        throw Error("table is existed, can't do restore");
      }
    }
  } catch (...) {
    if (conn) conn->get_pool()->add_back_to_dead(conn);
    throw;
  }
  status = EXECUTE_STATUS_COMPLETE;
}

int MySQLCheckMetaDataNode::check_field_type_exist(
    vector<string> &field_type_vec, string &field_type,
    vector<vector<string> > &one_column,
    vector<vector<string> > &remote_one_column) {
  for (const string &field : field_type_vec) {
    bool is_okA = false;
    bool is_okB = false;
    string tmp = "";
    for (auto &vec : one_column) {
      for (auto &str : vec) {
        if (field == str) {
          is_okA = true;
          tmp = field;
          break;
        }
      }
      if (is_okA) break;
    }
    for (auto &vec : remote_one_column) {
      for (auto &str : vec) {
        if (field == str) {
          is_okB = true;
          break;
        }
      }
      if (is_okB) break;
    }
    if (is_okB && is_okA) {
      field_type = tmp;
      return 0;
    } else if (is_okB || is_okA) {
      return -1;
    }
  }
  return 1;
}

/* class MySQLCheckMetaDataNode */
MySQLCheckMetaDataNode::MySQLCheckMetaDataNode(ExecutePlan *plan)
    : MySQLReturnOKNode(plan), error_packet(NULL) {
  remote_conn = NULL;
  remote_server = NULL;
  conn = NULL;
}
void MySQLCheckMetaDataNode::do_execute() {
  pre_options();
  bool is_error = false;
  string miss_msg = "table miss row[";
  string unequal_msg = "table unequal value row[";
  string error_message = "please check table : ";
  vector<string> field_type_vec;
  vector<string> check_metadata_base;
  check_metadata_base.push_back("dbscale");
  field_type_vec.push_back("cluster_id");
  field_type_vec.push_back("dbscale_cluster_id");
  if (enable_oracle_sequence) {
    check_metadata_base.push_back("oracle_sequence_db");
  }
  try {
    for (auto &database_name : check_metadata_base) {
      vector<vector<string> > table_vec;
      string check_table = "show tables from ";
      check_table = check_table + database_name.c_str() + ";";
      auto query_all_column = [&](Connection *&conn, string &query,
                                  vector<vector<string> > *rows) {
        try {
          conn->query_for_all_column(query.c_str(), rows);
        } catch (...) {
          if (conn) conn->get_pool()->add_back_to_dead(conn);
          conn = NULL;
          throw;
        }
      };
      query_all_column(conn, check_table, &table_vec);
      for (uint32_t i = 0; i < table_vec.size(); i++) {
        string tb_name = table_vec[i][0];
        one_column.clear();
        one_column.shrink_to_fit();
        remote_one_column.clear();
        remote_one_column.shrink_to_fit();
        string check_cluster_id_row_sql =
            "describe " + database_name + "." + tb_name + ";";
        query_all_column(conn, check_cluster_id_row_sql, &one_column);
        query_all_column(remote_conn, check_cluster_id_row_sql,
                         &remote_one_column);
        pre_check_table_structure_field_type(database_name, tb_name, one_column,
                                             remote_one_column);
        vector<vector<string> > data_vec;
        vector<vector<string> > remote_data_vec;
        vector<string> primary_key_vec;
        string field_type;
        int ret = check_field_type_exist(field_type_vec, field_type, one_column,
                                         remote_one_column);
        auto find_primary_key = [&primary_key_vec, this]() -> int {
          for (auto &vec : one_column) {
            if (vec.size() > 3 && vec[3] == "PRI") {
              primary_key_vec.emplace_back(vec[0]);
            }
          }
          return 0;
        };

        find_primary_key();
        const int EXIST_FIELD = 0;
        const int NOT_EXIST_FIELD = 1;
        string select_sql = "select * from " + database_name + ".";
        select_sql += tb_name;
        auto assemble_order_by_select_sql = [&select_sql,
                                             &primary_key_vec]() -> void {
          select_sql += " order by ";
          if (primary_key_vec.empty()) {
            select_sql += "1 asc;";
            return;
          }
          for (int i = 0; i < (int)primary_key_vec.size() - 1; i++) {
            select_sql += primary_key_vec[i];
            select_sql += " asc,";
          }
          select_sql += primary_key_vec.back();
          select_sql += " asc;";
          return;
        };
        if (ret == NOT_EXIST_FIELD) {
          assemble_order_by_select_sql();
          query_all_column(conn, select_sql, &data_vec);
          query_all_column(remote_conn, select_sql, &remote_data_vec);
        } else if (ret == EXIST_FIELD) {
          select_sql += " where ";
          select_sql += field_type + " = ";
          if (field_type == "dbscale_cluster_id")
            select_sql += to_string(dbscale_cluster_id);
          if (field_type == "cluster_id") {
            if (multiple_mode) {
              vector<string> v_online =
                  MultipleManager::instance()->get_online_child_vec();
              if (v_online.size() > 0) {
                for (unsigned int i = 0; i < v_online.size() - 1; i++) {
                  select_sql += v_online.at(i);
                  select_sql += " or cluster_id = ";
                }
                select_sql += v_online.back();
              } else {
                select_sql += to_string(Backend::instance()->get_cluster_id());
              }
            } else
              select_sql += to_string(Backend::instance()->get_cluster_id());
          }
          assemble_order_by_select_sql();
          conn->query_for_all_column(select_sql.c_str(), &data_vec);
          remote_conn->query_for_all_column(select_sql.c_str(),
                                            &remote_data_vec);
        } else {
          is_error = true;
          error_message +=
              "table not exist cluster_id or dbscale_cluster_id [" + tb_name +
              "] |";
          LOG_ERROR(
              "dbscale check metadata fail, table name %s need cluster_id "
              "field "
              "type, plz check remote cluster table, or local master table",
              tb_name.c_str());
          continue;
        }

        if (remote_data_vec.size() != data_vec.size()) {
          LOG_ERROR(
              "[find miss row]"
              "[dbscale check meta] fail, check_table %s , "
              "remote_data_vec.size "
              "= [%d], data_vec.size = [%d]\n",
              tb_name.c_str(), remote_data_vec.size(), data_vec.size());
          is_error = true;
          miss_msg += "(" + tb_name + " | " +
                      to_string(abs((long long)remote_data_vec.size() -
                                    (long long)data_vec.size())) +
                      " row)";
        } else {
          int ret = find_missing_value(data_vec, remote_data_vec, tb_name);
          if (ret != 0) {
            LOG_ERROR(
                "[dbscale check metadata] fail, please check database:dbscale "
                "datatable %s\n",
                tb_name.c_str());
            unequal_msg += "(" + tb_name + ")";
            is_error = true;
          }
        }
      }
    }
  } catch (ErrorPacketException &e) {
    throw ErrorPacketException();
  } catch (exception &e) {
    LOG_ERROR("check metadata get exception [%s]\n", e.what());
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, e.what(), "42S09");
    throw ErrorPacketException();
  } catch (...) {
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE,
                     "check metadata Error,plz check log", "42S09");
    throw ErrorPacketException();
  }
  if (is_error) {
    miss_msg += "]";
    unequal_msg += "]";
    error_message += miss_msg + " " + unequal_msg;
    LOG_ERROR("[dbscale check meta data] %s\n", error_message.c_str());
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, error_message.c_str(),
                     "42S09");
    throw ErrorPacketException();
  }
}

void MySQLCheckMetaDataNode::set_error_packet(uint16_t error_code,
                                              const char *error_message,
                                              const char *sqlstate,
                                              uint8_t number) {
  error_packet = Backend::instance()->get_new_packet();
  MySQLErrorResponse error(error_code, error_message, sqlstate, number);
  error.pack(error_packet);
}

void MySQLCheckMetaDataNode::clean() {
  if (conn) {
    conn->get_pool()->add_back_to_free(conn);
    conn = NULL;
  }
  if (remote_conn) {
    remote_conn->get_pool()->add_back_to_free(remote_conn);
    remote_conn = NULL;
  }
  if (remote_server) remote_server = NULL;

  if (error_packet) {
    delete error_packet;
    error_packet = NULL;
  }
}

Connection *MySQLCheckMetaDataNode::get_connection() {
  Backend *backend = Backend::instance();
  DataSpace *data_space = backend->get_config_data_space();
  if (!data_space) return NULL;
  return data_space->get_connection(session);
}

DataServer *MySQLCheckMetaDataNode::get_remote_dataserver() {
  Backend *backend = Backend::instance();
  map<string, DataServer *> data_servers;
  backend->get_data_servers(data_servers);
  map<string, DataServer *>::iterator it = data_servers.begin();
  for (; it != data_servers.end(); it++) {
    if (it->second->is_dbscale_server()) {
      return it->second;
    }
  }
  return NULL;
}

int MySQLCheckMetaDataNode::compare_meta_data_vec_one_row(
    vector<string> &local_vec, vector<string> &remote_vec) {
  int local_col_size = local_vec.size();
  for (int i = 0; i < local_col_size; i++) {
    if (local_vec[i] != remote_vec[i]) {
      return -1;
    }
  }
  return 0;
}

int MySQLCheckMetaDataNode::pre_check_table_structure_field_type(
    const string &database_name, const string &tb_name,
    vector<vector<string> > &one_column,
    vector<vector<string> > &remote_one_column) {
  if (one_column.size() != remote_one_column.size()) {
    string error_msg = "please check cluster metadata table name [" +
                       database_name + "." + tb_name +
                       "] extra structure field type";
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, error_msg.c_str(),
                     "42S09");
    throw ErrorPacketException();
  }
  int ret = find_missing_value(one_column, remote_one_column, tb_name);
  if (ret == -1) {
    string error_msg = "please check cluster metadata table name [" +
                       database_name + "." + tb_name +
                       "] unequal structure field type";
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, error_msg.c_str(),
                     "42S09");
    throw ErrorPacketException();
  }
  return 0;
}

int MySQLCheckMetaDataNode::pre_options() {
  if (!enable_disaster_mode) {
    LOG_ERROR("metadata check need set enable_disaster_mode.\n");
    throw DynamicRemoveSlaveFail(
        "metadata check need set enable_disaster_mode = 1");
  }
  if (slave_dbscale_mode) {
    LOG_ERROR(
        "not support slave cluster dbscale execute command, plz check "
        "[slave_dbscale_mode=0]\n");
    throw Error(
        "not support slave cluster dbscale execute command,plz check "
        "[slave_dbscale_mode=0]");
  }
  try {
    remote_server = get_remote_dataserver();
    if (!remote_server) {
      LOG_ERROR("get remote_server is fail\n");
      throw Error("get remote_server is fail");
    }

    conn = get_connection();
    if (!conn) {
      LOG_ERROR("get local conn fail\n");
      throw Error("get local conn fail");
    }
    const char *user = remote_server->get_user();
    // const char *passwd = remote_server->get_password();

    if (!user || !strcmp(user, "")) {
      LOG_ERROR("The user name must be configured\n");
      throw Error(
          "The user name must be configured,plz check user name configured");
    }
    remote_conn =
        remote_server->get_relate_server_datasource()->get_connection_force(
            NULL, true);
    // remote_conn = remote_server->create_connection(user, passwd, NULL);
    if (!remote_conn) {
      LOG_ERROR("get remote conn fail remote_server %s\n",
                remote_server->get_name());
      throw Error(
          "get remote conn fail,plz check conf, dataserver user or passwd");
    }

    if (remote_conn->ping() != PING_OK) {
      LOG_ERROR("remote server %s ping fail\n", remote_server->get_name());
      throw Error("remote server ping fail,plz check network connectivity");
    }
  } catch (Exception &e) {
    LOG_ERROR("metadata check got error %s", e.what());
    set_error_packet(ERROR_EXECUTE_NODE_FAILED_CODE, e.what(), "42S09");
    throw ErrorPacketException();
  }

  return 0;
}

int MySQLCheckMetaDataNode::find_missing_value(
    vector<vector<string> > &local_data_vec,
    vector<vector<string> > &remote_data_vec, const string &table_name) {
  string missing_value = "";
  int row_local_size = local_data_vec.size();
  int row_remote_size = remote_data_vec.size();
  bool is_find = false;
  bool is_local_miss = false;
  for (int i = 0; i < row_local_size; i++) {
    int ret = 0;
    for (int j = 0; j < row_remote_size; j++) {
      ret =
          compare_meta_data_vec_one_row(local_data_vec[i], remote_data_vec[j]);
      if (ret == 0) {
        is_find = true;
        break;
      }
    }
    if (!is_find) {
      string miss_val = "";
      for (uint32_t j = 0; j < local_data_vec[i].size() - 1; j++) {
        miss_val += local_data_vec[i][j];
        miss_val += ":";
      }
      miss_val += local_data_vec[i].back().c_str();
      is_local_miss = true;
      LOG_ERROR(
          "not exist row at slave cluster please check main cluster table %s "
          ",miss val : [%s]",
          table_name.c_str(), miss_val.c_str());
    } else {
      is_find = false;
    }
  }
  if (is_local_miss) return -1;
  LOG_DEBUG(
      "table[%s] is equal remote_data_vec_row = %d, "
      "local_data_vec_row = %d\n",
      table_name.c_str(), row_remote_size, row_local_size);
  return 0;
}

/*class MySQLDBScaleShowPrepareCacheNode*/
MySQLDBScaleShowPrepareCacheNode::MySQLDBScaleShowPrepareCacheNode(
    ExecutePlan *plan)
    : MySQLDBScaleShowNode(plan) {
  this->driver = (MySQLDriver *)plan->driver;
}

void MySQLDBScaleShowPrepareCacheNode::init_data() {
  set_head_packet(6);
  const char *catalog = "def";
  const char *schema = "information_schema";
  const char *table = "tables";
  const char *org_table = "tables";
  list<const char *> columns;
  columns.push_back("RealCacheSize");
  columns.push_back("TotalMemSize");
  columns.push_back("SQLMemSize");
  columns.push_back("HitCount");
  columns.push_back("PrepareCount");
  columns.push_back("CacheHit");
  list<const char *>::iterator it = columns.begin();
  for (; it != columns.end(); it++) {
    add_column_packet(catalog, schema, table, org_table, *it, *it, 8,
                      NAME_LEN + 1, MYSQL_TYPE_VAR_STRING, 0, 0, 0);
  }
  size_t cache_size;
  uint64_t prepare_count, cache_hit, total_mem_size, sql_mem_size;
  driver->get_prepare_cache_hit(cache_size, prepare_count, cache_hit,
                                total_mem_size, sql_mem_size);

  list<const char *> row_data;
  char cache_size_str[21];
  sprintf(cache_size_str, "%ld", cache_size);
  row_data.push_back(cache_size_str);

  char mem_size_str[21];
  sprintf(mem_size_str, "%ld", total_mem_size);
  row_data.push_back(mem_size_str);

  char sql_size_str[21];
  sprintf(sql_size_str, "%ld", sql_mem_size);
  row_data.push_back(sql_size_str);

  char hit_count_str[21];
  sprintf(hit_count_str, "%ld", cache_hit);
  row_data.push_back(hit_count_str);

  char prepare_count_str[21];
  sprintf(prepare_count_str, "%ld", prepare_count);
  row_data.push_back(prepare_count_str);

  char hit_str[21];
  if (prepare_count == 0)
    sprintf(hit_str, "%f", float(0));
  else
    sprintf(hit_str, "%f", float(float(cache_hit) / float(prepare_count)));
  row_data.push_back(hit_str);
  add_row_packet(row_data);
}

/*class MySQLDBScaleFlushPrepareCacheHitNode*/
MySQLDBScaleFlushPrepareCacheHitNode::MySQLDBScaleFlushPrepareCacheHitNode(
    ExecutePlan *plan)
    : MySQLReturnOKNode(plan) {
  this->driver = (MySQLDriver *)plan->driver;
}

void MySQLDBScaleFlushPrepareCacheHitNode::do_execute() {
  driver->reset_prepare_cache_hit();
}

}  // namespace mysql
}  // namespace dbscale
