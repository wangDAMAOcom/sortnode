/*
 * Copyright (C) 2013 Great OpenSource Inc. All Rights Reserved.
 */
#include <backend.h>
#include <basic.h>
#include <config.h>
#include <cross_node_join_manager.h>
#include <data_source.h>
#include <data_space.h>
#include <expression.h>
#include <handler.h>
#include <multiple.h>
#include <parser.h>
#include <partition_method.h>
#include <plan.h>
#include <routine.h>
#include <session.h>
#include <sql_exception.h>
#include <statement.h>
#include <string.h>
#include <sub_query.h>
#include <table_info.h>
#include <transfer_rule.h>

#include <boost/algorithm/string.hpp>
#include <boost/format.hpp>
#include <limits>
#include <vector>

#ifndef DBSCALE_DISABLE_SPARK
#include <spark_client.h>
#endif

#define DBSCALE_SPARK_TMP "dbscale_spark_tmp"
#define MAX_RECURSION_LEVEL 500

using namespace dbscale;
using namespace dbscale::sql;
using namespace dbscale::plan;
namespace dbscale {
namespace sql {

// TODO: Support mul partition keys if needed.

#define HAS_IDENTIFY_COLUMNS 0
#define NO_IDENTIFY_COLUMNS 1

enum condition_type {
  CONDITION_TYPE_NON,
  CONDITION_TYPE_EQUAL,
  CONDITION_TYPE_RANGE,
  CONDITION_TYPE_RANDOM
};

enum ConditionAndOr { CONDITION_AND, CONDITION_OR, CONDITION_NO_COND };

bool check_and_transform_autocommit_value(string &value) {
  if (!strcasecmp(value.c_str(), "ON") || !strcasecmp(value.c_str(), "'ON'") ||
      !strcasecmp(value.c_str(), "'TRUE'") || !strcasecmp(value.c_str(), "1")) {
    return true;
  }

  if (!strcasecmp(value.c_str(), "OFF") || !strcasecmp(value.c_str(), "'ON'") ||
      !strcasecmp(value.c_str(), "FALSE") || !strcasecmp(value.c_str(), "0")) {
    return false;
  }

  throw dbscale::sql::SQLError(
      "Variable 'autocommit' can't be set to the value", "42000", 1231);
}

inline bool is_terminator_has_null(char *term, unsigned int len) {
  if (len == 0) return false;

  for (unsigned int i = 0; i < len; ++i) {
    if (term[i] == '\0') return true;
  }
  return false;
}

bool is_executed_as_table(record_scan *rs) {
  return rs->subquerytype == SUB_SELECT_TABLE ||
         rs->subquerytype == SUB_SELECT_MUL_COLUMN;
}

static bool judge_need_apply_meta_datasource(
    map<DataSpace *, const char *> &spaces_map) {
  DataSpace *meta_dataspace = Backend::instance()->get_metadata_data_space();
  map<DataSpace *, const char *>::iterator it_space = spaces_map.begin();
  bool need_apply_meta = true;
  for (; it_space != spaces_map.end(); ++it_space) {
    if (is_share_same_server(meta_dataspace, it_space->first)) {
      need_apply_meta = false;
      break;
    }
  }
  return need_apply_meta;
}

CenterJoinGarbageCollector::CenterJoinGarbageCollector() {
  reconstruct_sql = NULL;
  generated_sql = NULL;
  table_vector_query = NULL;
  fake_rs_vector.clear();
  handler = NULL;
  new_table_vector = NULL;
}

void CenterJoinGarbageCollector::clear_garbages() {
  if (reconstruct_sql) delete reconstruct_sql;
  reconstruct_sql = NULL;

  Backend *backend = Backend::instance();
  DataSpace *ds =
      backend->get_data_space_for_table("mytest", "center_join_dataspace");
  if (generated_sql) {
    unsigned int generated_sql_size = generated_sql->size();
    unsigned int tmp_table_size = (generated_sql_size - 1) / 3;
    for (unsigned int i = tmp_table_size * 2 + 1; i < generated_sql_size; ++i) {
      string drop_sql = generated_sql->at(i);
      // TODO this method will throw exception, better not in destructor, fix it
      // in the future
      try {
        ds->execute_one_modify_sql(drop_sql.c_str(), handler, NULL);
      } catch (...) {
        LOG_ERROR("Drop center join tmp tables failed for sql [%s]\n",
                  drop_sql.c_str());
      }
    }
    delete generated_sql;
  }
  generated_sql = NULL;

  if (table_vector_query) delete table_vector_query;
  table_vector_query = NULL;

  if (new_table_vector) {
    delete new_table_vector;
    new_table_vector = NULL;
  }
  vector<record_scan *>::iterator it = fake_rs_vector.begin();
  for (; it != fake_rs_vector.end(); ++it) {
    delete *it;
  }
  fake_rs_vector.clear();

  vector<DataSpace *>::iterator it_ds = ds_vector.begin();
  for (; it_ds != ds_vector.end(); ++it_ds) {
    Schema *join_schema = backend->find_schema(TMP_TABLE_SCHEMA);
    join_schema->remove_table((*it_ds)->get_name());
    delete *it_ds;
  }
  ds_vector.clear();

  DataSpace *ds_spark =
      backend->get_data_space_for_table(spark_dst_schema.c_str(), NULL);
  if (!spark_sql_drop_sql.empty()) {
    try {
      ds_spark->execute_one_modify_sql(spark_sql_drop_sql.c_str(), handler,
                                       NULL);
    } catch (...) {
      LOG_ERROR("Drop center join tmp tables failed for sql [%s]\n",
                spark_sql_drop_sql.c_str());
    }
  }
  if (!spark_sql_table_name.empty())
    backend->remove_spark_sql_table_name(spark_sql_table_name);
  spark_sql_table_name.clear();
}

void check_partition_distinct(table_link *par_tb) {
  /*Assume that this sql will affect more than one partition of partition
   * table.*/
  /*Check the distinct usage for partition table. Currently, dbscale does
   * not support the partition table record scan has distinct opertion,
   * which should be done by using table subquery.*/
  record_scan *par_rs = par_tb->join->cur_rec_scan;
  if (par_rs->options & SQL_OPT_DISTINCT ||
      par_rs->options & SQL_OPT_DISTINCTROW) {
    LOG_ERROR(
        "Not support to use distinct[row] for partition table directly, plz "
        "check manual and use table subquery for distinct1 with options %d.\n",
        par_rs->options);
    throw UnSupportPartitionSQL(
        "UnSupport partition table sql with distinct, plz check manual and use "
        "table subquery for distinct.");
  }
}

void Statement::fulfill_tb_list(record_scan *rs, table_link *tl,
                                list<TableNameStruct> *tns_list) {
  if (tl->join->type == JOIN_NODE_SINGLE && tl->join->cur_rec_scan == rs) {
    TableNameStruct tns;
    tns.schema_name =
        string(tl->join->schema_name ? tl->join->schema_name : schema);
    tns.table_name = string(tl->join->table_name);
    tns.table_alias = string(tl->join->alias ? tl->join->alias : "");
    tns_list->push_back(tns);
  }
  if (tl->next) fulfill_tb_list(rs, tl->next, tns_list);
}

void inline append_column_fields_and_groupby_fragment(
    vector<TableColumnInfo> *column_info_vec, string &column_prefix,
    string &all_column_fields, string &groupby_fragment) {
  size_t column_total_count = column_info_vec->size();
  for (unsigned int i = 0; i < column_total_count; ++i) {
    all_column_fields.append(column_prefix);
    groupby_fragment.append(column_prefix);

    all_column_fields.append("`");
    all_column_fields.append((*column_info_vec)[i].column_name);
    // FIXME: uncomment below 2 line after issue #3294
    // all_column_fields.append(" AS ");
    // all_column_fields.append((*columns)[i]);
    all_column_fields.append("`, ");
    groupby_fragment.append("`");
    groupby_fragment.append((*column_info_vec)[i].column_name);
    groupby_fragment.append("`, ");
  }
}

bool need_hex(string column_type) {
  boost::to_lower(column_type);
  if (column_type.find("binary") != string::npos ||
      column_type.find("blob") != string::npos ||
      column_type.find("bit") != string::npos)
    return true;
  return false;
}

#define SAVE_STR_STMT(var, str)          \
  var = NULL;                            \
  if (str != NULL && strlen(str) != 0) { \
    tmp = new char[strlen(str) + 1];     \
    strncpy(tmp, str, strlen(str) + 1);  \
    var = tmp;                           \
    stmt_add_dynamic_str(tmp);           \
  }

void Statement::fulfill_hex_pos_real(const char *schema_name,
                                     const char *table_name,
                                     set<string> col_names) {
  hex_pos.clear();
  TableInfoCollection *tic = TableInfoCollection::instance();
  TableInfo *ti = tic->get_table_info_for_read(schema_name, table_name);
  vector<TableColumnInfo> *column_info_vec;
  try {
    column_info_vec =
        ti->element_table_column->get_element_vector_table_column_info(
            stmt_session);
  } catch (...) {
    LOG_ERROR(
        "Error occured when try to get table info column "
        "info(vector_table_column_info) of table [%s.%s]\n",
        schema_name, table_name);
    ti->release_table_info_lock();
    throw;
  }
  int pos = 0;
  vector<TableColumnInfo>::iterator it = column_info_vec->begin();
  for (; it != column_info_vec->end(); ++it) {
    if (need_hex(it->column_type)) {
      if (!col_names.empty()) {
        string tmp_name = it->column_name;
        boost::to_lower(tmp_name);
        if (col_names.count(tmp_name)) {
          hex_pos.insert(pos);
        }
      } else
        hex_pos.insert(pos);
    }
    ++pos;
  }
  ti->release_table_info_lock();
  LOG_DEBUG(
      "Statement::fulfill_hex_pos with hex_pos size %d for table [%s.%s].\n",
      hex_pos.size(), schema_name, table_name);
}

void Statement::fulfill_hex_pos(insert_op_node *insert_op) {
  table_link *modify_table = get_one_modify_table();
  name_item *cols_head = insert_op->insert_column_list;
  name_item *col = cols_head;
  const char *schema_name = modify_table->join->schema_name
                                ? modify_table->join->schema_name
                                : schema;
  const char *table_name = modify_table->join->table_name;
  set<string> col_names;
  if (col != NULL) do {
      string n(col->name);
      boost::to_lower(n);
      if (col_names.count(n)) {
        throw Error("Insert with duplicate col.");
      }
      col_names.insert(n);
      col = col->next;
    } while (col != cols_head);
  fulfill_hex_pos_real(schema_name, table_name, col_names);
  LOG_DEBUG("Statement::fulfill_hex_pos with hex_pos size %d for sql [%s].\n",
            hex_pos.size(), sql);
}

int fill_column_to_expend_columns(vector<string> &expend_columns,
                                  string column_prefix, const char *schema_name,
                                  const char *table_name, int field_pos,
                                  set<int> hex_pos, Session *stmt_session) {
  TableInfoCollection *tic = TableInfoCollection::instance();
  TableInfo *ti = tic->get_table_info_for_read(schema_name, table_name);

  vector<TableColumnInfo> *column_info_vec;
  try {
    column_info_vec =
        ti->element_table_column->get_element_vector_table_column_info(
            stmt_session);
  } catch (...) {
    LOG_ERROR(
        "Error occured when try to get table info column "
        "info(vector_table_column_info) of table [%s.%s]\n",
        schema_name, table_name);
    ti->release_table_info_lock();
    throw;
  }

  size_t column_total_count = column_info_vec->size();
  for (unsigned int i = 0; i < column_total_count; ++i) {
    string expend_column_str;
    if (hex_pos.count(field_pos)) {
      expend_column_str.append("hex(");
    }
    expend_column_str.append(column_prefix);
    expend_column_str.append("`");
    expend_column_str.append((*column_info_vec)[i].column_name);
    expend_column_str.append("`");
    if (hex_pos.count(field_pos)) {
      expend_column_str.append(")");
    }
    ++field_pos;
    expend_columns.push_back(expend_column_str);
  }
  ti->release_table_info_lock();
  return field_pos;
}

void Statement::handle_insert_select_hex_column() {
  if (st.type == STMT_INSERT_SELECT || st.type == STMT_REPLACE_SELECT) {
    LOG_DEBUG("Statement::handle_insert_select_hex_column with sql[%s].\n",
              sql);
    string tmp_select_sub_sql = select_sub_sql;
    try {
      vector<string> expend_columns;
      insert_op_node *insert_op = st.sql->insert_oper;
      fulfill_hex_pos(insert_op);
      expend_field_list(insert_op, expend_columns, sql);
      adjust_select_sub_sql_for_hex(expend_columns);
    } catch (exception &e) {
      LOG_DEBUG(
          "Statement::handle_insert_select_hex_column get exception %s.\n",
          e.what());
      select_sub_sql.assign(tmp_select_sub_sql.c_str());
      hex_pos.clear();
    }
  }
}

void Statement::adjust_select_sub_sql_for_hex(vector<string> expend_columns) {
  if (hex_pos.empty() || expend_columns.empty()) {
    hex_pos.clear();
    return;
  }
  vector<string>::iterator it = expend_columns.begin();
  string new_col_str;
  new_col_str.append((*it).c_str());
  ++it;
  for (; it != expend_columns.end(); ++it) {
    new_col_str.append(",");
    new_col_str.append((*it).c_str());
  }
  string new_sql("select ");
  new_sql.append(new_col_str.c_str());
  new_sql.append(" ");
  insert_op_node *insert_op = st.sql->insert_oper;
  record_scan *rs = insert_op->select_values;

  if (rs->from_pos >= 1) {
    new_sql.append(sql + rs->from_pos - 1);
  }
  LOG_DEBUG(
      "Statement::adjust_select_sub_sql_for_hex get new select_sub_sql is "
      "[%s].\n",
      new_sql.c_str());
  select_sub_sql.assign(new_sql.c_str());
}

void Statement::expend_field_list_real(field_item *field,
                                       vector<string> &expend_columns,
                                       const char *handle_sql,
                                       list<TableNameStruct> &tns_list,
                                       record_scan *rs) {
  int field_pos = 0;
  while (field) {
    bool has_added = false;
    if (field->field_expr && field->field_expr->type == EXPR_STR) {
      StrExpression *str_expr = (StrExpression *)(field->field_expr);
      LOG_DEBUG(
          "Statement::expend_field_list handle column %s with expend_columns "
          "size %d with star_flag %d tns_list size %d.\n",
          str_expr->str_value, expend_columns.size(),
          str_expr->get_has_star_flag() ? 1 : 0, tns_list.size());
      if (str_expr->get_has_star_flag()) {
        string field_string_value = string(str_expr->str_value);
        size_t first_dot_position = field_string_value.find(".");
        size_t second_dot_position =
            field_string_value.find(".", first_dot_position + 1);
        if (first_dot_position != string::npos && (!handle_sql || !rs)) {
          LOG_ERROR(
              "Statement::expend_field_list_real should has non NULL "
              "handle_sql and rs if not only *.\n");
          return;
        }

        if (first_dot_position == string::npos) {
          // field is '*'
          list<TableNameStruct>::iterator it = tns_list.begin();
          for (; it != tns_list.end(); ++it) {
            string column_prefix;
            column_prefix.append("`");
            if (!(*it).table_alias.empty())
              column_prefix.append((*it).table_alias);
            else
              column_prefix.append((*it).table_name);
            column_prefix.append("`.");

            field_pos = fill_column_to_expend_columns(
                expend_columns, column_prefix, (*it).schema_name.c_str(),
                (*it).table_name.c_str(), field_pos, hex_pos, stmt_session);
          }
        } else if (second_dot_position == string::npos) {
          // field is 't1.*'
          string table_name_in_field =
              field_string_value.substr(0, first_dot_position);
          string column_prefix;
          list<TableNameStruct>::iterator it = tns_list.begin();
          string schema_name, table_name;
          for (; it != tns_list.end(); ++it) {
            if (!(*it).table_alias.empty()) {
              if (!strcmp(it->table_alias.c_str(),
                          table_name_in_field.c_str())) {  // table alias match
                column_prefix.append("`");
                column_prefix.append((*it).table_alias);
                column_prefix.append("`.");
                break;
              }
            } else if (!strcmp(
                           it->table_name.c_str(),
                           table_name_in_field.c_str())) {  // table name match
              column_prefix.append("`");
              column_prefix.append((*it).table_name);
              column_prefix.append("`.");
              break;
            }
          }
          if (it == tns_list.end()) {
            char msg[128];
            sprintf(msg, "Table name [%s] do not found in table list.",
                    table_name_in_field.c_str());
            LOG_ERROR("%s\n", msg);
            throw Error(msg);
          }

          field_pos = fill_column_to_expend_columns(
              expend_columns, column_prefix, (*it).schema_name.c_str(),
              (*it).table_name.c_str(), field_pos, hex_pos, stmt_session);
        } else {
          // the expression is like 'test.t1.*'
          string schema_name_in_field =
              field_string_value.substr(0, first_dot_position);
          string table_name_in_field = field_string_value.substr(
              first_dot_position + 1,
              second_dot_position - first_dot_position - 1);
          string column_prefix =
              field_string_value.substr(0, second_dot_position + 1);

          map<string, string, strcasecomp> alias_table_map =
              get_alias_table_map(rs);
          if (alias_table_map.count(table_name_in_field))
            table_name_in_field = alias_table_map[table_name_in_field];

          field_pos = fill_column_to_expend_columns(
              expend_columns, column_prefix, schema_name_in_field.c_str(),
              table_name_in_field.c_str(), field_pos, hex_pos, stmt_session);
        }
        has_added = true;
      }
    }
    if (!has_added) {
      string expend_column;
      if (hex_pos.count(field_pos)) {
        expend_column.append("hex(");
        if (field->alias) {
          expend_column.append(handle_sql + field->head_pos - 1,
                               field->name_size);
          expend_column.append(")");
          expend_column.append(
              handle_sql + field->name_size + field->head_pos - 1,
              field->tail_pos - field->head_pos - field->name_size + 1);
        } else {
          expend_column.append(handle_sql + field->head_pos - 1,
                               field->tail_pos - field->head_pos + 1);
          expend_column.append(")");
        }

      } else
        expend_column.append(handle_sql + field->head_pos - 1,
                             field->tail_pos - field->head_pos + 1);
      ++field_pos;
      expend_columns.push_back(expend_column);
    }
    field = field->next;
  }
#ifdef DEBUG
  if (!expend_columns.empty()) {
    vector<string>::iterator it = expend_columns.begin();
    string tmp_str;
    tmp_str.append((*it).c_str());
    ++it;
    for (; it != expend_columns.end(); ++it) {
      tmp_str.append(",");
      tmp_str.append((*it).c_str());
    }
    LOG_DEBUG("Get expend column [%s] for Statement::expend_field_list.\n",
              tmp_str.c_str());
  }
#endif
}

void Statement::expend_field_list(insert_op_node *insert_op,
                                  vector<string> &expend_columns,
                                  const char *handle_sql) {
  record_scan *rs = insert_op->select_values;
  list<TableNameStruct> tns_list;
  table_link *tl = rs->first_table;
  if (!tl) {
    LOG_DEBUG(
        "Statement::expend_field_list skip due to no find table directly from "
        "select top rs.\n");
    return;
  }
  fulfill_tb_list(rs, tl, &tns_list);
  field_item *field = rs->field_list_head;
  expend_field_list_real(field, expend_columns, handle_sql, tns_list, rs);
}

// return true if replace DISTINCT with GROUP BY, otherwise false or throw
// exception.
bool Statement::check_and_replace_par_distinct_with_groupby(
    table_link *tb_link, const char *_query_sql) {
  record_scan *rs = tb_link->join->cur_rec_scan;
  if (!(rs->options & SQL_OPT_DISTINCT || rs->options & SQL_OPT_DISTINCTROW))
    return false;

  const char *handled_sql = NULL;
  bool handle_sql_local = false;
  if (_query_sql) {
    handled_sql = _query_sql;
    handle_sql_local = true;
  } else {
    handled_sql = sql;
  }
#ifdef DEBUG
  LOG_DEBUG(
      "sql [%s] in Statement::check_and_replace_par_distinct_with_groupby()\n",
      handled_sql);
#endif

  if (!rs->group_by_list) {
    string head_str, last_field_to_where_end_str, tail_str;
    string tmp_sql(handled_sql);

    list<TableNameStruct> tns_list;
    table_link *tl = rs->first_table;
    fulfill_tb_list(rs, tl, &tns_list);

    field_item *field = rs->field_list_head;
    string all_column_fields = string("");
    string groupby_fragment = string("");
    while (field) {
      if (field->field_expr && field->field_expr->type == EXPR_STR) {
        StrExpression *str_expr = (StrExpression *)(field->field_expr);
        if (str_expr->get_has_star_flag()) {
          string field_string_value = string(str_expr->str_value);
          size_t first_dot_position = field_string_value.find(".");
          size_t second_dot_position =
              field_string_value.find(".", first_dot_position + 1);

          if (first_dot_position == string::npos) {
            // field is '*'
            TableInfoCollection *tic = TableInfoCollection::instance();
            list<TableNameStruct>::iterator it = tns_list.begin();
            for (; it != tns_list.end(); ++it) {
              TableInfo *ti = tic->get_table_info_for_read(
                  (*it).schema_name.c_str(), (*it).table_name.c_str());
              vector<TableColumnInfo> *column_info_vec;
              try {
                column_info_vec =
                    ti->element_table_column
                        ->get_element_vector_table_column_info(stmt_session);
              } catch (...) {
                LOG_ERROR(
                    "Error occured when try to get table info column "
                    "info(vector_table_column_info) of table [%s.%s]\n",
                    (*it).schema_name.c_str(), (*it).table_name.c_str());
                ti->release_table_info_lock();
                throw;
              }

              string column_prefix;
              column_prefix.append("`");
              column_prefix.append((*it).schema_name);
              column_prefix.append("`.`");
              if (!(*it).table_alias.empty())
                column_prefix.append((*it).table_alias);
              else
                column_prefix.append((*it).table_name);
              column_prefix.append("`.");
              append_column_fields_and_groupby_fragment(
                  column_info_vec, column_prefix, all_column_fields,
                  groupby_fragment);
              ti->release_table_info_lock();
            }
          } else if (second_dot_position == string::npos) {
            // field is 't1.*'
            string table_name_in_field =
                field_string_value.substr(0, first_dot_position);
            TableInfoCollection *tic = TableInfoCollection::instance();
            TableInfo *ti = NULL;
            string column_prefix;
            list<TableNameStruct>::iterator it = tns_list.begin();
            string schema_name, table_name;
            for (; it != tns_list.end(); ++it) {
              if (!(*it).table_alias.empty()) {
                if (!strcmp(
                        it->table_alias.c_str(),
                        table_name_in_field.c_str())) {  // table alias match
                  ti = tic->get_table_info_for_read((*it).schema_name.c_str(),
                                                    (*it).table_name.c_str());
                  column_prefix.append("`");
                  column_prefix.append((*it).table_alias);
                  column_prefix.append("`.");
                  break;
                }
              } else if (!strcmp(it->table_name.c_str(),
                                 table_name_in_field
                                     .c_str())) {  // table name match
                ti = tic->get_table_info_for_read((*it).schema_name.c_str(),
                                                  (*it).table_name.c_str());
                column_prefix.append("`");
                column_prefix.append((*it).table_name);
                column_prefix.append("`.");
                break;
              }
            }
            if (it == tns_list.end()) {
              char msg[128];
              sprintf(msg, "Table name [%s] do not found in table list.",
                      table_name_in_field.c_str());
              LOG_ERROR("%s\n", msg);
              throw Error(msg);
            }

            vector<TableColumnInfo> *column_info_vec;
            try {
              column_info_vec =
                  ti->element_table_column
                      ->get_element_vector_table_column_info(stmt_session);
            } catch (...) {
              LOG_ERROR(
                  "Error occured when try to get table info column "
                  "info(vector_table_column_info) of table [%s.%s]\n",
                  (*it).schema_name.c_str(), (*it).table_name.c_str());
              ti->release_table_info_lock();
              throw;
            }

            append_column_fields_and_groupby_fragment(
                column_info_vec, column_prefix, all_column_fields,
                groupby_fragment);
            ti->release_table_info_lock();
          } else {
            // the expression is like 'test.t1.*'
            string schema_name_in_field =
                field_string_value.substr(0, first_dot_position);
            string table_name_in_field = field_string_value.substr(
                first_dot_position + 1,
                second_dot_position - first_dot_position - 1);
            string column_prefix =
                field_string_value.substr(0, second_dot_position + 1);

            map<string, string, strcasecomp> alias_table_map =
                get_alias_table_map(rs);
            if (alias_table_map.count(table_name_in_field))
              table_name_in_field = alias_table_map[table_name_in_field];
            TableInfoCollection *tic = TableInfoCollection::instance();
            TableInfo *ti = tic->get_table_info_for_read(
                schema_name_in_field.c_str(), table_name_in_field.c_str());

            vector<TableColumnInfo> *column_info_vec;
            try {
              column_info_vec =
                  ti->element_table_column
                      ->get_element_vector_table_column_info(stmt_session);
            } catch (...) {
              LOG_ERROR(
                  "Error occured when try to get table info column "
                  "info(vector_table_column_info) of table [%s.%s]\n",
                  schema_name_in_field.c_str(), table_name_in_field.c_str());
              ti->release_table_info_lock();
              throw;
            }
            append_column_fields_and_groupby_fragment(
                column_info_vec, column_prefix, all_column_fields,
                groupby_fragment);
            ti->release_table_info_lock();
          }
        } else {
          string expr_str = string(str_expr->str_value);
          size_t first_dot_pos = expr_str.find(".");
          size_t second_dot_pos = expr_str.find(".", first_dot_pos + 1);

          if (first_dot_pos == string::npos) {
            string str("`");
            str.append(expr_str);
            str.append("`");
            all_column_fields.append(str);
            groupby_fragment.append(str);

          } else if (second_dot_pos == string::npos) {
            string str("`");
            str.append(expr_str, 0, first_dot_pos);
            str.append("`.`");
            str.append(expr_str, first_dot_pos + 1,
                       expr_str.length() - first_dot_pos - 1);
            str.append("`");
            all_column_fields.append(str);
            groupby_fragment.append(str);
          } else {
            string str("`");
            str.append(expr_str, 0, first_dot_pos);
            str.append("`.`");
            str.append(expr_str, first_dot_pos + 1,
                       second_dot_pos - first_dot_pos - 1);
            str.append("`.`");
            str.append(expr_str, second_dot_pos + 1,
                       expr_str.length() - second_dot_pos - 1);
            str.append("`");
            all_column_fields.append(str);
            groupby_fragment.append(str);
          }

          if (field->alias != NULL) {
            all_column_fields.append(" AS `");
            all_column_fields.append(field->alias);
            all_column_fields.append("`");
          }
          all_column_fields.append(", ");
          groupby_fragment.append(", ");
        }
      } else {
        LOG_ERROR(
            "UnSupport partition table sql with distinct, plz check manual and "
            "use table subquery for distinct2.\n");
        throw UnSupportPartitionSQL(
            "UnSupport partition table sql with distinct, plz check manual and "
            "use table subquery for distinct.");
      }
      field = field->next;
    }
    boost::erase_tail(all_column_fields, 2);  // cut the tail ', '
    boost::erase_tail(groupby_fragment, 2);   // cut the tail ', '

    unsigned int distinct_start_pos;
    unsigned int distinct_end_pos;
    if (handle_sql_local) {
      distinct_start_pos = rs->distinct_start_pos - (rs->start_pos - 1);
      distinct_end_pos = rs->distinct_end_pos - (rs->start_pos - 1);
    } else {
      distinct_start_pos = rs->distinct_start_pos;
      distinct_end_pos = rs->distinct_end_pos;
    }

    head_str.assign(tmp_sql, 0, distinct_start_pos - 1);
    if (handle_sql_local) {
      head_str.append(tmp_sql, distinct_end_pos,
                      (rs->field_list_head->head_pos - (rs->start_pos - 1)) -
                          distinct_end_pos - 1);
      last_field_to_where_end_str.assign(
          tmp_sql, rs->field_list_tail->tail_pos - (rs->start_pos - 1),
          rs->opt_where_end_pos - rs->field_list_tail->tail_pos);
      tail_str.assign(
          tmp_sql, rs->opt_where_end_pos - (rs->start_pos - 1),
          tmp_sql.length() - (rs->opt_where_end_pos - (rs->start_pos - 1)));
    } else {
      head_str.append(tmp_sql, distinct_end_pos,
                      rs->field_list_head->head_pos - distinct_end_pos - 1);
      last_field_to_where_end_str.assign(
          tmp_sql, rs->field_list_tail->tail_pos,
          rs->opt_where_end_pos - rs->field_list_tail->tail_pos);
      tail_str.assign(tmp_sql, rs->opt_where_end_pos,
                      tmp_sql.length() - (rs->opt_where_end_pos));
    }

    no_distinct_sql.clear();
    no_distinct_sql.assign(head_str);
    LOG_DEBUG("change distinct sql from [%s] to [%s]\n", handled_sql,
              no_distinct_sql.c_str());
    no_distinct_sql.append(all_column_fields);
    LOG_DEBUG("change distinct sql from [%s] to [%s]\n", handled_sql,
              no_distinct_sql.c_str());
    no_distinct_sql.append(last_field_to_where_end_str);
    LOG_DEBUG("change distinct sql from [%s] to [%s]\n", handled_sql,
              no_distinct_sql.c_str());
    no_distinct_sql.append(" GROUP BY ");
    LOG_DEBUG("change distinct sql from [%s] to [%s]\n", handled_sql,
              no_distinct_sql.c_str());
    no_distinct_sql.append(groupby_fragment);
    LOG_DEBUG("change distinct sql from [%s] to [%s]\n", handled_sql,
              no_distinct_sql.c_str());
    no_distinct_sql.append(tail_str);
#ifdef DEBUG
    LOG_DEBUG("change distinct sql from [%s] to [%s]\n", handled_sql,
              no_distinct_sql.c_str());
#endif
    return true;
  } else {
    LOG_ERROR(
        "Not support to use distinct[row] with group by for partition table "
        "directly,"
        " plz check manual and use table subquery for distinct3 with options "
        "%d.\n",
        rs->options);
    throw UnSupportPartitionSQL(
        "UnSupport partition table sql with distinct,"
        " plz check manual and use table subquery for distinct.");
  }
}

DataSpace *get_prepare_sql_dataspace() {
  DataSpace *data_space;
  Backend *backend = Backend::instance();
  data_space = backend->get_metadata_data_space();
  if (!data_space) {
    data_space = backend->get_catalog();
  }

  return data_space;
}

inline Schema *get_alias_schema(
    table_link *table,
    map<table_link *, DataSpace *> &record_scan_all_table_spaces,
    const char *cur_schema) {
  DataSpace *space = NULL;
  if (record_scan_all_table_spaces.count(table))
    space = record_scan_all_table_spaces[table];

  if (!space) {
    Backend *backend = Backend::instance();
    space = backend->get_data_space_for_table(
        table->join->schema_name ? table->join->schema_name : cur_schema,
        table->join->table_name);
#ifdef DEBUG
    ACE_ASSERT(space);
#endif
  }
  if (space->get_dataspace_type() == SCHEMA_TYPE &&
      ((Schema *)space)->get_is_alias()) {
    return (Schema *)space;
  }

  return NULL;
}

inline bool is_shard_partition_table(
    table_link *table,
    map<table_link *, DataSpace *> &record_scan_all_table_spaces,
    const char *cur_schema, bool &is_tmp) {
  DataSpace *space = NULL;
  if (record_scan_all_table_spaces.count(table))
    space = record_scan_all_table_spaces[table];
  if (!space) {
    Backend *backend = Backend::instance();
    space = backend->get_data_space_for_table(
        table->join->schema_name ? table->join->schema_name : cur_schema,
        table->join->table_name);
#ifdef DEBUG
    ACE_ASSERT(space);
#endif
  }
  if (space->get_data_source()) {
    return false;
  }
  if (((Table *)space)->is_duplicated()) return false;
  PartitionedTable *part = (PartitionedTable *)space;
  if (part->get_partition_scheme()->is_shard()) {
    if (part->is_tmp_table_space()) is_tmp = true;
    return true;
  }
  return false;
}

inline void adjust_tmp_shard_table(const char *tmp_table, string &new_tb_name,
                                   unsigned virtual_machine_id,
                                   unsigned int partition_id) {
  new_tb_name.assign(tmp_table);
  new_tb_name.append("_shard_");
  char str[256];
  int len = sprintf(str, "%d", virtual_machine_id);
  str[len] = '\0';
  new_tb_name.append(str);
  len = sprintf(str, "%d", partition_id);
  str[len] = '\0';
  new_tb_name.append("_");
  new_tb_name.append(str);
  LOG_DEBUG("Adjust the shard table for tmp table %s to %s.\n", tmp_table,
            new_tb_name.c_str());
}

void adjust_shard_schema(const char *sql_schema, const char *cur_schema,
                         string &new_schema, unsigned int virtual_machine_id,
                         unsigned int partition_id) {
  const char *old_schema = NULL;
  if (sql_schema)
    old_schema = sql_schema;
  else
    old_schema = cur_schema;
  new_schema.clear();
  new_schema.append(old_schema);
  if (virtual_machine_id > 0) {
    Backend::instance()->get_shard_schema_string(virtual_machine_id,
                                                 partition_id, new_schema);
  }
}

void adjust_alias_schema(
    const char *old_sql, const char *cur_schema, stmt_node *st,
    map<table_link *, DataSpace *> &record_scan_all_table_spaces,
    string &new_sql, Schema *alias) {
#ifdef DEBUG
  ACE_ASSERT(st);
#endif
  vector<table_link *> alias_schema;
  Schema *tmp_schema = NULL;
  table_link *tmp_table = st->table_list_head;
  while (tmp_table) {
    tmp_schema =
        get_alias_schema(tmp_table, record_scan_all_table_spaces, cur_schema);
    if (tmp_schema != alias) {
      LOG_ERROR("Find a non_alias schema %s while expect alias schema %s.\n",
                tmp_schema ? tmp_schema->get_name() : "NULL",
                alias->get_name());
      throw Error("Unsupport usage for alias schema.");
    }
    alias_schema.push_back(tmp_table);
    tmp_table = tmp_table->next;
  }
  if (alias_schema.empty()) {
    new_sql.assign(old_sql);
    return;
  }
  new_sql.clear();
  string new_schema_tmp;
  const char *alias_real_name = alias->get_alias_real_name();
  vector<table_link *>::iterator it = alias_schema.begin();
  size_t start_copy_pos = 0;
  for (; it != alias_schema.end(); ++it) {
    new_sql.append(old_sql + start_copy_pos,
                   (*it)->start_pos - 1 - start_copy_pos);
    start_copy_pos = (*it)->end_pos;
    new_sql.append(alias_real_name);
    new_sql.append(".");
    new_sql.append((*it)->join->table_name);
  }
  new_sql.append(old_sql + start_copy_pos);
  LOG_DEBUG("After replace the alias schema, sql [%s] is adjusted to [%s].\n",
            old_sql, new_sql.c_str());
}

void adjust_virtual_machine_schema(
    unsigned int virtual_machine_id, unsigned int partition_id,
    const char *old_sql, const char *cur_schema, stmt_node *st,
    map<table_link *, DataSpace *> &record_scan_all_table_spaces,
    string &new_sql) {
#ifdef DEBUG
  ACE_ASSERT(virtual_machine_id);
  ACE_ASSERT(st);
#endif
  vector<table_link *> shard_tables;
  vector<table_link *> shard_tmp_tables;
  table_link *tmp_table = st->table_list_head;
  while (tmp_table) {
    bool is_tmp = false;
    if (!strcasecmp(tmp_table->join->table_name, "dual")) {
      LOG_DEBUG("SKip adjust shard schema for dual.\n");
      tmp_table = tmp_table->next;
      continue;
    }
    if (is_shard_partition_table(tmp_table, record_scan_all_table_spaces,
                                 cur_schema, is_tmp)) {
      if (is_tmp)
        shard_tmp_tables.push_back(tmp_table);
      else
        shard_tables.push_back(tmp_table);
    }
    tmp_table = tmp_table->next;
  }
  if (shard_tables.empty() && shard_tmp_tables.empty()) {
    new_sql.assign(old_sql);
    return;
  }
  new_sql.clear();
  string new_schema_tmp;
  vector<table_link *>::iterator it = shard_tables.begin();
  size_t start_copy_pos = 0;
  for (; it != shard_tables.end(); ++it) {
    new_sql.append(old_sql + start_copy_pos,
                   (*it)->start_pos - 1 - start_copy_pos);
    start_copy_pos = (*it)->end_pos;
    adjust_shard_schema((*it)->join->schema_name, cur_schema, new_schema_tmp,
                        virtual_machine_id, partition_id);
    new_sql.append("`");
    new_sql.append(new_schema_tmp.c_str());
    new_sql.append("`.`");
    new_sql.append((*it)->join->table_name);
    new_sql.append("`");
  }
  new_sql.append(old_sql + start_copy_pos);
  LOG_DEBUG(
      "After replace the shard machine schema, sql [%s] is adjusted to [%s].\n",
      old_sql, new_sql.c_str());
  if (!shard_tmp_tables.empty()) {
    string new_tb_tmp;
    vector<table_link *>::iterator it2 = shard_tmp_tables.begin();
    for (; it2 != shard_tmp_tables.end(); ++it2) {
      adjust_tmp_shard_table((*it2)->join->table_name, new_tb_tmp,
                             virtual_machine_id, partition_id);
      string old_tb((*it2)->join->table_name);
      replace_tmp_table_name(new_sql, old_tb, new_tb_tmp);
    }
    LOG_DEBUG(
        "After replace the tmp shard machine schema, sql is adjusted to "
        "[%s].\n",
        new_sql.c_str());
  }
}

inline PartitionedTable *get_part_ds_from_table(table_link *par_tb,
                                                const char *schema) {
  Backend *backend = Backend::instance();
  const char *schema_name =
      par_tb->join->schema_name ? par_tb->join->schema_name : schema;

  const char *table_name = par_tb->join->table_name;

  DataSpace *ds = backend->get_data_space_for_table(schema_name, table_name);
  return (PartitionedTable *)ds;
}

/* Whether two partition tables are using the same deplay topo*/
inline bool is_virtual_map_equal(PartitionedTable *pt1, PartitionedTable *pt2) {
  // check whether pt1 and pt2 with the virtual partition lay out originally,
  // which means the virtual_partition_id%real_parition_id=0
  if (pt1->is_virtual_map_nature() && pt2->is_virtual_map_nature()) return true;

  map<unsigned int, unsigned int> vi1 = pt1->get_virtual_map();
  map<unsigned int, unsigned int> vi2 = pt2->get_virtual_map();
  if (vi1.size() != vi2.size()) return false;
  // TODO: use a backend map to store the equal/unequal partition table space,
  // which will be deleted in the migration and insert for a new equal/unequal
  // find in issue #3412.
  map<unsigned int, unsigned int>::iterator it = vi1.begin();
  for (; it != vi1.end(); ++it) {
    if (it->second != vi2[it->first]) return false;
  }
  return true;
}
bool is_partition_table_cover(PartitionedTable *pt1, PartitionedTable *pt2) {
  if (pt1->get_key_num() != pt2->get_key_num()) return false;
  if (pt1->get_partition_type() != pt2->get_partition_type()) return false;
  if (pt1->get_partition_scheme() != pt2->get_partition_scheme()) return false;
  if (!is_virtual_map_equal(pt1, pt2)) return false;
  return true;
}

/* value could be key_name or table_name.key_name or
 * schema_name.table_name.key_name*/
bool column_name_equal_key(const char *value, const char *schema_name,
                           size_t schema_length, const char *table_name,
                           size_t table_length, const char *key_name,
                           size_t key_length) {
  const char *tmp = NULL, *tmp2 = NULL;
  size_t len = strlen(value);
  if (len == key_length + table_length + 1) {
    tmp = strchr(value, '.');
    if (!tmp) return false;
    if (strcasecmp(tmp + 1, key_name)) return false;
    if (lower_case_table_names)
      return strncasecmp(value, table_name, tmp - value) == 0;
    else
      return strncmp(value, table_name, tmp - value) == 0;
  }
  if (len == key_length + table_length + schema_length + 2) {
    tmp = strchr(value, '.');
    if (!tmp) return false;
    tmp2 = strchr(tmp + 1, '.');
    if (!tmp2) return false;
    if (strcasecmp(tmp2 + 1, key_name)) return false;
    if (lower_case_table_names) {
      if (strncasecmp(tmp + 1, table_name, tmp2 - tmp - 1)) return false;
      return strncasecmp(value, schema_name, tmp - value) == 0;
    } else {
      if (strncmp(tmp + 1, table_name, tmp2 - tmp - 1)) return false;
      return strncmp(value, schema_name, tmp - value) == 0;
    }
  }
  return false;
}

inline bool column_name_equal_key_alias_without_len(const char *value,
                                                    const char *schema_name,
                                                    const char *table_name,
                                                    const char *key_name,
                                                    const char *alias_name) {
  bool ret = false;
  ret = (strcasecmp(value, key_name) == 0);
  if (ret) return true;
  size_t schema_length = strlen(schema_name);
  if (!schema_length) return false;

  size_t key_length = strlen(key_name);
  size_t table_length = strlen(table_name);
  if (alias_name)  // if table has alias, we only can use alias to find the
                   // equality
    ret = column_name_equal_key(value, schema_name, schema_length, alias_name,
                                strlen(alias_name), key_name, key_length);
  else {
    ret = column_name_equal_key(value, schema_name, schema_length, table_name,
                                table_length, key_name, key_length);
  }
  return ret;
}

inline bool column_name_equal_key_alias(
    const char *value, const char *schema_name, size_t schema_length,
    const char *table_name, size_t table_length, const char *key_name,
    const char *alias_name) {
  bool ret = false;
  ret = (strcasecmp(value, key_name) == 0);
  if (ret) return true;
  if (!schema_length) return false;
  size_t key_length = strlen(key_name);
  if (alias_name)  // if table has alias, we only can use alias to find the
                   // equality
    ret = column_name_equal_key(value, schema_name, schema_length, alias_name,
                                strlen(alias_name), key_name, key_length);
  else {
    ret = column_name_equal_key(value, schema_name, schema_length, table_name,
                                table_length, key_name, key_length);
  }
  return ret;
}

/* check whether this str_expr is possible belonging to a (sub)condition of
 * a "where" or "having".*/
condition_type check_condition_type(StrExpression *expr) {
  if (!expr->parent) return CONDITION_TYPE_NON;

  expr_type type = expr->parent->type;
  switch (type) {
    case EXPR_LIKE:
    case EXPR_NOTLIKE:
    case EXPR_IN:
    case EXPR_NOT_IN:
    case EXPR_IS:
    case EXPR_IS_NOT:
    case EXPR_REGEXP:
    case EXPR_NOTREGEXP:
      return CONDITION_TYPE_RANDOM;
    case EXPR_EQ:
      return CONDITION_TYPE_EQUAL;
    case EXPR_GR:
    case EXPR_LESS:
    case EXPR_GE:
    case EXPR_LESSE:
    case EXPR_NE:
      return CONDITION_TYPE_RANGE;
    default:
      return CONDITION_TYPE_NON;
  }
  return CONDITION_TYPE_NON;
}

RangeType check_range_type(StrExpression *expr, bool is_left) {
  expr_type type = expr->parent->type;
  switch (type) {
    case EXPR_GR:
      if (!is_left) return RANGE_TYPE_LT;
      return RANGE_TYPE_GT;
    case EXPR_LESS:
      if (!is_left) return RANGE_TYPE_GT;
      return RANGE_TYPE_LT;
    case EXPR_GE:
      if (!is_left) return RANGE_TYPE_LE;
      return RANGE_TYPE_GE;
    case EXPR_LESSE:
      if (!is_left) return RANGE_TYPE_GE;
      return RANGE_TYPE_LE;
    default:
      return RANGE_TYPE_EQ;
  }
}

inline const char *Statement::get_str_from_simple_value(Expression *expr) {
  if (expr->type == EXPR_INT) return ((IntExpression *)expr)->str_value;
  if (expr->type == EXPR_STRING) return ((StrExpression *)expr)->str_value;
  if (expr->type == EXPR_FLOAT) return ((FloatExpression *)expr)->str_value;
  if (expr->type == EXPR_BOOL) return ((BoolExpression *)expr)->str_value;
  return NULL;
}

inline const char *Statement::get_gmp_from_simple_value(Expression *expr) {
  if (expr->type == EXPR_INT) return ((IntExpression *)expr)->str_value;
  if (expr->type == EXPR_FLOAT) return ((FloatExpression *)expr)->str_value;
  if (expr->type == EXPR_BOOL) return ((BoolExpression *)expr)->str_value;

  try {
    if (expr->is_null()) return NULL;
  } catch (...) {
    return NULL;
  }

#ifdef DEBUG
  LOG_DEBUG("expr type is %d\n", expr->type);
#endif
  if (expr->type == EXPR_STR) return NULL;
  GMP_Expression gmp_expr;
  mpf_init2(gmp_expr.value, DECIMAL_STORE_BIT);
  try {
    expr->get_gmp_value(&gmp_expr);
  } catch (...) {
    mpf_clear(gmp_expr.value);
    return NULL;
  }

  char *expr_str = new char[GMP_N_DIGITS + 5];
  gmp_sprintf(expr_str, "%.Ff", gmp_expr.value);
  mpf_clear(gmp_expr.value);
  simple_expression.push_back(expr_str);
  need_clean_simple_expression = true;
#ifdef DEBUG
  LOG_DEBUG("gmp value = [%s]\n", expr_str);
#endif
  return (const char *)expr_str;
}

inline const char *Statement::expr_is_simple_value(Expression *expr) {
  if (expr->type == EXPR_INT) return ((IntExpression *)expr)->str_value;
  if (expr->type == EXPR_STRING) return ((StrExpression *)expr)->str_value;
  if (expr->type == EXPR_FLOAT) return ((FloatExpression *)expr)->str_value;
  if (expr->type == EXPR_BOOL) return ((BoolExpression *)expr)->str_value;

  try {
    if (expr->is_null()) {
      if (expr->type == EXPR_NULL) {
        // use 0 rather than NULL, in case the partition method is mod
        return null_key_str.c_str();
      }
      return NULL;
    }
  } catch (...) {
    return NULL;
  }

#ifdef DEBUG
  LOG_DEBUG("expr type is %d\n", expr->type);
#endif
  if (expr->type == EXPR_STR) return NULL;
  GMP_Expression gmp_expr;
  mpf_init2(gmp_expr.value, DECIMAL_STORE_BIT);
  try {
    expr->get_gmp_value(&gmp_expr);
  } catch (...) {
    mpf_clear(gmp_expr.value);
    return NULL;
  }
  char *expr_str = new char[GMP_N_DIGITS + 5];
  gmp_sprintf(expr_str, "%.Ff", gmp_expr.value);
  mpf_clear(gmp_expr.value);
  simple_expression.push_back(expr_str);
  need_clean_simple_expression = true;
#ifdef DEBUG
  LOG_DEBUG("gmp value = [%s]\n", expr_str);
#endif
  return (const char *)expr_str;
}

inline Expression *get_peer(Expression *expr) {
  CompareExpression *cmp_expr = (CompareExpression *)expr->parent;
  const Expression *peer = NULL;
  peer = cmp_expr->left == expr ? cmp_expr->right : cmp_expr->left;

  return (Expression *)peer;
}

inline Expression *get_bool_binary_peer(Expression *expr) {
  BoolBinaryExpression *cmp_expr = (CompareExpression *)expr->parent;
  const Expression *peer = NULL;
  peer = cmp_expr->left == expr ? cmp_expr->right : cmp_expr->left;

  return (Expression *)peer;
}

inline pair<Expression *, bool> get_range_peer(Expression *expr) {
  CompareExpression *cmp_expr = (CompareExpression *)expr->parent;
  Expression *peer = cmp_expr->left;
  bool is_left;
  if (peer == expr) {
    peer = cmp_expr->right;
    is_left = true;
  } else {
    peer = cmp_expr->left;
    is_left = false;
  }

  pair<Expression *, bool> peer_pair(peer, is_left);
  return peer_pair;
}

inline bool is_column(Expression *expr) {
  /* TODO: EXPR_STR contains not only column name, but also routine name, and
   * other special names. Fix it to identify the column name.*/
  if (expr->type != EXPR_STR && expr->type != EXPR_UNIT) return false;
  return true;
}

inline bool expr_equal_alias(Expression *expr, const char *alias) {
  if (expr->type == EXPR_STR && alias != NULL) {
    string str;
    const char *col_pos;
    expr->to_string(str);
    col_pos = str.c_str();

    if (strcasecmp(col_pos, alias) == 0) {
      return true;
    }
  }

  return false;
}

inline void split_column_expr(string value, string &schema_name,
                              string &table_name, string &column_name) {
  unsigned int first_dot_position = value.find(".");
  unsigned int second_dot_position = value.find(".", first_dot_position + 1);
  if (first_dot_position == (unsigned)string::npos) {
    column_name = value;
  } else if (second_dot_position == (unsigned)string::npos) {
    table_name = value.substr(0, first_dot_position);
    column_name = value.substr(first_dot_position + 1);
  } else {
    schema_name = value.substr(0, first_dot_position);
    table_name = value.substr(first_dot_position + 1,
                              second_dot_position - first_dot_position - 1);
    column_name = value.substr(second_dot_position + 1);
  }
}

inline bool expr_equal_column(Expression *expr1, Expression *expr2) {
  if (expr1->type == EXPR_STR && expr2->type == EXPR_STR) {
    string value1, schema1, table1, column1;
    string value2, schema2, table2, column2;
    expr1->to_string(value1);
    expr2->to_string(value2);
    split_column_expr(value1, schema1, table1, column1);
    split_column_expr(value2, schema2, table2, column2);
    if (!schema1.empty() && !schema2.empty()) {
      if (lower_case_table_names &&
          strcasecmp(schema1.c_str(), schema2.c_str()))
        return false;
      if (!lower_case_table_names && strcmp(schema1.c_str(), schema2.c_str()))
        return false;
    }
    if (!table1.empty() && !table2.empty()) {
      if (lower_case_table_names && strcasecmp(table1.c_str(), table2.c_str()))
        return false;
      if (!lower_case_table_names && strcmp(table1.c_str(), table2.c_str()))
        return false;
    }
    if (!strcasecmp(column1.c_str(), column2.c_str())) return true;
  }
  return false;
}

int item_in_select_fields_pos(Expression *expr, record_scan *rs) {
  field_item *field = rs->field_list_head;
  int column_index = 0;

  while (field) {
    if (field->field_expr && (field->field_expr->is_equal(expr) ||
                              expr_equal_alias(expr, field->alias) ||
                              expr_equal_column(field->field_expr, expr))) {
      return column_index;
    }
    ++column_index;
    field = field->next;
  }

  return COLUMN_INDEX_UNDEF;
}

inline bool cmp_peer_is_column(Expression *expr) {
  return is_column(get_peer(expr));
}

ConditionAndOr is_and_or_condition(Expression *cond, record_scan *rs) {
  Expression *condition = rs->condition;

  Expression *tmp = cond;

  ConditionAndOr ret = CONDITION_AND;

  while (tmp->parent) {
    tmp = tmp->parent;
    if (tmp->type == EXPR_OR)
      ret = CONDITION_OR;
    else if (tmp->type != EXPR_AND) {
      /*The type should be EXPR_AND or EXPR_OR, otherwise the query result may
       * be wrong.
       *
       * For example:
       *
       *  select * from t1 where ((c5 =1) and (c6=1)) =0;
       *
       *  We should not use c5 or c6 for and_condition*/
      ret = CONDITION_NO_COND;
      return ret;
    }
  }

  if (tmp != condition && !tmp->is_join_condition) ret = CONDITION_NO_COND;

  return ret;
}

bool is_not_condition(Expression *cond) {
  Expression *tmp = cond;

  while (tmp->parent) {
    tmp = tmp->parent;
    if (tmp->type == EXPR_NOT) return true;
  }

  return false;
}

inline bool is_acceptable_join_par_table(join_node *join) {
  join_node *parent = join->upper;
  if (!parent) return true;
  if ((parent->join_bit & JT_RIGHT) && parent->right != join) return false;
  return is_acceptable_join_par_table(parent);
}

/*This function only check current record scan and its parent, and do not
 * consider the parent of parent record_scan.*/
bool is_subquery_in_select_list(record_scan *rs) {
  if (rs->subquerytype == SUB_SELECT_ONE_VALUE && rs->condition_belong) {
    Expression *tmp = rs->condition_belong;
    while (tmp->parent) tmp = tmp->parent;
    if (tmp == rs->upper->condition || tmp == rs->upper->having) return false;
    return true;
  }
  return false;
}

inline DataServer *get_write_server_of_source(DataSource *source) {
  DataServer *ret = NULL;
  if (source->is_load_balance()) {
    LoadBalanceDataSource *lb = (LoadBalanceDataSource *)source;
    ret = get_write_server_of_source(lb->get_data_sources()[0]);
  } else {
    ret = source->get_master_server();
  }
  LOG_DEBUG("Get dataserver [%s%@] from source [%s%@].\n", ret->get_name(), ret,
            source->get_name(), source);
  return ret;
}

bool is_simple_expression(Expression *expr) {
  if (expr->type == EXPR_INT || expr->type == EXPR_STRING ||
      expr->type == EXPR_FLOAT || expr->type == EXPR_BOOL)
    return true;
  return false;
}

const char *get_simple_expression_value(Expression *expr) {
  if (expr->type == EXPR_INT) return ((IntExpression *)expr)->str_value;
  if (expr->type == EXPR_STRING) return ((StrExpression *)expr)->str_value;
  if (expr->type == EXPR_FLOAT) return ((FloatExpression *)expr)->str_value;
  if (expr->type == EXPR_BOOL) return ((BoolExpression *)expr)->str_value;
  return NULL;
}

void trace_condition_find_or_expression(Expression *expr,
                                        vector<Expression *> *or_expr) {
  if (expr->type == EXPR_AND) {
    BoolBinaryCaculateExpression *and_expr =
        (BoolBinaryCaculateExpression *)expr;
    trace_condition_find_or_expression(and_expr->left, or_expr);
    trace_condition_find_or_expression(and_expr->right, or_expr);
  } else if (expr->type == EXPR_OR) {
    or_expr->push_back(expr);
  }
}

vector<Expression *> find_top_or_expression(Expression *expr) {
  vector<Expression *> ret;
  trace_condition_find_or_expression(expr, &ret);
  return ret;
}

bool check_need_condition_or_equality(Expression *expr, const char *schema_name,
                                      const char *table_name,
                                      const char *key_name,
                                      const char *alias_name,
                                      vector<const char *> *key_value_vector) {
  if (!expr) return false;
  if (expr->type == EXPR_OR) {
    BoolBinaryCaculateExpression *or_expr =
        (BoolBinaryCaculateExpression *)expr;
    bool ret1 = check_need_condition_or_equality(or_expr->left, schema_name,
                                                 table_name, key_name,
                                                 alias_name, key_value_vector);
    bool ret2 = check_need_condition_or_equality(or_expr->right, schema_name,
                                                 table_name, key_name,
                                                 alias_name, key_value_vector);
    return ret1 && ret2;
  } else if (expr->type == EXPR_EQ) {
    CompareExpression *eq_expr = (CompareExpression *)expr;
    if (eq_expr->left->type == EXPR_STR &&
        is_simple_expression(eq_expr->right)) {
      StrExpression *str_expr = (StrExpression *)eq_expr->left;
      if (column_name_equal_key_alias_without_len(str_expr->str_value,
                                                  schema_name, table_name,
                                                  key_name, alias_name)) {
        const char *right_value = get_simple_expression_value(eq_expr->right);
        key_value_vector->push_back(right_value);
        return true;
      }
    }
    if (is_simple_expression(eq_expr->left) &&
        eq_expr->right->type == EXPR_STR) {
      StrExpression *str_expr = (StrExpression *)eq_expr->right;
      if (column_name_equal_key_alias_without_len(str_expr->str_value,
                                                  schema_name, table_name,
                                                  key_name, alias_name)) {
        const char *left_value = get_simple_expression_value(eq_expr->left);
        key_value_vector->push_back(left_value);
        return true;
      }
    }
  }
  return false;
}

vector<const char *> get_or_condition_key_values(Expression *expr,
                                                 const char *schema_name,
                                                 const char *table_name,
                                                 const char *key_name,
                                                 const char *alias_name) {
  vector<const char *> key_value_vector;
  if (!expr) return key_value_vector;
  vector<Expression *> or_exprs = find_top_or_expression(expr);
  vector<Expression *>::iterator it = or_exprs.begin();
  for (; it != or_exprs.end(); ++it) {
    bool can_find_equality = check_need_condition_or_equality(
        *it, schema_name, table_name, key_name, alias_name, &key_value_vector);
    if (can_find_equality && !key_value_vector.empty()) {
      return key_value_vector;
    }
  }

  key_value_vector.clear();
  return key_value_vector;
}

void Statement::check_column_name(set<string> *valid_columns,
                                  set<string> &curr_column_names,
                                  const char *column_name,
                                  const char *full_table_name) {
  string s(column_name);
  boost::to_lower(s);
  if (!valid_columns->count(s)) {
    char msg[512];
    snprintf(msg, sizeof(msg),
             "Unknown column '%s' in 'field list' of table %s", column_name,
             full_table_name);
    LOG_ERROR("%s\n", msg);
    throw dbscale::sql::SQLError(msg, "42S22", ERROR_UNKNOWN_COLUMN_CODE);
  }
  if (!curr_column_names.count(s)) {
    curr_column_names.insert(s);
  } else {
    char msg[512];
    snprintf(msg, sizeof(msg), "Column '%s' specified twice of table %s",
             column_name, full_table_name);
    LOG_ERROR("%s\n", msg);
    throw dbscale::sql::SQLError(msg, "42000", ERROR_DUPLICATE_COLUMN_CODE);
  }
}

void Statement::check_column_count_name(PartitionedTable *part_space,
                                        name_item *list, int &column_count) {
  column_count = 0;
  name_item *head = list;
  name_item *tmp = head;
  set<string> curr_column_names;

  ACE_RW_Mutex *tc_lock = NULL;
  try {
    tc_lock = part_space->get_table_columns_lock(full_table_name);
    tc_lock->acquire_read();
    set<string> *valid_columns = part_space->get_table_columns(full_table_name);
    do {
      check_column_name(valid_columns, curr_column_names, tmp->name,
                        full_table_name.c_str());
      tmp = tmp->next;
    } while (tmp != head);
    tc_lock->release();
  } catch (...) {
    tc_lock->release();
    throw;
  }
  column_count = curr_column_names.size();
}

void Statement::check_column_count_name(PartitionedTable *part_space,
                                        Expression *assign_list) {
  set<string> curr_column_names;
  ListExpression *expr_list = (ListExpression *)assign_list;
  expr_list_item *head = expr_list->expr_list_head;
  expr_list_item *tmp = head;
  CompareExpression *cmp_tmp = NULL;
  StrExpression *str_tmp = NULL;
  ACE_RW_Mutex *tc_lock = NULL;

  try {
    tc_lock = part_space->get_table_columns_lock(full_table_name);
    tc_lock->acquire_read();

    set<string> *valid_columns = part_space->get_table_columns(full_table_name);
    do {
      cmp_tmp = (CompareExpression *)tmp->expr;
      str_tmp = (StrExpression *)cmp_tmp->left;
      check_column_name(valid_columns, curr_column_names, str_tmp->str_value,
                        full_table_name.c_str());
      tmp = tmp->next;
    } while (tmp != head);
    tc_lock->release();
  } catch (...) {
    tc_lock->release();
    throw;
  }
}

void release_avg_list(list<AvgDesc> &avg_list);
Statement::~Statement() {
  map<record_scan *, map<const char *, RangeValues *> >::iterator it_rs;
  for (it_rs = record_scan_par_key_ranges.begin();
       it_rs != record_scan_par_key_ranges.end(); ++it_rs) {
    map<const char *, RangeValues *> range_map = it_rs->second;
    map<const char *, RangeValues *>::iterator it;
    for (it = range_map.begin(); it != range_map.end(); ++it) {
      delete it->second;
    }
    record_scan_par_key_ranges[it_rs->first].clear();
  }

  FieldExpression *field_expr;
  while (!new_field_expressions.empty()) {
    field_expr = new_field_expressions.front();
    new_field_expressions.pop_front();
    delete field_expr;
  }

  while (!re_parser_stmt_list.empty()) {
    Statement *tmp = re_parser_stmt_list.front();
    tmp->free_resource();
    delete tmp;
    re_parser_stmt_list.pop_front();
  }

  stmt_dynamic_add_str_cleanup();
  clear_simple_expression();

  while (!union_all_stmts.empty()) {
    union_all_stmts.front()->free_resource();
    delete union_all_stmts.front();
    union_all_stmts.front() = NULL;
    union_all_stmts.pop_front();
  }
  vector<SeparatedExecNode *>::iterator it = exec_nodes.begin();
  for (; it != exec_nodes.end(); ++it) {
    (*it)->clean_node();
    delete (*it);
  }
  exec_nodes.clear();
  map<record_scan *, vector<vector<TableStruct *> *> *>::iterator it_table =
      record_scan_join_tables.begin();
  for (; it_table != record_scan_join_tables.end(); ++it_table) {
    vector<vector<TableStruct *> *>::iterator it_table_vector =
        it_table->second->begin();
    for (; it_table_vector != it_table->second->end(); ++it_table_vector) {
      vector<TableStruct *>::iterator it_table_struct =
          (*it_table_vector)->begin();
      for (; it_table_struct != (*it_table_vector)->end(); ++it_table_struct) {
        delete (*it_table_struct);
      }
      delete (*it_table_vector);
    }
    delete it_table->second;
  }
  clear_simple_expression();
  release_avg_list(avg_list);
}
void Statement::clean_separated_node() {
  vector<SeparatedExecNode *>::iterator it = exec_nodes.begin();
  for (; it != exec_nodes.end(); ++it) {
    (*it)->clean_node();
    delete (*it);
  }
  exec_nodes.clear();
  need_clean_exec_nodes = false;
  record_scan_exec_node_map.clear();
}

/* Whether the given partition table's key is equal to any of the equality of
 * the stored key of related record_scan.*/
bool Statement::is_all_par_table_equal(record_scan *rs, table_link *table,
                                       const char *kept_key,
                                       const char *key_name) {
  const char *schema_name =
      table->join->schema_name ? table->join->schema_name : schema;
  const char *table_name = table->join->table_name;
  const char *table_alias = table->join->alias;

  vector<string>::iterator it;
  vector<string> *vec = &record_scan_par_key_equality[rs][kept_key];
  it = vec->begin();
  /*skip the first, which is the key itself and not the equality.  otherwise
   * if the two partition tables' key name is the same, such as c1. dbscale
   * will always consider these two partition tables can be merged.
   */
  if (it != vec->end()) ++it;
  for (; it != vec->end(); ++it) {
    if (column_name_equal_key_alias((*it).c_str(), schema_name,
                                    strlen(schema_name), table_name,
                                    strlen(table_name), key_name, table_alias))
      return true;
  }

  return false;
}

bool Statement::is_CNJ_all_par_table_equal(record_scan *rs, table_link *table1,
                                           table_link *table2,
                                           const char *kept_key,
                                           const char *key_name) {
  const char *schema_name =
      table2->join->schema_name ? table2->join->schema_name : schema;
  const char *table_name = table2->join->table_name;
  const char *table_alias = table2->join->alias;

  const char *schema_name_tmp =
      table1->join->schema_name ? table1->join->schema_name : schema;
  const char *table_name_tmp = table1->join->table_name;
  const char *table_alias_tmp = table1->join->alias;
  string full_column(schema_name_tmp);
  full_column.append(".");
  if (table_alias_tmp)
    full_column.append(table_alias_tmp);
  else
    full_column.append(table_name_tmp);
  full_column.append(".");
  full_column.append(kept_key);

  vector<string>::iterator it;
  vector<string> *vec =
      &record_scan_par_key_equality_full_table[rs][full_column];
  it = vec->begin();
  /*skip the first, which is the key itself and not the equality.  otherwise
   * if the two partition tables' key name is the same, such as c1. dbscale
   * will always consider these two partition tables can be merged.
   */

  if (it != vec->end()) ++it;
  for (; it != vec->end(); ++it) {
    if (column_name_equal_key_alias((*it).c_str(), schema_name,
                                    strlen(schema_name), table_name,
                                    strlen(table_name), key_name, table_alias))
      return true;
  }

  return false;
}

/* get the possible partitions to execute sql.
 *
 * Return value: false, only need some partions, which is stored in par_id.
 *               true, need all partitions.*/
bool Statement::get_partitions_all_par_tables(record_scan *rs,
                                              PartitionMethod *method,
                                              vector<unsigned int> *par_id) {
  /*TODO: some partition method may support range condition, so we need to add
   * more interface to PartitionMethod, and let PartitionMethod to decide how
   * to handle.*/
  table_link *par_tb = one_table_node.only_one_table
                           ? one_table_node.table
                           : record_scan_par_table_map[rs];
  DataSpace *tmp_space = one_table_node.only_one_table
                             ? one_table_node.space
                             : (record_scan_all_table_spaces.count(par_tb)
                                    ? record_scan_all_table_spaces[par_tb]
                                    : NULL);
  if (!tmp_space || !tmp_space->is_partitioned()) {
    LOG_ERROR("Got error when get partitions for table.\n");
    throw Error("Not support sql, fail to get table partitions.");
  }
  PartitionedTable *par_table = (PartitionedTable *)tmp_space;
  vector<const char *> *key_names = par_table->get_key_names();

  if (method->get_type() == PART_HASH && hash_method_cs && key_names &&
      !key_names->empty()) {
    const char *schema_name =
        par_tb->join->schema_name ? par_tb->join->schema_name : schema;
    const char *table_name = par_tb->join->table_name;
    bool is_column_collation_cs_or_bin = false;
    bool is_data_type_string_like = true;
    get_column_collation_using_schema_table(
        schema_name, table_name, key_names->at(0),
        is_column_collation_cs_or_bin, is_data_type_string_like);
    if (is_data_type_string_like && !is_column_collation_cs_or_bin) return true;
  }

  try {
    string replace_null_char = stmt_session->get_query_sql_replace_null_char();
    method->fullfil_part_ids(this, rs, key_names, par_id, replace_null_char);
  } catch (...) {
    clear_simple_expression();
    throw;
  }

  if (rs->need_range && record_scan_par_key_ranges.count(rs)) {
    map<const char *, RangeValues *> range_map = record_scan_par_key_ranges[rs];
    map<const char *, RangeValues *>::iterator it;
    for (it = range_map.begin(); it != range_map.end(); ++it) {
      delete it->second;
    }
    record_scan_par_key_ranges[rs].clear();
  }
  if (need_clean_simple_expression) clear_simple_expression();
  if (par_id->empty()) return true;

  return false;
}

void Statement::clear_simple_expression() {
  vector<char *>::iterator sit;
  LOG_DEBUG("clear_simple_expression\n");
  for (sit = simple_expression.begin(); sit != simple_expression.end(); ++sit) {
    char *tmp = *sit;
    delete[] tmp;
  }
  simple_expression.clear();
  need_clean_simple_expression = false;
}

void Statement::get_partitons_one_record_scan(record_scan *rs,
                                              PartitionMethod *method,
                                              unsigned int partition_num,
                                              vector<unsigned int> *par_ids) {
  bool need_all_partition = true;
  need_all_partition = get_partitions_all_par_tables(rs, method, par_ids);
  if (need_all_partition) {
    par_ids->clear();
    unsigned int i = 0;
    for (; i < partition_num; ++i) par_ids->push_back(i);
    return;
  }
  unsigned int par_ids_size = par_ids->size();
  if (par_ids_size == 0 || par_ids_size > partition_num) {
    LOG_ERROR(
        "Got abnormal number of parittion ids [%d] with"
        " partition_num [%d], the sql is [%s].\n",
        par_ids_size, partition_num, sql);
    throw UnSupportPartitionSQL("Got abnormal number of parittion ids");
  }
}

// split one Str-value into schema, table, and column.
void split_value_into_schema_table_column(string value, string &schema_name,
                                          string &table_name,
                                          string &column_name) {
  vector<string> split_name;
  boost::split(split_name, value, boost::is_any_of("."));
  if (split_name.size() == 1) {
    column_name = split_name.at(0);
  } else if (split_name.size() == 2) {
    table_name = split_name.at(0);
    column_name = split_name.at(1);
  } else {
    schema_name = split_name.at(0);
    table_name = split_name.at(1);
    column_name = split_name.at(2);
  }
}

int get_pos_from_field_list(Expression *expr, record_scan *rs) {
  int position = 0;
  string group_schema;
  string group_table;
  string group_column;
  string local_group_str;
  expr->to_string(local_group_str);
  split_value_into_schema_table_column(local_group_str, group_schema,
                                       group_table, group_column);

  field_item *fi = rs->field_list_head;
  if (fi) {
    do {
      string local_field_str;
      if (fi->field_expr && fi->field_expr->type == EXPR_STR) {
        fi->field_expr->to_string(local_field_str);
        string schema_name;
        string table_name;
        string column_name;
        split_value_into_schema_table_column(local_field_str, schema_name,
                                             table_name, column_name);
        if (!strcasecmp(column_name.c_str(), group_column.c_str()) &&
            !fi->alias) {
          break;
        }
      }
      ++position;
      fi = fi->next;
    } while (fi && fi != rs->field_list_tail);
  }
  LOG_DEBUG("%s pos is %d.\n", local_group_str.c_str(), position);

  return position;
}

string construct_string_seperated_with_comma(set<string> string_set) {
  string ret;
  bool need_comma = false;
  set<string>::iterator it = string_set.begin();
  for (; it != string_set.end(); ++it) {
    if (need_comma) ret.append(", ");
    need_comma = true;
    ret.append(*it);
  }
  return ret;
}

void adjust_key_according_type(set<string> &index_set,
                               map<int, string> &column_type_map) {
  /*the string in column_type_map is like `xmmc` varchar(1100). the string in
   * index_set is like `xmmc`*/
  set<string> ret_set;
  set<string>::iterator it = index_set.begin();
  for (; it != index_set.end(); ++it) {
    if (ret_set.size() > MAX_INDEX_TMP_COLUMNS) break;
    map<int, string>::iterator it2 = column_type_map.begin();
    bool has_insert_key = false;
    for (; it2 != column_type_map.end(); ++it2) {
      if (ret_set.size() > MAX_INDEX_TMP_COLUMNS) break;
      string c_type = it2->second;
      string k_column = *it;
      // k_column.append(" ");
      size_t pos = c_type.find(k_column.c_str());
      if (pos != std::string::npos) {
        if (c_type.find("blob", pos + k_column.length()) != std::string::npos ||
            c_type.find("text", pos + k_column.length()) != std::string::npos) {
          string tmp = *it;
          tmp.append("(50)");
          ret_set.insert(tmp);
          has_insert_key = true;
          break;
        }

        size_t pos1 = c_type.find("varchar", pos + k_column.length());
        if (pos1 == std::string::npos)
          pos1 = c_type.find("char", pos + k_column.length());
        if (pos1 != std::string::npos) {
          size_t start_pos = c_type.find("(", pos1);
          if (start_pos != std::string::npos) {
            size_t end_pos = c_type.find(")", start_pos);
            if (end_pos != std::string::npos &&
                end_pos - start_pos >
                    3) {  // larger 3 means the length number of char/varchar
                          // inside () is larger than 99
              string tmp = *it;
              tmp.append("(50)");
              ret_set.insert(tmp);
              has_insert_key = true;
              break;
            }
          }
        }
      }
    }
    if (!has_insert_key) ret_set.insert(*it);
  }
  index_set = ret_set;
}

/*
 * DBScale 

   21.
 2.rs

   1.
 ReconstructSQL::local_execute_query_to_createoriginal_stmt->get_table_index_for_rs(original_rs->upper,
 default_schema, table_alias, column_vector, tmp_string_map)
   rsalias

   Statement::get_table_index_for_rs
      a. indexrange
      b. index
      c. indexrange
      d. range index = column_vector+range_index
      e.  range index  equality index equality_index
      f. equality index = column_vector+equality_index

   2.
 rscross_join_utils.ccDataMoveJoinInfo::handle_equal_columns

   :
   1.
 10001099
   2. blobtextvarchar/char
    Statement::adjust_key_according_type
   rs
 DataMoveJoinInfo::handle_equal_columnsMAX_INDEX_TMP_COLUMNS

   construct_sql.ccConstructMethod::generate_create_column_related_string
 * */

/* get table index for an record_scan
   The returned sql is the index we recommanded.
   1. find the candidates column of index, range to range_index(col<2),
   simple equal to column_vector(col=2), multiple equality to
   equality_index(tt.col=t.c1)
   2. construct range_index, column_vector, equality_index into index string.
   we construct two indexes, one is for range, one is for equality
   range index = column_vector+range_index.
   if range index contains equality_index, ignore equality_index.
   equality index = column_vector+equality_index.
*/
string Statement::get_table_index_for_rs(record_scan *rs,
                                         const char *table_schema,
                                         const char *table_alias,
                                         set<string> column_vector,
                                         map<int, string> &column_type_map) {
  expr_list_item *str_list_head = rs->str_expr_list_head;
  expr_list_item *tmp = str_list_head;
  StrExpression *str_expr = NULL;
  condition_type cond_type = CONDITION_TYPE_NON;

  set<string> index_set;
  set<string> range_index;
  set<string> equality_index;
  string index;
  if (!tmp) return index;

  // 1. find the candidates column of index
  do {
    str_expr = (StrExpression *)tmp->expr;
    cond_type = check_condition_type(str_expr);
    if (cond_type == CONDITION_TYPE_NON || cond_type == CONDITION_TYPE_RANDOM) {
      tmp = tmp->next;
      continue;
    }
    ConditionAndOr and_or_type = is_and_or_condition(str_expr->parent, rs);
    if (and_or_type != CONDITION_AND) {
      tmp = tmp->next;
      continue;
    }

    bool can_be_key = false;
    string schema_name;
    string table_name;
    string column_name;
    split_value_into_schema_table_column(str_expr->str_value, schema_name,
                                         table_name, column_name);
    // check the column name is in column vector or not
    if (!column_vector.count(column_name)) {
      tmp = tmp->next;
      continue;
    }

    if (schema_name.empty() && table_name.empty()) {
      if (column_vector.count(column_name)) can_be_key = true;
    } else if (schema_name.empty()) {
      if (!strcasecmp(table_schema, schema) &&
          !strcasecmp(table_name.c_str(), table_alias))
        can_be_key = true;
    } else {
      if (!strcasecmp(schema_name.c_str(), table_schema) &&
          !strcasecmp(table_name.c_str(), table_alias))
        can_be_key = true;
    }

    if (!can_be_key) {
      tmp = tmp->next;
      continue;
    }

    // Add ` for column
    string tmp_column_name = "`";
    tmp_column_name.append(column_name);
    tmp_column_name.append("`");
    column_name = tmp_column_name;

    Expression *peer_expr = get_peer(str_expr);
    bool peer_is_simple_value = expr_is_simple_value(peer_expr);
    if (cond_type == CONDITION_TYPE_EQUAL && peer_is_simple_value) {
      index_set.insert(column_name);
      if (equality_index.count(column_name)) equality_index.erase(column_name);
      if (range_index.count(column_name)) range_index.erase(column_name);
    }

    if (cond_type == CONDITION_TYPE_EQUAL && !index_set.count(column_name) &&
        !peer_is_simple_value) {
      equality_index.insert(column_name);
    }

    if (cond_type == CONDITION_TYPE_RANGE && !index_set.count(column_name) &&
        peer_is_simple_value) {
      range_index.insert(column_name);
    }
    tmp = tmp->next;
  } while (tmp != str_list_head);

  adjust_key_according_type(index_set, column_type_map);
  adjust_key_according_type(equality_index, column_type_map);
  adjust_key_according_type(range_index, column_type_map);

  // 2. construct range_index, column_vector, equality_index into index string.

  if (range_index.empty() && equality_index.empty() && !index_set.empty()) {
    index.append(" KEY `ind_where` (");
    string index_set_string = construct_string_seperated_with_comma(index_set);
    index.append(index_set_string);
    index.append(")");
  }

  if (!range_index.empty()) {
    index.append(" KEY `ind_where` (");

    string index_set_string = construct_string_seperated_with_comma(index_set);
    index.append(index_set_string);

    set<string>::iterator it = range_index.begin();
    if (!index_set_string.empty()) index.append(", ");
    index.append(*it);

    index.append(")");
  }

  if (!equality_index.empty()) {
    if (!index.empty()) index.append(",");
    index.append(" KEY `ind_equality` (");
    string index_set_string = construct_string_seperated_with_comma(index_set);
    index.append(index_set_string);

    if (!index_set_string.empty()) index.append(", ");

    string equality_index_string =
        construct_string_seperated_with_comma(equality_index);
    index.append(equality_index_string);
    index.append(")");
  }
  return index;
}

/*
 * Get a vector<all equality str> for each key (it can be a const value or
 * a column name), it stored in map<rs, map<key, vector<all equality str>>>.
 * Also store map<rs, map<key, const value>> (this const value is from
 * 'and'condition) and map<rs, map<key, vector<'or' const value>>>.  When find
 * the first partition table we fullfil the above two map.*/
void Statement::fullfil_par_key_equality(
    record_scan *rs, const char *schema_name, const char *table_name,
    const char *table_alias, const char *key_name,
    vector<string> *added_equality_vector) {
  map<const char *, bool> find_equality;
  expr_list_item *str_list_head = rs->str_expr_list_head;
  expr_list_item *tmp = str_list_head;
  StrExpression *str_expr = NULL;
  condition_type cond_type = CONDITION_TYPE_NON;
  ConditionAndOr and_or_type = CONDITION_AND;
  const char *schema_tmp = schema_name;
  const char *table_tmp = table_name;
  const char *key_tmp = NULL;
  bool need_to_find_or = true;
  bool need_range = rs->need_range;
  record_scan_par_key_values[rs][key_name].clear();
  vector<const char *> or_equality_value = get_or_condition_key_values(
      rs->condition, schema_name, table_name, key_name, table_alias);
  if (!or_equality_value.empty()) {
    record_scan_par_key_values[rs][key_name] = or_equality_value;
  }
  need_clean_record_scan_par_key_equality = true;

  record_scan_par_key_equality[rs][key_name].push_back(key_name);
  vector<string> *vec = &(record_scan_par_key_equality[rs][key_name]);
  if (added_equality_vector && !added_equality_vector->empty())
    vec->insert(vec->end(), added_equality_vector->begin(),
                added_equality_vector->end());
  RangeValues *range_values = NULL;
#ifdef DEBUG
  LOG_DEBUG(
      "record_scan_par_key_equality size is %d with key_name size %d "
      "record_scan_par_key_values is %d.\n",
      record_scan_par_key_equality.size(), vec->size(),
      record_scan_par_key_values.size());
#endif

  if (need_range) {
    if (record_scan_par_key_ranges[rs].count(key_name) != 0) {
      if (record_scan_par_key_ranges[rs][key_name] != NULL) {
        record_scan_par_key_ranges[rs][key_name]->clear();
      }
    }
    record_scan_par_key_ranges[rs][key_name] = NULL;

    if (!tmp) {
      range_values = new RangeValues();
      record_scan_par_key_ranges[rs][key_name] = range_values;
      need_clean_record_scan_par_key_ranges = true;
      return;
    }

    range_values = new RangeValues();
  }

  if (!tmp) {
    return;
  }

  // use to deal with a=1 and b=1
  // 1. while first find a=1, will record the '1' to key_values_set,
  //    after find b=1, push b to keys directly
  // 2. while first find b=1, will record 1 to non_key_values_map,
  //    and b to the map's set, then if find a=1,
  //    push all set's value to keys
  set<string> key_values_set;
  map<string, map<string, const char *> > non_key_values_map;

  unsigned int i = 0;
  for (; i < vec->size(); ++i) {
    string tmp_str = vec->at(i);
    key_tmp = tmp_str.c_str();
    do {
      str_expr = (StrExpression *)tmp->expr;
      cond_type = check_condition_type(str_expr);
      if (cond_type == CONDITION_TYPE_NON) {
        tmp = tmp->next;
        continue;
      }
      if (schema_tmp) {
        if (!column_name_equal_key_alias_without_len(str_expr->str_value,
                                                     schema_tmp, table_tmp,
                                                     key_tmp, table_alias)) {
          // only the first time, try to fullfil the non_key_values_map,
          // for the next loops, we will only use key=1 to find column=1 in the
          // map, cause it will record all non_key=1 in the first loop, it's
          // correct
          if (i == 0 && is_column(str_expr) &&
              cond_type == CONDITION_TYPE_EQUAL &&
              is_and_or_condition(str_expr->parent, rs) == CONDITION_AND) {
            Expression *peer = get_peer(str_expr);
            const char *peer_value = expr_is_simple_value(peer);

            if (peer_value && !is_column(peer)) {
              // if '1' in the key_values_set, when find b=1,
              // will push b to keys' vector,
              // else, will record '1' to non_key_values_map,
              // wait for whether '1' will appear in key=1
              if (key_values_set.count(peer_value)) {
                if (!find_equality.count(str_expr->str_value)) {
                  vec->push_back(str_expr->str_value);
                  find_equality[str_expr->str_value] = true;
                }
              } else {
                if (non_key_values_map.count(peer_value)) {
                  non_key_values_map[peer_value][str_expr->str_value] =
                      str_expr->str_value;
                } else {
                  map<string, const char *> s_map;
                  s_map[str_expr->str_value] = str_expr->str_value;
                  non_key_values_map[peer_value] = s_map;
                }
              }
            }
          }

          tmp = tmp->next;
          continue;
        }
      } else {
        if (strcasecmp(str_expr->str_value, key_tmp)) {
          tmp = tmp->next;
          continue;
        }
      }
      if (!is_column(str_expr)) {
        tmp = tmp->next;
        continue;
      }
      and_or_type = is_and_or_condition(str_expr->parent, rs);
      switch (and_or_type) {
        case CONDITION_NO_COND:
          break;
        case CONDITION_AND: {
          // if and condition belong to a not expr, ignore it anyway.
          if (is_not_condition(str_expr)) {
            break;
          }
          switch (cond_type) {
            case CONDITION_TYPE_RANDOM:
              // ignore it, for and condition we only care about equal in hash,
              // and sequence range in range, if there is no equal or sequence
              // range we will not consider it.
              break;
            case CONDITION_TYPE_RANGE: {
              pair<Expression *, bool> peer_pair = get_range_peer(str_expr);
              Expression *peer = peer_pair.first;
              bool is_left = peer_pair.second;
              const char *peer_value = expr_is_simple_value(peer);

              if (!peer_value || is_column(peer)) break;

              if (need_range) {
                RangeType range_type = check_range_type(str_expr, is_left);
                Range *range = new Range(range_type, peer_value);
                range_values->add_and_range(range);
              }

              if (str_expr->copy_expr != NULL) {
                Expression *copy_expr = str_expr->copy_expr;
                pair<Expression *, bool> peer_pair = get_range_peer(copy_expr);
                Expression *peer = peer_pair.first;
                bool is_left = peer_pair.second;
                const char *peer_value = expr_is_simple_value(peer);

                if (!peer_value || is_column(peer)) break;

                if (need_range) {
                  RangeType range_type = check_range_type(str_expr, is_left);
                  Range *range = new Range(range_type, peer_value);
                  range_values->add_and_range(range);
                }
              }
              break;
            }
            case CONDITION_TYPE_EQUAL: {
              Expression *peer = get_peer(str_expr);
              const char *peer_value = expr_is_simple_value(peer);

              if (!peer_value) {
                if (is_column(peer)) {
                  peer_value = ((StrExpression *)peer)->str_value;
                  if (!peer_value) break;
                  if (find_equality.count(peer_value)) break;

                  vec->push_back(peer_value);
                  find_equality[peer_value] = true;
                }
              } else {
                // Only store one value for 'and condition'
                record_scan_par_key_values[rs][key_name].clear();
                record_scan_par_key_values[rs][key_name].push_back(peer_value);

                // for a=1, will record '1' to key_values_set,
                // and then push all value in non_key_values_map[1] to keys
                key_values_set.insert(peer_value);
                if (non_key_values_map.count(peer_value)) {
                  map<string, const char *>::iterator it_begin =
                      non_key_values_map[peer_value].begin();
                  map<string, const char *>::iterator it_end =
                      non_key_values_map[peer_value].end();
                  map<string, const char *>::iterator it_map;
                  for (it_map = it_begin; it_map != it_end; ++it_map) {
                    const char *col_value = it_map->second;
                    if (!find_equality.count(col_value)) {
                      vec->push_back(col_value);
                      find_equality[col_value] = true;
                    }
                  }
                }

                if (need_range) {
                  Range *range = new Range(RANGE_TYPE_EQ, peer_value);
                  range_values->clear();
                  range_values->add_and_range(range);
                }

                need_to_find_or = false;
              }
              break;
            }
            default:
              break;
          }
          break;
        }
        case CONDITION_OR: {
          if (!need_to_find_or) break;

          if (is_not_condition(str_expr) ||
              cond_type == CONDITION_TYPE_RANDOM) {
            record_scan_par_key_values[rs][key_name].clear();

            if (need_range) {
              range_values->clear();
            }

            need_to_find_or = false;

            break;
          }
          if (cond_type == CONDITION_TYPE_RANGE) {
            record_scan_par_key_values[rs][key_name].clear();

            if (need_range) {
              pair<Expression *, bool> peer_pair = get_range_peer(str_expr);
              Expression *peer = peer_pair.first;
              bool is_left = peer_pair.second;
              const char *peer_value = expr_is_simple_value(peer);

              if (!peer_value || is_column(peer)) {
                range_values->clear();
                need_to_find_or = false;
              } else {
                RangeType range_type = check_range_type(str_expr, is_left);
                Range *range = new Range(range_type, peer_value);
                range_values->add_or_range(range);
              }

              if (str_expr->copy_expr != NULL) {
                Expression *copy_expr = str_expr->copy_expr;
                pair<Expression *, bool> peer_pair = get_range_peer(copy_expr);
                Expression *peer = peer_pair.first;
                bool is_left = peer_pair.second;
                const char *peer_value = expr_is_simple_value(peer);

                if (!peer_value || is_column(peer)) {
                  range_values->clear();
                  need_to_find_or = false;
                } else {
                  RangeType range_type = check_range_type(str_expr, is_left);
                  Range *range = new Range(range_type, peer_value);
                  range_values->add_or_range(range);
                }
              }
            }
            break;
          }
          if (cond_type == CONDITION_TYPE_EQUAL) {
            Expression *peer = get_peer(str_expr);
            const char *peer_value = expr_is_simple_value(peer);

            if (!peer_value || is_column(peer)) {
              record_scan_par_key_values[rs][key_name].clear();
              if (need_range) {
                range_values->clear();
              }
              need_to_find_or = false;
              break;
            } else {
              if (need_range) {
                RangeType range_type = check_range_type(str_expr, true);
                Range *range = new Range(range_type, peer_value);
                range_values->add_or_range(range);
              }
            }
          }
        }
      }
      tmp = tmp->next;
    } while (tmp != str_list_head);

    if (vec->size() == 1) {
      // There is no related equal condition found
      vec->clear();
    }

    if (need_range) {
      record_scan_par_key_ranges[rs][key_name] = range_values;
      need_clean_record_scan_par_key_ranges = true;
    }
    schema_tmp = table_tmp = NULL;
  }
}
void Statement::fullfil_par_key_equality_full_table(
    record_scan *rs, const char *schema_name, const char *table_name,
    const char *table_alias, const char *key_name,
    vector<string> *added_equality_vector) {
  map<const char *, bool> find_equality;
  expr_list_item *str_list_head = rs->str_expr_list_head;
  expr_list_item *tmp = str_list_head;
  StrExpression *str_expr = NULL;
  condition_type cond_type = CONDITION_TYPE_NON;
  ConditionAndOr and_or_type = CONDITION_AND;
  const char *schema_tmp = schema_name;
  const char *table_tmp = table_name;
  const char *key_tmp = NULL;

  if (!tmp) return;

  string full_key_name(schema_name);
  full_key_name.append(".");
  if (table_alias)
    full_key_name.append(table_alias);
  else
    full_key_name.append(table_name);
  full_key_name.append(".");
  full_key_name.append(key_name);

  if (!record_scan_par_key_equality_full_table[rs][full_key_name].empty())
    return;
  record_scan_par_key_equality_full_table[rs][full_key_name].clear();
  record_scan_par_key_equality_full_table[rs][full_key_name].push_back(
      key_name);
  vector<string> *vec =
      &(record_scan_par_key_equality_full_table[rs][full_key_name]);
  if (added_equality_vector && !added_equality_vector->empty())
    vec->insert(vec->end(), added_equality_vector->begin(),
                added_equality_vector->end());

  // use to deal with a=1 and b=1 1. while first find a=1, will record the '1'
  // to key_values_set, after find b=1, push b to keys directly 2. while first
  // find b=1, will record 1 to non_key_values_map, and b to the map's set,
  // then if find a=1, push all set's value to keys
  set<string> key_values_set;
  map<string, map<string, const char *> > non_key_values_map;

  unsigned int i = 0;
  for (; i < vec->size(); ++i) {
    string tmp_str = vec->at(i);
    key_tmp = tmp_str.c_str();
    do {
      str_expr = (StrExpression *)tmp->expr;
      cond_type = check_condition_type(str_expr);
      if (cond_type == CONDITION_TYPE_NON) {
        tmp = tmp->next;
        continue;
      }
      if (schema_tmp) {
        if (!column_name_equal_key_alias_without_len(str_expr->str_value,
                                                     schema_tmp, table_tmp,
                                                     key_tmp, table_alias)) {
          // only the first time, try to fullfil the non_key_values_map,
          // for the next loops, we will only use key=1 to find column=1 in the
          // map, cause it will record all non_key=1 in the first loop, it's
          // correct
          if (i == 0 && is_column(str_expr) &&
              cond_type == CONDITION_TYPE_EQUAL &&
              is_and_or_condition(str_expr->parent, rs) == CONDITION_AND) {
            Expression *peer = get_peer(str_expr);
            const char *peer_value = expr_is_simple_value(peer);

            if (peer_value && !is_column(peer)) {
              // if '1' in the key_values_set, when find b=1,
              // will push b to keys' vector,
              // else, will record '1' to non_key_values_map,
              // wait for whether '1' will appear in key=1
              if (key_values_set.count(peer_value)) {
                if (!find_equality.count(str_expr->str_value)) {
                  vec->push_back(str_expr->str_value);
                  find_equality[str_expr->str_value] = true;
                }
              } else {
                if (non_key_values_map.count(peer_value)) {
                  non_key_values_map[peer_value][str_expr->str_value] =
                      str_expr->str_value;
                } else {
                  map<string, const char *> s_map;
                  s_map[str_expr->str_value] = str_expr->str_value;
                  non_key_values_map[peer_value] = s_map;
                }
              }
            }
          }

          tmp = tmp->next;
          continue;
        }
      } else {
        if (strcasecmp(str_expr->str_value, key_tmp)) {
          tmp = tmp->next;
          continue;
        }
      }
      and_or_type = is_and_or_condition(str_expr->parent, rs);
      switch (and_or_type) {
        case CONDITION_NO_COND:
          break;
        case CONDITION_AND: {
          // if and condition belong to a not expr, ignore it anyway.
          if (is_not_condition(str_expr)) {
            break;
          }
          switch (cond_type) {
            case CONDITION_TYPE_EQUAL: {
              Expression *peer = get_peer(str_expr);
              const char *peer_value = expr_is_simple_value(peer);

              if (!peer_value) {
                if (is_column(peer)) {
                  peer_value = ((StrExpression *)peer)->str_value;
                  if (!peer_value) break;
                  if (find_equality.count(peer_value)) break;

                  vec->push_back(peer_value);
                  find_equality[peer_value] = true;
                }
              } else {
                // for a=1, will record '1' to key_values_set, and then push
                // all value in non_key_values_map[1] to keys
                key_values_set.insert(peer_value);
                if (non_key_values_map.count(peer_value)) {
                  map<string, const char *>::iterator it_begin =
                      non_key_values_map[peer_value].begin();
                  map<string, const char *>::iterator it_end =
                      non_key_values_map[peer_value].end();
                  map<string, const char *>::iterator it_map;
                  for (it_map = it_begin; it_map != it_end; ++it_map) {
                    const char *col_value = it_map->second;
                    if (!find_equality.count(col_value)) {
                      vec->push_back(col_value);
                      find_equality[col_value] = true;
                    }
                  }
                }
              }
              break;
            }
            default:
              break;
          }
          break;
        }
        default:
          break;
      }
      tmp = tmp->next;
    } while (tmp != str_list_head);

    if (vec->size() == 1) {
      // There is no related equal condition found
      vec->clear();
    }

    schema_tmp = table_tmp = NULL;
  }
}

/* Get the parition id of the insert row.*/
unsigned int Statement::get_partition_one_insert_row(
    Expression *column_values, vector<unsigned int> *key_pos_vec,
    int64_t auto_inc_val, int value_count, PartitionMethod *method,
    string replace_null_char) {
  vector<const char *> key_values;
  ListExpression *expr_list = (ListExpression *)column_values;
  expr_list_item *head = expr_list->expr_list_head;
  vector<unsigned int>::iterator it = key_pos_vec->begin();

  for (; it != key_pos_vec->end(); ++it) {
    expr_list_item *tmp = head;
    unsigned int i = 0, key_pos = *it;
    const char *value = NULL;

    /**
     * key_pos == value_count means that the auto_increment key is
     * also a partitioned key but not specified in column list
     */
    if (auto_inc_status != NO_AUTO_INC_FIELD && ((int)key_pos == value_count)) {
#ifdef DEBUG
      LOG_DEBUG(
          "Partition key not specified but is auto_increment field, set it "
          "internally"
          " to calculate partition id.\n");
#endif
      char value_tmp[21];
      sprintf(value_tmp, "%ld", auto_inc_val);
      value = (const char *)value_tmp;
      key_values.push_back(value);
      continue;
    }

    /**
     * set to a proper value if auto_increment_field is 0 or NULL
     */
    if (key_pos == (unsigned int)auto_increment_key_pos &&
        auto_inc_status == AUTO_INC_VALUE_NULL) {
#ifdef DEBUG
      LOG_DEBUG(
          "Partition key is auto_increment field,"
          " since its value is 0/NULL, set it internally to calculate "
          "partition id.\n");
#endif
      char value_tmp[21];
      sprintf(value_tmp, "%ld", auto_inc_val);
      value = (const char *)value_tmp;
      key_values.push_back(value);
      continue;
    }

    for (; i < key_pos; ++i) tmp = tmp->next;
    Expression *expr_key = tmp->expr;
    value = expr_is_simple_value(expr_key);

    if (!value) {
      LOG_ERROR(
          "The partition key value of stmt [%s] is not a literal,"
          "it can not be handled by DBScale.\n",
          sql);
      // TODO: handle this situation by send the stmt "select expr_key"
      throw UnSupportPartitionSQL(
          "The partition key value of stmt is not a literal");
    }
    key_values.push_back(value);
  }

  unsigned int partition_id = 0;
  try {
    if (replace_null_char.empty())
      replace_null_char = stmt_session->get_query_sql_replace_null_char();
    partition_id = method->get_partition_id(&key_values, replace_null_char);
  } catch (...) {
    clear_simple_expression();
    throw;
  }
  if (need_clean_simple_expression) clear_simple_expression();
  return partition_id;
}

/*Get partition id for a insert asign stmt (insert into t set c=1). We assume
 * that the asign_list should not be NULL.*/
unsigned int Statement::get_partition_insert_asign(
    Expression *asign_list, const char *schema_name, size_t schema_len,
    const char *table_name, size_t table_len, const char *table_alias,
    vector<const char *> *key_names, int64_t auto_inc_val,
    PartitionMethod *method) {
  vector<const char *> key_values;
  ListExpression *expr_list = (ListExpression *)asign_list;
  expr_list_item *head = expr_list->expr_list_head;
  vector<const char *>::iterator it = key_names->begin();

  for (; it != key_names->end(); ++it) {
    expr_list_item *tmp = head;
    CompareExpression *cmp_tmp = NULL;
    StrExpression *str_tmp = NULL;
    Expression *value_expr = NULL;
    const char *key_name = *it;

    do {
      cmp_tmp = (CompareExpression *)tmp->expr;
      str_tmp = (StrExpression *)cmp_tmp->left;
      if (column_name_equal_key_alias(str_tmp->str_value, schema_name,
                                      schema_len, table_name, table_len,
                                      key_name, table_alias)) {
        value_expr = cmp_tmp->right;
        break;
      }
      tmp = tmp->next;
    } while (tmp != head);
    const char *value;
    bool part_key_is_auto_inc_key = false;
    if (!auto_inc_key_name.empty()) {
      part_key_is_auto_inc_key = column_name_equal_key_alias(
          auto_inc_key_name.c_str(), schema_name, schema_len, table_name,
          table_len, key_name, table_alias);
    }

    if (!value_expr) {
      if (part_key_is_auto_inc_key) {
#ifdef DEBUG
        LOG_DEBUG(
            "Partition key not specified but is auto_increment field, set it "
            "internally"
            " to calculate partition id.\n");
#endif
        char value_tmp[21];
        sprintf(value_tmp, "%ld", auto_inc_val);
        value = (const char *)value_tmp;
      } else {
        LOG_ERROR(
            "Cannot find partition key [%s] from assign list of sql [%s].\n",
            key_name, sql);
        throw UnSupportPartitionSQL(
            "Cannot find partition key from assign list");
      }
    } else if (part_key_is_auto_inc_key &&
               auto_inc_status == AUTO_INC_VALUE_NULL) {
#ifdef DEBUG
      LOG_DEBUG(
          "Partition key is auto_increment field,"
          " since its value is 0/NULL, set it internally to calculate "
          "partition id.\n");
#endif
      char value_tmp[21];
      sprintf(value_tmp, "%ld", auto_inc_val);
      value = (const char *)value_tmp;
    } else {
      value = expr_is_simple_value(value_expr);
      if (!value) {
        LOG_ERROR(
            "The insert value of key of stmt [%s] is not a literal,"
            "it can not be handled by DBScale.\n",
            sql);
        // TODO: handle this situation by send the stmt "select expr_key"
        throw UnSupportPartitionSQL(
            "The insert value of key of stmt is not a literal");
      }
    }
    key_values.push_back(value);
  }

  unsigned int partition_id = 0;
  try {
    string replace_null_char = stmt_session->get_query_sql_replace_null_char();
    partition_id = method->get_partition_id(&key_values, replace_null_char);
  } catch (...) {
    clear_simple_expression();
    throw;
  }
  if (need_clean_simple_expression) clear_simple_expression();
  return partition_id;
}

bool Statement::get_auto_inc_from_insert_assign(
    Expression *assign_list, const char *schema_name, size_t schema_len,
    const char *table_name, size_t table_len, const char *table_alias,
    expr_list_item **auto_inc_expr_item) {
  *auto_inc_expr_item = NULL;
  expr_list_item *head = ((ListExpression *)assign_list)->expr_list_head;
  expr_list_item *tmp = head;
  CompareExpression *cmp_tmp = NULL;
  StrExpression *str_tmp = NULL;

  do {
    cmp_tmp = (CompareExpression *)tmp->expr;
    str_tmp = (StrExpression *)cmp_tmp->left;
    if (column_name_equal_key_alias(str_tmp->str_value, schema_name, schema_len,
                                    table_name, table_len,
                                    auto_inc_key_name.c_str(), table_alias)) {
      *auto_inc_expr_item = tmp;
      return true;
    }
    tmp = tmp->next;
  } while (tmp != head);

  LOG_DEBUG("Auto_increment field [%s] is not find in assign list\n",
            auto_inc_key_name.c_str());
  return false;
}

void Statement::check_int_value(int64_t parsed_val, int64_t &curr_auto_inc_val,
                                const char *schema_name, const char *table_name,
                                PartitionedTable *part_space,
                                bool is_multi_row_insert, Session *s) {
  if (parsed_val == LLONG_MAX || parsed_val == LLONG_MIN) {
    part_space->rollback_auto_inc_value(full_table_name, old_auto_inc_val,
                                        is_multi_row_insert);
    LOG_ERROR(
        "Auto_increment value of stmt [%s] is not valid, value out of range.\n",
        sql);
    throw Error("Auto_increment value is not valid, value out of range.");
  } else if (parsed_val == 0) {
    set_auto_inc_status(AUTO_INC_VALUE_NULL);
  } else if (parsed_val == curr_auto_inc_val) {
    set_auto_inc_status(AUTO_INC_NO_NEED_MODIFY);
  } else if (parsed_val > curr_auto_inc_val) {
    LOG_DEBUG(
        "Set auto_increment_value for table %s.%s to bigger one, old=%Q, "
        "new=%Q\n",
        schema_name, table_name, curr_auto_inc_val, parsed_val);
    part_space->set_auto_increment_value(full_table_name, parsed_val);
    curr_auto_inc_val = parsed_val;
    set_auto_inc_status(AUTO_INC_NO_NEED_MODIFY);
  } else if (parsed_val < curr_auto_inc_val) {
    part_space->rollback_auto_inc_value(full_table_name, old_auto_inc_val,
                                        is_multi_row_insert);
    if (check_auto_increment_value &&
        !part_space->is_auto_inc_partitioning_key(schema_name, table_name, s)) {
      LOG_ERROR(
          "Auto increment value is not valid with parsed_val %d and "
          "curr_auto_inc_val %d.\n",
          parsed_val, curr_auto_inc_val);
      throw Error(
          "Auto_increment value is not valid."
          " Cannot guarantee uniqueness of auto-increment field if value is"
          " not larger than the last insert id when auto-increment field is"
          " not the partition key.");
    }
    LOG_DEBUG(
        "Auto_increment value [%Q] is not big enough, but auto_increment field "
        "is also"
        " partition key, so treat as valid.\n",
        parsed_val);
    set_auto_inc_status(AUTO_INC_NO_NEED_MODIFY);
  }
}

// check for save inc value in session for reduce lock.
int64_t Statement::check_inc_int_value(
    int64_t parsed_val, int64_t curr_auto_inc_val, PartitionedTable *part_space,
    Session *session, const char *schema_name, const char *table_name) {
  if (parsed_val == LLONG_MAX || parsed_val == LLONG_MIN) {
    LOG_ERROR(
        "Auto_increment value of stmt [%s] is not valid, value out of range.\n",
        sql);
    throw Error("Auto_increment value is not valid, value out of range.");
  } else if (parsed_val == 0) {
    set_auto_inc_status(AUTO_INC_VALUE_NULL);
    return curr_auto_inc_val;
  } else if (parsed_val > curr_auto_inc_val) {
    LOG_DEBUG(
        "Set auto_increment_value for table %s.%s to bigger one, old=%Q, "
        "new=%Q\n",
        schema_name, table_name, curr_auto_inc_val, parsed_val);
    if (session->set_auto_increment_value(full_table_name, parsed_val)) {
      set_auto_inc_status(AUTO_INC_NO_NEED_MODIFY);
      return parsed_val;
    } else {
      ACE_Thread_Mutex *row_lock = NULL;

      row_lock = part_space->get_row_autoinc_lock(full_table_name);
      if (row_lock) row_lock->acquire();

      int inc_step;  // not used
      curr_auto_inc_val = part_space->get_auto_increment_value(
          full_table_name, schema_name, table_name, 1, inc_step);
      if (curr_auto_inc_val <= parsed_val) {
        part_space->set_auto_increment_value(full_table_name, parsed_val);
        set_auto_inc_status(AUTO_INC_NO_NEED_MODIFY);
        if (row_lock) row_lock->release();
        return parsed_val;
      } else {
        if (row_lock) row_lock->release();
        if (check_auto_increment_value) {
          LOG_ERROR(
              "Auto increment value is not valid with parsed_val %d and "
              "curr_auto_inc_val %d.\n",
              parsed_val, curr_auto_inc_val);
          throw Error(
              "Auto_increment value is not valid."
              " Cannot guarantee uniqueness of auto-increment field if value is"
              " not larger than the last insert id when auto-increment field is"
              " not the partition key.");
        } else {
          set_auto_inc_status(AUTO_INC_NO_NEED_MODIFY);
          return parsed_val;
        }
      }
    }
  } else if (parsed_val == curr_auto_inc_val) {
    set_auto_inc_status(AUTO_INC_NO_NEED_MODIFY);
    return curr_auto_inc_val;
  } else if (parsed_val < curr_auto_inc_val) {
    if (check_auto_increment_value && !part_space->is_auto_inc_partitioning_key(
                                          schema_name, table_name, session)) {
      LOG_ERROR(
          "Auto increment value is not valid with parsed_val %d and "
          "curr_auto_inc_val %d.\n",
          parsed_val, curr_auto_inc_val);
      throw Error(
          "Auto_increment value is not valid."
          " Cannot guarantee uniqueness of auto-increment field if value is"
          " not larger than the last insert id when auto-increment field is"
          " not the partition key.");
    }
    LOG_DEBUG(
        "Auto_increment value [%Q] is not big enough, but auto_increment field "
        "is also"
        " partition key, so treat as valid.\n",
        parsed_val);
    set_auto_inc_status(AUTO_INC_NO_NEED_MODIFY);
  }
  return curr_auto_inc_val;
}

// used only when there is an auto_increment field in partitioned table
int64_t Statement::get_auto_increment_value(PartitionedTable *part_space,
                                            ExecutePlan *plan,
                                            const char *schema_name,
                                            const char *table_name,
                                            Expression *expr, int row_num,
                                            bool is_multi_row_insert) {
  int64_t curr_auto_inc_val = 0;
  if (!multiple_mode &&
      plan->session->is_contain_auto_inc_value(full_table_name)) {
    curr_auto_inc_val = plan->session->get_auto_inc_value(full_table_name);
    is_multi_row_insert = true;
    if (!expr) {
      return curr_auto_inc_val;
    }
    int64_t parsed_val = 0;
    if (expr->type == EXPR_INT) {
      parsed_val = ((IntExpression *)expr)->get_integer_value();
      check_inc_int_value(parsed_val, curr_auto_inc_val, part_space,
                          plan->session, schema_name, table_name);
    } else if (expr->type == EXPR_NULL) {
      set_auto_inc_status(AUTO_INC_VALUE_NULL);
    } else if (expr->type == EXPR_STRING || st.type == STMT_LOAD) {
      const char *str = static_cast<StrExpression *>(expr)->str_value;
      if (is_natural_number(str)) {
        parsed_val = strtoll(str, NULL, 10);
        check_inc_int_value(parsed_val, curr_auto_inc_val, part_space,
                            plan->session, schema_name, table_name);
      } else if (!strcasecmp(str, "NULL")) {
        set_auto_inc_status(AUTO_INC_VALUE_NULL);
      } else {
        LOG_ERROR(
            "The auto_increment field [%s] should be positive INT or NULL\n",
            str);
        throw Error("The auto_increment field should be positive INT or NULL.");
      }
    } else {
      LOG_ERROR(
          "The auto_increment field of stmt [%s] should be positive INT or "
          "NULL\n",
          sql);
      throw Error("The auto_increment field should be positive INT or NULL.");
    }
    return curr_auto_inc_val;
  } else {
    ACE_Thread_Mutex *row_lock = NULL;
    try {
      int insert_times = 0;
      bool updated_part_space_cache = false;
      if (auto_inc_lock_mode == AUTO_INC_LOCK_MODE_INTERLEAVED &&
          !multiple_mode) {
        insert_times = plan->session->get_insert_inc_times(full_table_name);
        if (insert_times > 3) {
          row_num = row_num > INSERT_INC_NUM ? row_num : INSERT_INC_NUM;
          is_multi_row_insert = true;
        }
      }
      if (auto_inc_step != 0 &&
          multi_row_insert_saved_auto_inc_value + auto_inc_step <=
              multi_row_insert_max_valid_auto_inc_value) {
        multi_row_insert_saved_auto_inc_value += auto_inc_step;
        curr_auto_inc_val = multi_row_insert_saved_auto_inc_value;
      } else {
        row_lock = part_space->get_row_autoinc_lock(full_table_name);
        if (row_lock) row_lock->acquire();
        curr_auto_inc_val =
            part_space->get_auto_inc_value(plan->handler, this, schema_name,
                                           table_name, row_num, auto_inc_step);
        if (row_num > 1) {
          multi_row_insert_saved_auto_inc_value = curr_auto_inc_val;
          multi_row_insert_max_valid_auto_inc_value =
              multi_row_insert_saved_auto_inc_value +
              (row_num - 1) * auto_inc_step;
        }
        updated_part_space_cache = true;
      }
      if (!expr) {
        if (row_lock) {
          row_lock->release();
          row_lock = NULL;
        }
        if (auto_inc_lock_mode == AUTO_INC_LOCK_MODE_INTERLEAVED &&
            !multiple_mode && updated_part_space_cache) {
          if (insert_times > 3) {
            multi_row_insert_saved_auto_inc_value =
                multi_row_insert_max_valid_auto_inc_value;
            plan->session->set_auto_inc_value(full_table_name,
                                              curr_auto_inc_val + 1,
                                              row_num - 1, insert_times);
          } else {
            plan->session->set_auto_inc_value(full_table_name, 0, 0,
                                              insert_times + 1);
          }
        }
        return curr_auto_inc_val;
      }
      int64_t parsed_val = 0;
      if (expr->type == EXPR_INT) {
        parsed_val = ((IntExpression *)expr)->get_integer_value();
        if (multiple_mode) {
          if (parsed_val == 0)
            set_auto_inc_status(AUTO_INC_VALUE_NULL);
          else if (instance_option_value["record_auto_increment_delete_value"]
                       .uint_val &&
                   plan->session->mul_can_set_auto_increment_value(
                       full_table_name, parsed_val)) {
            // for oltp test, when in multiple_mode, dbscale should can insert
            // automent value if the value has been deleted by this session
            curr_auto_inc_val = parsed_val;
            set_auto_inc_status(AUTO_INC_NO_NEED_MODIFY);
          } else if (get_session()->has_bridge_mark_sql() &&
                     get_session()
                         ->get_session_option("is_bridge_session")
                         .bool_val) {
            if (parsed_val > curr_auto_inc_val)
              part_space->set_auto_increment_value(full_table_name, parsed_val);
            curr_auto_inc_val = parsed_val;
            set_auto_inc_status(AUTO_INC_NO_NEED_MODIFY);
          } else if (!check_auto_increment_value) {
            curr_auto_inc_val = parsed_val;
            set_auto_inc_status(AUTO_INC_NO_NEED_MODIFY);
          } else {
            LOG_ERROR(
                "In multiple_mode, dbscale do not support partition table set "
                "auto_increment value\n");
            throw Error(
                "In multiple_mode, dbscale do not support partition table set "
                "auto_increment value");
          }
        } else {
          check_int_value(parsed_val, curr_auto_inc_val, schema_name,
                          table_name, part_space, is_multi_row_insert,
                          plan->session);
        }
      } else if (expr->type == EXPR_NULL) {
        set_auto_inc_status(AUTO_INC_VALUE_NULL);
      } else if (expr->type == EXPR_STRING || st.type == STMT_LOAD) {
        const char *str = static_cast<StrExpression *>(expr)->str_value;
        if (is_natural_number(str)) {
          parsed_val = strtoll(str, NULL, 10);
          if (multiple_mode) {
            if (parsed_val == 0)
              set_auto_inc_status(AUTO_INC_VALUE_NULL);
            else if (instance_option_value["record_auto_increment_delete_value"]
                         .uint_val &&
                     plan->session->mul_can_set_auto_increment_value(
                         full_table_name, parsed_val)) {
              // for oltp test, when in multiple_mode, dbscale should can insert
              // automent value if the value has been deleted by this session
              curr_auto_inc_val = parsed_val;
              set_auto_inc_status(AUTO_INC_NO_NEED_MODIFY);
            } else if (get_session()->has_bridge_mark_sql() &&
                       get_session()
                           ->get_session_option("is_bridge_session")
                           .bool_val) {
              if (parsed_val > curr_auto_inc_val)
                part_space->set_auto_increment_value(full_table_name,
                                                     parsed_val);
              curr_auto_inc_val = parsed_val;
              set_auto_inc_status(AUTO_INC_NO_NEED_MODIFY);
            } else if (!check_auto_increment_value) {
              curr_auto_inc_val = parsed_val;
              set_auto_inc_status(AUTO_INC_NO_NEED_MODIFY);
            } else {
              LOG_ERROR(
                  "In multiple_mode, dbscale do not support partition table "
                  "set auto_increment value\n");
              throw Error(
                  "In multiple_mode, dbscale do not support partition table "
                  "set auto_increment value");
            }
          } else {
            check_int_value(parsed_val, curr_auto_inc_val, schema_name,
                            table_name, part_space, is_multi_row_insert,
                            plan->session);
          }
        } else if (!strcasecmp(str, "NULL")) {
          set_auto_inc_status(AUTO_INC_VALUE_NULL);
        } else {
          part_space->rollback_auto_inc_value(full_table_name, old_auto_inc_val,
                                              is_multi_row_insert);
          LOG_ERROR(
              "The auto_increment field should be positive INT or NULL\n");
          throw Error(
              "The auto_increment field should be positive INT or NULL.");
        }
      } else {
        part_space->rollback_auto_inc_value(full_table_name, old_auto_inc_val,
                                            is_multi_row_insert);
        LOG_ERROR(
            "The auto_increment field of stmt [%s] should be positive INT or "
            "NULL\n",
            sql);
        throw Error("The auto_increment field should be positive INT or NULL.");
      }
      if (row_lock) {
        row_lock->release();
        row_lock = NULL;
      }

      if (auto_inc_lock_mode == AUTO_INC_LOCK_MODE_INTERLEAVED &&
          !multiple_mode) {
        if (insert_times > 3) {
          plan->session->set_auto_inc_value(full_table_name,
                                            curr_auto_inc_val + 1, row_num - 1,
                                            insert_times);
        } else {
          plan->session->set_auto_inc_value(full_table_name, 0, 0,
                                            insert_times + 1);
        }
      }
      if (row_num > 1 && auto_inc_step != 0) {
        if (curr_auto_inc_val > multi_row_insert_saved_auto_inc_value) {
          int64_t extra_v =
              curr_auto_inc_val - multi_row_insert_saved_auto_inc_value;
          multi_row_insert_saved_auto_inc_value = curr_auto_inc_val;
          int m = extra_v % auto_inc_step;
          if (m != 0) {
            multi_row_insert_saved_auto_inc_value += (auto_inc_step - m);
          }
        }
      }

      return curr_auto_inc_val;
    } catch (...) {
      if (row_lock) {
        row_lock->release();
        row_lock = NULL;
      }

      throw;
    }
  }
}

int64_t Statement::get_auto_inc_value_one_insert_row(
    PartitionedTable *part_space, ExecutePlan *plan, const char *schema_name,
    const char *table_name, Expression *expr, int row_num) {
  ACE_Thread_Mutex *stmt_lock =
      part_space->get_stmt_autoinc_lock(full_table_name);
  if (stmt_lock) stmt_lock->acquire();
  try {
    int64_t curr_auto_inc_val = get_auto_increment_value(
        part_space, plan, schema_name, table_name, expr, row_num, false);
    if (stmt_lock) stmt_lock->release();
    return curr_auto_inc_val;
  } catch (...) {
    if (stmt_lock) stmt_lock->release();
    throw;
  }
}

int64_t Statement::get_auto_inc_value_multi_insert_row(
    PartitionedTable *part_space, ExecutePlan *plan, const char *schema_name,
    const char *table_name, Expression *expr, int row_num) {
  return get_auto_increment_value(part_space, plan, schema_name, table_name,
                                  expr, row_num, true);
}

void Statement::init_auto_increment_params(ExecutePlan *plan,
                                           const char *schema_name,
                                           const char *table_name,
                                           PartitionedTable *part_space) {
  int alter_table = part_space->set_auto_increment_info(
      plan->handler, this, schema_name, table_name, true, true);
  if (alter_table == 1) {
    Driver *driver = Driver::get_driver();
    driver->acquire_session_mutex();
    set<Session *, compare_session> *session_set = driver->get_session_set();
    set<Session *, compare_session>::iterator it = session_set->begin();
    for (; it != session_set->end(); ++it) {
      (*it)->reset_auto_increment_info(full_table_name);
    }
    driver->release_session_mutex();
  }
  if (auto_inc_status != NO_AUTO_INC_FIELD) {
    auto_increment_key_pos =
        part_space->get_auto_increment_key_pos(full_table_name);
    auto_inc_key_name = part_space->get_auto_increment_key(full_table_name);
  }
}

/**
 * For multiple row insert statement only.
 *
 * roll back last_insert_id to the value before current multiple row
 * insert statement is analyticed;
 * we could also rollback auto_increment_value to the same point if
 * current auto_inc_lock_mode is TRADITIONAL or CONSECUTIVE, but not
 * if INTERLEAVED cause we do not hold the mutex in that condition.
 */
void Statement::rollback_auto_inc_params(ExecutePlan *plan,
                                         PartitionedTable *part_table) {
  part_table->rollback_auto_inc_value(full_table_name, old_auto_inc_val, true);
  int enable_last_insert_id_session =
      plan->handler->get_session()
          ->get_session_option("enable_last_insert_id")
          .int_val;
  if (enable_last_insert_id_session)
    plan->handler->get_session()->set_last_insert_id(old_last_insert_id);
}

/* Get the key position from the given column list, such as insert. The
 * position is start from 0. For this function, we assume that the list is not
 * NULL.*/
unsigned int Statement::get_key_pos_from_column_list(
    name_item *list, const char *schema_name, size_t schema_len,
    const char *table_name, size_t table_len, const char *table_alias,
    const char *key_name) {
  name_item *head = list;
  name_item *tmp = head;
  unsigned int pos = 0;

  do {
    if (column_name_equal_key_alias(tmp->name, schema_name, schema_len,
                                    table_name, table_len, key_name,
                                    table_alias)) {
#ifdef DEBUG
      LOG_DEBUG("Find key pos [%d] from column list of sql [%s].\n", (int)pos,
                sql);
#endif
      return pos;
    }
    tmp = tmp->next;
    ++pos;
  } while (tmp != head);

  if (auto_inc_status == AUTO_INC_NOT_SPECIFIED) {
    if (column_name_equal_key_alias(auto_inc_key_name.c_str(), schema_name,
                                    schema_len, table_name, table_len, key_name,
                                    table_alias)) {
      /**
       * partition field is also auto_increment field,
       * but not specified in column list, since pos's value marks
       * end() of key_vec, use it to mark such situation.
       */
      return pos;
    }
  }

  LOG_ERROR("Cannot find partition key [%s] from column list of sql [%s].\n",
            key_name, sql);
  throw UnSupportPartitionSQL(
      "Cannot find partition key from column list of sql");
  return 0;
}

int Statement::get_auto_inc_pos_from_column_list(
    name_item *list, const char *schema_name, size_t schema_len,
    const char *table_name, size_t table_len, const char *table_alias) {
  name_item *head = list;
  name_item *tmp = head;
  int pos = 0;
  do {
    if (column_name_equal_key_alias(tmp->name, schema_name, schema_len,
                                    table_name, table_len,
                                    auto_inc_key_name.c_str(), table_alias)) {
      return pos;
    }
    tmp = tmp->next;
    ++pos;
  } while (tmp != head);
#ifdef DEBUG
  LOG_DEBUG(
      "Auto_increment field [%s] not specified in column list, statement is "
      "[%s]\n",
      auto_inc_key_name.c_str(), sql);
#endif
  set_auto_inc_status(AUTO_INC_NOT_SPECIFIED);
  return -1;
}

bool Statement::select_need_divide(record_scan *top_select,
                                   record_scan *partition,
                                   unsigned int dataspace_merge_start,
                                   const char *schema_name, size_t schema_len,
                                   const char *table_name, size_t table_len,
                                   const char *table_alias,
                                   vector<const char *> *key_names) {
  /* If the partition record_scan is not the top one, it and all its upper
   * record_scan, except top record_scan, should not contain
   * order_by|limit|group_by, otherwise this select_part should be divided.*/
  record_scan *tmp = partition;
  while (tmp != top_select) {
    if (tmp->order_by_list || tmp->limit || tmp->group_by_list) {
      LOG_DEBUG(
          "Record scan of partition table contains order_by|limit|group_by,"
          " so the select part of sql[%s] must be divided for execution.",
          sql);
      return true;
    }
    tmp = tmp->upper;
#ifdef DEBUG
    ACE_ASSERT(tmp);
#endif
  }

  /*When Partition record_scan is a sub_select condition,
   If the condition type is 'exists', it can not execute without divided;
   (may allow in the future)
   If the condition type is 'in'|'not in'|'compare', the select list of
   partition record_scan should contain the partition key.*/
  if (partition->condition_belong) {
    Expression *expr = partition->condition_belong->parent;
    if (expr == NULL) return true;
    switch (expr->type) {
      case EXPR_EXISTS: {
        // TODO: we need to ensure that: the condition contains
        //"par.key=[outer_table.column]"
        return true;
        break;
      }
      case EXPR_EQ:
      case EXPR_IN: {
        // select field list should contain all 'par.key'
        bool field_has_all_key = true;
        vector<const char *>::iterator it = key_names->begin();
        for (; it != key_names->end(); ++it) {
          bool field_has_key = false;
          const char *key_name = *it;
          field_item *tmp = partition->field_list_head;
          while (tmp) {
            Expression *expr_field = tmp->field_expr;
            if (expr_field && expr_field->type == EXPR_STR) {
              StrExpression *str_field = (StrExpression *)expr_field;
              if (is_column(str_field)) {
                if (column_name_equal_key_alias(
                        str_field->str_value, schema_name, schema_len,
                        table_name, table_len, key_name, table_alias)) {
                  field_has_key = true;
                  break;
                }
              }
            }
            tmp = tmp->next;
          }
          if (!field_has_key) {
            field_has_all_key = false;
            break;
          }
        }
        if (!field_has_all_key) {
          LOG_DEBUG(
              "Fail to find the partition_key in the field list of"
              " partition record_scan of sql[%s].\n",
              sql);
          return true;
        }
        break;
      }
      default:  // NOT_IN, other compare
      {
        return true;
      }
    }
  }
  /*The select part should only has one dataspace(table or higher level) after
   * merge, otherwise it should be divided.*/
  if (!can_dataspace_merge(dataspace_merge_start)) return true;

  return false;
}

/* Try to merge dataspaces from spaces[dataspace_merge_start] to
 * spaces[spaces.size()]*/
bool Statement::can_dataspace_merge(unsigned int dataspace_merge_start) {
  unsigned int i = dataspace_merge_start;
  DataSpace *merge_space = spaces[i];
  ++i;
  for (; i < spaces.size(); ++i) {
    if (stmt_session->is_dataspace_cover_session_level(merge_space, spaces[i]))
      continue;
    else if (stmt_session->is_dataspace_cover_session_level(spaces[i],
                                                            merge_space))
      merge_space = spaces[i];
    else {
      LOG_DEBUG("Fail to merge the dataspaces of sql [%s].\n", sql);
      return false;
    }
  }
  spaces[dataspace_merge_start] = merge_space;
  need_clean_spaces = true;
  return true;
}

/* Check whether the set list expression contain the given columns*/
bool set_list_contain_key(Expression *update_set_list,
                          vector<const char *> *columns,
                          const char *schema_name, const char *table_name,
                          const char *table_alias, string &key_value) {
  ListExpression *expr_list = (ListExpression *)update_set_list;
  expr_list_item *head = expr_list->expr_list_head;

  vector<const char *>::iterator it = columns->begin();

  for (; it != columns->end(); ++it) {
    expr_list_item *tmp = head;
    CompareExpression *cmp_tmp = NULL;
    StrExpression *str_tmp = NULL;
    const char *key_name = *it;

    do {
      cmp_tmp = (CompareExpression *)tmp->expr;
      str_tmp = (StrExpression *)cmp_tmp->left;
      if (column_name_equal_key_alias(
              str_tmp->str_value, schema_name, strlen(schema_name), table_name,
              strlen(table_name), key_name, table_alias)) {
        Expression *right_expr = cmp_tmp->right;
        const char *str_val = right_expr->str_value;
        key_value = str_val ? str_val : "";
        return true;
      }
      tmp = tmp->next;
    } while (tmp != head);
  }

  return false;
}

bool set_list_contain_key(Expression *update_set_list,
                          vector<string *> *columns, const char *schema_name,
                          const char *table_name, const char *table_alias,
                          string &key_value) {
  vector<const char *> column_values;
  vector<string *>::iterator it = columns->begin();
  for (; it != columns->end(); ++it) column_values.push_back((*it)->c_str());
  return set_list_contain_key(update_set_list, &column_values, schema_name,
                              table_name, table_alias, key_value);
}

bool Statement::check_and_assemble_multi_send_node(ExecutePlan *plan) {
  ACE_UNUSED_ARG(plan);
#ifndef DBSCALE_TEST_DISABLE

  dbscale_test_info *test_info = plan->session->get_dbscale_test_info();
  if (!strcasecmp(test_info->test_case_name.c_str(), "multi_send_node") &&
      !strcasecmp(test_info->test_case_operation.c_str(), "working_thread") &&
      !strcasecmp(sql, "select * FROM test_multi_send_node")) {
    Backend *backend = Backend::instance();
    DataSpace *data_space =
        backend->get_data_space_for_table("test", "test_multi_send_node");

    TransferRule *rule = backend->dispatch_rule;
    /* Construct the dispatch strategy */
    DataSpace *data_space_send =
        backend->get_data_space_for_table("test", "test_multi_send_dataspace");
    vector<const char *> *key_names =
        ((PartitionedTable *)data_space_send)->get_key_names();
    PartitionMethod *method =
        ((PartitionedTable *)data_space_send)->get_partition_method();
    DispatchStrategy *dispatch_strategy = rule->get_dispatch_strategy();
    dispatch_strategy->method = method;
    dispatch_strategy->key_names = key_names;  // key names should be one column
                                               // in table test_multi_send_node
    dispatch_strategy->local_key_name = string(key_names->at(0));
    dispatch_strategy->part_table = (PartitionedTable *)data_space_send;
    dispatch_strategy->dup_table = NULL;

    assemble_dispatch_packet_plan(plan, rule, data_space, data_space_send,
                                  "select * FROM test_multi_send_node", NULL);
    return true;
  } else if (!strcasecmp(test_info->test_case_name.c_str(),
                         "multi_send_node") &&
             !strcasecmp(test_info->test_case_operation.c_str(),
                         "working_thread_dup") &&
             !strcasecmp(sql, "select * FROM test_multi_send_node")) {
    Backend *backend = Backend::instance();
    DataSpace *data_space =
        backend->get_data_space_for_table("test", "test_multi_send_node");

    TransferRule *rule = backend->dispatch_rule;
    /* Construct the dispatch strategy */
    DataSpace *data_space_send =
        backend->get_data_space_for_table("test", "test_multi_send_dataspace");
    DuplicatedTable *dup_tab = new DuplicatedTable(
        (PartitionedTable *)data_space_send, "duplicated", "");
    DispatchStrategy *dispatch_strategy = rule->get_dispatch_strategy();
    dispatch_strategy->dup_table = dup_tab;

    Schema *join_schema = backend->find_schema(TMP_TABLE_SCHEMA);
    join_schema->add_table(dup_tab);

    assemble_dispatch_packet_plan(plan, rule, data_space, data_space_send,
                                  "select * FROM test_multi_send_node", NULL);
    return true;
  } else {
    return false;
  }
#else
  return false;
#endif
}

const char *Statement::adjust_stmt_sql_for_alias(DataSpace *dataspace,
                                                 const char *old_sql) {
  string new_sql_tmp;
  const char *used_sql = old_sql;

  if (!dataspace) {
    LOG_ERROR("Internal error for Statement::adjust_stmt_sql_for_shard.\n");
    throw Error("Internal error for Statement::adjust_stmt_sql_for_shard.");
  }

  if (dataspace->get_dataspace_type() == SCHEMA_TYPE &&
      ((Schema *)dataspace)->get_is_alias()) {
    Parser *parser = Driver::get_driver()->get_parser();
    Statement *new_stmt = parser->parse(old_sql, stmt_allow_dot_in_ident, true,
                                        NULL, NULL, NULL, ctype);
    re_parser_stmt_list.push_back(new_stmt);
    new_stmt->check_and_refuse_partial_parse();

    adjust_alias_schema(old_sql, schema, new_stmt->get_stmt_node(),
                        record_scan_all_table_spaces, new_sql_tmp,
                        (Schema *)dataspace);
    record_shard_sql_str(new_sql_tmp);  // just reuse the shard_sql_vec to store
                                        // the alias schema sql
    used_sql = get_last_shard_sql();
  }

  return used_sql;
}

const char *Statement::adjust_stmt_sql_for_shard(DataSpace *dataspace,
                                                 const char *old_sql) {
  string new_sql_tmp;
  const char *used_sql = old_sql;
  if (!dataspace) {
    LOG_ERROR("Internal error for Statement::adjust_stmt_sql_for_shard.\n");
    throw Error("Internal error for Statement::adjust_stmt_sql_for_shard.");
  }

  if (dataspace->get_virtual_machine_id() > 0) {
    adjust_virtual_machine_schema(dataspace->get_virtual_machine_id(),
                                  dataspace->get_partition_id(), old_sql,
                                  schema, get_latest_stmt_node(),
                                  record_scan_all_table_spaces, new_sql_tmp);
    record_shard_sql_str(new_sql_tmp);
    used_sql = get_last_shard_sql();
  }
  return used_sql;
}

void Statement::check_stmt_sql_for_dbscale_row_id(stmt_type type,
                                                  record_scan *scanner) {
  if (type != STMT_SELECT) return;
  field_item *field = scanner->field_list_head;
  if (!field) return;
  if (field->alias && (!strcasecmp(field->alias, "dbscale_row_id") ||
                       !strcasecmp(field->alias, "dbscale_row_num"))) {
    is_first_column_dbscale_row_id = true;
  }
}

void Statement::assemble_connect_by_plan(ExecutePlan *plan,
                                         DataSpace *dataspace) {
  record_scan *rs = st.scanner;
  string new_sql;
  if (!rs->from_pos) {
    LOG_ERROR("Not support connect by without from table.\n");
    throw Error("Not support connect by without from table.");
  }
  if (rs->from_pos >= strlen(sql)) {
    LOG_ERROR("Fail to get from pos for connect by sql.\n");
    throw Error("Fail to get from pos for connect by sql.");
  }

  new_sql.append(sql + rs->start_pos - 1, rs->from_pos - rs->start_pos);
  list<string>::reverse_iterator it = new_select_items.rbegin();
  for (; it != new_select_items.rend(); ++it) {
    new_sql.append(",");
    new_sql.append(*it);
    new_sql.append(" ");
  }

  new_sql.append(sql + rs->from_pos - 1);
  record_statement_stored_string(new_sql);
  sql = statement_stored_string.back().c_str();
  Statement *new_stmt = NULL;
  re_parser_stmt(&(st.scanner), &new_stmt, sql);

  const char *used_sql = adjust_stmt_sql_for_shard(dataspace, sql);

  ExecuteNode *fetch_node = plan->get_fetch_node(dataspace, used_sql);
  ExecuteNode *conn_node = plan->get_connect_by_node(connect_by_desc);
  conn_node->add_child(fetch_node);
  ExecuteNode *filter_node = plan->get_project_node(new_select_items.size());
  filter_node->add_child(conn_node);
  ExecuteNode *send_node = plan->get_send_node();
  send_node->add_child(filter_node);
  plan->set_start_node(send_node);
}

void Statement::assemble_select_plain_value_plan(ExecutePlan *plan,
                                                 const char *str_value,
                                                 const char *alias_name) {
  LOG_DEBUG("Assemble select plain value plan.\n");
  ExecuteNode *node = plan->get_select_plain_value_node(str_value, alias_name);
  plan->set_start_node(node);
}

void Statement::assemble_direct_exec_plan(ExecutePlan *plan,
                                          DataSpace *dataspace) {
  LOG_DEBUG("Assemble direct exec plan.\n");
  if (plan->get_is_parse_transparent()) {
    ExecuteNode *node = plan->get_direct_execute_node(dataspace, sql);
    plan->set_start_node(node);
    LOG_DEBUG("Assemble sql [%s] parse transparent.\n", sql);
    return;
  }
  if (Backend::instance()->need_deal_with_metadata(st.type, &st))
    st.need_apply_metadata = !is_share_same_server(
        Backend::instance()->get_metadata_data_space(), dataspace);
  if (is_connect_by) {
    assemble_connect_by_plan(plan, dataspace);
    return;
  }

  const char *used_sql = adjust_stmt_sql_for_shard(dataspace, sql);

  if (union_all_sub) {
    ExecuteNode *node = plan->get_fetch_node(dataspace, used_sql);
    union_all_nodes.push_back(node);
  } else {
    if (is_into_outfile()) {
      ExecuteNode *fetch_node = plan->get_fetch_node(dataspace, used_sql);
      ExecuteNode *send_node = plan->get_into_outfile_node();
      send_node->add_child(fetch_node);
      plan->set_start_node(send_node);
    } else if (is_select_into()) {
      ExecuteNode *fetch_node = plan->get_fetch_node(dataspace, used_sql);
      ExecuteNode *send_node = plan->get_select_into_node();
      send_node->add_child(fetch_node);
      plan->set_start_node(send_node);
    } else {
      plan->session->set_is_simple_direct_stmt(true);
      ExecuteNode *node = plan->get_direct_execute_node(dataspace, used_sql);
      plan->set_start_node(node);
    }
  }
}

void Statement::assemble_dbscale_estimate_select_plan(ExecutePlan *plan,
                                                      DataSpace *dataspace,
                                                      Statement *statement) {
  LOG_DEBUG("Assemble estimate select exec plan.\n");
  ExecuteNode *node = plan->get_estimate_select_node(dataspace, sql, statement);
  plan->set_start_node(node);
}

void Statement::assemble_dbscale_estimate_select_partition_plan(
    ExecutePlan *plan, vector<unsigned int> *par_ids,
    PartitionedTable *par_table, const char *sql, Statement *statement) {
  LOG_DEBUG("Assemble estimate select exec plan.\n");
  ExecuteNode *node = plan->get_estimate_select_partition_node(
      par_ids, par_table, sql, statement);
  plan->set_start_node(node);
}

void Statement::assemble_direct_exec_plan(ExecutePlan *plan,
                                          DataSpace *dataspace,
                                          string &customized_sql) {
  LOG_DEBUG("Assemble direct execute plan with generated sql [%s].\n",
            customized_sql.c_str());
  const char *used_sql =
      adjust_stmt_sql_for_shard(dataspace, customized_sql.c_str());

  ExecuteNode *node = plan->get_direct_execute_node(dataspace, used_sql);
  plan->set_start_node(node);
}

void Statement::assemble_forward_master_role_plan(ExecutePlan *plan,
                                                  bool is_slow_query) {
  LOG_DEBUG("Assemble forward master role plan %@ for sql %s.\n", plan, sql);

  // session use db expired(dropped)
  Session *s = plan->session;
  string plan_cur_schema;
  s->get_schema(plan_cur_schema);
  if (enable_acl) {
    if (!s->is_contain_schema(plan_cur_schema)) {
      // operation about other database db
      if (st.table_list_tail && st.table_list_tail->join) {
        const char *schema_name = st.table_list_tail->join->schema_name;
        if (schema_name) {
          // sql schema name is dropped database
          string tmp_schema(schema_name);
          if (!s->is_contain_schema(tmp_schema)) {
            string err;
            err.append(tmp_schema);
            err.append(" doesn't exist");
            throw Error(err.c_str());
          }
          plan->set_forward_sql_target_db(tmp_schema);
        }
      } else if (st.type == STMT_CREATE_DB || st.type == STMT_DROP_DB) {
        plan->set_forward_sql_target_db(default_login_schema);
      } else if (is_dbscale_command(st.type)) {
        if (!s->is_contain_schema(plan_cur_schema))
          plan->set_forward_sql_target_db(default_login_schema);
      } else {
        throw Error("No database selected", ERROR_NO_DATABASE_SELECTED_CODE);
      }
    }
  }
  ExecuteNode *node = plan->get_forward_master_role_node(sql, is_slow_query);
  plan->set_start_node(node);
  plan->set_is_forward_plan();
}

void Statement::assenble_dbscale_reset_zoo_info_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble dbscale reset zoo info node.\n");
  ExecuteNode *node = plan->get_reset_zoo_info_node();
  plan->set_start_node(node);
}

void Statement::assemble_one_partition_plan(ExecutePlan *plan,
                                            DataSpace *dataspace) {
  LOG_DEBUG("Assemble one partition plan.\n");
  assemble_direct_exec_plan(plan, dataspace);
  if (st.type <= STMT_DDL_START || st.type >= STMT_DDL_END) {
    plan->session->set_execute_plan_touch_partition_nums(1);
  }
}

void Statement::assemble_one_partition_plan(ExecutePlan *plan,
                                            DataSpace *dataspace,
                                            string &customized_sql) {
  LOG_DEBUG("Assemble one partition plan with generated sql [%s].\n",
            customized_sql.c_str());
  assemble_direct_exec_plan(plan, dataspace, customized_sql);
}

void clean_up_execute_nodes(vector<ExecuteNode *> *nodes) {
  vector<ExecuteNode *>::iterator it = nodes->begin();
  for (; it != nodes->end(); ++it) {
    (*it)->clean();
    delete (*it);
  }
  nodes->clear();
}

void Statement::assemble_mul_par_result_set_plan(ExecutePlan *plan,
                                                 vector<unsigned int> *par_ids,
                                                 PartitionedTable *par_table,
                                                 bool need_check_distinct) {
  LOG_DEBUG("Assemble mul par result set plan [%d] partitions of [%s].\n",
            par_ids->size(), par_table->get_name());
  vector<ExecuteNode *> nodes;
  generate_select_plan_nodes(plan, par_ids, par_table, &nodes, sql,
                             need_check_distinct);
  if (union_all_sub) {
    if (is_into_outfile()) {
      clean_up_execute_nodes(&nodes);
      LOG_ERROR("Can not SELECT INTO OUTFILE in UNION ALL statement.\n");
      throw NotSupportedError(
          "Can not SELECT INTO OUTFILE in UNION ALL statement.");
    }
    unsigned int i = 0;
    for (; i < nodes.size(); ++i) {
      union_all_nodes.push_back(nodes[i]);
    }
  } else if (fe_join) {
    unsigned int i = 0;
    for (; i < nodes.size(); ++i) {
      fe_join_nodes.push_back(nodes[i]);
    }
  } else {
    ExecuteNode *send_node;
    try {
      if (is_into_outfile()) {
        send_node = plan->get_into_outfile_node();
        if (plan->get_fetch_node_no_thread())
          plan->set_fetch_node_no_thread(false);
      } else if (is_select_into()) {
        send_node = plan->get_select_into_node();
        if (plan->get_fetch_node_no_thread())
          plan->set_fetch_node_no_thread(false);
      } else {
        send_node = plan->get_send_node();
        if (is_handle_sub_query()) plan->set_fetch_node_no_thread(false);
        send_node->set_is_first_column_dbscale_row_id(
            is_first_column_dbscale_row_id);
      }
    } catch (...) {
      clean_up_execute_nodes(&nodes);
      throw;
    }

    unsigned int i = 0;
    for (; i < nodes.size(); ++i) {
      send_node->add_child(nodes[i]);
    }
    plan->set_start_node(send_node);
  }
}

long get_integer_value_from_expr(IntExpression *expr) {
  return expr->get_integer_value();
}

long Statement::get_limit_clause_offset_value(IntExpression *limit_offset,
                                              ExecutePlan *plan,
                                              record_scan *rs) {
  if (limit_offset) {
    return get_integer_value_from_expr(limit_offset);
  } else if (rs->limit->offset_is_uservar) {
    if (plan->session->is_call_store_procedure()) {
      map<string, string> *user_var_map = plan->session->get_user_var_map();
      string var_name(rs->limit->offset_uservar_str);
      boost::to_upper(var_name);
      string var_value;
      if (user_var_map->count(var_name)) {
        var_value = (*user_var_map)[var_name];
        if (!is_natural_number(var_value.c_str())) {
          LOG_ERROR("Offset value [%s] of limit clause is invalid.\n",
                    var_value.c_str());
          throw Error("Offset value of limit clause is invalid");
        }
        return atoi(var_value.c_str());
      } else {
        LOG_ERROR(
            "Offset value of limit clause is invalid, uservar not exist\n");
        throw Error("Offset value of limit clause is invalid");
      }
    }
  }
  return 0;
}

ExecuteNode *Statement::generate_limit_subplan(ExecutePlan *plan,
                                               record_scan *rs,
                                               ExecuteNode **tail_node) {
  long offset = 0;
  long num = 0;
  IntExpression *limit_offset = rs->limit->offset;
  IntExpression *limit_num = rs->limit->num;

  offset = get_limit_clause_offset_value(limit_offset, plan, rs);
  if (limit_num)
    num = get_integer_value_from_expr(limit_num);
  else {
    LOG_ERROR("limit num is not correct for sql [%s].\n",
              fetch_sql_tmp.c_str());
    throw Error("limit num is not correct");
  }
  ExecuteNode *limit = plan->get_limit_node(offset, num);

  if (offset) {
    unsigned int limit_pos = rs->limit_pos;
    string new_sql(fetch_sql_tmp.c_str(), limit_pos + 4);
    new_sql.append(" ");
    char new_num[128];
    size_t len = sprintf(new_num, "%ld", offset + num);
    new_sql.append(new_num, len);
    fetch_sql_tmp.clear();
    fetch_sql_tmp.append(new_sql.c_str());
    LOG_DEBUG("Limit node rebuild the fetch sql %s.\n", fetch_sql_tmp.c_str());
  }

  *tail_node = limit;

  return limit;
}

void Statement::init_uservar(ExecutePlan *plan, Session *work_session) {
  if (!work_session) return;
  Session *session = plan->session;
  map<string, string> *user_var_map = work_session->get_user_var_map();
  map<string, string>::iterator it = user_var_map->begin();
  for (; it != user_var_map->end(); it++) {
    bool is_str = false;
    if (!it->second.empty()) {
      is_str = (it->second[0] == '\'');
    }
    session->add_user_var_value(it->first, it->second, is_str);
  }
}

void Statement::assemble_dispatch_packet_plan(
    ExecutePlan *plan, TransferRule *rule, DataSpace *exec_space,
    DataSpace *send_space, const char *sql, Session *work_session) {
  PartitionScheme *send_scheme = NULL;
  PartitionScheme *exec_scheme = NULL;
  if (!send_space->get_data_source()) {
    if (!send_space->is_duplicated())
      send_scheme = ((PartitionedTable *)send_space)->get_partition_scheme();
    else
      send_scheme = ((DuplicatedTable *)send_space)
                        ->get_partition_table()
                        ->get_partition_scheme();
  }
  if (!exec_space->get_data_source()) {
    if (!exec_space->is_duplicated())
      exec_scheme = ((PartitionedTable *)exec_space)->get_partition_scheme();
    else
      exec_scheme = ((DuplicatedTable *)exec_space)
                        ->get_partition_table()
                        ->get_partition_scheme();
  }

  if ((exec_scheme && exec_scheme->is_shard()) ||
      (send_scheme && send_scheme->is_shard())) {
    LOG_ERROR(
        "Unsupport shard partition table for data_read cross node join.\n");

    throw NotSupportedError(
        "Unsupport shard partition table for data_read cross node join.");
  }

  ExecuteNode *dispatch_packet_node = NULL;
  if (send_space->get_data_source())
    dispatch_packet_node = plan->get_send_node();
  else
    dispatch_packet_node = plan->get_dispatch_packet_node(rule);
  if (exec_space->get_data_source()) {
    ExecuteNode *fetch_node = plan->get_fetch_node(exec_space, sql);
    dispatch_packet_node->add_child(fetch_node);
  } else {
    // If it is a subquery, then the sql was not handled by reconstruct, we
    // should regenerate it.
    if (rule->is_sub_query()) {
      re_parser_stmt(&(st.scanner), &(plan->statement), sql);
      plan->statement->set_fe_join(true);
      plan->statement->set_default_schema(schema);
      plan->statement->init_uservar(plan, work_session);
      try {
        plan->statement->generate_execution_plan(plan);
#ifndef DBSCALE_TEST_DISABLE
        Backend *bk = Backend::instance();
        dbscale_test_info *test_info = bk->get_dbscale_test_info();
        if (!strcasecmp(test_info->test_case_name.c_str(), "execution_plan") &&
            !strcasecmp(test_info->test_case_operation.c_str(),
                        "fail_in_assemble_dispatch_packet_plan")) {
          throw UnSupportPartitionSQL(
              "dbscale_test operation execution_plan for case "
              "fail_in_assemble_dispatch_packet_plan");
        }
#endif
      } catch (...) {
        work_session->set_fe_error(true);
        if (dispatch_packet_node) {
          vector<ExecuteNode *> fe_join_nodes_tmp =
              plan->statement->get_fe_join_nodes();
          for (unsigned int i = 0; i < fe_join_nodes_tmp.size(); ++i) {
            dispatch_packet_node->add_child(fe_join_nodes_tmp[i]);
          }
          dispatch_packet_node->clean();
          delete dispatch_packet_node;
          dispatch_packet_node = NULL;
        }
        LOG_ERROR("got error when generate execution plan.\n");
        throw;
      }
      vector<ExecuteNode *> fe_join_nodes_tmp =
          plan->statement->get_fe_join_nodes();
      for (unsigned int i = 0; i < fe_join_nodes_tmp.size(); ++i) {
        dispatch_packet_node->add_child(fe_join_nodes_tmp[i]);
      }
    } else {
      DataSpace *space = NULL;
      unsigned int i = 0;
      PartitionedTable *par_table = (PartitionedTable *)exec_space;
      unsigned int partition_num = par_table->get_real_partition_num();
      for (; i < partition_num; ++i) {
        space = par_table->get_partition(i);
        ExecuteNode *fetch_node = plan->get_fetch_node(space, sql);
        dispatch_packet_node->add_child(fetch_node);
      }
      plan->session->set_execute_plan_touch_partition_nums(-1);
    }
  }
  plan->set_start_node(dispatch_packet_node);
}

void Statement::assemble_mul_par_modify_plan(ExecutePlan *plan,
                                             vector<unsigned int> *par_ids,
                                             PartitionedTable *par_table) {
  LOG_DEBUG("Assemble mul par modify plan [%d] partitions of [%s].\n",
            par_ids->size(), par_table->get_name());
  DataSpace *space = NULL;
  map<DataSpace *, const char *> spaces_map;

  // map to mark how many SQLs each dataspace executed.
  // 3 means [CREATE DB stmt + USE DB stmt + CREATE TB stmt]
  // 2 means [CREATE DB stmt + CREATE TB stmt]
  // 1 means others
  map<DataSpace *, int> sql_count_map;
  vector<unsigned int>::iterator it = par_ids->begin();
  bool need_handle_create_database_ok = false;
  for (; it != par_ids->end(); ++it) {
    space = par_table->get_partition(*it);
    if (!space->get_virtual_machine_id()) {
      if (st.type == STMT_CREATE_TB) {
        table_link *table = one_table_node.only_one_table ? one_table_node.table
                                                          : par_tables[0];
        const char *schema_name =
            table->join->schema_name ? table->join->schema_name : schema;
        string new_sql = Backend::instance()->fetch_create_db_sql_from_metadata(
            plan->session, schema_name, true, NULL);
        int has_use = 0;
        if (plan->session->get_schema() &&
            strcmp(plan->session->get_schema(), "")) {
          if (!lower_case_compare(plan->session->get_schema(), schema_name)) {
            /*Do the use schema only for the current schema is the created
             * schema.*/
            new_sql.append(";USE `");
            new_sql.append(plan->session->get_schema());
            new_sql.append("`");
            has_use = 1;
          }
        } else {
          new_sql.append(";USE `");
          new_sql.append(DEFAULT_LOGIN_SCHEMA);
          new_sql.append("`");
          has_use = 1;
        }
        new_sql.append(";");
        new_sql.append(sql);
        record_shard_sql_str(new_sql);
        spaces_map[space] = get_last_shard_sql();
        sql_count_map[space] = 2 + has_use;
        LOG_DEBUG(
            "create table stmt changed from [%s] to [%s] with sqls num %d\n",
            sql, new_sql.c_str(), sql_count_map[space]);
        need_handle_create_database_ok = true;
      } else {
        spaces_map[space] = sql;
        sql_count_map[space] = 1;
      }
    } else {
      string new_sql_tmp;
      adjust_virtual_machine_schema(space->get_virtual_machine_id(),
                                    space->get_partition_id(), sql, schema,
                                    get_latest_stmt_node(),
                                    record_scan_all_table_spaces, new_sql_tmp);
      if (st.type == STMT_CREATE_TB) {
        string new_schema_tmp;
        table_link *table = one_table_node.only_one_table ? one_table_node.table
                                                          : par_tables[0];
        adjust_shard_schema(table->join->schema_name, schema, new_schema_tmp,
                            space->get_virtual_machine_id(),
                            space->get_partition_id());
        string new_sql = Backend::instance()->fetch_create_db_sql_from_metadata(
            plan->session,
            table->join->schema_name ? table->join->schema_name : schema, true,
            new_schema_tmp.c_str());
        new_sql.append(";");
        new_sql.append(new_sql_tmp.c_str());
        record_shard_sql_str(new_sql);
        spaces_map[space] = get_last_shard_sql();
        sql_count_map[space] = 2;
        need_handle_create_database_ok = true;
      } else {
        record_shard_sql_str(new_sql_tmp);
        spaces_map[space] = get_last_shard_sql();
        sql_count_map[space] = 1;
      }
    }
  }
  if (Backend::instance()->need_deal_with_metadata(st.type, &st))
    st.need_apply_metadata = judge_need_apply_meta_datasource(spaces_map);
  ExecuteNode *node = plan->get_mul_modify_node(
      spaces_map, need_handle_create_database_ok, &sql_count_map);
  if (!is_cross_node_join()) plan->session->reset_table_without_given_schema();
  plan->session->set_affected_servers(AFFETCED_MUL_SERVER);
  plan->set_start_node(node);

  if (st.type == STMT_UPDATE || st.type == STMT_DELETE ||
      st.type == STMT_INSERT || st.type == STMT_REPLACE) {
    if (par_ids->size() == par_table->get_real_partition_num()) {
      plan->session->set_execute_plan_touch_partition_nums(-1);
    } else {
      plan->session->set_execute_plan_touch_partition_nums(par_ids->size());
    }
  }
}

void Statement::assemble_mul_part_insert_plan(ExecutePlan *plan,
                                              PartitionedTable *part_table) {
  LOG_DEBUG("Assemble mul partition insert plan [%d] partitions of [%s].\n",
            multi_insert_id_sqls.size(), part_table->get_name());
  DataSpace *dataspace = NULL;
  map<DataSpace *, const char *> spaces_map;

  if (close_cross_node_transaction) {
    LOG_ERROR(
        "Refuse to execute cross node transaction when "
        "close_cross_node_transaction != 0\n");
    throw Error(
        "Refuse to execute cross node transaction when "
        "close_cross_node_transaction != 0");
  }

  map<unsigned int, string>::iterator it = multi_insert_id_sqls.begin();
  for (; it != multi_insert_id_sqls.end(); ++it) {
    dataspace = part_table->get_partition(it->first);
    const char *used_sql =
        adjust_stmt_sql_for_shard(dataspace, it->second.c_str());
    spaces_map[dataspace] = used_sql;
  }
  ExecuteNode *node = plan->get_mul_modify_node(spaces_map);
  plan->session->set_affected_servers(AFFETCED_MUL_SERVER);
  plan->set_start_node(node);
  if (multi_insert_id_sqls.size() == part_table->get_real_partition_num()) {
    plan->session->set_execute_plan_touch_partition_nums(-1);
  } else {
    plan->session->set_execute_plan_touch_partition_nums(
        multi_insert_id_sqls.size());
  }
}

bool Statement::prepare_two_phase_sub_sql(vector<string *> **columns,
                                          table_link *modify_table) {
  bool can_quick;
  if (st.type == STMT_UPDATE || st.type == STMT_DELETE)
    *columns = new vector<string *>();
  try {
    can_quick = generate_two_phase_modify_sub_sqls(*columns, modify_table);
  } catch (exception &e) {
    vector<string *>::iterator it = (*columns)->begin();
    for (; it != (*columns)->end(); ++it) {
      delete *it;
    }
    (*columns)->clear();
    delete *columns;
    throw;
  }
  return can_quick;
}

void build_simple_two_phase_plan(ExecutePlan *plan, DataSpace *modify_space,
                                 stmt_node *st, vector<ExecuteNode *> *nodes,
                                 vector<string *> *columns, bool can_quick,
                                 const char *modify_sub_sql,
                                 bool is_replace_set_value = false) {
  ExecuteNode *ok_node = plan->get_ok_node();
  ExecuteNode *ok_merge_node = plan->get_ok_merge_node();
  ExecuteNode *update_select;

  if (st->type == STMT_INSERT_SELECT || st->type == STMT_REPLACE_SELECT) {
    update_select = plan->get_insert_select_node(modify_sub_sql, nodes);
  } else {
    update_select = plan->get_modify_select_node(
        modify_sub_sql, columns, can_quick, nodes, is_replace_set_value);
  }
  ok_node->add_child(ok_merge_node);
  ok_merge_node->add_child(update_select);

  ExecuteNode *modify_node = plan->get_modify_node(modify_space);
  update_select->add_child(modify_node);

  plan->set_start_node(ok_node);
}

void Statement::assemble_two_phase_insert_part_select_part_plan(
    ExecutePlan *plan, table_link *modify_table, vector<unsigned int> *key_pos,
    PartitionedTable *par_table_select, vector<unsigned int> *par_ids) {
  const char *schema_name = modify_table->join->schema_name
                                ? modify_table->join->schema_name
                                : schema;
  const char *table_name = modify_table->join->table_name;

  LOG_DEBUG(
      "Assemble two phase insert partition select partition plan of table "
      "[%s.%s].\n",
      schema_name, table_name);

  Backend *backend = Backend::instance();

#ifdef DEBUG
  bool is_par_table = backend->table_is_partitioned(schema_name, table_name);
  bool is_dup_table = backend->table_is_duplicated(schema_name, table_name);
  ACE_ASSERT(is_par_table || is_dup_table);
#endif
  DataSpace *dspace =
      backend->get_data_space_for_table(schema_name, table_name);

  bool is_duplicated = false;
  PartitionedTable *par_table = NULL;
  if (((Table *)dspace)->is_duplicated()) {
    is_duplicated = true;
    par_table = ((DuplicatedTable *)dspace)->get_partition_table();
  } else {
    if (close_cross_node_transaction) {
      LOG_ERROR(
          "Refuse to execute cross node transaction when "
          "close_cross_node_transaction != 0\n");
      throw Error(
          "Refuse to execute cross node transaction when "
          "close_cross_node_transaction != 0");
    }
    par_table = (PartitionedTable *)dspace;
  }

  // columns will be deleted by the clean of node.
  vector<string *> *columns = NULL;
  prepare_two_phase_sub_sql(&columns, modify_table);
  handle_insert_select_hex_column();

  vector<ExecuteNode *> nodes;
  record_scan *select_rs = NULL;

  int load_insert =
      plan->session->get_session_option("use_load_data_for_insert_select")
          .int_val;
  if (!is_duplicated && st.type == STMT_INSERT_SELECT &&
      !plan->statement->is_cross_node_join() &&
      auto_inc_status == NO_AUTO_INC_FIELD &&
      (load_insert == FIFO_INSERT_SELECT ||
       load_insert == LOAD_INSERT_SELECT)) {
    ExecuteNode *parent_node = NULL;
    if (load_insert == FIFO_INSERT_SELECT) {
      LOG_DEBUG("Assemble INSERT SELECT from partition table plan via fifo.\n");
      prepare_insert_select_via_fifo(plan, NULL, schema_name, table_name, true);
    } else if (load_insert == LOAD_INSERT_SELECT) {
      LOG_DEBUG("Assemble INSERT SELECT from partition table plan via LOAD.\n");
      prepare_insert_select_via_load(plan, schema_name, table_name);
    }
    select_rs = generate_select_plan_nodes(plan, par_ids, par_table_select,
                                           &nodes, select_sub_sql.c_str());
    if (select_rs &&
        check_can_pushdown_for_modify_select(*par_table_select, select_rs,
                                             nodes, par_table, NULL)) {
      LOG_DEBUG("Found a pushdown modify select sql\n");
      free_re_parser_list();
      st.scanner = get_stmt_node()->scanner;
      plan->statement = this;
      assemble_modify_node_according_to_fetch_node(*plan, nodes, par_table);
      clean_up_execute_nodes(&nodes);
      return;
    }
    if (load_insert == FIFO_INSERT_SELECT)
      parent_node = plan->get_into_outfile_node();
    else if (load_insert == LOAD_INSERT_SELECT)
      parent_node =
          plan->get_load_select_partition_node(schema_name, table_name);
    size_t nodes_num = nodes.size();
    for (size_t i = 0; i < nodes_num; ++i) {
      parent_node->add_child(nodes[i]);
    }
    plan->set_start_node(parent_node);
    return;
  }

  ExecuteNode *update_select = NULL;
  select_rs = generate_select_plan_nodes(plan, par_ids, par_table_select,
                                         &nodes, select_sub_sql.c_str());
  if (select_rs && check_can_pushdown_for_modify_select(
                       *par_table_select, select_rs, nodes, par_table, NULL)) {
    LOG_DEBUG("Found a pushdown modify select sql\n");
    free_re_parser_list();
    assemble_modify_node_according_to_fetch_node(*plan, nodes, par_table);
    clean_up_execute_nodes(&nodes);
    return;
  }

  update_select = plan->get_par_insert_select_node(
      modify_sub_sql.c_str(), par_table, par_table->get_partition_method(),
      *key_pos, &nodes, schema_name, table_name, is_duplicated);

  ExecuteNode *ok_node = plan->get_ok_node();
  ExecuteNode *ok_merge_node = plan->get_ok_merge_node();
  ok_node->add_child(ok_merge_node);
  ok_merge_node->add_child(update_select);

  /*For modify normal table, we add modify node directly; for modify partition
   * table, we add modify node dynamically during execution.*/

  plan->session->set_affected_servers(AFFETCED_MUL_SERVER);
  plan->set_start_node(ok_node);
}
void Statement::assemble_two_phase_modify_partition_plan(
    ExecutePlan *plan,
    DataSpace *space,  // only for insert_select
    PartitionMethod *method, table_link *modify_table,
    vector<unsigned int> *key_pos,  // only for insert_select
    vector<unsigned int> *par_ids)  // only for update/delete_select
{
  LOG_DEBUG("Assemble two phase modify partition plan of table [%s].\n",
            modify_table->join->table_name);

  const char *schema_name = modify_table->join->schema_name
                                ? modify_table->join->schema_name
                                : schema;
  const char *table_name = modify_table->join->table_name;

  Backend *backend = Backend::instance();

#ifdef DEBUG
  bool is_par_table = backend->table_is_partitioned(schema_name, table_name);
  bool is_dup_table = backend->table_is_duplicated(schema_name, table_name);
  ACE_ASSERT(is_par_table || is_dup_table);
#endif
  DataSpace *dspace =
      backend->get_data_space_for_table(schema_name, table_name);

  bool is_duplicated = false;
  PartitionedTable *par_table = NULL;
  if (((Table *)dspace)->is_duplicated()) {
    is_duplicated = true;
    par_table = ((DuplicatedTable *)dspace)->get_partition_table();
  } else {
    if (close_cross_node_transaction) {
      LOG_ERROR(
          "Refuse to execute cross node transaction when "
          "close_cross_node_transaction != 0\n");
      throw Error(
          "Refuse to execute cross node transaction when "
          "close_cross_node_transaction != 0");
    }
    par_table = (PartitionedTable *)dspace;
  }

  if (!is_duplicated) {
    int load_insert =
        plan->session->get_session_option("use_load_data_for_insert_select")
            .int_val;
    if (st.type == STMT_INSERT_SELECT && load_insert == FIFO_INSERT_SELECT &&
        auto_inc_status == NO_AUTO_INC_FIELD &&
        !plan->statement->is_union_table_sub() &&
        !plan->statement->is_cross_node_join() &&
        !plan->session->is_in_explain()) {
      vector<ExecuteNode *> nodes;
      LOG_DEBUG("Assemble INSERT SELECT from partition table plan via fifo.\n");
      prepare_insert_select_via_fifo(plan, NULL, schema_name, table_name, true);
      const char *used_sql =
          adjust_stmt_sql_for_shard(space, select_sub_sql.c_str());
      if (used_sql == select_sub_sql.c_str())
        used_sql = adjust_stmt_sql_for_alias(space, select_sub_sql.c_str());
      ExecuteNode *fetch_node = plan->get_fetch_node(space, used_sql);
      ExecuteNode *send_node = plan->get_into_outfile_node();

      send_node->add_child(fetch_node);
      plan->set_start_node(send_node);
      return;
    } else if (st.type == STMT_INSERT_SELECT &&
               load_insert == LOAD_INSERT_SELECT &&
               auto_inc_status == NO_AUTO_INC_FIELD &&
               !plan->statement->is_union_table_sub() &&
               !plan->statement->is_cross_node_join() &&
               !plan->session->is_in_explain()) {
      vector<ExecuteNode *> nodes;
      LOG_DEBUG("Assemble INSERT SELECT from partition table plan via LOAD.\n");
      prepare_insert_select_via_load(plan, schema_name, table_name);
      const char *used_sql =
          adjust_stmt_sql_for_shard(space, select_sub_sql.c_str());
      if (used_sql == select_sub_sql.c_str())
        used_sql = adjust_stmt_sql_for_alias(space, select_sub_sql.c_str());

      ExecuteNode *fetch_node = plan->get_fetch_node(space, used_sql);
      ExecuteNode *load_node =
          plan->get_load_select_partition_node(schema_name, table_name);
      load_node->add_child(fetch_node);
      plan->set_start_node(load_node);
      return;
    }
  }

  // columns will be deleted by the clean of node.
  vector<string *> *columns = NULL;
  bool can_quick = prepare_two_phase_sub_sql(&columns, modify_table);
  handle_insert_select_hex_column();

  if (st.type == STMT_DELETE &&
      (st.scanner->order_by_list == NULL && st.scanner->limit != NULL &&
       st.scanner->limit->offset == NULL)) {
    vector<string *>::iterator it = columns->begin();
    for (; it != columns->end(); ++it) {
      delete *it;
    }
    columns->clear();
    delete columns;
    assemble_modify_limit_partition_plan(plan, par_ids, par_table);
    return;
  }

  ExecuteNode *ok_node = plan->get_ok_node();
  ExecuteNode *ok_merge_node = plan->get_ok_merge_node();

  ExecuteNode *update_select = NULL;
  vector<ExecuteNode *> nodes;

  if (st.type == STMT_INSERT_SELECT || st.type == STMT_REPLACE_SELECT) {
    ExecuteNode *fetch_node;
#ifndef DBSCALE_DISABLE_SPARK
    if (spark_insert) {
      analysis_spark_sql_join(plan);
      fetch_node = spark_node;
    } else {
#endif
      /*The select part only need one fetch node to execute the select_sql
       * directly. Cause the select part of insert select stmt here, doesnot
       * contain the partition table.*/
      const char *used_sql =
          adjust_stmt_sql_for_shard(space, select_sub_sql.c_str());
      if (used_sql == select_sub_sql.c_str())
        used_sql = adjust_stmt_sql_for_alias(space, select_sub_sql.c_str());
      fetch_node = plan->get_fetch_node(space, used_sql);
#ifndef DBSCALE_DISABLE_SPARK
    }
#endif
    nodes.push_back(fetch_node);
    update_select = plan->get_par_insert_select_node(
        modify_sub_sql.c_str(), par_table, method, *key_pos, &nodes,
        schema_name, table_name, is_duplicated);
    plan->session->set_execute_plan_touch_partition_nums(-1);
  } else {
    generate_select_plan_nodes(plan, par_ids, par_table, &nodes,
                               select_sub_sql.c_str());
    update_select = plan->get_par_modify_select_node(
        modify_sub_sql.c_str(), par_table, method, columns, can_quick, &nodes);
  }
  ok_node->add_child(ok_merge_node);
  ok_merge_node->add_child(update_select);

  /*For modify normal table, we add modify node directly; for modify partition
   * table, we add modify node dynamically during execution.*/
  plan->session->set_affected_servers(AFFETCED_MUL_SERVER);
  plan->set_start_node(ok_node);
}

void Statement::assemble_two_phase_modify_no_partition_plan(
    ExecutePlan *plan, DataSpace *modify_space, DataSpace *select_space,
    table_link *modify_table) {
  LOG_DEBUG("Assemble two phase modify no partition plan.\n");
  // columns will be deleted by the clean of node.
  vector<string *> *columns = NULL;
  bool can_quick = prepare_two_phase_sub_sql(&columns, modify_table);
  handle_insert_select_hex_column();

  ExecuteNode *fetch_node;
#ifndef DBSCALE_DISABLE_SPARK
  if (spark_insert) {
    analysis_spark_sql_join(plan);
    fetch_node = spark_node;
  } else {
#endif
    const char *used_sql =
        adjust_stmt_sql_for_shard(select_space, select_sub_sql.c_str());
    if (used_sql == select_sub_sql.c_str())
      used_sql =
          adjust_stmt_sql_for_alias(select_space, select_sub_sql.c_str());

    fetch_node = plan->get_fetch_node(select_space, used_sql);
#ifndef DBSCALE_DISABLE_SPARK
  }
#endif

  vector<ExecuteNode *> nodes;
  nodes.push_back(fetch_node);

  build_simple_two_phase_plan(plan, modify_space, &st, &nodes, columns,
                              can_quick, modify_sub_sql.c_str(),
                              is_update_set_subquery);
}

void Statement::assemble_modify_limit_partition_plan(
    ExecutePlan *plan, vector<unsigned int> *par_ids,
    PartitionedTable *par_table) {
  long num = 0;
  IntExpression *limit_num = st.scanner->limit->num;
  if (limit_num) num = get_integer_value_from_expr(limit_num);

  string tmp_sql(sql);
  ExecuteNode *node = plan->get_modify_limit_node(
      st.scanner, par_table, *par_ids, (unsigned int)num, tmp_sql);
  plan->session->set_affected_servers(AFFETCED_MUL_SERVER);
  plan->set_start_node(node);

  LOG_DEBUG("Assemble delete limit plan [%d] partitions of [%s].\n",
            par_ids->size(), par_table->get_name());
  return;
}

void Statement::assemble_two_phase_modify_normal_plan(
    ExecutePlan *plan, vector<unsigned int> *par_ids,
    PartitionedTable *par_table, table_link *modify_table) {
  LOG_DEBUG("Assemble two phase modify normal plan.\n");
  Backend *backend = Backend::instance();
  const char *schema_name = modify_table->join->schema_name
                                ? modify_table->join->schema_name
                                : schema;
  const char *table_name = modify_table->join->table_name;
  DataSpace *modify_space =
      backend->get_data_space_for_table(schema_name, table_name);
  DataServer *modify_server =
      get_write_server_of_source(modify_space->get_data_source());

  vector<ExecuteNode *> nodes;
  record_scan *select_rs = NULL;
  do {
    if (st.type == STMT_INSERT_SELECT) {
      bool is_external_load =
          modify_server && modify_server->get_is_external_load();
      int load_insert =
          plan->session->get_session_option("use_load_data_for_insert_select")
              .int_val;
      bool should_load_insert = auto_inc_status == NO_AUTO_INC_FIELD &&
                                !plan->statement->is_cross_node_join();
      bool insert_select_use_fifo =
          should_load_insert && (load_insert == FIFO_INSERT_SELECT);
      bool insert_select_use_load =
          should_load_insert && (load_insert == LOAD_INSERT_SELECT);
      ExecuteNode *parent_node = NULL;
      if (is_external_load || insert_select_use_fifo) {
        LOG_DEBUG(
            "Assemble INSERT SELECT from partition table plan via fifo.\n");
        prepare_insert_select_via_fifo(plan, modify_server, schema_name,
                                       table_name, !is_external_load);
      } else if (insert_select_use_load) {
        LOG_DEBUG(
            "Assemble INSERT SELECT from partition table plan via LOAD.\n");
        prepare_insert_select_via_load(plan, schema_name, table_name);
      } else {
        break;
      }
      select_rs = generate_select_plan_nodes(plan, par_ids, par_table, &nodes,
                                             select_sub_sql.c_str());
      if (select_rs && check_can_pushdown_for_modify_select(
                           *par_table, select_rs, nodes, NULL, modify_space)) {
        LOG_DEBUG("Found a pushdown modify select sql\n");
        free_re_parser_list();
        st.scanner = get_stmt_node()->scanner;
        plan->statement = this;
        assemble_modify_node_according_to_fetch_node(*plan, nodes, NULL);
        clean_up_execute_nodes(&nodes);
        return;
      }
      if (is_external_load || insert_select_use_fifo) {
        parent_node = plan->get_into_outfile_node();
      } else if (insert_select_use_load) {
        parent_node = plan->get_load_select_node(schema_name, table_name);
      }
      size_t nodes_num = nodes.size();
      for (size_t i = 0; i < nodes_num; ++i) {
        parent_node->add_child(nodes[i]);
      }
      plan->set_start_node(parent_node);
      return;
    }
  } while (0);

  LOG_DEBUG("Assemble two phase modify normal plan [%d] partitions of [%s].\n",
            par_ids->size(), par_table->get_name());
  // columns will be deleted by the clean of node.
  vector<string *> *columns = NULL;

  bool can_quick = prepare_two_phase_sub_sql(&columns, modify_table);
  handle_insert_select_hex_column();
  select_rs = generate_select_plan_nodes(plan, par_ids, par_table, &nodes,
                                         select_sub_sql.c_str());
  if (select_rs && check_can_pushdown_for_modify_select(
                       *par_table, select_rs, nodes, NULL, modify_space)) {
    LOG_DEBUG("Found a pushdown modify select sql\n");
    free_re_parser_list();
    assemble_modify_node_according_to_fetch_node(*plan, nodes, par_table);
    clean_up_execute_nodes(&nodes);
    return;
  }

  build_simple_two_phase_plan(plan, modify_space, &st, &nodes, columns,
                              can_quick, modify_sub_sql.c_str());
}

void Statement::check_load_local_infile(ExecutePlan *plan) {
  if (plan->session->get_local_infile() == false) {
    throw dbscale::sql::SQLError(
        "The used command is not allowed with this MySQL version", "42000",
        1148);
  }
}

void Statement::assemble_load_local_plan(ExecutePlan *plan,
                                         DataSpace *dataspace) {
  check_load_local_infile(plan);
  ExecuteNode *load_node = plan->get_load_local_node(dataspace, sql);
  plan->session->set_load_data_local_command(true);
  plan->set_start_node(load_node);
}

void Statement::assemble_kill_plan(ExecutePlan *plan, int killed_cluster_id,
                                   uint32_t kid) {
  ExecuteNode *kill_node = plan->get_kill_node(killed_cluster_id, kid);
  plan->set_start_node(kill_node);
}

void Statement::assemble_load_data_infile_external_plan(
    ExecutePlan *plan, DataSpace *dataspace, DataServer *dataserver) {
  ExecuteNode *load_node =
      plan->get_load_data_infile_external_node(dataspace, dataserver);
  plan->set_start_node(load_node);
}

void Statement::assemble_load_local_external_plan(ExecutePlan *plan,
                                                  DataSpace *dataspace,
                                                  DataServer *dataserver) {
  ExecuteNode *load_node =
      plan->get_load_local_external_node(dataspace, dataserver, sql);
  plan->session->set_load_data_local_command(true);
  plan->set_start_node(load_node);
}

void Statement::assemble_partition_load_local_plan(ExecutePlan *plan,
                                                   PartitionedTable *par_table,
                                                   const char *schema_name,
                                                   const char *table_name) {
  LOG_DEBUG("assemble partition_load_local plan for table [%s.%s].\n",
            schema_name, table_name);
  check_load_local_infile(plan);
  ExecuteNode *load_node = plan->get_load_local_part_table_node(
      par_table, sql, schema_name, table_name);
  plan->session->set_load_data_local_command(true);
  plan->session->set_affected_servers(AFFETCED_MUL_SERVER);
  plan->set_start_node(load_node);
}
void Statement::assemble_partition_load_data_infile_plan(
    ExecutePlan *plan, PartitionedTable *par_table, const char *schema_name,
    const char *table_name) {
  ExecuteNode *load_node = plan->get_load_data_infile_part_table_node(
      par_table, sql, schema_name, table_name);
  plan->session->set_affected_servers(AFFETCED_MUL_SERVER);
  plan->set_start_node(load_node);
}

void Statement::assemble_load_data_infile_plan(ExecutePlan *plan,
                                               DataSpace *dataspace) {
  ExecuteNode *load_node = plan->get_load_data_infile_node(dataspace, sql);
  plan->set_start_node(load_node);
}

void Statement::assemble_dbscale_set_plan(ExecutePlan *plan) {
  ExecuteNode *dynamic_configuration_node =
      plan->get_dynamic_configuration_node();
  plan->set_start_node(dynamic_configuration_node);
}

void Statement::assemble_dbscale_set_rep_strategy_plan(ExecutePlan *plan) {
  ExecuteNode *rep_strategy_node = plan->get_rep_strategy_node();
  plan->set_start_node(rep_strategy_node);
}

void Statement::assemble_dbscale_set_server_weight(ExecutePlan *plan) {
  ExecuteNode *set_server_weight_node = plan->get_set_server_weight_node();
  plan->set_start_node(set_server_weight_node);
}

void Statement::assemble_show_option_plan(ExecutePlan *plan) {
  ExecuteNode *show_option_node = plan->get_show_option_node();
  plan->set_start_node(show_option_node);
}

void Statement::assemble_show_dynamic_option_plan(ExecutePlan *plan) {
  ExecuteNode *show_dynamic_option_node = plan->get_show_dynamic_option_node();
  plan->set_start_node(show_dynamic_option_node);
}

void Statement::assemble_change_startup_config_plan(ExecutePlan *plan) {
  ExecuteNode *change_startup_config_node =
      plan->get_change_startup_config_node();
  plan->set_start_node(change_startup_config_node);
}

void Statement::assemble_show_changed_startup_config_plan(ExecutePlan *plan) {
  ExecuteNode *show_changed_startup_config_node =
      plan->get_show_changed_startup_config_node();
  plan->set_start_node(show_changed_startup_config_node);
}

void Statement::assemble_dbscale_clean_monitor_point_plan(ExecutePlan *plan) {
  ExecuteNode *clean_node = plan->get_dbscale_clean_monitor_point_node();
  plan->set_start_node(clean_node);
}

void Statement::assemble_dbscale_purge_monitor_point_plan(ExecutePlan *plan) {
  ExecuteNode *purge_node = plan->get_dbscale_purge_monitor_point_node();
  plan->set_start_node(purge_node);
}

void Statement::assemble_dbscale_get_global_consistence_point_plan(
    ExecutePlan *plan) {
  if (!multiple_mode ||
      (multiple_mode && MultipleManager::instance()->get_is_cluster_master())) {
    ExecuteNode *consistence_node =
        plan->get_dbscale_global_consistence_point_node();
    plan->set_start_node(consistence_node);
  } else {
    assemble_forward_master_role_plan(plan, true);
  }
}
void Statement::assemble_show_engine_lock_waiting_status_plan(ExecutePlan *plan,
                                                              int engine_type) {
  ExecuteNode *node = plan->get_show_engine_lock_waiting_node(engine_type);
  plan->set_start_node(node);
}
void Statement::assemble_show_dataserver_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble show dataserver plan\n");
  ExecuteNode *node = plan->get_show_dataserver_node();
  plan->set_start_node(node);
}

void Statement::assemble_show_backend_threads_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble show backend threads plan\n");
  ExecuteNode *node = plan->get_show_backend_threads_node();
  plan->set_start_node(node);
}

void Statement::assemble_show_partition_scheme_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble show partition scheme plan\n");
  ExecuteNode *node = plan->get_show_partition_scheme_node();
  plan->set_start_node(node);
}

void Statement::assemble_show_schema_plan(ExecutePlan *plan,
                                          const char *schema_name) {
  LOG_DEBUG("Assemble show schema plan\n");
  ExecuteNode *node = plan->get_show_schema_node(schema_name);
  plan->set_start_node(node);
}
void Statement::assemble_show_table_plan(ExecutePlan *plan, const char *schema,
                                         const char *table, bool use_like) {
  LOG_DEBUG("Assemble show table dataspace plan\n");
  ExecuteNode *node = plan->get_show_table_node(schema, table, use_like);
  plan->set_start_node(node);
}

void Statement::assemble_show_migrate_clean_tables_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble show migrate clean tables plan\n");
  if (multiple_mode && !MultipleManager::instance()->get_is_cluster_master()) {
    LOG_ERROR("slave dbscale do not migrate table\n");
    throw NotSupportedError("slave dbscale do not support migrate table.");
  }
  Backend *backend = Backend::instance();
  DataSpace *data_space = backend->get_config_data_space();

  ExecuteNode *node = plan->get_direct_execute_node(data_space, sql);
  plan->set_start_node(node);
}

void Statement::assemble_migrate_clean_tables_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble migrate clean tables plan\n");
  if (multiple_mode && !MultipleManager::instance()->get_is_cluster_master()) {
    LOG_ERROR("slave dbscale do not migrate table\n");
    throw NotSupportedError("slave dbscale do not support migrate table.");
  }

  const char *migrate_id = st.sql->migrate_clean_oper->migrate_id;
  ExecuteNode *node = plan->get_migrate_clean_node(migrate_id);
  plan->set_start_node(node);
}

bool check_need_show_weight(DataSource *datasource) {
  DataSourceType datasource_type = datasource->get_data_source_type();
  if (datasource_type == DATASOURCE_TYPE_REPLICATION ||
      datasource_type == DATASOURCE_TYPE_RWSPLIT) {
    DataSource *lb_ds =
        ((ReplicationDataSource *)datasource)->get_read_source();
    SchedulePolicy type =
        ((LoadBalanceDataSource *)lb_ds)->get_scheduler_type();
    if (type == SCHEDULE_POLICY_WEIGHT) {
      return true;
    }
  }
  if (datasource_type == DATASOURCE_TYPE_LOAD_BALANCE) {
    SchedulePolicy type =
        ((LoadBalanceDataSource *)datasource)->get_scheduler_type();
    if (type == SCHEDULE_POLICY_WEIGHT) {
      return true;
    }
  }
  return false;
}

void Statement::assemble_show_datasource_plan(ExecutePlan *plan) {
  const char *name = st.sql->show_datasource_oper->data_source_name;
  const char *type_name = st.sql->show_datasource_oper->data_source_type;
  list<const char *> names;
  list<DataSource *> type_data_sources;
  Backend *backend = Backend::instance();
  bool need_show_weight = false;
  if (type_name) {
    int type = -1;
    if (!strcasecmp(type_name, "SERVER"))
      type = DATASOURCE_TYPE_SERVER;
    else if (!strcasecmp(type_name, "RWSPLIT"))
      type = DATASOURCE_TYPE_RWSPLIT;
    else if (!strcasecmp(type_name, "LOAD_BALANCE"))
      type = DATASOURCE_TYPE_LOAD_BALANCE;
    else if (!strcasecmp(type_name, "REPLICATION"))
      type = DATASOURCE_TYPE_REPLICATION;
    else if (!strcasecmp(type_name, "SHARE_DISK"))
      type = DATASOURCE_TYPE_SHARE_DISK;
    else if (!strcasecmp(type_name, "READ_ONLY"))
      type = DATASOURCE_TYPE_READ_ONLY;
    else if (!strcasecmp(type_name, "MGR"))
      type = DATASOURCE_TYPE_MGR;
    else {
      LOG_DEBUG("Unknown DataSource type.\n");
      throw UnknownDataSourceException("Unknown DataSource type.");
    }
    backend->find_data_source_by_type(type, type_data_sources);
    if (!type_data_sources.empty()) {
      list<DataSource *>::iterator it = type_data_sources.begin();
      for (; it != type_data_sources.end(); ++it) {
        if (!need_show_weight) need_show_weight = check_need_show_weight((*it));
        names.push_back((*it)->get_name());
      }
    } else {
      LOG_DEBUG("No DataSource of the type.\n");
      throw UnknownDataSourceException("No DataSource of the type.");
    }
  } else {
    names.push_back(name);
    DataSource *datasource = backend->find_data_source(name);
    if (datasource) need_show_weight = check_need_show_weight(datasource);
  }
  ExecuteNode *datasource_node =
      plan->get_show_data_source_node(names, need_show_weight);
  plan->set_start_node(datasource_node);
}

void Statement::assemble_dbscale_show_partition_plan(ExecutePlan *plan) {
  ExecuteNode *partition_node = plan->get_show_partition_node();
  plan->set_start_node(partition_node);
}

void Statement::assemble_dbscale_show_user_sql_count_plan(ExecutePlan *plan,
                                                          const char *user_id) {
  LOG_DEBUG("Assemble show user sql count plan.\n");
  ExecuteNode *count_node = plan->get_show_user_sql_count_node(user_id);
  plan->set_start_node(count_node);
}

void Statement::assemble_dbscale_show_auto_increment_info(ExecutePlan *plan) {
  LOG_DEBUG("assemble_dbscale_show_auto_increment_info\n");
  join_node *node = st.sql->auto_increment_info_oper->table;
  const char *table_name = node->table_name;
  const char *schema_name =
      node->schema_name ? node->schema_name : plan->session->get_schema();
  Backend *backend = Backend::instance();
  DataSpace *space = backend->get_data_space_for_table(schema_name, table_name);
  if (space->get_dataspace_type() == TABLE_TYPE &&
      ((Table *)space)->is_partitioned()) {
    ExecuteNode *node =
        plan->get_show_auto_increment_info_node((PartitionedTable *)space);
    plan->set_start_node(node);
  } else {
    LOG_ERROR(
        "Not find this partition table [%s.%s] to show auto_increment info\n",
        schema_name, table_name);
    throw Error("Not a partitioned table to show auto_increment info.");
  }
}

void Statement::assemble_dbscale_set_auto_increment_offset(ExecutePlan *plan) {
  LOG_DEBUG("assemble_dbscale_set_auto_increment_offset\n");
  join_node *node = st.sql->auto_increment_info_oper->table;
  const char *table_name = node->table_name;
  const char *schema_name =
      node->schema_name ? node->schema_name : plan->session->get_schema();

  string full_table_name;
  splice_full_table_name(schema_name, table_name, full_table_name);
  if (!Backend::instance()->has_auto_increment_field(full_table_name)) {
    LOG_ERROR("Not find table [%s.%s] has auto_increment info\n", schema_name,
              table_name);
    throw Error("Not find table has auto_increment info.");
  }
  Backend *backend = Backend::instance();
  DataSpace *space = backend->get_data_space_for_table(schema_name, table_name);
  if (space->get_dataspace_type() == TABLE_TYPE &&
      ((Table *)space)->is_partitioned()) {
    ExecuteNode *node =
        plan->get_set_auto_increment_offset_node((PartitionedTable *)space);
    plan->set_start_node(node);
  } else {
    LOG_ERROR(
        "Not find this partition table [%s.%s] to set_auto_increment_offset.\n",
        schema_name, table_name);
    throw Error("Not a partitioned table to set_auto_increment_offset.");
  }
}
void Statement::assemble_dbscale_show_virtual_map_plan(ExecutePlan *plan) {
  LOG_DEBUG("assemble dbscale show virtual map plan\n");
  if (!st.table_list_head)  // statemet does't contains table
    throw Error("Not find table name.");
  table_link *link = st.table_list_head;
  join_node *node = link->join;
  const char *table_name = node->table_name;
  const char *schema_name =
      node->schema_name ? node->schema_name : plan->session->get_schema();
  Backend *backend = Backend::instance();
  DataSpace *space = backend->get_data_space_for_table(schema_name, table_name);
  if (space->get_dataspace_type() == TABLE_TYPE &&
      ((Table *)space)->is_partitioned()) {
    ExecuteNode *node = NULL;
    if (st.type == STMT_DBSCALE_SHOW_VIRTUAL_MAP)
      node = plan->get_show_virtual_map_node((PartitionedTable *)space);
    if (st.type == STMT_DBSCALE_SHOW_SHARD_MAP)
      node = plan->get_show_shard_map_node((PartitionedTable *)space);
    plan->set_start_node(node);
  } else {
    LOG_ERROR("Not find this partition table [%s.%s]\n", schema_name,
              table_name);
    throw Error("Not find this partition table.");
  }
}

void Statement::assemble_dbscale_mul_sync_plan(ExecutePlan *plan) {
  LOG_DEBUG("assemble dbscale mul sync plan\n");
  const char *sync_topic = st.sql->mul_sync_oper->topic_name;
  const char *sync_param = st.sql->mul_sync_oper->sync_param;
  const char *sync_cond = st.sql->mul_sync_oper->sync_cond;
  const char *sync_state = st.sql->mul_sync_oper->sync_state;
  unsigned long version_id = st.sql->mul_sync_oper->version_id;
  ExecuteNode *node = plan->get_dbscale_mul_sync_node(
      sync_topic, sync_state, sync_param, sync_cond, version_id);
  plan->set_start_node(node);
}

void Statement::assemble_dbscale_show_status_plan(ExecutePlan *plan) {
  LOG_DEBUG("assemble dbscale show status plan\n");
  ExecuteNode *node = plan->get_show_status_node();
  plan->set_start_node(node);
}

void Statement::assemble_show_catchup_plan(ExecutePlan *plan,
                                           const char *source_name) {
  LOG_DEBUG("assemble dbscale show catchup plan.\n");
  ExecuteNode *node = plan->get_show_catchup_node(source_name);
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_skip_wait_catchup_plan(
    ExecutePlan *plan, const char *source_name) {
  LOG_DEBUG("Assemble dbscale force kill catchup plan.\n");
  ExecuteNode *node = plan->get_dbscale_skip_wait_catchup_node(source_name);
  plan->set_start_node(node);
}

void Statement::assemble_show_transaction_sqls_plan(ExecutePlan *plan) {
  ExecuteNode *show_transaction_sqls_node =
      plan->get_show_transaction_sqls_node();
  plan->set_start_node(show_transaction_sqls_node);
}

void Statement::assemble_federated_thread_plan(ExecutePlan *plan,
                                               const char *table_name) {
  LOG_DEBUG("Start assemble federated thread plan [%s].\n", sql);
  Backend *backend = Backend::instance();
  DataSpace *send_space =
      backend->get_data_space_for_table(TMP_TABLE_SCHEMA, table_name);
  DataSource *send_source = send_space->get_data_source();
  CrossNodeJoinManager *cross_join_manager =
      backend->get_cross_node_join_manager();
  Session *work_session = NULL;
  if (send_source) {
    work_session = cross_join_manager->get_work_session(table_name);
  } else {
    work_session =
        cross_join_manager->get_federated_table_work_session(table_name);
  }
  if (!work_session) {
    LOG_ERROR(
        "Fail to get work session from join manager for table [%s] in assemble "
        "federated thread plan.\n",
        table_name);
    throw Error("Fail to get work session from join manager.");
  }
  plan->session->get_session_option("cross_node_join_method").int_val = 1;
  plan->session->get_session_option("max_federated_cross_join_rows").ulong_val =
      work_session->get_session_option("max_federated_cross_join_rows")
          .ulong_val;
  LOG_DEBUG("Federated table [%s] get work session [%@].\n", table_name,
            work_session);
  TransferRuleManager *rule_manager = work_session->get_transfer_rule_manager();
  TransferRule *rule =
      rule_manager->get_transfer_rule_by_remote_table_name(table_name);
  DispatchStrategy *dispatch_strategy = rule->get_dispatch_strategy();
  string execution_plan_sql = rule->get_execution_plan_sql();
  DataSpace *exec_space = rule->get_data_space();
  DataSource *exec_source = exec_space->get_data_source();
  LOG_DEBUG("Get federated execute sql [%s].\n", execution_plan_sql.c_str());
  plan->set_federated(true);
  if (send_source && exec_source) {
    ExecuteNode *node =
        plan->get_direct_execute_node(exec_space, execution_plan_sql.c_str());
    plan->set_start_node(node);
    return;
  }
  if (!send_source) {
    if (send_space->is_duplicated())
      send_space = ((DuplicatedTable *)send_space)->get_partition_table();
    vector<const char *> *key_names =
        ((PartitionedTable *)send_space)->get_key_names();
    PartitionMethod *method =
        ((PartitionedTable *)send_space)->get_partition_method();
    dispatch_strategy->method = method;
    dispatch_strategy->key_names = key_names;
    dispatch_strategy->part_table = (PartitionedTable *)send_space;
  }

  assemble_dispatch_packet_plan(plan, rule, exec_space, send_space,
                                execution_plan_sql.c_str(), work_session);
}

bool groupby_on_par_key(order_item *groupby, const char *schema_name,
                        const char *table_name, const char *table_alias,
                        const char *key) {
  field_item *item = groupby->field;
  Expression *field_expr = item->field_expr;
  if (field_expr->type == EXPR_STR) {
    const char *str = ((StrExpression *)field_expr)->str_value;
    if (table_name && column_name_equal_key_alias(
                          str, schema_name, strlen(schema_name), table_name,
                          strlen(table_name), key, table_alias))
      return true;
  }
  return false;
}

/* Whether the group by clause of record scan contains all the partition keys
 * of the partition table of this record scan.
 *
 * TODO: currently if there are several partition tables, we will only use the
 * partition keys of the first partition table. Fix it.*/
bool Statement::group_by_all_keys(record_scan *rs) {
  if (!(record_scan_par_table_map.count(rs) ||
        (one_table_node.only_one_table && one_table_node.rs == rs)) ||
      !rs->group_by_list)
    return false;
  table_link *par_table = one_table_node.only_one_table
                              ? one_table_node.table
                              : record_scan_par_table_map[rs];
  const char *schema_name =
      par_table->join->schema_name ? par_table->join->schema_name : schema;
  const char *table_name = par_table->join->table_name;
  const char *table_alias = par_table->join->alias;
  PartitionedTable *par_space =
      (PartitionedTable *)(one_table_node.only_one_table
                               ? one_table_node.space
                               : record_scan_all_table_spaces[par_table]);
  vector<const char *> *key_names = par_space->get_key_names();

  vector<const char *>::iterator it_keys;
  for (it_keys = key_names->begin(); it_keys != key_names->end(); ++it_keys) {
    bool find_key = false;
    order_item *item = rs->group_by_list;
    if (item) {
      do {
        if (groupby_on_par_key(item, schema_name, table_name, table_alias,
                               *it_keys)) {
          find_key = true;
          break;
        }
        item = item->next;
      } while (item != rs->group_by_list);
    }
    if (!find_key) return false;
  }

  return true;
}

ExecuteNode *Statement::generate_order_by_subplan(
    ExecutePlan *plan, record_scan *rs, list<AggregateDesc> &aggr_list,
    list<AvgDesc> &avg_list, ExecuteNode **tail_node) {
  ExecuteNode *top_node = NULL;
  ExecuteNode *bottom_node = NULL;
  string new_sql;

  string original_sql(fetch_sql_tmp);
  const char *query_sql = original_sql.c_str();

  /* If the groupby list contains all partition keys, all the records related
   * to the same group will located in only one partition. In that case, we
   * can consider the groupby as a orderby, so that we can ignore the
   * aggregate functions and push order by and limit clauses to the fetch
   * nodes. Finally, we can get a better performance.*/

  // generate new SQL
  new_sql.append(query_sql + rs->start_pos - 1, rs->from_pos - rs->start_pos);
#ifndef DBSCALE_TEST_DISABLE
  Backend *bk = Backend::instance();
  dbscale_test_info *test_info = bk->get_dbscale_test_info();
  if (!strcasecmp(test_info->test_case_name.c_str(), "reparse_sql") &&
      !strcasecmp(test_info->test_case_operation.c_str(), "wrong_from_pos")) {
    new_sql.append("   ");
    LOG_DEBUG("test reparse_sql with wrong_from_pos\n");
  }
#endif

  if (rs->group_by_list) {
    if (!groupby_all_par_keys) {
      ExecuteNode *group_node = plan->get_group_node(&group_by_list, aggr_list);
      bottom_node = group_node;
      top_node = group_node;
      if (!avg_list.empty()) {
        ExecuteNode *avg_node = plan->get_avg_node(avg_list);
        avg_node->add_child(top_node);
        top_node = avg_node;
      }
      if (!select_expr_list.empty()) {
        ExecuteNode *calculate_expr_node =
            plan->get_expr_calculate_node(select_expr_list);
        calculate_expr_node->add_child(top_node);
        top_node = calculate_expr_node;
      }

      /* group by with having with aggregate function */
      if (rs->having) {
        if (!simple_having) {  // having with aggregate function
          ExecuteNode *having_node = NULL;
          having_node = plan->get_expr_filter_node(this->having);
          having_node->add_child(top_node);
          top_node = having_node;
          new_sql.append(query_sql + rs->from_pos - 1,
                         rs->having_pos - rs->from_pos);
          if (rs->order_by_list) {
            if (!order_in_group_flag) {
              ExecuteNode *order_node =
                  plan->get_single_sort_node(&order_by_list);
              order_node->add_child(top_node);
              top_node = order_node;
            }
          }
        } else {  // with having which can push down
          if (rs->order_by_list) {
            new_sql.append(query_sql + rs->from_pos - 1,
                           rs->order_pos - rs->from_pos);
            if (!order_in_group_flag) {
              ExecuteNode *order_node =
                  plan->get_single_sort_node(&order_by_list);
              order_node->add_child(top_node);
              top_node = order_node;
            }
          } else if (rs->limit) {
            new_sql.append(query_sql + rs->from_pos - 1,
                           rs->limit_pos - rs->from_pos);
          } else {
            new_sql.append(query_sql + rs->from_pos - 1);
          }
        }
      } else {  // without having
        if (rs->order_by_list) {
          new_sql.append(query_sql + rs->from_pos - 1,
                         rs->order_pos - rs->from_pos);
          if (!order_in_group_flag) {
            ExecuteNode *order_node =
                plan->get_single_sort_node(&order_by_list);
            order_node->add_child(top_node);
            top_node = order_node;
          }
        } else if (rs->limit) {
          new_sql.append(query_sql + rs->from_pos - 1,
                         rs->limit_pos - rs->from_pos);
        } else {
          new_sql.append(query_sql + rs->from_pos - 1);
        }
      }
    } else {  // group by can be push down
      if (!avg_list.empty()) release_avg_list(avg_list);
      if (rs->order_by_list) {
        if (order_in_group_flag) {
          ExecuteNode *sort_node = plan->get_sort_node(&group_by_list);
          top_node = sort_node;
          bottom_node = sort_node;
        } else {
          ExecuteNode *sort_node = plan->get_sort_node(&order_by_list);
          top_node = sort_node;
          bottom_node = sort_node;
        }
      } else {
        ExecuteNode *sort_node = plan->get_sort_node(&group_by_list);
        top_node = sort_node;
        bottom_node = sort_node;
      }
      if (MYSQL_VERSION_8 ==
              Backend::instance()->get_backend_server_version() &&
          !rs->order_by_list) {
        if (rs->limit) {
          new_sql.append(query_sql + rs->from_pos - 1,
                         rs->limit_pos - rs->from_pos);
        } else {
          new_sql.append(query_sql + rs->from_pos - 1);
        }
        order_item *oi = rs->group_by_list;
        if (oi) {
          new_sql.append(" ORDER BY ");
          while (oi) {
            string s;
            oi->field->field_expr->to_string(s);
            if (oi->field->field_expr->is_plain_expression()) {
              size_t pos = 0;
              while (pos < s.size() && s.find(".", pos) != string::npos) {
                pos = s.find(".", pos);
                s.replace(pos, 1, "`.`");
                pos += 3;
              }
              new_sql.append("`").append(s).append("`, ");
            } else {
              new_sql.append(s).append(", ");
            }
            oi = oi->next;
            if (oi == rs->group_by_list) {
              new_sql.erase(new_sql.size() - 2);
              new_sql.append(" ");
              break;
            }
          }
        }
        if (rs->limit) new_sql.append(query_sql + rs->limit_pos - 1);
      } else {
        new_sql.append(query_sql + rs->from_pos - 1);
      }
    }
  } else {  // without group by
    if (rs->order_by_list) {
      ExecuteNode *sort_node = plan->get_sort_node(&order_by_list);
      top_node = sort_node;
      bottom_node = sort_node;
    }
    if (!avg_list.empty()) release_avg_list(avg_list);

    new_sql.append(query_sql + rs->from_pos - 1);
  }

  order_item *oi = rs->group_by_list;
  if (Backend::instance()->get_backend_server_version() == MYSQL_VERSION_8 &&
      oi && !groupby_all_par_keys) {
    new_sql.append(" ORDER BY ");
    while (oi) {
      string s;
      oi->field->field_expr->to_string(s);
      if (oi->field->field_expr->is_plain_expression()) {
        size_t pos = 0;
        while (pos < s.size() && s.find(".", pos) != string::npos) {
          pos = s.find(".", pos);
          s.replace(pos, 1, "`.`");
          pos += 3;
        }
        new_sql.append("`").append(s).append("`, ");
      } else {
        new_sql.append(s).append(", ");
      }
      oi = oi->next;
      if (oi == rs->group_by_list) {
        new_sql.erase(new_sql.size() - 2);
        break;
      }
    }
  }
  fetch_sql_tmp = new_sql;
  LOG_DEBUG("Orginal SQL: %s\n", query_sql);
  LOG_DEBUG("    New SQL: %s\n", new_sql.c_str());

  *tail_node = bottom_node;
  return top_node;
}

void Statement::reset_avg_sql(string column_name, AvgDesc &desc) {
  string tmp_str = "";

  if (desc.count_index == 0) {
    tmp_str = "COUNT(";
    tmp_str.append(column_name);
    tmp_str += ") ";

    new_select_items.push_back(tmp_str);
    desc.count_index = --new_select_item_num;
  }
  if (desc.sum_index == 0) {
    tmp_str = "SUM(";
    tmp_str.append(column_name);
    tmp_str += ") ";

    new_select_items.push_back(tmp_str);
    desc.sum_index = --new_select_item_num;
  }
}

void init_avgdesc(AvgDesc &desc) {
  desc.count_index = 0;
  desc.avg_index = 0;
  desc.sum_index = 0;
  desc.mpf_inited = true;
  mpf_init2(desc.value, DECIMAL_STORE_BIT);
}
void release_avg_list(list<AvgDesc> &avg_list) {
  for (auto &desc : avg_list) {
    if (desc.mpf_inited) mpf_clear(desc.value);
  }
  avg_list.clear();
}

void Statement::handle_avg_function(record_scan *rs, list<AvgDesc> &avg_list) {
  AvgDesc desc;
  CurrentStatementFunctionType type;
  int field_num = 0;
  field_item *field = rs->field_list_head;
  while (field) {
    if (field->field_expr) {
      type = field->field_expr->get_cur_func_type();
      if (type == AGGREGATE_TYPE_AVG) {
        init_avgdesc(desc);
        desc.avg_index = field_num;
        string column_name = "";
        try {
          if (dynamic_cast<FunctionExpression *>(field->field_expr)) {
            ((FunctionExpression *)field->field_expr)
                ->param_list->to_string(column_name);
          } else if (ArithmeticUnaryExpression *unary_expr =
                         dynamic_cast<ArithmeticUnaryExpression *>(
                             field->field_expr)) {
            // means select -avg() ...
            if (FunctionExpression *avg_expr =
                    (dynamic_cast<FunctionExpression *>(unary_expr->right))) {
              avg_expr->param_list->to_string(column_name);
            } else {
              LOG_ERROR(
                  "Unsupport multi ArithmeticUnaryExpression before AVG "
                  "function\n");
            }
          } else {
            LOG_ERROR(
                "Unsupport complex Arithmetic expression before AVG "
                "function\n");
          }
        } catch (...) {
          if (desc.mpf_inited) mpf_clear(desc.value);
          throw;
        }
        reset_avg_sql(column_name, desc);
        avg_list.push_back(desc);
      }
    }
    ++field_num;
    field = field->next;
  }
  LOG_DEBUG("in get_avg_function avg_list.size=[%d]\n", avg_list.size());
}

bool Statement::check_field_list_invalid_distinct(record_scan *rs) {
  if (!(rs->options & SQL_OPT_DISTINCT || rs->options & SQL_OPT_DISTINCTROW))
    return false;
  bool contain_valid_field = false;
  field_item *field = rs->field_list_head;
  while (field) {
    if (field->field_expr && field->field_expr->type != EXPR_STR) {
      contain_valid_field = true;
      break;
    }
    field = field->next;
  }
  return contain_valid_field;
}

bool Statement::check_field_list_contains_invalid_function(record_scan *rs) {
  bool contain_valid_function = false;
  field_item *field = rs->field_list_head;
  while (field) {
    if (field->field_expr &&
        is_expression_contain_invaid_function(field->field_expr, rs, false)) {
      contain_valid_function = true;
      break;
    }
    field = field->next;
  }
  return contain_valid_function;
}

void Statement::handle_select_expr(record_scan *rs) {
  SelectExprDesc desc;
  int field_num = 0;
  field_item *field = rs->field_list_head;
  while (field) {
    if (!field->field_expr ||
        !is_expression_contain_aggr(field->field_expr, rs, false)) {
      field = field->next;
      ++field_num;
      continue;
    }
    need_deal_field_expr = false;
    handle_expression(field->field_expr, NULL, rs, FIELD_EXPR);
    if (need_deal_field_expr) {
      desc.index = field_num;
      desc.expr = field->field_expr;
      select_expr_list.push_back(desc);
    }
    ++field_num;
    field = field->next;
  }
}

bool Statement::has_and_rebuild_autocommit_sql(ExecutePlan *plan) {
  bool is_on = plan->session->get_auto_commit_is_on();
  const char *auto_commit_value = "0";
  if (is_on) {
    auto_commit_value = "1";
  }
  bool has_autocommit = false;
  int head_pos = 0, tail_pos = 0;
  string replace_sql;
  const char *tmp = sql_tmp.c_str();
  session_var_item *session_var_list = st.session_var_list_head;
  while (session_var_list != NULL) {
    string var_name = session_var_list->value;
    boost::to_upper(var_name);
    if (!strcmp(var_name.c_str(), "AUTOCOMMIT")) {
      replace_sql.append(tmp + tail_pos,
                         session_var_list->start_pos_at_flag - tail_pos - 1);
      head_pos = session_var_list->start_pos_at_flag;
      tail_pos = session_var_list->end_pos;
      replace_sql.append(auto_commit_value);
      if (session_var_list->rs->where_pos == 0 ||
          session_var_list->rs->where_pos >
              (unsigned int)session_var_list->start_pos_at_flag) {
        replace_sql.append(" AS '");
        replace_sql.append(tmp + head_pos - 1, tail_pos - head_pos + 1);
        replace_sql.append("'");
      }
      has_autocommit = true;
    }
    session_var_list = session_var_list->next;
  }

  if (has_autocommit) {
    replace_sql.append(tmp + tail_pos, strlen(tmp) - tail_pos);
    LOG_DEBUG("replace sql is %s.\n", replace_sql.c_str());
    sql_tmp = replace_sql;
  }

  return has_autocommit;
}

void Statement::replace_sql_function_field(
    PreviousStatementFunctionType func_type, string &sql, record_scan *rs,
    ExecutePlan *plan) {
#ifdef DEBUG
  ACE_ASSERT(func_type == FUNCTION_TYPE_ROW_COUNT ||
             func_type == FUNCTION_TYPE_LAST_INSERT_ID);
#endif

  int head_pos = 0, tail_pos = 0;
  char new_num[128];
  int64_t number = 0;
  field_item *field = rs->field_list_tail;
  switch (func_type) {
    case FUNCTION_TYPE_ROW_COUNT: {
      head_pos = rs->row_count_head_pos;
      tail_pos = rs->row_count_tail_pos;
      number = plan->session->get_affected_rows();
      field_item *tmp_field = field;
      while (tmp_field) {
        if (tmp_field->field_expr->has_function_type(FUNCTION_TYPE_ROW_COUNT)) {
          if (!tmp_field->alias) {
            sql = add_original_field(sql.c_str(), tmp_field->head_pos,
                                     tmp_field->tail_pos);
          }
        }
        tmp_field = tmp_field->pre;
      }
      break;
    }
    case FUNCTION_TYPE_LAST_INSERT_ID: {
      head_pos = rs->last_insert_id_head_pos;
      tail_pos = rs->last_insert_id_tail_pos;
      number = plan->handler->get_session()->get_last_insert_id();
      field_item *tmp_field = field;
      while (tmp_field) {
        if (tmp_field->field_expr->has_function_type(
                FUNCTION_TYPE_LAST_INSERT_ID)) {
          if (!tmp_field->alias) {
            sql = add_original_field(sql.c_str(), tmp_field->head_pos,
                                     tmp_field->tail_pos);
          }
        }
        tmp_field = tmp_field->pre;
      }
      break;
    }
    default: {
#ifdef DEBUG
      ACE_ASSERT(0);
#endif
      break;
    }
  }
  string head_str, tail_str;
  head_str.assign(sql, 0, head_pos - 1);
  tail_str.assign(sql, tail_pos, sql.length() - tail_pos);

  int len = sprintf(new_num, "%ld", number);
  head_str.append(new_num, len);

  sql.clear();
  sql.append(head_str.c_str());
  sql.append(tail_str.c_str());
}

bool Statement::is_set_password(ExecutePlan *plan) {
  if (!st.sql->set_oper || st.sql->set_oper->names) return false;

  expr_list_item *head =
      ((ListExpression *)st.sql->set_oper->set_list)->expr_list_head;
  CompareExpression *cmp = (CompareExpression *)head->expr;
  VarExpression *var_expression = (VarExpression *)cmp->left;
  if (head == head->next &&
      !strcasecmp(var_expression->str_value, "PASSWORD")) {
    if (plan->session->is_in_lock()) {
      LOG_ERROR("Forbid set password in lock table mode.\n");
      throw NotSupportedError("Not support set password in lock table mode.");
    }
    const char *com_str = NULL;
    if (st.sql->set_oper->user_name) {
      com_str = st.sql->set_oper->user_name;
    } else {
      com_str = plan->session->get_username();
    }
    if (!strcmp(dbscale_internal_user, com_str)) {
      LOG_ERROR("Not support change password for dbscale internal user.\n");
      throw NotSupportedError(
          "Not support set password for dbscale internal user.");
    }
    if (!strcmp(admin_user, com_str)) {
      LOG_ERROR("Not support change password for admin user now.\n");
      throw NotSupportedError("Not support set password for admin user now.");
    }
    Backend *backend = Backend::instance();
    map<string, DataSource *> so_map;
    backend->get_data_sources(so_map);
    map<string, DataSource *>::iterator so_it = so_map.begin();
    for (; so_it != so_map.end(); so_it++) {
      const char *kept_user = so_it->second->get_user();
      if (kept_user && !strcmp(kept_user, com_str)) {
        LOG_ERROR("Not support change password for dbscale keep user.\n");
        throw NotSupportedError(
            "Not support change password for dbscale keep user.");
      }
    }
    map<string, DataServer *> se_map;
    backend->get_data_servers(se_map);
    map<string, DataServer *>::iterator se_it = se_map.begin();
    for (; se_it != se_map.end(); se_it++) {
      const char *kept_user = se_it->second->get_user();
      if (kept_user && !strcmp(kept_user, com_str)) {
        LOG_ERROR("Not support change password for dbscale keep user.\n");
        throw NotSupportedError(
            "Not support change password for dbscale keep user.");
      }
    }

    // the SET PASSWORD stmt will be:
    // 1. SET PASSWORD=password('xxx'), the password is a function
    // 2. SET PASSWORD = 'xxx', the password is a simple string
    StrExpression *str_expr = NULL;
    Expression *expr = cmp->right;
    if (expr->type == EXPR_FUNC) {
      FunctionExpression *func_expr = (FunctionExpression *)expr;
      StrExpression *name_expr = func_expr->name;
      if (strcasecmp(name_expr->str_value, "PASSWORD") == 0) {
        ListExpression *param_list = func_expr->param_list;
        if (param_list->expr_list_head->expr->type == EXPR_STRING) {
          str_expr = (StrExpression *)(param_list->expr_list_head->expr);
        }
      }
    } else if (expr->type == EXPR_STRING) {
      str_expr = (StrExpression *)expr;
    }

    if (str_expr != NULL) {
      const char *password = str_expr->str_value;
      string err_msg;
      bool is_legal = validate_password_complex(password, err_msg);
      if (!is_legal) {
        LOG_ERROR("Password is illegal, %s.\n", err_msg.c_str());
        throw Error(err_msg.c_str());
      }
    }

    replace_set_pass_host();
    st.type = STMT_SET_PASSWORD;
    return true;
  }
  return false;
}

void Statement::deal_set_var() {
  if (!st.sql->set_oper) return;

  if (st.sql->set_oper->names) {
    uservar_list.clear();
    sessionvar_list.clear();
    sessionvar_list.push_back("CHARACTER_SET_CLIENT");
    sessionvar_list.push_back("CHARACTER_SET_RESULTS");
    string select_var_sql(
        "SELECT @@CHARACTER_SET_CLIENT, @@CHARACTER_SET_RESULTS");
    if (!st.sql->set_oper->no_character_set_connection) {
      sessionvar_list.push_back("CHARACTER_SET_CONNECTION");
      select_var_sql.append(", @@CHARACTER_SET_CONNECTION");
    }
    set_select_var_sql(select_var_sql);
    return;
  }

  uservar_list.clear();
  sessionvar_list.clear();
  expr_list_item *head = NULL, *tmp;
  head = ((ListExpression *)st.sql->set_oper->set_list)->expr_list_head;
  tmp = head;

  string uservar_sql;
  string sessionvar_sql;
  CompareExpression *cmp;
  VarExpression *var_expression;
  do {
    cmp = (CompareExpression *)tmp->expr;
    var_expression = (VarExpression *)cmp->left;
    string var(var_expression->str_value);
    boost::to_upper(var);
    if (var_expression->var_scope == VAR_SCOPE_USER) {
      if (uservar_list.size() != 0) {
        uservar_sql.append(", ");
      }
      uservar_sql.append("@");
      uservar_sql.append(var);
      uservar_list.push_back(var);
    } else if (var_expression->var_scope == VAR_SCOPE_SESSION) {
      if (useful_session_vars.count(var)) {
        if (sessionvar_list.size() != 0) {
          sessionvar_sql.append(", ");
        }
        sessionvar_sql.append("@@");
        sessionvar_sql.append(var);
        sessionvar_list.push_back(var);
      }
    }

    tmp = tmp->next;
  } while (tmp != head);

  if (uservar_list.empty() && sessionvar_list.empty()) {
    return;
  }
  string select_var_sql("SELECT ");
  if (uservar_list.empty()) {
    select_var_sql += sessionvar_sql;
    set_select_var_sql(select_var_sql);
    return;
  }
  select_uservar_flag = true;
  select_var_sql += uservar_sql;
  if (!sessionvar_list.empty()) {
    select_var_sql += ",";
    select_var_sql += sessionvar_sql;
  }
  set_select_var_sql(select_var_sql);
}

void Statement::handle_session_var(ExecutePlan *plan) {
  if (!st.sql->set_oper || st.sql->set_oper->names) {
    has_ignore_session_var = false;
    return;
  }

  Backend *backend = Backend::instance();
  unsigned int pos = 0;
  unsigned int var_num = 0;
  bool is_auto_commit = false;
  expr_list_item *head = NULL, *tmp;
  head = ((ListExpression *)st.sql->set_oper->set_list)->expr_list_head;
  tmp = head;

  CompareExpression *cmp;
  VarExpression *var_expression;
  string next_trx_level_value = "";
  do {
    cmp = (CompareExpression *)tmp->expr;
    var_expression = (VarExpression *)cmp->left;
    string var(var_expression->str_value);
    boost::to_upper(var);
    if (get_session()->is_in_transaction() &&
        var_expression->var_scope == VAR_SCOPE_SESSION &&
        var == "TRANSACTION_ISOLATION") {
      LOG_ERROR(
          "Transaction characteristics can't be changed while a transaction is "
          "in progress.\n");
      throw HandlerError(
          "Transaction characteristics can't be changed while a transaction is "
          "in progress.",
          1568);
    }
    if ((static_cast<VarExpression *>(var_expression))->next_trx_level) {
      if (boost::regex_match(
              var, boost::regex("TX_ISOLATION", boost::regex::icase))) {
        LOG_ERROR(
            "Unsupport set @@TX_ISOLATION, please use set "
            "@@TRANSACTION_ISOLATION.\n");
        throw NotSupportedError(
            "Unsupport set @@TX_ISOLATION, please use set "
            "@@TRANSACTION_ISOLATION.");
      }
      boost::regex support_level(
          "READ-COMMITTED|READ-UNCOMMITTED|REPEATABLE-READ",
          boost::regex::icase);
      if (!boost::regex_match(cmp->right->str_value, support_level)) {
        string err_msg = "Unsupport set @@TRANSACTION_ISOLATION = ";
        err_msg += cmp->right->str_value;
        LOG_ERROR("%s.\n", err_msg.c_str());
        throw NotSupportedError(err_msg.c_str());
      }
      next_trx_level_value.assign(cmp->right->str_value);
      has_ignore_session_var = true;
      session_var_sql.append(sql, pos, cmp->start_pos - pos - 1);
      pos = (tmp->next != head) ? tmp->next->expr->start_pos : cmp->end_pos;
      tmp = tmp->next;
      continue;
    }

    if (is_forbidden_session_var(
            var, Backend::instance()->get_backend_server_version() ==
                     MYSQL_VERSION_MARIADB)) {
      LOG_WARN("Ignore the unsupport variable %s.\n", var.c_str());
      has_ignore_session_var = true;
      session_var_sql.append(sql, pos, cmp->start_pos - pos - 1);
      pos = (tmp->next != head) ? tmp->next->expr->start_pos : cmp->end_pos;
      tmp = tmp->next;
      continue;
    }

    if (var_expression->var_scope != VAR_SCOPE_SESSION) {
      if (var_expression->var_scope == VAR_SCOPE_GLOBAL) {
        has_ignore_session_var = true;
        session_var_sql.append(sql, pos, cmp->start_pos - pos - 1);
        pos = (tmp->next != head) ? tmp->next->expr->start_pos : cmp->end_pos;
        tmp = tmp->next;
        continue;
      }
      tmp = tmp->next;
      ++var_num;
      continue;
    } else if (var == "TRANSACTION_ISOLATION") {
      next_trx_level_value.clear();
      get_session()->overwrite_session_next_trx_level();
    }
    if (!strcmp(var.c_str(), "AUTOCOMMIT")) {
      string value;
      try {
        cmp->right->to_string(value);
      } catch (exception &e) {
        LOG_ERROR("Unsupport set session.\n");
        throw NotSupportedError("Unsupport set session");
      }
      bool auto_value = check_and_transform_autocommit_value(value);
      plan->session->set_stmt_auto_commit_value(auto_value);
      is_auto_commit = true;
    }
    if (is_auto_commit || backend->is_ignore_session_var(var)) {
      has_ignore_session_var = true;
      session_var_sql.append(sql, pos, cmp->right->start_pos - pos - 1);
      session_var_sql.append("@@");
      session_var_sql.append(var);
      pos = cmp->right->end_pos;
      is_auto_commit = false;
    } else {
      useful_session_vars.insert(var);
      ++var_num;
    }
    tmp = tmp->next;
  } while (tmp != head);

  if (!next_trx_level_value.empty()) {
    next_trx_level_value = "\'" + next_trx_level_value + "\'";
    get_session()->prepare_set_session_next_trx_level(next_trx_level_value);
  }
  if (!get_session()->get_auto_commit_is_on() ||
      !get_session()->get_stmt_auto_commit_int_value()) {
    get_session()->set_session_next_trx_level();
  }

  if (has_ignore_session_var) {
    if (var_num == 0) {
      session_var_sql.assign("");
      LOG_DEBUG("After rebuild ignore/skip all session var.\n");
    } else {
      session_var_sql.append(sql, pos, strlen(sql) - pos);
      LOG_DEBUG("rebuild session var sql : %s.\n", session_var_sql.c_str());
    }
  }
}

void Statement::build_uservar_sql(var_item *var_item_list, ExecutePlan *plan) {
  Session *session = plan->session;
  map<string, string> *user_var_map = session->get_user_var_map();

  string var_sql = "SET ";
  string select_var_sql;
  set<string> uservar_set;
  while (var_item_list != NULL) {
    string var_name = var_item_list->value;
    boost::to_upper(var_name);
    if (!uservar_set.count(var_name)) {
      uservar_set.insert(var_name);

      var_sql.append("@");
      var_sql.append(var_name);
      var_sql.append("=");

      if (user_var_map->count(var_name)) {
        var_sql.append((*user_var_map)[var_name]);
      } else {
        var_sql.append("NULL");
      }
      var_sql.append(", ");
    }

    if (var_item_list->expr) {
      VarExpression *expr = (VarExpression *)var_item_list->expr;
      if (user_var_map->count(var_name)) {
        expr->set_user_var_null(false);
        expr->set_user_var_value(((*user_var_map)[var_name]).c_str());
      } else {
        expr->set_user_var_null(true);
      }
    }

    var_item_list = var_item_list->next;
  }

  boost::erase_tail(var_sql, 2);
  session->set_var_sql(var_sql);
  LOG_DEBUG("Build variable sql is [%s].\n", var_sql.c_str());

  if (select_uservar_flag) {
    return;
  }
  if (uservar_set.empty()) {
    return;
  }
  if (st.type == STMT_CALL) {
    uservar_list.clear();
    select_uservar_flag = true;
    select_var_sql = "SELECT ";
    for (set<string>::iterator it = uservar_set.begin();
         it != uservar_set.end(); ++it) {
      string var_name = *it;
      uservar_list.push_back(var_name);

      select_var_sql.append("@");
      select_var_sql.append(var_name);
      select_var_sql.append(", ");
    }
  }
  boost::erase_tail(select_var_sql, 2);
  this->set_select_var_sql(select_var_sql);
  LOG_DEBUG("Build select variable sql is [%s].\n", select_var_sql.c_str());
}

void Statement::deal_select_assign_uservar(record_scan *rs) {
  field_item *field = rs->field_list_head;
  uservar_list.clear();
  string select_var_sql = "SELECT ";
  unsigned int i = 0;
  while (field) {
    // the field_item should like "@v:=expr"
    Expression *expr = field->field_expr;

    if (expr && expr->type == EXPR_STR) {
      if (select_uservar_flag) {
        if (strchr(((StrExpression *)expr)->str_value, '*') != NULL) {
          LOG_ERROR(
              "The assign user variable field in SELECT statement"
              " must appear after field like '*' or 'TABLE.*.'\n");
          throw NotSupportedError(
              "The assign user variable field must appear after field like '*' "
              "or 'TABLE.*.'");
        }
      }
    }

    if (expr && expr->type != EXPR_ASSIGN) {
      ++i;
      field = field->next;
      continue;
    }

    select_uservar_flag = true;
    CompareExpression *cmp_expr = (CompareExpression *)expr;
    VarExpression *var_expr = (VarExpression *)(cmp_expr->left);
    string var_name(var_expr->str_value);
    boost::to_upper(var_name);
    pair<unsigned int, string> uservar_pair(i++, var_name);
    select_uservar_vec.push_back(uservar_pair);
    need_clean_select_uservar_vec = true;
    uservar_list.push_back(var_name);
    select_var_sql.append("@");
    select_var_sql.append(var_name);
    select_var_sql.append(", ");
    field = field->next;
  }
  if (select_uservar_flag) {
    select_field_num = i;
    boost::erase_tail(select_var_sql, 2);
    this->set_select_var_sql(select_var_sql);
  }
}

void Statement::rebuild_found_rows_sql(record_scan *rs, ExecutePlan *plan) {
  field_item *field = rs->field_list_tail;
  join_node *tables = rs->join_tables;
  if (!tables || (tables && tables->table_name &&
                  !strcasecmp(tables->table_name, "dual"))) {
    while (field) {
      replace_func_found_rows_with_value(field, rs, plan, field);
      field = field->pre;
    }
  }
  if (tables && tables->type == JOIN_NODE_SUBSELECT) {
    rebuild_found_rows_sql(tables->sub_select, plan);
  }
  if (tables && tables->type == JOIN_NODE_JOIN) {
    while (tables->type == JOIN_NODE_JOIN) {
      join_node *tables_tmp = tables->right;
      if (tables_tmp->type == JOIN_NODE_SUBSELECT) {
        rebuild_found_rows_sql(tables_tmp->sub_select, plan);
      }
      tables = tables->left;
    }
    if (tables->type == JOIN_NODE_SUBSELECT) {
      rebuild_found_rows_sql(tables->sub_select, plan);
    }
  }
}

void Statement::replace_func_found_rows_with_value(field_item *field,
                                                   const record_scan *scan,
                                                   ExecutePlan *plan,
                                                   field_item *top_field) {
  if (field->field_expr->has_function_type(FUNCTION_TYPE_FOUND_ROWS)) {
    if (!top_field->alias) {
      sql_tmp = add_original_field(sql_tmp.c_str(), top_field->head_pos,
                                   top_field->tail_pos);
    }
    sql_tmp = replace_found_rows(sql_tmp.c_str(), plan,
                                 scan->found_rows_pos_list->head_pos,
                                 scan->found_rows_pos_list->tail_pos);
  } else if (field->field_expr->type == EXPR_SUBSELECT) {
    SubSelectExpression *sub_expr = (SubSelectExpression *)field->field_expr;
    const record_scan *sub_select = sub_expr->sub_select;
    field_item *sub_field = sub_select->field_list_tail;
    replace_func_found_rows_with_value(sub_field, sub_select, plan, top_field);
  }
}

string Statement::replace_found_rows(const char *query, ExecutePlan *plan,
                                     unsigned int head_pos,
                                     unsigned int tail_pos) {
  string tmp_sql(query);
  char new_num[32];
  uint64_t number = 0;
  string head_str, tail_str, tmp_str, add_str;

  head_str.assign(tmp_sql, 0, head_pos - 1);
  tail_str.assign(tmp_sql, tail_pos, tmp_sql.length() - tail_pos);
  add_str.assign(tmp_sql, head_pos - 1, tail_pos - head_pos + 1);

  number = plan->session->get_found_rows();
  int len = sprintf(new_num, "%llu", (unsigned long long)number);
  head_str.append(new_num, len);
  if ((tail_pos + 1) >= (head_pos + len)) {
    tmp_str.assign(tail_pos + 1 - head_pos - len, ' ');
  }
  head_str.append(tmp_str);
  tmp_sql.clear();
  tmp_sql.append(head_str.c_str());
  tmp_sql.append(tail_str.c_str());

  return tmp_sql;
}

string Statement::add_original_field(const char *query, unsigned int head_pos,
                                     unsigned int tail_pos) {
  string tmp_sql(query);
  string head_str, tail_str, tmp_str, add_str;

  head_str.assign(tmp_sql, 0, tail_pos);
  tail_str.assign(tmp_sql, tail_pos, tmp_sql.length() - tail_pos);
  add_str.assign(tmp_sql, head_pos - 1, tail_pos - head_pos + 1);

  head_str.append(" AS '");
  head_str.append(add_str.c_str());
  head_str.append("'");
  tmp_sql.clear();
  tmp_sql.append(head_str.c_str());
  tmp_sql.append(tail_str.c_str());

  return tmp_sql;
}

bool function_is_valid(CurrentStatementFunctionType type) {
  return type == AGGREGATE_TYPE_MIN || type == AGGREGATE_TYPE_MAX ||
         type == AGGREGATE_TYPE_COUNT || type == AGGREGATE_TYPE_SUM ||
         type == AGGREGATE_TYPE_AVG;
}

void get_aggregate_functions(record_scan *rs, list<AggregateDesc> &aggr_list) {
  AggregateDesc desc;
  CurrentStatementFunctionType type;
  unsigned int column_index = 0;

  field_item *field = rs->field_list_head;
  while (field) {
    if (field->field_expr) {
      type = field->field_expr->get_cur_func_type();
      if (type != AGGREGATE_TYPE_NON) {
        if (function_is_valid(type)) {
          if (type != AGGREGATE_TYPE_AVG) {
            desc.column_index = column_index;
            desc.type = type;
            aggr_list.push_back(desc);
          }
        }
      }
    }
    ++column_index;
    field = field->next;
  }
}

Expression *get_expr_in_select_items(int column_index, record_scan *rs) {
  field_item *field = rs->field_list_head;
  int i = 0;

  while (i++ < column_index) {
    if (!field) {
      LOG_ERROR("Fail to find [%d] column in fields.\n", column_index);
      throw Error("Fail to find correct column in select list.");
    }
    field = field->next;
  }

  if (!field) {
    LOG_ERROR("Fail to find [%d] column in fields.\n", column_index);
    throw Error("Fail to find correct column in select list.");
  }
  return field->field_expr;
}

int Statement::create_new_select_item(Expression *expr, record_scan *rs,
                                      bool for_order_by_list) {
  list<string>::iterator it;
  string expr_str;
  int column_index;

  if (expr->type == EXPR_INT) {
    if (rs->is_contain_star) {
      LOG_ERROR(
          "Not support ORDER BY or GROUP BY column index with '*' in select "
          "list.\n");
      throw Error(
          "Not support ORDER BY or GROUP BY column index with '*' in select "
          "list.");
    }
    column_index = (int)(((IntExpression *)expr)->get_integer_value()) - 1;
    if (column_index < 0) {
      LOG_ERROR("Unknown column in order clause.\n");
      throw Error("Unknown column in order clause.");
    }
  } else {
    column_index = item_in_select_fields_pos(expr, rs);
  }

  if (column_index != COLUMN_INDEX_UNDEF) {
    expr = get_expr_in_select_items(column_index, rs);
  }

  if (!rs->is_contain_star && column_index != COLUMN_INDEX_UNDEF) {
    return column_index;
  } else {
    expr->to_string(expr_str);
    int i = 0;
    for (it = new_select_items.begin(); it != new_select_items.end();
         it++, i--) {
      if (strcasecmp(it->c_str(), expr_str.c_str()) == 0) {
        return i - 1;
      }
    }
    if (expr->type == EXPR_STR) {
      size_t first_dot_pos = expr_str.find(".");
      size_t second_dot_pos = expr_str.find(".", first_dot_pos + 1);

      if (first_dot_pos == string::npos) {
        string str("`");
        str.append(expr_str);
        str.append("`");
        new_select_items.push_back(str);
      } else if (second_dot_pos == string::npos) {
        string str("`");
        str.append(expr_str, 0, first_dot_pos);
        str.append("`.`");
        str.append(expr_str, first_dot_pos + 1,
                   expr_str.length() - first_dot_pos - 1);
        str.append("`");
        new_select_items.push_back(str);
      } else {
        string str("`");
        str.append(expr_str, 0, first_dot_pos);
        str.append("`.`");
        str.append(expr_str, first_dot_pos + 1,
                   second_dot_pos - first_dot_pos - 1);
        str.append("`.`");
        str.append(expr_str, second_dot_pos + 1,
                   expr_str.length() - second_dot_pos - 1);
        str.append("`");
        new_select_items.push_back(str);
      }
    } else {
      if (for_order_by_list && !expr->check_contains_str_expression()) {
        LOG_ERROR(
            "Only support ORDER BY contains column names or one single int "
            "number.\n");
        throw Error(
            "Only support ORDER BY contains column names or one single int "
            "number.");
      }
      new_select_items.push_back(expr_str);
    }
    return --new_select_item_num;
  }
}

bool Statement::is_expression_contain_invaid_function(
    Expression *expr, record_scan *rs, bool error_on_unknown_expr,
    bool is_check_for_update) {
  FieldFunctionSituation type = expression_contain_function_type(
      expr, rs, error_on_unknown_expr, is_check_for_update);
  if (type == CONTAINS_OTHER_FUNC) {
    return true;
  }
  return false;
}

bool Statement::is_expression_contain_aggr(Expression *expr, record_scan *rs,
                                           bool error_on_unknown_expr,
                                           bool is_check_for_update) {
  bool ret = false;
  FieldFunctionSituation type = expression_contain_function_type(
      expr, rs, error_on_unknown_expr, is_check_for_update);
  if (type == CONTAINS_VALID_AGGR)
    ret = true;
  else if (type == CONTAINS_NO_FUNCTION)
    ret = false;
  else {
    string expr_str;
    expr->to_string(expr_str);
    LOG_ERROR("Unsupport aggregate function in expression %s.\n",
              expr_str.c_str());
    throw UnSupportPartitionSQL(
        "Unsupport expression with "
        "invalid aggregate function.");
  }
  return ret;
}

FieldFunctionSituation Statement::expression_contain_function_type(
    Expression *expr, record_scan *rs, bool error_on_unknown_expr,
    bool is_check_for_update) {
  expr_type type = expr->type;
  switch (type) {
    case EXPR_IS:
    case EXPR_IS_NOT:
    case EXPR_EQ:
    case EXPR_GR:
    case EXPR_LESS:
    case EXPR_GE:
    case EXPR_LESSE:
    case EXPR_NE:
    case EXPR_EQ_N:
    case EXPR_AND:
    case EXPR_OR:
    case EXPR_XOR: {
      BoolBinaryExpression *bi_expr = (BoolBinaryExpression *)expr;
      FieldFunctionSituation ret = expression_contain_function_type(
          bi_expr->left, rs, error_on_unknown_expr, is_check_for_update);
      if (ret == CONTAINS_VALID_AGGR || ret == CONTAINS_OTHER_FUNC)
        return ret;
      else
        return expression_contain_function_type(
            bi_expr->right, rs, error_on_unknown_expr, is_check_for_update);
    } break;

    case EXPR_IF_COND:
    case EXPR_WHEN_THEN: {
      ConditionExpression *cond_expr = (ConditionExpression *)expr;
      FieldFunctionSituation ret = expression_contain_function_type(
          cond_expr->left, rs, error_on_unknown_expr, is_check_for_update);
      if (ret == CONTAINS_VALID_AGGR || ret == CONTAINS_OTHER_FUNC)
        return ret;
      else
        return expression_contain_function_type(
            cond_expr->right, rs, error_on_unknown_expr, is_check_for_update);
    } break;

    case EXPR_ADD:
    case EXPR_SUB:
    case EXPR_MUL:
    case EXPR_DIV: {
      ArithmeticBinaryExpression *bi_expr = (ArithmeticBinaryExpression *)expr;
      FieldFunctionSituation ret = expression_contain_function_type(
          bi_expr->left, rs, error_on_unknown_expr, is_check_for_update);
      if (ret == CONTAINS_VALID_AGGR || ret == CONTAINS_OTHER_FUNC)
        return ret;
      else
        return expression_contain_function_type(
            bi_expr->right, rs, error_on_unknown_expr, is_check_for_update);
    } break;

    case EXPR_MINUS: {
      ArithmeticUnaryExpression *un_expr = (ArithmeticUnaryExpression *)expr;
      return expression_contain_function_type(
          un_expr->right, rs, error_on_unknown_expr, is_check_for_update);
    } break;

    case EXPR_IF:
    case EXPR_IFNULL:
    case EXPR_CASE: {
      bool contains_valid_function = false;
      TerCondExpression *case_expr = (TerCondExpression *)expr;
      expr_list_item *item = case_expr->list_expr->expr_list_head;
      do {
        FieldFunctionSituation ret = expression_contain_function_type(
            item->expr, rs, error_on_unknown_expr, is_check_for_update);
        if (ret == CONTAINS_OTHER_FUNC)
          return ret;
        else if (ret == CONTAINS_VALID_AGGR)
          contains_valid_function = true;
        item = item->next;
      } while (item != case_expr->list_expr->expr_list_head);
      if (case_expr->else_expr) {
        FieldFunctionSituation ret = expression_contain_function_type(
            case_expr->else_expr, rs, error_on_unknown_expr,
            is_check_for_update);
        if (ret == CONTAINS_OTHER_FUNC || ret == CONTAINS_VALID_AGGR)
          return ret;
      }
      if (contains_valid_function) return CONTAINS_VALID_AGGR;
      return CONTAINS_NO_FUNCTION;
    } break;

    case EXPR_FUNC:
    case EXPR_STR: {
      if (is_check_for_update) return CONTAINS_VALID_AGGR;
      int col_index;
      CurrentStatementFunctionType func_type;
      col_index = item_in_select_fields_pos(expr, rs);
      if (col_index != COLUMN_INDEX_UNDEF) {
        expr = get_expr_in_select_items(col_index, rs);
      }
      func_type = expr->get_cur_func_type();
      if (func_type != AGGREGATE_TYPE_NON) {
        if (function_is_valid(func_type)) {
          return CONTAINS_VALID_AGGR;
        } else {
          return CONTAINS_OTHER_FUNC;
        }
      }

      if (expr->check_contains_aggregate_function()) return CONTAINS_OTHER_FUNC;
      return CONTAINS_NO_FUNCTION;
    } break;

    case EXPR_INT:
    case EXPR_STRING:
    case EXPR_FLOAT:
    case EXPR_BOOL:
    case EXPR_VAR:
      break;

    case EXPR_NULL:
      break;

    default: {
      if (error_on_unknown_expr) {
        LOG_ERROR("Filter expression %d is too complex.\n", type);
        throw NotImplementedError("Filter expression is too complex.");
      }
      return CONTAINS_NO_FUNCTION;
    } break;
  }
  return CONTAINS_NO_FUNCTION;
}

void Statement::handle_expression(Expression *expr, Expression **parent,
                                  record_scan *rs,
                                  STMT_EXPR_TYPE stmt_expr_type) {
  expr_type type = expr->type;
  switch (type) {
    case EXPR_EQ:
    case EXPR_GR:
    case EXPR_LESS:
    case EXPR_GE:
    case EXPR_LESSE:
    case EXPR_NE:
    case EXPR_EQ_N:
    case EXPR_AND:
    case EXPR_OR:
    case EXPR_XOR: {
      BoolBinaryExpression *bi_expr = (BoolBinaryExpression *)expr;
      handle_expression(bi_expr->left, &bi_expr->left, rs, stmt_expr_type);
      handle_expression(bi_expr->right, &bi_expr->right, rs, stmt_expr_type);
    } break;

    case EXPR_IF_COND:
    case EXPR_WHEN_THEN: {
      ConditionExpression *cond_expr = (ConditionExpression *)expr;
      handle_expression(cond_expr->left, &cond_expr->left, rs, stmt_expr_type);
      handle_expression(cond_expr->right, &cond_expr->right, rs,
                        stmt_expr_type);
    } break;

    case EXPR_ADD:
    case EXPR_SUB:
    case EXPR_MUL:
    case EXPR_DIV: {
      ArithmeticBinaryExpression *bi_expr = (ArithmeticBinaryExpression *)expr;
      handle_expression(bi_expr->left, &bi_expr->left, rs, stmt_expr_type);
      handle_expression(bi_expr->right, &bi_expr->right, rs, stmt_expr_type);
    } break;
    case EXPR_IS:
    case EXPR_IS_NOT: {
      TruthExpression *truth_expr = (TruthExpression *)expr;
      handle_expression(truth_expr->left, &truth_expr->left, rs,
                        stmt_expr_type);
      handle_expression(truth_expr->right, &truth_expr->right, rs,
                        stmt_expr_type);
    } break;
    case EXPR_MINUS: {
      ArithmeticUnaryExpression *un_expr = (ArithmeticUnaryExpression *)expr;
      handle_expression(un_expr->right, &un_expr->right, rs, stmt_expr_type);
    } break;

    case EXPR_IF:
    case EXPR_IFNULL:
    case EXPR_CASE: {
      TerCondExpression *ter_cond_expr = (TerCondExpression *)expr;
      expr_list_item *item = ter_cond_expr->list_expr->expr_list_head;
      do {
        handle_expression(item->expr, &item->expr, rs, stmt_expr_type);
        item = item->next;
      } while (item != ter_cond_expr->list_expr->expr_list_head);
      if (ter_cond_expr->else_expr)
        handle_expression(ter_cond_expr->else_expr, &ter_cond_expr->else_expr,
                          rs, stmt_expr_type);
    } break;

    case EXPR_FUNC:
    case EXPR_STR: {
      int col_index;
      FieldExpression *field_expr;

      if (stmt_expr_type == HAVING_EXPR) {
        col_index = create_new_select_item(expr, rs);
        if (expr->get_cur_func_type() == AGGREGATE_TYPE_AVG)
          add_avg_func(col_index, expr);
        Expression *tmp = NULL;
        if (parent && *parent) tmp = *parent;
        field_expr = get_field_expression(col_index, tmp);
        new_field_expressions.push_back(field_expr);
        need_clean_new_field_expressions = true;
        if (parent && *parent) {
          *parent = field_expr;
        }
      } else if (stmt_expr_type == FIELD_EXPR) {
        if (parent && *parent) {  // don't deal 'select sum() from table', deal
                                  // 'select sum()+1 from table'
          col_index = create_new_select_item(expr, rs);
          if (expr->get_cur_func_type() == AGGREGATE_TYPE_AVG)
            add_avg_func(col_index, expr);
          field_expr = get_field_expression(col_index, *parent);
          new_field_expressions.push_back(field_expr);
          need_clean_new_field_expressions = true;
          *parent = field_expr;
          need_deal_field_expr = true;
        }
      }
    } break;

    case EXPR_INT:
    case EXPR_STRING:
    case EXPR_FLOAT:
    case EXPR_BOOL:
      break;

    case EXPR_NULL:
      break;

    default:
      if (stmt_expr_type == HAVING_EXPR) {
        LOG_ERROR("Filter expression %d is too complex.", type);
        throw NotImplementedError("Filter expression is too complex.");
      }
      break;
  }
}

void Statement::add_avg_func(int field_num, Expression *expr) {
  AvgDesc desc;
  init_avgdesc(desc);
  try {
    desc.avg_index = field_num;
    string column_name = "";
    ((FunctionExpression *)expr)->param_list->to_string(column_name);
    reset_avg_sql(column_name, desc);
  } catch (...) {
    if (desc.mpf_inited) mpf_clear(desc.value);
    throw;
  }
  avg_list.push_back(desc);
}
ExecuteNode *Statement::generate_aggregate_by_subplan(
    ExecutePlan *plan, record_scan *rs, list<AggregateDesc> &aggr_list,
    list<AvgDesc> &avg_list, ExecuteNode **tail_node) {
  ACE_ASSERT(aggr_list.size());
  ACE_UNUSED_ARG(rs);

  ExecuteNode *head_node = NULL;
  ExecuteNode *aggr_node = plan->get_aggr_node(aggr_list);
  *tail_node = aggr_node;
  head_node = aggr_node;
  if (!avg_list.empty()) {
    ExecuteNode *avg_node = plan->get_avg_node(avg_list);
    avg_node->add_child(head_node);
    head_node = avg_node;
  }
  if (!select_expr_list.empty()) {
    ExecuteNode *calculate_expr_node =
        plan->get_expr_calculate_node(select_expr_list);
    calculate_expr_node->add_child(head_node);
    head_node = calculate_expr_node;
  }

  if (!simple_having) {
    string new_sql;

    ExecuteNode *having_node = plan->get_expr_filter_node(having);
    having_node->add_child(head_node);
    head_node = having_node;
    new_sql.append(fetch_sql_tmp, 0, rs->having_pos - rs->start_pos);

    LOG_DEBUG("Orginal SQL: %s\n", fetch_sql_tmp.c_str());
    fetch_sql_tmp = new_sql;
    LOG_DEBUG("    New SQL: %s\n", new_sql.c_str());
  }
  return head_node;
}

void Statement::re_parser_stmt(record_scan **new_rs, Statement **new_stmt,
                               const char *query_sql) {
  ACE_UNUSED_ARG(new_rs);
  Parser *parser = Driver::get_driver()->get_parser();
  *new_stmt = parser->parse(query_sql, stmt_allow_dot_in_ident, true, NULL,
                            NULL, NULL, ctype);
  re_parser_stmt_list.push_back(*new_stmt);
  if ((*new_stmt)->is_partial_parsed()) {
    LOG_ERROR(
        "not support partial parse when do sql reparse, the sql is [%s]\n",
        query_sql);
    throw Error("got partial parsed sql when do sql reparse");
  }
  *new_rs = (*new_stmt)->get_stmt_node()->scanner;
}

void Statement::set_charset_and_isci_accroding_to_collation(
    string &collation_name, CharsetType &ctype, bool &is_cs) {
  ctype = CHARSET_TYPE_OTHER;
  is_cs = false;
  if (collation_name.find("utf8") != string::npos) {
    ctype = CHARSET_TYPE_UTF8;
    if (collation_name.find("utf8mb4") != string::npos) {
      ctype = CHARSET_TYPE_UTF8MB4;
    }
  }
  if (collation_name.find("bin") != string::npos) is_cs = true;
}

void Statement::add_group_item_list(order_item *start,
                                    list<SortDesc> &group_list,
                                    record_scan *rs) {
  order_item *item = start;
  if (item == NULL) return;
  need_clean_group_by_list = true;
  Expression *expr;
  int column_index;
  string collation_name;
  SortDesc og_desc;
  do {
    expr = item->field->field_expr;
    column_index = create_new_select_item(expr, rs, true);
    collation_name = get_column_collation(expr, rs);
    og_desc.column_index = column_index;
    og_desc.sort_order = item->order;
    set_charset_and_isci_accroding_to_collation(collation_name, og_desc.ctype,
                                                og_desc.is_cs);
    if (!og_desc.is_cs) {
      if (!collation_name.empty())  // The collaction is not empty, so it is a
                                    // char and not utf8_bin/utf8mb4_bin
        rs->group_by_ci = true;
    }
    group_list.push_back(og_desc);
    item = item->next;
  } while (item != start);
}

TableStruct Statement::get_table_struct_from_record_scan(string column_name,
                                                         record_scan *rs) {
  table_link *tl = rs->first_table;
  while (tl) {
    if (tl->join->cur_rec_scan == rs) {
      string schema_name =
          tl->join->schema_name ? tl->join->schema_name : schema;
      string table_name = tl->join->table_name;

      TableInfoCollection *tic = TableInfoCollection::instance();
      TableInfo *ti =
          tic->get_table_info_for_read(schema_name.c_str(), table_name.c_str());
      map<string, TableColumnInfo *, strcasecomp> *column_info_map;
      try {
        column_info_map =
            ti->element_table_column
                ->get_element_map_columnname_table_column_info(stmt_session);
      } catch (...) {
        LOG_ERROR(
            "Error occured when try to get table info column "
            "info(map_columnname_table_column_info) of table [%s.%s]\n",
            schema_name.c_str(), table_name.c_str());
        ti->release_table_info_lock();
        throw;
      }

      string tmp_colname(column_name);
      boost::to_lower(tmp_colname);
      if (column_info_map != NULL) {
        if (column_info_map->count(tmp_colname)) {
          ti->release_table_info_lock();
          TableStruct ret;
          ret.schema_name = schema_name;
          ret.table_name = table_name;
          ret.alias = table_name;
          return ret;
        }
      }
      ti->release_table_info_lock();
    }
    tl = tl->next;
  }
  TableStruct ret;
  return ret;
}

// If return true means this table_name is belong to subquery, which not be
// able to get the real table_name
bool adjust_alias_to_table_name_in_rs(record_scan *rs, string &table_name,
                                      string &schema_name, const char *schema) {
  LOG_DEBUG("adjust_alias_to_table_name_in_rs for table_name %s with %@.\n",
            table_name.c_str(), rs->first_table);
  table_link *table = rs->first_table;

  // TODO: should record the map<table_name, alias> for one record_scan in
  // statement.
  if (!table)  // no table means it is a table subquery stmt
    return true;
  bool find_match_table = false;
  while (table) {
    if (table->join->alias &&
        !strcasecmp(table->join->alias, table_name.c_str())) {
      if (table->join->sub_select) return true;
      table_name.assign(
          table->join->table_name);  // replace the alias to the real table name
      schema_name.assign(table->join->schema_name ? table->join->schema_name
                                                  : schema);
      return false;
    }
    if (table->join->table_name &&
        !strcasecmp(table->join->table_name, table_name.c_str()))
      find_match_table = true;
    table = table->next;
    if (table && table->join->cur_rec_scan != rs) break;
  }
  if (find_match_table) return false;
  /*This table is not find in current record_scan, it may be the merged
   * record_scan. In this situation, we should consider it as not find
   * table.*/
  return true;
}

Expression *find_group_by_alias_expr(string group_str, record_scan *rs) {
  field_item *fi = rs->field_list_head;
  if (fi) {
    do {
      if (fi->alias && !strcasecmp(fi->alias, group_str.c_str())) {
        return fi->field_expr;
      }
      fi = fi->next;
    } while (fi && fi != rs->field_list_tail);
  }

  return NULL;
}

string Statement::get_column_collation(Expression *expr, record_scan *rs,
                                       bool check_alias) {
#ifdef DEBUG
  string tmp_str;
  expr->to_string(tmp_str);
  LOG_DEBUG("Statement::get_column_collation handle expr %s with rs %@.\n",
            tmp_str.c_str(), this);
#endif
  string ret;
  bool has_adjust_alias = false;
  bool fail_to_get_real_table_name = false;
  if (expr->type == EXPR_STR) {
    string column_string;
    expr->to_string(column_string);
    string schema_name;
    string table_name;
    string column_name;
    split_value_into_schema_table_column(column_string, schema_name, table_name,
                                         column_name);
    if (table_name.empty()) {
      TableStruct ts = get_table_struct_from_record_scan(column_name, rs);
      if (check_alias && ts.schema_name.empty()) {
        Expression *tmp_expr = find_group_by_alias_expr(column_string, rs);
        if (tmp_expr) {
          if (tmp_expr == expr) {
            string tmp_str;
            expr->to_string(tmp_str);
            LOG_WARN(
                "Fail to Statement::get_column_collation with expr [%s] for "
                "sql [%s].\n",
                tmp_str.c_str(), sql);
            return ret;
          }
          return get_column_collation(tmp_expr, rs, false);
        }
        return ret;  // this means a empty result
      }
      schema_name = ts.schema_name;
      table_name = ts.table_name;
    } else if (schema_name.empty()) {
      /*we should adjust the alias to the real table name, otherwise the
       * get_column_collation_using_schema_table may fail.
       *
       *  Here we should adjust the possible alias to the real table name,
       *  such as select * from test.t1 as t2, test1.t2 as t1 group by t1.c1;
       *  the c1 should be check from table test1.t2*/
      fail_to_get_real_table_name =
          adjust_alias_to_table_name_in_rs(rs, table_name, schema_name, schema);
      has_adjust_alias = true;
      if (schema_name.empty()) schema_name = schema;
    }
    if (!strcasecmp(schema_name.c_str(),
                    "dbscale_tmp"))  // TODO: support to check collaction for
                                     // dbscale_tmp table
      return ret;
    if (fail_to_get_real_table_name ||
        (!has_adjust_alias &&
         adjust_alias_to_table_name_in_rs(rs, table_name, schema_name, schema)))
      return ret;
    bool is_column_collation_cs_or_bin = false;
    bool is_data_type_string_like = true;
    ret = get_column_collation_using_schema_table(
        schema_name, table_name, column_name, is_column_collation_cs_or_bin,
        is_data_type_string_like);
  } else {
    list<Expression *> str_list;
    expr->get_str_expression(&str_list);
    list<Expression *>::iterator it = str_list.begin();
    for (; it != str_list.end(); ++it) {
      if (*it == expr) {
        string tmp_str;
        expr->to_string(tmp_str);
        LOG_WARN(
            "Fail to Statement::get_column_collation with expr [%s] for sql "
            "[%s].\n",
            tmp_str.c_str(), sql);
        return ret;
      }
      ret = get_column_collation(*it, rs, check_alias);
      if (!ret.empty()) {
        break;
      }
    }
  }
  return ret;
}

string Statement::get_column_collation_using_schema_table(
    string schema_name, string table_name, string column_name,
    bool &is_column_collation_cs_or_bin, bool &is_data_type_string_like) {
  string ret;
  TableInfoCollection *tic = TableInfoCollection::instance();
  TableInfo *ti =
      tic->get_table_info_for_read(schema_name.c_str(), table_name.c_str());
  map<string, TableColumnInfo *, strcasecomp> *column_info_map;
  try {
    column_info_map =
        ti->element_table_column->get_element_map_columnname_table_column_info(
            stmt_session);
  } catch (...) {
    LOG_ERROR(
        "Error occured when try to get table info column "
        "info(map_columnname_table_column_info) of table [%s.%s]\n",
        schema_name.c_str(), table_name.c_str());
    ti->release_table_info_lock();
    return ret;  // we should not just skip get collation if the table not find.
  }

  string tmp_colname(column_name);
  boost::to_lower(tmp_colname);
  if (column_info_map != NULL) {
    if (column_info_map->count(tmp_colname)) {
      TableColumnInfo *table_col_info = (*column_info_map)[tmp_colname];
      ret = table_col_info->collation_name;
      is_column_collation_cs_or_bin =
          table_col_info->is_column_collation_cs_or_bin;
      is_data_type_string_like = table_col_info->is_data_type_string_like;
    }
  }
  ti->release_table_info_lock();

  return ret;
}

void Statement::add_order_item_list(order_item *start,
                                    list<SortDesc> &order_list,
                                    record_scan *rs) {
  return add_group_item_list(start, order_list, rs);
}

bool order_by_in_group_by(order_item *order, order_item *group) {
  if (order == NULL) {
    return true;
  }

  if (group == NULL) {
    return false;
  }

  order_item *item1 = order;
  order_item *item2 = group;
  Expression *expr1;
  Expression *expr2;
  do {
    expr1 = item1->field->field_expr;
    expr2 = item2->field->field_expr;
    if (!expr1->is_equal(expr2) || item1->order != item2->order) {
      return false;
    }

    item1 = item1->next;
    item2 = item2->next;
  } while (item1 != order && item2 != group);

  if (item1 != order) {
    return false;
  }

  return true;
}

/* add select item if necessary */
void Statement::prepare_select_item(record_scan *rs) {
  list<AggregateDesc> aggr_list;

  // If there is having or with_rollup we can not deal with it easily.
  if (rs->group_by_list && rs->is_with_rollup) {
    LOG_ERROR("Do not support rollup currently.\n");
    throw UnSupportPartitionSQL("Do not support rollup currently");
  }

  order_in_group_flag =
      order_by_in_group_by(rs->order_by_list, rs->group_by_list);

  groupby_all_par_keys = group_by_all_keys(rs);
  if (rs->group_by_list || rs->having) {
    add_group_item_list(rs->group_by_list, group_by_list, rs);
    if (groupby_all_par_keys && hash_method_cs && rs->group_by_ci)
      groupby_all_par_keys =
          false;  // If the partition key collection is ci, but the partition
                  // method handle as cs, the group by should not pushdown.
    if (!groupby_all_par_keys || (!rs->order_by_list || order_in_group_flag)) {
      if (rs->having) {
        simple_having = !is_expression_contain_aggr(rs->having, rs, true);
        if (!simple_having) {
          handle_expression(rs->having, NULL, rs, HAVING_EXPR);
          having = rs->having;
        }
      }
    }
  }

  handle_avg_function(rs, avg_list);
  handle_select_expr(rs);

  if (rs->order_by_list) {
    if (!rs->group_by_list || !order_in_group_flag)
      add_order_item_list(rs->order_by_list, order_by_list, rs);
  }
}

void Statement::modify_select_item(record_scan **new_rs, Statement **new_stmt,
                                   const char *query_sql, size_t sql_len) {
  string new_sql;
  list<string>::reverse_iterator it;
  record_scan *rs = *new_rs;

  if (!rs || !rs->from_pos) {
    LOG_ERROR("Unsupport sql for Statement::modify_select_item for sql [%s].\n",
              sql);
    throw Error("Unsupport sql for Statement::modify_select_item.");
  }
  if (rs->from_pos >= sql_len) {
    re_parser_stmt(new_rs, new_stmt, query_sql);
    rs = *new_rs;
  }

  new_sql.append(query_sql + rs->start_pos - 1, rs->from_pos - rs->start_pos);
  for (it = new_select_items.rbegin(); it != new_select_items.rend(); ++it) {
    new_sql.append(",");
    new_sql.append(*it);
    new_sql.append(" ");
  }

  new_sql.append(query_sql + rs->from_pos - 1);
  re_parser_stmt(new_rs, new_stmt, new_sql.c_str());
  fetch_sql_tmp = new_sql;
}

order_item *get_last_groupby(order_item *groupby) {
  order_item *head = groupby;
  order_item *tmp = head;
  while (tmp && tmp->next != head) {
    tmp = tmp->next;
  }

  return tmp;
}

bool is_group_by_partition_key(record_scan *rs, table_link *par_table,
                               const char *schema) {
  if (!par_table || !par_table->join) {
    throw Error(
        "Fail to get par table or its join in the param of "
        "is_group_by_partition_key.\n");
  }
  const char *schema_name =
      par_table->join->schema_name ? par_table->join->schema_name : schema;
  const char *table_name = par_table->join->table_name;
  const char *tb_alias = par_table->join->alias;
  PartitionedTable *par_space = get_part_ds_from_table(par_table, schema);
  if (!par_space) {
    throw Error("Fail to get par space in is_group_by_partition_key.\n");
  }
  vector<const char *> *key_names = par_space->get_key_names();
  if (!key_names) {
    throw Error("Fail to get key names in is_group_by_partition_key.\n");
  }
  if (key_names->size() == 1) {
    order_item *groupby = get_last_groupby(rs->group_by_list);
    if (groupby && groupby_on_par_key(groupby, schema_name, table_name,
                                      tb_alias, key_names->at(0))) {
      return true;
    }
  }
  return false;
}

void Statement::check_single_par_table_in_sub_query() {
  if (par_table_num == 1) {
    table_link *par_table = par_tables[0];
    record_scan *rs = par_table->join->cur_rec_scan;
    record_scan *temp = rs;
    while (temp->upper && temp->upper->field_list_head) {
      for (field_item *fi = temp->upper->field_list_head;
           fi != temp->upper->field_list_tail->next; fi = fi->next) {
        if (fi->field_expr == temp->condition_belong) {
          LOG_ERROR(
              "Unsupport sql, the sql is [%s], Unsupport single partition "
              "table in sub query\n",
              sql);
          throw UnSupportPartitionSQL(
              "Unsupport single partition table in sub query");
        }
      }
      temp = temp->upper;
    }
    while (rs->join_belong) {
      if (rs_contains_aggr_group_limit(rs, par_table)) {
        LOG_ERROR(
            "Unsupport sql, the sql is [%s], Unsupport single partition table "
            "in"
            " sub query\n",
            sql);
        throw UnSupportPartitionSQL(
            "Unsupport single partition table in sub query");
      }
      rs = rs->join_belong->cur_rec_scan;
    }
  }
}

bool Statement::rs_contains_aggr_group_limit(record_scan *rs,
                                             table_link *par_table) {
  list<AggregateDesc> aggr_list;
  int avg_num = 0;
  get_aggregate_functions(rs, aggr_list);
  field_item *field = rs->field_list_head;
  while (field) {
    if (field->field_expr) {
      if (is_expression_contain_aggr(field->field_expr, rs, false)) {
        ++avg_num;
        break;
      }
    }
    field = field->next;
  }
  bool group_by_par_key = false;
  try {
    if (rs->group_by_list && is_group_by_partition_key(rs, par_table, schema))
      group_by_par_key = true;
  } catch (Exception &e) {
    LOG_ERROR("[%s] get exception in is_group_by_partition_key due to %s.\n",
              sql, e.what());
    throw;
  }

  if (!group_by_par_key &&
      (rs->group_by_list || !aggr_list.empty() || rs->having || avg_num)) {
    return 1;
  }
  if ((rs->limit || rs->order_by_list)) {
    return 1;
  }
  return 0;
}

record_scan *Statement::generate_select_plan_nodes(
    ExecutePlan *plan, vector<unsigned int> *par_ids,
    PartitionedTable *par_table, vector<ExecuteNode *> *nodes,
    const char *query_sql, bool need_check_distinct) {
  record_scan *new_rs;
  Statement *new_stmt = NULL;

  if (st.scanner->field_list_head == NULL) {
    re_parser_stmt(&new_rs, &new_stmt, query_sql);
  } else {
    new_rs = st.scanner;
  }

  ExecuteNode *tmp = NULL;
  ExecuteNode *head = NULL;
  ExecuteNode *tail = NULL;
  const char *fetch_sql = query_sql;
  try {
    if (par_tables.size() > 0) {
      table_link *par_table = par_tables[0];
      if (need_check_distinct &&
          check_and_replace_par_distinct_with_groupby(par_table, fetch_sql)) {
        // sql has update by replacing DISTINCT with GROUP BY, so need reparse.
        fetch_sql = no_distinct_sql.c_str();
        re_parser_stmt(&new_rs, &new_stmt, fetch_sql);
      }
    }
    check_single_par_table_in_sub_query();

    list<AggregateDesc> aggr_list;
    fetch_sql_tmp.append(fetch_sql);

    prepare_select_item(new_rs);
    if (!new_select_items.empty()) {
      modify_select_item(&new_rs, &new_stmt, fetch_sql_tmp.c_str(),
                         fetch_sql_tmp.length());
      fetch_sql = fetch_sql_tmp.c_str();
    }

    get_aggregate_functions(new_rs, aggr_list);
    if (!new_rs->limit && !new_rs->order_by_list && !new_rs->group_by_list &&
        aggr_list.empty() && avg_list.empty()) {
      ExecuteNode *fetch_node = NULL;
      DataSpace *dataspace = NULL;

      if (is_connect_by) {
        ExecuteNode *conn_node = plan->get_connect_by_node(connect_by_desc);
        ExecuteNode *filter_node =
            plan->get_project_node(new_select_items.size());
        filter_node->add_child(conn_node);

        unsigned int i = 0, id;
        for (; i < par_ids->size(); ++i) {
          id = par_ids->at(i);
          dataspace = par_table->get_partition(id);
          const char *used_sql =
              adjust_stmt_sql_for_shard(dataspace, fetch_sql);
          fetch_node = plan->get_fetch_node(dataspace, used_sql);
          conn_node->add_child(fetch_node);
        }
        nodes->push_back(filter_node);
      } else {
        unsigned int i = 0, id;
        for (; i < par_ids->size(); ++i) {
          id = par_ids->at(i);
          dataspace = par_table->get_partition(id);
          const char *used_sql =
              adjust_stmt_sql_for_shard(dataspace, fetch_sql);
          fetch_node = plan->get_fetch_node(dataspace, used_sql);
          nodes->push_back(fetch_node);
        }
      }

      if (par_ids->size() == par_table->get_real_partition_num()) {
        plan->session->set_execute_plan_touch_partition_nums(-1);
      } else {
        plan->session->set_execute_plan_touch_partition_nums(par_ids->size());
      }

      if (st.type == STMT_SELECT && max_fetch_node_threads == 1)
        plan->set_fetch_node_no_thread(true);
      return new_rs;
    }
    if (new_rs->is_contain_star && !aggr_list.empty()) {
      LOG_ERROR(
          "Unsupport sql, the sql is [%s], Unsupport select * and aggregate "
          "function"
          "for PartitionedTable",
          sql);
      release_avg_list(avg_list);
      throw UnSupportPartitionSQL(
          "Unsupport select * and aggregate function for PartitionedTable");
    }

    if (new_rs->limit) {
      ExecuteNode *limit = generate_limit_subplan(plan, new_rs, &tmp);
      if (!head) head = limit;
      if (tail) tail->add_child(limit);
      tail = tmp;
    }

    if (!aggr_list.empty() && !new_rs->group_by_list) {
      ExecuteNode *aggr = generate_aggregate_by_subplan(plan, new_rs, aggr_list,
                                                        avg_list, &tmp);
      if (!head) head = aggr;
      if (tail) tail->add_child(aggr);
      tail = tmp;
    } else if (new_rs->group_by_list || new_rs->order_by_list) {
      ExecuteNode *og =
          generate_order_by_subplan(plan, new_rs, aggr_list, avg_list, &tmp);
      if (!head) head = og;
      if (tail) tail->add_child(og);
      tail = tmp;
      // TODO: if there is order by or group by and there is other child node(s)
      // after it, which needs info from new_rs, we may need to re-parse the sql
      // to get correct new_rs
    }

    if (is_connect_by) {
      ExecuteNode *conn_node = plan->get_connect_by_node(connect_by_desc);
      conn_node->add_child(head);
      head = conn_node;
    }

    /* add project node */
    if (!new_select_items.empty()) {
      ExecuteNode *filter_node =
          plan->get_project_node(new_select_items.size());
      filter_node->add_child(head);
      head = filter_node;
    }

    if (fetch_sql_tmp.size()) {
      fetch_sql = fetch_sql_tmp.c_str();
      if (strcmp(query_sql, fetch_sql) != 0) {
        LOG_DEBUG("sql[%s] has changed to [%s], reparse for sql\n", query_sql,
                  fetch_sql);
        re_parser_stmt(&new_rs, &new_stmt, fetch_sql);
      }
    }
    vector<unsigned int>::iterator it = par_ids->begin();
    for (; it != par_ids->end(); ++it) {
      const char *used_sql =
          adjust_stmt_sql_for_shard(par_table->get_partition(*it), fetch_sql);
      ExecuteNode *fetch_node =
          plan->get_fetch_node(par_table->get_partition(*it), used_sql);
      tail->add_child(fetch_node);
    }
    if (par_ids->size() == par_table->get_real_partition_num()) {
      plan->session->set_execute_plan_touch_partition_nums(-1);
    } else {
      plan->session->set_execute_plan_touch_partition_nums(par_ids->size());
    }

    nodes->push_back(head);
  } catch (Exception &e) {
    clean_up_execute_nodes(nodes);
    throw;
  }
  return new_rs;
}

table_link *Statement::get_one_modify_table() {
  table_link *modify_table = st.scanner->first_table;
  if (modify_table->join->upper) {  // upper means join with other table.

    // TODO: support the sql rebuild of delete/update with multiple normal
    // tables.
    if (st.type == STMT_UPDATE) {
      if (Backend::instance()->is_centralized_cluster()) return modify_table;
      if (is_tables_share_same_dataserver(modify_table)) return modify_table;
      vector<table_link *> modify_tables;
      get_modify_tables(&modify_tables);
      if (modify_tables.size() == 1) {
        modify_table = modify_tables[0];
        if (modify_table && can_tables_associate(modify_table) &&
            table_contains_set_list(modify_table))
          return modify_table;
      }
      LOG_ERROR(
          "Unsupport sql[%s] with more than 1 modify table when "
          "this sql needs seperated execution.\n",
          sql);
      throw UnSupportPartitionSQL(
          "Unsupport sql with more than 1 modify table when this sql needs "
          "seperated execution");
    } else if (st.type == STMT_DELETE &&
               st.sql->delete_oper->delete_list->next !=
                   st.sql->delete_oper->delete_list &&
               !can_tables_associate(modify_table)) {
      LOG_ERROR(
          "Unsupport sql[%s] with more than 1 modify table when "
          "this sql needs seperated execution.\n",
          sql);
      throw UnSupportPartitionSQL(
          "Unsupport sql with more than 1 modify table when this sql needs "
          "seperated execution");
    } else if (st.type != STMT_UPDATE && st.type != STMT_DELETE) {
      if (can_tables_associate(modify_table))
        return modify_table;
      else {
        LOG_ERROR(
            "Unsupport sql[%s] with more than 1 modify table when "
            "this sql needs seperated execution.\n",
            sql);
        throw UnSupportPartitionSQL(
            "Unsupport sql with more than 1 modify table when this sql needs "
            "seperated execution");
      }
    }
    modify_table =
        find_table_link(st.sql->delete_oper->delete_list->name, st.scanner);
    if (!modify_table) {
      LOG_ERROR("Fail to get modify table for this sql.\n");
      throw Error("Fail to get modify table for this sql.");
    }
  }
  return modify_table;
}

bool Statement::first_table_contains_all_data(table_link *tables) {
  table_link *table1 = tables;
  Backend *backend = Backend::instance();
  const char *schema1 =
      table1->join->schema_name ? table1->join->schema_name : schema;
  DataSpace *space1 =
      backend->get_data_space_for_table(schema1, table1->join->table_name);

  table_link *table2 = table1->next;
  while (table2) {
    const char *schema2 =
        table2->join->schema_name ? table2->join->schema_name : schema;
    DataSpace *space2 =
        backend->get_data_space_for_table(schema2, table2->join->table_name);

    if (!stmt_session->is_dataspace_cover_session_level(space1, space2)) {
      return false;
    }
    table2 = table2->next;
  }
  return true;
}

bool Statement::table_contains_all_data(table_link *tables,
                                        table_link *first_table) {
  table_link *table1 = tables;
  Backend *backend = Backend::instance();
  const char *schema1 =
      table1->join->schema_name ? table1->join->schema_name : schema;
  DataSpace *space1 =
      backend->get_data_space_for_table(schema1, table1->join->table_name);
  table_link *table2 = first_table;
  while (table2) {
    const char *schema2 =
        table2->join->schema_name ? table2->join->schema_name : schema;
    DataSpace *space2 =
        backend->get_data_space_for_table(schema2, table2->join->table_name);

    if (!stmt_session->is_dataspace_cover_session_level(space1, space2)) {
      return false;
    }
    table2 = table2->next;
  }
  return true;
}

void Statement::get_modify_tables(vector<table_link *> *modify_tables) {
  string full_name;
  set<string> tables;
  table_link *modify_table;
  Expression *set_list = st.sql->update_oper->update_set_list;
  ListExpression *expr_list = (ListExpression *)set_list;
  expr_list_item *head = expr_list->expr_list_head;

  expr_list_item *tmp = head;
  CompareExpression *cmp_tmp = NULL;
  StrExpression *str_tmp = NULL;

  do {
    cmp_tmp = (CompareExpression *)tmp->expr;
    str_tmp = (StrExpression *)cmp_tmp->left;
    str_tmp->to_string(full_name);
    string full_table_name, tmp_schema_name, table, column;
    split_value_into_schema_table_column(full_name, tmp_schema_name, table,
                                         column);
    if (tmp_schema_name.empty()) {
      full_table_name.append(table);
      modify_table = find_table_link(full_table_name.c_str(), st.scanner, true);
    } else {
      full_table_name.append(tmp_schema_name).append(".").append(table);
      modify_table = find_table_link(full_table_name.c_str(), st.scanner);
    }
    if (lower_case_table_names) boost::to_lower(full_table_name);
    if (st.type == STMT_UPDATE && full_table_name.size() == 0 &&
        column == full_name && modify_tables->size()) {
      full_name.clear();
      tmp = tmp->next;
      continue;
    }
    if (tables.find(full_table_name) == tables.end()) {
      modify_tables->push_back(modify_table);
      tables.insert(full_table_name);
    }
    full_name.clear();
    tmp = tmp->next;
  } while (tmp != head);
}

bool Statement::can_tables_associate(table_link *tables) {
  table_link *table1 = tables;
  while (table1) {
    table_link *table2 = table1->next;
    while (table2) {
      Backend *backend = Backend::instance();
      const char *schema1 =
          table1->join->schema_name ? table1->join->schema_name : schema;
      DataSpace *space1 =
          backend->get_data_space_for_table(schema1, table1->join->table_name);
      const char *schema2 =
          table2->join->schema_name ? table2->join->schema_name : schema;
      DataSpace *space2 =
          backend->get_data_space_for_table(schema2, table2->join->table_name);

      if (!stmt_session->is_dataspace_cover_session_level(space2, space1) &&
          !stmt_session->is_dataspace_cover_session_level(space1, space2)) {
        return false;
      }
      table2 = table2->next;
    }
    table1 = table1->next;
  }

  return true;
} /* can_tables_associate */

bool Statement::is_tables_share_same_dataserver(table_link *tables) {
  table_link *table1 = tables;
  if (!table1) return false;
  Backend *backend = Backend::instance();
  const char *schema1 =
      table1->join->schema_name ? table1->join->schema_name : schema;
  DataSpace *space1 =
      backend->get_data_space_for_table(schema1, table1->join->table_name);
  table_link *table2 = table1->next;
  while (table2) {
    const char *schema2 =
        table2->join->schema_name ? table2->join->schema_name : schema;
    DataSpace *space2 =
        backend->get_data_space_for_table(schema2, table2->join->table_name);

    if (!is_share_same_server(space1, space2)) {
      return false;
    }
    table2 = table2->next;
  }
  return true;
} /* is_tables_share_same_dataserver */

bool Statement::table_contains_set_list(table_link *tables) {
  string column;
  Expression *set_list = st.sql->update_oper->update_set_list;
  ListExpression *expr_list = (ListExpression *)set_list;
  expr_list_item *head = expr_list->expr_list_head;

  expr_list_item *tmp = head;
  CompareExpression *cmp_tmp = NULL;
  StrExpression *str_tmp = NULL;
  do {
    cmp_tmp = (CompareExpression *)tmp->expr;
    str_tmp = (StrExpression *)cmp_tmp->left;
    str_tmp->to_string(column);
    if (get_column_in_table(column, tables).empty()) return false;
    tmp = tmp->next;
    column.clear();
  } while (tmp != head);

  return true;
}

/*Fulfill the vector 'key_pos_real' with the real key pos of insert stmt. If
 * the insert column list is not exist, return the define key pos, otherwise
 * get the real key pos from the insert column list.*/
void Statement::fulfill_insert_key_pos(
    name_item *column_list, vector<const char *> *key_names,
    const char *schema_name, size_t schema_len, const char *table_name,
    size_t table_len, const char *table_alias,
    vector<unsigned int> *key_pos_def, vector<unsigned int> *key_pos_real) {
  if (!column_list) {
    *key_pos_real = *key_pos_def;
    return;
  }

  vector<const char *>::iterator it = key_names->begin();
  for (; it != key_names->end(); ++it) {
    unsigned int key_pos = 0;
    const char *key_name = *it;
    key_pos = get_key_pos_from_column_list(column_list, schema_name, schema_len,
                                           table_name, table_len, table_alias,
                                           key_name);
    key_pos_real->push_back(key_pos);
  }
}

void Statement::handle_one_par_table_mul_spaces(ExecutePlan *plan) {
  table_link *par_table =
      one_table_node.only_one_table ? one_table_node.table : par_tables[0];
  stmt_type type = st.type;
  record_scan *rs = par_table->join->cur_rec_scan;
  DataSpace *dspace = one_table_node.only_one_table
                          ? one_table_node.space
                          : record_scan_all_table_spaces[par_table];
  PartitionedTable *par_space = NULL;
  bool par_is_duplicated = false;
  if (((Table *)dspace)->is_duplicated()) {
    par_space = ((DuplicatedTable *)dspace)->get_partition_table();
    par_is_duplicated = true;
#ifdef DEBUG
    /*Currenlty, duplicated code will only be used fro cross node join.*/
    ACE_ASSERT(type == STMT_INSERT_SELECT);
#endif
  } else
    par_space = (PartitionedTable *)(dspace);

  vector<const char *> *key_names = par_space->get_key_names();
  const char *schema_name =
      par_table->join->schema_name ? par_table->join->schema_name : schema;
  const char *table_name = par_table->join->table_name;
  const char *table_alias = par_table->join->alias;

  switch (type) {
    case STMT_SELECT: {
      if (spaces.size() > 1) {
        LOG_ERROR("Unsupport the select [%s], which need to be divided.\n",
                  sql);
        throw UnSupportPartitionSQL(
            "Unsuport the select, which need to be divided");
      }

      handle_one_par_table_one_spaces(plan);
      break;
    }
      // For below 5 cases, we need to check this sql match the following
      // pattern: {operate normal table by using data from partition table} or
      //{operate partition table by using data from normal table}.  Then we
      // need to merge all dataspace except the dataspace of top record_scan.
    case STMT_UPDATE:
    case STMT_DELETE: {
      if (spaces.size() > 2 ||
          record_scan_one_space_tmp_map[st.scanner] != spaces[0]) {
        LOG_ERROR("Unsupport the sql [%s], which need to be divided.\n", sql);
        throw UnSupportPartitionSQL(
            "Unsuport the sql, which need to be divided");
      }

      if (type == STMT_UPDATE) {
        vector<table_link *> modify_tables;
        get_modify_tables(&modify_tables);
        if (modify_tables.size() == 1 && modify_tables[0]) {
          update_on_one_server = table_contains_all_data(
              modify_tables[0], st.scanner->first_table);
        }
      }
      /* If the modify table is the partition table and without limit
       * constrant, we donot need to divide it into 'one modify sql + one
       * select sql'
       */
      if (((!(st.scanner->first_table->next &&
              st.scanner->first_table->next->join->cur_rec_scan ==
                  st.scanner) &&
            st.scanner->first_table == par_table) ||
           update_on_one_server || delete_on_one_server) &&
          !rs->limit) {
        handle_one_par_table_one_spaces(plan);
        break;
      }
      plan->session->set_is_complex_stmt(true);

      table_link *modify_table = get_one_modify_table();

      // Update partition key columns is forbidden currently. TODO: support it
      if (table_link_same_dataspace(modify_table, par_table) &&
          st.type == STMT_UPDATE) {
        Expression *set_list = st.sql->update_oper->update_set_list;
        string key_value = "";
        if (set_list_contain_key(set_list, key_names, schema_name, table_name,
                                 table_alias, key_value)) {
          LOG_ERROR(
              "Unsupport SQL[%s], which modify the partition key columns "
              "[%s].\n",
              sql, (*key_names)[0]);
          throw UnSupportPartitionSQL(
              "Unsupport to modify the partition key columns");
        }
      }
      vector<unsigned int> par_ids;
      unsigned int partition_num = par_space->get_partition_num();
      PartitionMethod *method = par_space->get_partition_method();
      get_partitons_one_record_scan(rs, method, partition_num, &par_ids);
      par_space->replace_real_par_id_vec_from_virtual_ids(&par_ids);

      if (par_ids.size() > 1) {
        if (!is_acceptable_join_par_table(par_table->join)) {
          LOG_ERROR("Unsupport join type for partition table with sql [%s].\n",
                    sql);
          throw UnSupportPartitionSQL(
              "Unsupport join type for partition table");
        }
        check_partition_distinct(par_table);
      }

      Backend *backend = Backend::instance();
      const char *schema_name = modify_table->join->schema_name
                                    ? modify_table->join->schema_name
                                    : schema;
      const char *table_name = modify_table->join->table_name;
      DataSpace *modify_space =
          backend->get_data_space_for_table(schema_name, table_name);

      if (!table_link_same_dataspace(modify_table, par_table)) {
        if (!modify_space->get_data_source())
          assemble_two_phase_modify_partition_plan(
              plan, NULL, method, modify_table, NULL, &par_ids);
        else
          assemble_two_phase_modify_normal_plan(plan, &par_ids, par_space,
                                                modify_table);
      } else {
        assemble_two_phase_modify_partition_plan(plan, NULL, method,
                                                 modify_table, NULL, &par_ids);
      }

      break;
    }
    case STMT_REPLACE_SELECT:
    case STMT_INSERT_SELECT: {
#ifdef DEBUG
      ACE_ASSERT(st.sql->insert_oper);
#endif
      plan->session->set_is_complex_stmt(true);
      insert_op_node *insert_op = st.sql->insert_oper;
      if (rs == st.scanner) {  // insert/replace into a partition table
        if (spaces.size() > 2 ||
            (record_scan_dependent_map.count(st.scanner->children_begin) &&
             !record_scan_dependent_map[st.scanner->children_begin].empty())) {
          // The select part can not be merged.
          LOG_ERROR("Unsupport the SQL [%s], which need to be divided.\n", sql);
          throw UnSupportPartitionSQL(
              "Unsupport the SQL, which need to be divided");
        }
      }

      table_link *modify_table = get_one_modify_table();

      vector<unsigned int> par_ids;
      PartitionMethod *method = NULL;
      if (par_is_duplicated) {
#ifdef DEBUG
        ACE_ASSERT(modify_table == par_table);
#endif
      } else {
        unsigned int partition_num = par_space->get_partition_num();
        method = par_space->get_partition_method();
        get_partitons_one_record_scan(rs, method, partition_num, &par_ids);
        par_space->replace_real_par_id_vec_from_virtual_ids(&par_ids);

        if (par_ids.size() > 1) {
          if (!is_acceptable_join_par_table(par_table->join)) {
            LOG_ERROR(
                "Unsupport join type for partition table with sql [%s].\n",
                sql);
            throw UnSupportPartitionSQL(
                "Unsupport join type for partition table");
          }
        }
      }

      if (!table_link_same_dataspace(modify_table, par_table)) {
        assemble_two_phase_modify_normal_plan(plan, &par_ids, par_space,
                                              modify_table);
      } else {
        if (par_ids.size() > 1) {
          check_partition_distinct(par_table);
        }
        vector<unsigned int> key_pos_real;
        if (!par_is_duplicated) {
          set_full_table_name(schema_name, table_name);
          vector<unsigned int> key_pos_def;
          par_space->get_key_pos_vec(schema_name, table_name, key_pos_def,
                                     stmt_session);
          int column_count = 0;
          init_auto_increment_params(plan, schema_name, table_name, par_space);
          if (auto_inc_status != NO_AUTO_INC_FIELD &&
              insert_op->insert_column_list) {
            check_column_count_name(par_space, insert_op->insert_column_list,
                                    column_count);
            rectify_auto_inc_key_pos(insert_op->insert_column_list, schema_name,
                                     strlen(schema_name), table_name,
                                     strlen(table_name), table_alias);
          }
          fulfill_insert_key_pos(insert_op->insert_column_list, key_names,
                                 schema_name, strlen(schema_name), table_name,
                                 strlen(table_name), table_alias, &key_pos_def,
                                 &key_pos_real);
        }

        assemble_two_phase_modify_partition_plan(
            plan, spaces[0], method, modify_table, &key_pos_real, NULL);
      }

      break;
    }
    case STMT_DBSCALE_ESTIMATE: {
      if (spaces.size() > 2) {
        LOG_ERROR("Unsupport estimate partition sql type, the sql is [%s].\n",
                  sql);
        throw UnSupportPartitionSQL("Unsupport estimate partition sql type");
      }
      if ((spaces[0])->get_dataspace_type() == TABLE_TYPE &&
          st.estimate_type == STMT_INSERT_SELECT) {
        Table *first_table = (Table *)spaces[0];
        if (first_table->is_partitioned()) {
          PartitionedTable *par_space = (PartitionedTable *)spaces[0];
          vector<unsigned int> *par_ids = new vector<unsigned int>();
          Statement *stmt = plan->statement;
          record_scan *rs =
              stmt->get_stmt_node()->sql->insert_oper->select_values;
          unsigned int partition_num = par_space->get_partition_num();
          PartitionMethod *method = par_space->get_partition_method();
          stmt->get_partitons_one_record_scan(rs, method, partition_num,
                                              par_ids);
          par_space->replace_real_par_id_vec_from_virtual_ids(par_ids);
          assemble_dbscale_estimate_select_partition_plan(plan, par_ids,
                                                          par_space, sql, this);
        } else {
          assemble_dbscale_estimate_select_plan(plan, spaces[0], this);
        }
      } else {
        LOG_ERROR("Unsupport estimate partition sql type, the sql is [%s].\n",
                  sql);
        throw UnSupportPartitionSQL("Unsupport estimate partition sql type");
      }
    } break;
    case STMT_CREATE_SELECT: {
      // TODO: implement it.
      break;
    }
      // Prepare stmt support need to be added in the future.
    case STMT_PREPARE: {
      // TODO:Implement it;
      break;
    }
    case STMT_TRUNCATE: {
      // TODO: implement it by seperate the partition table and normal table.
      // For partition table, send truncate to all parititons; for normal
      // table, send truncate to master.
      break;
    }
    case STMT_CREATE_TB: {
      if (par_space->get_key_names()->size() == 1) {
        const char *par_key = par_space->get_key_names()->front();
        if (!is_partition_table_contain_partition_key(par_key)) {
          LOG_ERROR("Fail to find the partition key [%s] from table %s.%s\n",
                    par_key, schema_name, table_name);
          throw Error("Fail to find partition key from table.");
        }
      } else {
        LOG_ERROR("Not specify partition key for table %s.%s\n", schema_name,
                  table_name);
        throw Error("Not specify partition key for table.");
      }
      if (st.sql->create_tb_oper->has_foreign_key) {
        LOG_INFO(
            "Create table defination contains foreign key restriction, "
            "please make sure these keys are partitioned appropriately.\n");
      }
      // TODO: implement it by send this request to parent dataspace of
      // partition table (master).
      break;
    }

    default:
      LOG_ERROR("Unsupport partition sql type, the sql is [%s].\n", sql);
      throw UnSupportPartitionSQL("Unsupport partition sql type");
      break;
  }
}

void Statement::prepare_insert_select_via_load(ExecutePlan *plan,
                                               const char *schema,
                                               const char *table) {
  if (schema == NULL || table == NULL) {
    LOG_ERROR(
        "Unsupport INSERT SELECT statement for load-insert-select"
        " without schema or table name, the sql is [%s].\n",
        sql);
    throw NotSupportedError(
        "Unsupport INSERT SELECT statement for"
        "load-insert-select without schema or table name.");
  }
  if (st.sql->insert_oper->insert_column_list) {
    LOG_ERROR(
        "Unsupport INSERT SELECT statement for load-insert-select"
        " with insert column list, the sql is [%s].\n",
        sql);
    throw NotSupportedError(
        "Unsupport INSERT SELECT statement for"
        "load-insert-select with insert column list.");
  }
  record_scan *insert_select = st.sql->insert_oper->select_values;
  unsigned int start_pos = insert_select->start_pos;
  select_sub_sql.clear();
  select_sub_sql.append(sql + start_pos - 1, strlen(sql) - start_pos + 1);

  handle_insert_select_hex_column();
  re_parser_stmt(&(st.scanner), &(plan->statement), select_sub_sql.c_str());
}

void Statement::prepare_insert_select_via_fifo(ExecutePlan *plan,
                                               DataServer *modify_server,
                                               const char *schema,
                                               const char *table,
                                               bool load_insert) {
  if (!load_insert) {
    if (access(modify_server->get_local_load_script(),
               F_OK)) {  // script not found
      LOG_ERROR("Could not find load script.\n");
      throw Error("Could not find load script, please check.");
    }
  }
  if (schema == NULL || table == NULL) {
    LOG_ERROR(
        "Unsupport INSERT SELECT statement for external-load table or "
        "load-insert-select"
        " without schema or table name, the sql is [%s].\n",
        sql);
    throw NotSupportedError(
        "Unsupport INSERT SELECT statement for external-load"
        "table or load-insert-select without schema or table name.");
  }
  if (st.sql->insert_oper->insert_column_list) {
    LOG_ERROR(
        "Unsupport INSERT SELECT statement for external-load table or "
        "load-insert-select"
        " with insert column list, the sql is [%s].\n",
        sql);
    throw NotSupportedError(
        "Unsupport INSERT SELECT statement for external-load"
        " table or load-insert-select with insert column list.");
  }
  Backend *backend = Backend::instance();
  record_scan *insert_select = st.sql->insert_oper->select_values;
  unsigned int start_pos = insert_select->start_pos;
  select_sub_sql.clear();
  select_sub_sql.append(sql + start_pos - 1, strlen(sql) - start_pos + 1);

  handle_insert_select_hex_column();

  int fifo_id = backend->create_fifo();
  char fifo_name[30];
  sprintf(fifo_name, "/tmp/tmp_fifo_%d", fifo_id);

  outfile_sql = select_sub_sql;
  outfile_sql.append(" into outfile '");
  outfile_sql.append(fifo_name);
  if (load_insert) {
    if (strlen(
            plan->session->get_session_option("load_insert_select_fields_term")
                .char_val) > 1 ||
        strlen(
            plan->session->get_session_option("load_insert_select_lines_term")
                .char_val) > 1) {
      LOG_ERROR(
          "DBScale does not support FIELDS/LINES TERMINATED BY with multiple "
          "characters "
          "when use load for insert select.\n");
      throw NotImplementedError(
          "Load insert select with FIELDS or LINES TERMINATED BY has multiple "
          "characters");
    }
    char fields_term =
        plan->session->get_session_option("load_insert_select_fields_term")
            .char_val[0];
    char lines_term =
        plan->session->get_session_option("load_insert_select_lines_term")
            .char_val[0];
    outfile_sql.append("' fields terminated by '");
    outfile_sql.append(1, fields_term);
    outfile_sql.append("' enclosed by '\"' lines terminated by '");
    outfile_sql.append(1, lines_term);
    outfile_sql.append("'");
  } else {
    outfile_sql.append("' fields terminated by ',' enclosed by '\"'");
  }
  re_parser_stmt(&(st.scanner), &(plan->statement), outfile_sql.c_str());
  plan->statement->insert_select_modify_schema = schema;
  plan->statement->insert_select_modify_table = table;
  if (load_insert) {
    plan->statement->set_default_schema(get_schema());
    plan->statement->use_load_insert_select = true;
  } else {
    plan->statement->is_insert_select_via_fifo = true;
    plan->statement->local_load_script = modify_server->get_local_load_script();
  }
}

bool key_equal_in_expr(record_scan *rs, const char *schema1,
                       const char *schema2, const char *table1,
                       const char *table2, const char *table1_alias,
                       const char *table2_alias, const char *key1,
                       const char *key2) {
  expr_list_item *str_list_head = rs->str_expr_list_head;
  expr_list_item *tmp = str_list_head;
  StrExpression *str_expr1, *str_expr2;
  condition_type cond_type;
  ConditionAndOr and_or_type;
  do {
    str_expr1 = (StrExpression *)tmp->expr;
    cond_type = check_condition_type(str_expr1);
    if (cond_type != CONDITION_TYPE_EQUAL) {
      tmp = tmp->next;
      continue;
    }
    if (!column_name_equal_key_alias(str_expr1->str_value, schema1,
                                     strlen(schema1), table1, strlen(table1),
                                     key1, table1_alias)) {
      tmp = tmp->next;
      continue;
    }

    and_or_type = is_and_or_condition(str_expr1->parent, rs);
    switch (and_or_type) {
      case CONDITION_AND: {
        Expression *peer = get_peer(str_expr1);
        if (peer->type == EXPR_STR) {
          str_expr2 = (StrExpression *)peer;
          if (column_name_equal_key_alias(str_expr2->str_value, schema2,
                                          strlen(schema2), table2,
                                          strlen(table2), key2, table2_alias)) {
            return true;
          }
        }
      }
      default:
        tmp = tmp->next;
        break;
    }
  } while (tmp != str_list_head);

  return false;
}
void Statement::assemble_show_pool_info_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble show pool info plan.\n");
  ExecuteNode *node = plan->get_show_pool_info_node();
  plan->set_start_node(node);
}
void Statement::assemble_show_pool_version_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble show pool version plan.\n");
  ExecuteNode *node = plan->get_show_pool_version_node();
  plan->set_start_node(node);
}
void Statement::assemble_show_execution_profile_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble show execution profile.\n");
  ExecuteNode *node = plan->get_show_execution_profile_node();
  plan->set_start_node(node);
}
void Statement::assemble_show_lock_usage_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble show lock usage plan.\n");
  ExecuteNode *node = plan->get_show_lock_usage_node();
  plan->set_start_node(node);
}
void Statement::handle_mul_par_table_mul_spaces(ExecutePlan *plan) {
  stmt_type type = st.type;
  if (st.has_where_rownum) {
    LOG_ERROR("Unsupport par table use rownum.\n");
    throw UnSupportPartitionSQL("Unsupport par table use rownum.");
  }

  switch (type) {
    case STMT_INSERT_SELECT:
    case STMT_REPLACE_SELECT: {
      if (par_table_num == 2) {
#ifdef DEBUG
        ACE_ASSERT(st.sql->insert_oper);
#endif
        table_link *par_insert = par_tables[1];
        table_link *par_select = par_tables[0];
        record_scan *rs_select = par_select->join->cur_rec_scan;
        Table *space_insert = (Table *)record_scan_all_table_spaces[par_insert];
        Table *space_select = (Table *)record_scan_all_table_spaces[par_select];
#ifdef DEBUG
        ACE_ASSERT(!space_select->is_duplicated());
#endif
        bool insert_is_duplicated = false;
        PartitionedTable *par_space_insert = NULL;
        if (space_insert->is_duplicated()) {
          insert_is_duplicated = true;
        } else {
          par_space_insert = (PartitionedTable *)space_insert;
        }
        const char *schema_name_insert = par_insert->join->schema_name
                                             ? par_insert->join->schema_name
                                             : schema;
        const char *table_name_insert = par_insert->join->table_name;
        const char *table_alias_insert = par_insert->join->alias;
        PartitionedTable *par_space_select = (PartitionedTable *)space_select;

        insert_op_node *insert_op = st.sql->insert_oper;
        if (spaces.size() > 2 ||
            (record_scan_dependent_map.count(st.scanner->children_begin) &&
             !record_scan_dependent_map[st.scanner->children_begin].empty())) {
          LOG_ERROR("Unsupport the SQL [%s], which need to be divided.\n", sql);
          throw UnSupportPartitionSQL(
              "Unsupport the SQL, which need to be divided.");
        }

#ifdef DEBUG
        table_link *modify_table = get_one_modify_table();
        // after can_dataspace_merge, the modify_table must be par_table
        // currently
        ACE_ASSERT(table_link_same_dataspace(par_insert, modify_table));
#endif
        vector<unsigned int> key_pos_real;
        set_full_table_name(schema_name_insert, table_name_insert);
        if (!insert_is_duplicated) {
          vector<unsigned int> key_pos_def;
          par_space_insert->get_key_pos_vec(
              schema_name_insert, table_name_insert, key_pos_def, stmt_session);
          int column_count = 0;
          init_auto_increment_params(plan, schema_name_insert,
                                     table_name_insert, par_space_insert);
          if (auto_inc_status != NO_AUTO_INC_FIELD &&
              insert_op->insert_column_list) {
            check_column_count_name(
                par_space_insert, insert_op->insert_column_list, column_count);
            rectify_auto_inc_key_pos(
                insert_op->insert_column_list, schema_name_insert,
                strlen(schema_name_insert), table_name_insert,
                strlen(table_name_insert), table_alias_insert);
          }
          vector<const char *> *key_names_insert =
              par_space_insert->get_key_names();
          fulfill_insert_key_pos(insert_op->insert_column_list,
                                 key_names_insert, schema_name_insert,
                                 strlen(schema_name_insert), table_name_insert,
                                 strlen(table_name_insert), table_alias_insert,
                                 &key_pos_def, &key_pos_real);
        }

        vector<unsigned int> par_ids;
        unsigned int partition_num = par_space_select->get_partition_num();
        PartitionMethod *method = par_space_select->get_partition_method();
        get_partitons_one_record_scan(rs_select, method, partition_num,
                                      &par_ids);
        par_space_select->replace_real_par_id_vec_from_virtual_ids(&par_ids);

        if (par_ids.size() > 1) {
          if (!is_acceptable_join_par_table(par_select->join)) {
            LOG_ERROR(
                "Unsupport join type for partition table with sql [%s].\n",
                sql);
            throw UnSupportPartitionSQL(
                "Unsupport join type for partition table");
          }
        }
        assemble_two_phase_insert_part_select_part_plan(
            plan, par_insert, &key_pos_real, par_space_select, &par_ids);
        return;
      }
      break;
    }
    case STMT_DBSCALE_ESTIMATE: {
      if (spaces.size() > 2) break;
      if (st.estimate_type == STMT_INSERT_SELECT) {
        PartitionedTable *par_space = (PartitionedTable *)spaces[0];
        vector<unsigned int> *par_ids = new vector<unsigned int>();
        Statement *stmt = plan->statement;
        record_scan *rs =
            stmt->get_stmt_node()->sql->insert_oper->select_values;
        unsigned int partition_num = par_space->get_partition_num();
        PartitionMethod *method = par_space->get_partition_method();
        stmt->get_partitons_one_record_scan(rs, method, partition_num, par_ids);
        par_space->replace_real_par_id_vec_from_virtual_ids(par_ids);
        assemble_dbscale_estimate_select_partition_plan(plan, par_ids,
                                                        par_space, sql, this);
        return;
      }
    }
    default:
      break;
  }
  LOG_ERROR("Unsupport partition sql, the sql is [%s].\n", sql);
  throw UnSupportPartitionSQL(
      "Unsupport partition sql type for mul par table mul spaces.");
}

bool Statement::can_two_par_table_record_scan_merge(record_scan *rs1,
                                                    record_scan *rs2) {
  table_link *par_table1 = record_scan_par_table_map[rs1];
  table_link *par_table2 = record_scan_par_table_map[rs2];

  // the same upper select
  if (rs1->upper == rs2->upper && rs1->subquerytype == SUB_SELECT_TABLE &&
      rs2->subquerytype == SUB_SELECT_TABLE) {
    const char *schema_name1 =
        par_table1->join->schema_name ? par_table1->join->schema_name : schema;
    const char *table_name1 = par_table1->join->table_name;
    const char *schema_name2 =
        par_table2->join->schema_name ? par_table2->join->schema_name : schema;
    const char *table_name2 = par_table2->join->table_name;
    const char *tb_alias1 = par_table1->join->alias;
    const char *tb_alias2 = par_table2->join->alias;
    PartitionedTable *par_space1 =
        (PartitionedTable *)record_scan_all_table_spaces[par_table1];
    PartitionedTable *par_space2 =
        (PartitionedTable *)record_scan_all_table_spaces[par_table2];

    // the same table
    if (!strcasecmp(schema_name1, schema_name2) &&
        !strcasecmp(table_name1, table_name2)) {
      vector<const char *> *key_names1 = par_space1->get_key_names();
      vector<const char *> *key_names2 = par_space2->get_key_names();
      if (key_names1->size() == 1 && key_names2->size() == 1) {
        // group by on partition key
        order_item *groupby1 = get_last_groupby(rs1->group_by_list);
        order_item *groupby2 = get_last_groupby(rs2->group_by_list);
        if (groupby1 && groupby2 &&
            groupby_on_par_key(groupby1, schema_name1, table_name1, tb_alias1,
                               key_names1->at(0)) &&
            groupby_on_par_key(groupby2, schema_name2, table_name2, tb_alias2,
                               key_names2->at(0))) {
          const char *outer_alias1 = rs1->join_belong->alias;
          const char *outer_alias2 = rs2->join_belong->alias;
          if (key_equal_in_expr(rs1->upper, schema_name1, schema_name2,
                                table_name1, table_name2, outer_alias1,
                                outer_alias2, key_names1->at(0),
                                key_names2->at(0))) {
            return true;
          }
        }
      }
    }
  }
  return false;
}

bool Statement::can_par_global_table_record_scan_merge(record_scan *rs1,
                                                       record_scan *rs2) {
  table_link *par_table1 = record_scan_par_table_map[rs1];
  DataSpace *global_space = record_scan_one_space_tmp_map[rs2];
  if (rs2->is_select_union)
    return false;
  else if (!global_space)
    return true;

  if (rs1->order_by_list || rs1->limit) return false;
  // the same upper select
  if (rs1->upper == rs2->upper && rs1->subquerytype == SUB_SELECT_TABLE &&
      rs2->subquerytype == SUB_SELECT_TABLE) {
    const char *schema_name1 =
        par_table1->join->schema_name ? par_table1->join->schema_name : schema;
    const char *table_name1 = par_table1->join->table_name;
    const char *tb_alias1 = par_table1->join->alias;
    PartitionedTable *par_space1 =
        (PartitionedTable *)record_scan_all_table_spaces[par_table1];

    vector<const char *> *key_names1 = par_space1->get_key_names();
    if (key_names1->size() == 1) {
      // group by on partition key
      order_item *groupby1 = get_last_groupby(rs1->group_by_list);
      if (groupby1 && groupby_on_par_key(groupby1, schema_name1, table_name1,
                                         tb_alias1, key_names1->at(0))) {
        if (stmt_session->is_dataspace_cover_session_level(par_space1,
                                                           global_space))
          return true;
      }
    }
  }
  return false;
}

void Statement::assemble_modify_all_partition_plan(
    ExecutePlan *plan, PartitionedTable *par_space) {
  vector<unsigned int> par_ids;
  unsigned int partition_num = par_space->get_real_partition_num();
  par_ids.clear();
  unsigned int i = 0;
  for (; i < partition_num; ++i) par_ids.push_back(i);

  assemble_mul_par_modify_plan(plan, &par_ids, par_space);
  if (st.type <= STMT_DDL_START || st.type >= STMT_DDL_END) {
    plan->session->set_execute_plan_touch_partition_nums(-1);
  }
}

void Statement::build_insert_sql_assign_auto_inc(string &insert_sql,
                                                 Expression *value_expr,
                                                 int64_t curr_auto_inc_val) {
  char auto_inc_str_val[21];
  snprintf(auto_inc_str_val, sizeof(auto_inc_str_val), "%ld",
           curr_auto_inc_val);
  if (auto_inc_status != AUTO_INC_NOT_SPECIFIED) {
    int pos_before = value_expr->start_pos;
    int pos_after = value_expr->end_pos;
    insert_sql.append(sql, pos_before - 1);
    insert_sql.append(auto_inc_str_val);
    insert_sql.append(&sql[pos_after]);
  } else if (auto_inc_status == AUTO_INC_NOT_SPECIFIED) {
    insert_sql.append(sql);
    insert_sql.append(", ");
    insert_sql.append(auto_inc_key_name);
    insert_sql.append("=");
    insert_sql.append(auto_inc_str_val);
  }
}

void Statement::build_insert_sql_auto_inc(string &insert_sql,
                                          insert_op_node *insert,
                                          expr_list_item *auto_inc_expr_item,
                                          int64_t curr_auto_inc_val) {
  build_insert_sql_prefix(insert_sql, insert);
  append_insert_sql_row_auto_inc(insert_sql, insert->insert_values,
                                 auto_inc_expr_item, curr_auto_inc_val);
  boost::erase_tail(insert_sql, 1);
}

void Statement::build_insert_sql_prefix(string &sql_prefix,
                                        insert_op_node *insert) {
  sql_prefix.clear();
  if (auto_inc_status == AUTO_INC_NOT_SPECIFIED) {
    sql_prefix.append(sql, insert->column_list_end_pos);
    sql_prefix.append(", ");
    sql_prefix.append(auto_inc_key_name);
    sql_prefix.append(") VALUES ");
  } else {
    sql_prefix.append(sql, insert->keyword_values_end_pos);
    sql_prefix.append(" ");
  }
}

void Statement::append_insert_sql_row(string &part_sql,
                                      ListExpression *expr_list) {
  int start_pos = expr_list->expr_list_head->expr->start_pos - 1;
  int end_pos = expr_list->expr_list_tail->expr->end_pos - 1;
  part_sql.append("(");
  part_sql.append(&sql[start_pos], end_pos - start_pos + 1);
  part_sql.append("),");
}

void Statement::append_insert_sql_row_auto_inc(
    string &part_sql, insert_row *row, expr_list_item *auto_inc_expr_item,
    int64_t curr_auto_inc_val) {
  char auto_inc_str_val[21];
  int start_pos = 0;
  int end_pos = 0;
  snprintf(auto_inc_str_val, 21, "%ld", curr_auto_inc_val);
  if (auto_inc_status != AUTO_INC_NOT_SPECIFIED) {
    start_pos = ((ListExpression *)row->column_values)
                    ->expr_list_head->expr->start_pos -
                1;
    end_pos = auto_inc_expr_item->expr->start_pos - 1;
    part_sql.append("(");
    part_sql.append(&sql[start_pos], end_pos - start_pos);
    part_sql.append(auto_inc_str_val);
    if (auto_inc_expr_item !=
        ((ListExpression *)row->column_values)->expr_list_tail) {
      part_sql.append(", ");
      start_pos = auto_inc_expr_item->next->expr->start_pos - 1;
      end_pos = ((ListExpression *)row->column_values)
                    ->expr_list_tail->expr->end_pos -
                1;
      part_sql.append(&sql[start_pos], end_pos - start_pos + 1);
    }
    part_sql.append("),");
  } else if (auto_inc_status == AUTO_INC_NOT_SPECIFIED) {
    start_pos = ((ListExpression *)row->column_values)
                    ->expr_list_head->expr->start_pos -
                1;
    end_pos =
        ((ListExpression *)row->column_values)->expr_list_tail->expr->end_pos -
        1;
    part_sql.append("(");
    part_sql.append(&sql[start_pos], end_pos - start_pos + 1);
    part_sql.append(", ");
    part_sql.append(auto_inc_str_val);
    part_sql.append("),");
  }
}

void Statement::check_value_count_auto_increment(int column_count,
                                                 int value_count,
                                                 PartitionedTable *part_space,
                                                 int row_index) {
  int field_count = 0;
  ACE_RW_Mutex *tc_lock = NULL;

  if (!column_count) {
    tc_lock = part_space->get_table_columns_lock(full_table_name);
    tc_lock->acquire_read();
    field_count = part_space->get_table_field_count(full_table_name);
    ;
    tc_lock->release();
  }
  bool is_column_count_ok =
      column_count ? value_count == column_count : value_count == field_count;
  if (!is_column_count_ok) {
    char msg[70];
    snprintf(msg, sizeof(msg), "%s%d",
             "Column count doesn't match value count at row ", row_index + 1);
    LOG_ERROR("%s\n", msg);
    throw dbscale::sql::SQLError(msg, "21S01",
                                 ERROR_COLUMN_COUNT_NOT_MATCH_CODE);
  }
}

expr_list_item *Statement::get_auto_inc_expr(expr_list_item *head) {
  expr_list_item *item = NULL;
  if (auto_inc_status != AUTO_INC_NOT_SPECIFIED) {
    item = head;
    int i = 0;
    for (; i < auto_increment_key_pos; ++i) {
      item = item->next;
    }
  }
  return item;
}

void Statement::rectify_auto_inc_key_pos(
    name_item *column_list, const char *schema_name, size_t schema_len,
    const char *table_name, size_t table_len, const char *table_alias) {
  int ret = get_auto_inc_pos_from_column_list(
      column_list, schema_name, schema_len, table_name, table_len, table_alias);
  if (auto_increment_key_pos != ret) {
    LOG_DEBUG("Set real auto_increment_key_pos, old=%d, new=%d\n",
              auto_increment_key_pos, ret);
    auto_increment_key_pos = ret;
  }
}

void fulfill_set_col_names(
    ListExpression *set_list,
    map<string, CompareExpression *, strcasecomp> &set_col_names) {
  expr_list_item *head = set_list->expr_list_head;
  expr_list_item *tmp = head;
  CompareExpression *cmp_tmp = NULL;
  StrExpression *str_tmp = NULL;
  do {
    cmp_tmp = (CompareExpression *)tmp->expr;
    str_tmp = (StrExpression *)cmp_tmp->left;
    string set_col_name = str_tmp->str_value;
    size_t pos = set_col_name.find_first_of('.');
    if (pos != string::npos) {
      if (set_col_name.find_first_of('.', pos + 1) != string::npos) {
        throw Error("do not support column name contain more than 1 dot(.)");
      }
      set_col_name = set_col_name.substr(pos + 1);
    }
    set_col_names[set_col_name] = cmp_tmp;
    tmp = tmp->next;
  } while (tmp != head);
}

void Statement::check_auto_inc_value_in_update_set_list(
    Expression *set_list,
    map<string, CompareExpression *, strcasecomp> &set_col_names,
    PartitionedTable *par_space, ExecutePlan *plan, const char *schema_name,
    const char *table_name) {
  ListExpression *expr_list = (ListExpression *)set_list;
  fulfill_set_col_names(expr_list, set_col_names);
  if (set_col_names.count(auto_inc_key_name)) {
    // auto inc value should
    // 1. not 0 or NULL
    // 2. be valid at dbscale level
    CompareExpression *ce = set_col_names[auto_inc_key_name];
    Expression *se = ce->right;
    string val = se->str_value ? se->str_value : "";
    if (val.empty() || !is_natural_number(val.c_str()) ||
        !strcmp(val.c_str(), "0")) {
      LOG_ERROR(
          "if set, should set auto_inc column exactly when modify partition "
          "key column value. SQL[%s]\n",
          sql);
      throw Error(
          "if set, should set auto increment column exactly (not 0 or NULL) "
          "when modify partition key column value");
    } else {  // check auto inc validity
      IntExpression ie(val.c_str());
      get_auto_inc_value_one_insert_row(par_space, plan, schema_name,
                                        table_name, &ie);
    }
  }
}

void check_par_key_eq_in_where_clause(
    Expression *where_expr, const char *schema_name, const char *table_name,
    vector<const char *> *key_names, const char *table_alias,
    string &key_value_in_where_clause, bool &par_key_eq_in_where_clause)

{
#ifdef DEBUG
  ACE_ASSERT(where_expr);
#endif
  vector<CompareExpression *> where_comp_exprs;
  if (where_expr->type == EXPR_EQ) {
    where_comp_exprs.push_back((CompareExpression *)where_expr);
  } else {
    ((BoolBinaryCaculateExpression *)where_expr)
        ->get_all_comp_exprs(&where_comp_exprs);
  }
  vector<CompareExpression *>::iterator it = where_comp_exprs.begin();
  for (; it != where_comp_exprs.end(); ++it) {
    CompareExpression *cmp_tmp = *it;
    if (cmp_tmp->type != EXPR_EQ) {
      continue;
    }
    StrExpression *str_tmp = (StrExpression *)cmp_tmp->left;
    if (column_name_equal_key_alias(
            str_tmp->str_value, schema_name, strlen(schema_name), table_name,
            strlen(table_name), key_names->at(0), table_alias)) {
      Expression *right_expr = cmp_tmp->right;
      const char *str_val = right_expr->str_value;
      key_value_in_where_clause = str_val ? str_val : "";
      par_key_eq_in_where_clause = true;
      break;
    }
  }
}

bool check_is_all_plain_value(Expression *expr) {
  bool ret = true;
  if (!expr) {
    ret = false;
  } else if (expr->type != EXPR_EQ) {
    if (expr->type == EXPR_AND || expr->type == EXPR_OR ||
        expr->type == EXPR_XOR) {
      ret = ((BoolBinaryCaculateExpression *)expr)->is_all_plain_expr();
    } else if (expr->is_plain_expression()) {
      ret = true;
    } else {
      ret = false;
    }
  } else {
    Expression *right = ((CompareExpression *)expr)->right;
    ret = right->is_plain_expression();
  }
  return ret;
}

bool check_is_all_and_expr(Expression *where_expr) {
  bool is_where_clause_and_expr = true;
  if (!where_expr) {
    is_where_clause_and_expr = false;
  } else if (where_expr->type != EXPR_EQ) {
    if (where_expr->type == EXPR_AND) {  // should be like: a=b and c<d and e=f
                                         // ... where 'a' is partition key
      is_where_clause_and_expr =
          ((BoolBinaryCaculateExpression *)where_expr)->is_all_and_expr();
    } else {  //"where 1" is not valid
      is_where_clause_and_expr = false;
    }
  }
  return is_where_clause_and_expr;
}

void fetch_rows_to_update(DataSpace *dataspace_where_clause, string &fetch_sql,
                          vector<vector<string> > &result_vec,
                          vector<vector<bool> > &result_null_vec,
                          int &fetch_row_count, bool is_count) {
  Connection *conn = NULL;
  try {
    conn = dataspace_where_clause->get_connection(NULL);
    if (!conn) throw Error("Fail to get connection to fetch_rows_to_update");
    conn->query_for_all_column(fetch_sql.c_str(), &result_vec,
                               &result_null_vec);
    conn->get_pool()->add_back_to_free(conn);
    conn = NULL;
  } catch (...) {
    if (conn) {
      conn->get_pool()->add_back_to_dead(conn);
      conn = NULL;
    }
    throw;
  }
  if (is_count) {
    fetch_row_count = atoi(result_vec[0][0].c_str());
  } else {
    fetch_row_count = result_vec.size();
  }
  LOG_DEBUG("get [%d] rows\n", fetch_row_count);
}

void build_update_sql_insert_part(
    string &insert_head, string &update_sql_insert_part,
    int fetch_col_names_size, int fetch_row_count,
    map<string, TableColumnInfo *, strcasecomp> *column_info_map,
    vector<vector<string> > &result_vec, vector<vector<bool> > &result_null_vec,
    vector<string> &fetch_col_names, const char *charset) {
  for (int i = 0; i < fetch_row_count;) {
    if (result_vec.empty()) {
      // all column are in set list
      update_sql_insert_part.append(insert_head);
    } else {
      // some column are in result_vec, we need append it at end of insert_head
      string insert_sql = insert_head;
      for (int j = 0; j < fetch_col_names_size; ++j) {
        insert_sql.append(",`");
        insert_sql.append(fetch_col_names[j]);
        if (result_null_vec[i][j]) {
          insert_sql.append("`=NULL");
        } else {
          if ((*column_info_map)[fetch_col_names[j]]->is_data_type_blob_like) {
            throw Error("can not update row when blob column not NULL");
          }
          insert_sql.append("`=\'");
          if ((*column_info_map)[fetch_col_names[j]]
                  ->is_data_type_string_like) {
            string val = result_vec[i][j];
            add_translation_char_for_special_char(val, charset);
            insert_sql.append(val);
          } else {
            insert_sql.append(result_vec[i][j]);
          }
          insert_sql.append("\'");
        }
      }
      update_sql_insert_part.append(insert_sql);
    }
    ++i;
    if (i < fetch_row_count) {
      update_sql_insert_part.append(";");
    }
  }
}

void build_fetch_sql_for_update_sql(
    string &fetch_sql, int set_col_count, int table_column_count,
    map<string, TableColumnInfo *, strcasecomp> *column_info_map,
    map<string, CompareExpression *, strcasecomp> &set_col_names,
    vector<string> &fetch_col_names, string &new_schema_tmp,
    const char *table_name, const char *sql, stmt_node &st) {
  fetch_sql = "select ";
  if (set_col_count == table_column_count) {
    fetch_sql.append("count(*)");
  } else {
    map<string, TableColumnInfo *, strcasecomp>::iterator ittc =
        column_info_map->begin();
    for (; ittc != column_info_map->end();) {
      if (!set_col_names.count(ittc->second->column_name)) {
        fetch_sql.append("`");
        fetch_sql.append(ittc->second->column_name);
        fetch_sql.append("`");
        fetch_sql.append(", ");
        fetch_col_names.push_back(ittc->second->column_name);
      }
      ++ittc;
    }
    boost::erase_tail(fetch_sql, 2);
  }

  fetch_sql.append(" from `");
  fetch_sql.append(new_schema_tmp);
  fetch_sql.append("`.`");
  fetch_sql.append(table_name);
  fetch_sql.append("` ");
  fetch_sql.append(sql, st.scanner->where_pos - 1, strlen(sql));
  LOG_DEBUG("generate sql [%s] to fetch rows for update sql [%s]\n",
            fetch_sql.c_str(), sql);
}

void Statement::handle_one_par_table_one_spaces(ExecutePlan *plan) {
  table_link *par_table =
      one_table_node.only_one_table ? one_table_node.table : par_tables[0];
  PartitionedTable *par_space =
      (PartitionedTable *)(one_table_node.only_one_table ? one_table_node.space
                                                         : spaces[0]);
  record_scan *rs = par_table->join->cur_rec_scan;
  vector<const char *> *key_names = par_space->get_key_names();
  const char *schema_name =
      par_table->join->schema_name ? par_table->join->schema_name : schema;
  const char *table_name = par_table->join->table_name;
  const char *table_alias = par_table->join->alias;
  stmt_type type = st.type;

  set_sql_schema(schema_name);

  set_full_table_name(schema_name, table_name);

  if (st.has_where_rownum) {
    LOG_ERROR("Unsupport par table use rownum.\n");
    throw UnSupportPartitionSQL("Unsupport par table use rownum.");
  }

  switch (type) {
    case STMT_INSERT:
    case STMT_REPLACE: {
#ifndef DBSCALE_TEST_DISABLE
      if (on_test_stmt) {
        dbscale_test_info *test_info = plan->session->get_dbscale_test_info();
        if (!strcasecmp(test_info->test_case_name.c_str(), "auto_increment") &&
            !strcasecmp(test_info->test_case_operation.c_str(),
                        "table_non_exist") &&
            !strcasecmp(sql, "INSERT INTO test.t1 (c1,c2,c3) VALUES (0,0,0)")) {
          full_table_name = string("test.table_non_exist");
          table_name = "table_not_exist";
          LOG_DEBUG(
              "Do dbscale test operation 'table_non_exist' for case "
              "'auto_increment'\n");
        }
      }
#endif
      size_t schema_len = strlen(schema_name);
      size_t table_len = strlen(table_name);

      int64_t curr_auto_inc_val = 0;
      insert_op_node *insert = NULL;
      if (plan->session->is_call_store_procedure()) {
        insert = st.sql->insert_oper;
      } else {
        insert = get_latest_stmt_node()->sql->insert_oper;
      }
      vector<unsigned int> key_pos_def;
      par_space->get_key_pos_vec(schema_name, table_name, key_pos_def,
                                 stmt_session);
      vector<unsigned int> key_pos_real;
      unsigned int part_id = 0;
      PartitionMethod *method = par_space->get_partition_method();
      int column_count = 0;
      int value_count = 0;

      init_auto_increment_params(plan, schema_name, table_name, par_space);

      if (insert->insert_column_list && auto_inc_status != NO_AUTO_INC_FIELD) {
        if (auto_inc_lock_mode != AUTO_INC_LOCK_MODE_INTERLEAVED ||
            insert->insert_row_num > 1) {
          /*If only one row and
           *  auto_inc_lock_mode == AUTO_INC_LOCK_MODE_INTERLEAVED
           *
           *  The checking work can be pushed down to backend server.*/
          check_column_count_name(par_space, insert->insert_column_list,
                                  column_count);
        }
        rectify_auto_inc_key_pos(insert->insert_column_list, schema_name,
                                 schema_len, table_name, table_len,
                                 table_alias);
      }
      fulfill_insert_key_pos(insert->insert_column_list, key_names, schema_name,
                             schema_len, table_name, table_len, table_alias,
                             &key_pos_def, &key_pos_real);
      int enable_last_insert_id_session =
          plan->handler->get_session()
              ->get_session_option("enable_last_insert_id")
              .int_val;
      if (insert->insert_row_num > 1) {
        int row_index = 0;
        string sql_prefix;
        expr_list_item *auto_inc_expr_item = NULL;
        multi_insert_id_sqls.clear();
        build_insert_sql_prefix(sql_prefix, insert);
        bool need_update_last_insert_id = enable_last_insert_id_session;
        if (auto_inc_status != NO_AUTO_INC_FIELD &&
            need_update_last_insert_id) {
          old_last_insert_id =
              plan->handler->get_session()->get_last_insert_id();
        }
        insert_row *row = insert->insert_values;
        ACE_Thread_Mutex *stmt_lock = NULL;
        if (auto_inc_status != NO_AUTO_INC_FIELD) {
          stmt_lock = par_space->get_stmt_autoinc_lock(full_table_name);
          if (stmt_lock) stmt_lock->acquire();
        }
        try {
          for (; row_index < insert->insert_row_num; ++row_index) {
            ListExpression *expr_list = (ListExpression *)row->column_values;
            if (auto_inc_status != NO_AUTO_INC_FIELD) {
              value_count = expr_list->list_size;
              check_value_count_auto_increment(column_count, value_count,
                                               par_space, row_index);
              auto_inc_expr_item = get_auto_inc_expr(expr_list->expr_list_head);
              curr_auto_inc_val = get_auto_inc_value_multi_insert_row(
                  par_space, plan, schema_name, table_name,
                  auto_inc_expr_item ? auto_inc_expr_item->expr : NULL,
                  insert->insert_row_num - row_index);
            }
            part_id = get_partition_one_insert_row(
                row->column_values,
                key_pos_real.size() ? &key_pos_real : &key_pos_def,
                curr_auto_inc_val, value_count, method, "");
            part_id = par_space->get_real_par_id_from_virtual_id(part_id);
            if (!multi_insert_id_sqls.count(part_id)) {
              multi_insert_id_sqls[part_id].append(sql_prefix);
            }
            if (auto_inc_status == NO_AUTO_INC_FIELD ||
                auto_inc_status == AUTO_INC_NO_NEED_MODIFY) {
              append_insert_sql_row(multi_insert_id_sqls[part_id], expr_list);
            } else {
              append_insert_sql_row_auto_inc(multi_insert_id_sqls[part_id], row,
                                             auto_inc_expr_item,
                                             curr_auto_inc_val);
              if (need_update_last_insert_id &&
                  (auto_inc_status == AUTO_INC_VALUE_NULL ||
                   auto_inc_status == AUTO_INC_NOT_SPECIFIED)) {
                plan->handler->get_session()->set_last_insert_id(
                    curr_auto_inc_val);
                need_update_last_insert_id = false;
              }
            }
            row = row->next;
          }
        } catch (...) {
          if (auto_inc_status != NO_AUTO_INC_FIELD) {
            rollback_auto_inc_params(plan, par_space);
            if (stmt_lock) stmt_lock->release();
          }
          throw;
        }
        if (auto_inc_status != NO_AUTO_INC_FIELD) {
          if (stmt_lock) stmt_lock->release();
        }

        map<unsigned int, string>::iterator it = multi_insert_id_sqls.begin();
        string on_dup_info;
        if (insert->on_dup_update_list_end_pos -
                insert->on_dup_update_list_start_pos >
            1) {
          on_dup_info = string(" ") +
                        string(sql, insert->on_dup_update_list_start_pos - 1,
                               insert->on_dup_update_list_end_pos -
                                   insert->on_dup_update_list_start_pos + 1);
        }
        for (; it != multi_insert_id_sqls.end(); ++it) {
          boost::erase_tail(it->second, 1);
          if (!on_dup_info.empty()) {
            it->second.append(on_dup_info);
          }
        }

        if (multi_insert_id_sqls.size() == 1) {  // only one partition is used.
          DataSpace *dataspace = par_space->get_partition(part_id);
          assemble_one_partition_plan(plan, dataspace,
                                      multi_insert_id_sqls[part_id]);
          plan->session->set_execute_plan_touch_partition_nums(1);
        } else {
          assemble_mul_part_insert_plan(plan, par_space);
          if (multi_insert_id_sqls.size() ==
              par_space->get_real_partition_num()) {
            plan->session->set_execute_plan_touch_partition_nums(-1);
          } else {
            plan->session->set_execute_plan_touch_partition_nums(
                multi_insert_id_sqls.size());
          }
        }
        break;
      } else if (insert->insert_values) {
        expr_list_item *auto_inc_expr_item = NULL;
        if (auto_inc_status != NO_AUTO_INC_FIELD) {
          ListExpression *le =
              (ListExpression *)insert->insert_values->column_values;
          expr_list_item *head = le->expr_list_head;
          value_count = le->list_size;
          if (auto_inc_lock_mode != AUTO_INC_LOCK_MODE_INTERLEAVED ||
              (column_count > 0 && column_count != value_count)) {
            /*Only has one row, and AUTO_INC_LOCK_MODE_INTERLEAVED, so the
             * error checking can be pushed down to backend server. */
            check_value_count_auto_increment(column_count, value_count,
                                             par_space, 0);
          }
          auto_inc_expr_item = get_auto_inc_expr(head);
          curr_auto_inc_val = get_auto_inc_value_one_insert_row(
              par_space, plan, schema_name, table_name,
              auto_inc_expr_item ? auto_inc_expr_item->expr : NULL);
        }

        part_id = get_partition_one_insert_row(
            insert->insert_values->column_values,
            key_pos_real.size() ? &key_pos_real : &key_pos_def,
            curr_auto_inc_val, value_count, method, "");
        part_id = par_space->get_real_par_id_from_virtual_id(part_id);
        if (auto_inc_status != NO_AUTO_INC_FIELD &&
            auto_inc_status != AUTO_INC_NO_NEED_MODIFY) {
          build_insert_sql_auto_inc(insert_sql, insert, auto_inc_expr_item,
                                    curr_auto_inc_val);
          if (enable_last_insert_id_session &&
              (auto_inc_status == AUTO_INC_VALUE_NULL ||
               auto_inc_status == AUTO_INC_NOT_SPECIFIED)) {
            plan->handler->get_session()->set_last_insert_id(curr_auto_inc_val);
          }
          DataSpace *dataspace = par_space->get_partition(part_id);
          assemble_one_partition_plan(plan, dataspace, insert_sql);
          plan->session->set_execute_plan_touch_partition_nums(1);
          return;
        }
      } else if (insert->assign_list) {
        Expression *value_expr = NULL;
        if (auto_inc_status != NO_AUTO_INC_FIELD) {
          if (auto_inc_lock_mode != AUTO_INC_LOCK_MODE_INTERLEAVED)
            check_column_count_name(par_space, insert->assign_list);
          expr_list_item *expr_item = NULL;
          if (get_auto_inc_from_insert_assign(insert->assign_list, schema_name,
                                              schema_len, table_name, table_len,
                                              table_alias, &expr_item)) {
            value_expr = ((CompareExpression *)expr_item->expr)->right;
          } else {
            set_auto_inc_status(AUTO_INC_NOT_SPECIFIED);
          }
          curr_auto_inc_val = get_auto_inc_value_one_insert_row(
              par_space, plan, schema_name, table_name, value_expr);
        }

        part_id = get_partition_insert_asign(
            insert->assign_list, schema_name, schema_len, table_name, table_len,
            table_alias, key_names, curr_auto_inc_val, method);
        part_id = par_space->get_real_par_id_from_virtual_id(part_id);
        if (auto_inc_status != NO_AUTO_INC_FIELD &&
            auto_inc_status != AUTO_INC_NO_NEED_MODIFY) {
          build_insert_sql_assign_auto_inc(insert_sql, value_expr,
                                           curr_auto_inc_val);
          if (enable_last_insert_id_session &&
              (auto_inc_status == AUTO_INC_VALUE_NULL ||
               auto_inc_status == AUTO_INC_NOT_SPECIFIED)) {
            plan->handler->get_session()->set_last_insert_id(curr_auto_inc_val);
          }
          DataSpace *dataspace = par_space->get_partition(part_id);
          assemble_one_partition_plan(plan, dataspace, insert_sql);
          plan->session->set_execute_plan_touch_partition_nums(1);
          return;
        }
      } else {
        LOG_ERROR("Unsupport insert stmt [%s].\n", sql);
        throw UnSupportPartitionSQL("Unsupport insert stmt");
      }
      DataSpace *dataspace = par_space->get_partition(part_id);
      assemble_one_partition_plan(plan, dataspace);
      plan->session->set_execute_plan_touch_partition_nums(1);
      return;
    }
    case STMT_UPDATE: {
      Expression *set_list = st.sql->update_oper->update_set_list;
      string key_value_in_set_list = "";
      if (set_list_contain_key(set_list, key_names, schema_name, table_name,
                               table_alias, key_value_in_set_list)) {
        // update set list constain partition key
        Expression *where_expr = st.sql->update_oper->condition;
        if (!check_is_all_and_expr(where_expr)) {
          LOG_ERROR(
              "only support update partition key with where clause with type "
              "AND "
              "or directly equivalent condition. Unsupport SQL[%s]\n",
              sql);
          throw UnSupportPartitionSQL(
              "only support update partition key with where clause with type "
              "AND "
              "or directly equivalent condition");
        }
        expr_list_item *head = ((ListExpression *)set_list)->expr_list_head;
        expr_list_item *tmp = head;
        do {
          CompareExpression *cmp_tmp = (CompareExpression *)tmp->expr;
          Expression *right = cmp_tmp->right;
          if (!right->is_plain_expression()) {
            throw UnSupportPartitionSQL(
                "only support plain expression in set list");
          }
          tmp = tmp->next;
        } while (tmp != head);
        if (!check_is_all_plain_value(where_expr)) {
          throw UnSupportPartitionSQL(
              "only support plain or NULL expression in where clause");
        }

        // the parititon key in where clause should be an EXPR_EQ, like c1=2
        string key_value_in_where_clause = "";
        bool par_key_eq_in_where_clause = false;
        check_par_key_eq_in_where_clause(
            where_expr, schema_name, table_name, key_names, table_alias,
            key_value_in_where_clause, par_key_eq_in_where_clause);
        if (!par_key_eq_in_where_clause) {
          LOG_ERROR(
              "only support update partition key when where "
              "clause also has partition key, and it should be in equivalent "
              "condition. Unsupport SQL[%s]\n",
              sql);
          throw UnSupportPartitionSQL(
              "only support update partition key when where "
              "clause also has partition key, and it should be in equivalent "
              "condition");
        }

        if (key_value_in_set_list.empty() ||
            key_value_in_where_clause.empty()) {
          LOG_ERROR(
              "fail to get partition key value from sql, key_value_in_set_list "
              "[%s], key_value_in_where_clause [%s], sql [%s]\n",
              key_value_in_set_list.c_str(), key_value_in_where_clause.c_str(),
              sql);
          if (key_value_in_set_list.empty()) {
            throw Error("fail to get partition key value from set list");
          }
          if (key_value_in_where_clause.empty()) {
            throw Error("fail to get partition key value from where clause");
          }
        }

        unsigned int part_id_set_list = -1;
        unsigned int part_id_where_clause = -2;
        vector<const char *> key_values;
        key_values.push_back(key_value_in_set_list.c_str());
        PartitionMethod *method = par_space->get_partition_method();
        string replace_null_char =
            plan->session->get_query_sql_replace_null_char();
        part_id_set_list =
            method->get_partition_id(&key_values, replace_null_char);
        part_id_set_list =
            par_space->get_real_par_id_from_virtual_id(part_id_set_list);

        if (key_value_in_set_list != key_value_in_where_clause) {
          key_values.clear();
          key_values.push_back(key_value_in_where_clause.c_str());
          part_id_where_clause =
              method->get_partition_id(&key_values, replace_null_char);
          part_id_where_clause =
              par_space->get_real_par_id_from_virtual_id(part_id_where_clause);
        }

        map<string, CompareExpression *, strcasecomp> set_col_names;
        bool update_del_quick_limit =
            plan->session->get_session_option("update_delete_quick_limit")
                .int_val;
        init_auto_increment_params(plan, schema_name, table_name, par_space);
        if (!auto_inc_key_name.empty()) {
          check_auto_inc_value_in_update_set_list(set_list, set_col_names,
                                                  par_space, plan, schema_name,
                                                  table_name);
        }

        DataSpace *dataspace_set_list =
            par_space->get_partition(part_id_set_list);
        if ((key_value_in_set_list == key_value_in_where_clause) ||
            (part_id_set_list == part_id_where_clause)) {
          // the update sql do not move row from one partition to another, we
          // can try to generate direct execution plan
          assemble_direct_exec_plan(plan, dataspace_set_list);
          plan->session->set_execute_plan_touch_partition_nums(1);
          return;
        }

        // situation: part_id_set_list != part_id_where_clause
        if (!plan->session->is_in_transaction()) {
          LOG_ERROR(
              "session should in transaction when modify "
              "partition key column value from one partition to another. "
              "SQL[%s]\n",
              sql);
          throw Error(
              "session should in transaction when modify partition key column "
              "value from one partition to another");
        }
        if (set_col_names.empty()) {
          ListExpression *expr_list = (ListExpression *)set_list;
          fulfill_set_col_names(expr_list, set_col_names);
        }

        if (update_del_quick_limit) {
          DataSpace *dataspace_where_clause = NULL;
          string new_schema_tmp;
          dataspace_where_clause =
              par_space->get_partition(part_id_where_clause);
          adjust_shard_schema(par_table->join->schema_name, schema,
                              new_schema_tmp,
                              dataspace_where_clause->get_virtual_machine_id(),
                              dataspace_where_clause->get_partition_id());
          update_sql_delete_part = "delete from `";
          update_sql_delete_part.append(new_schema_tmp);
          update_sql_delete_part.append("`.`");
          update_sql_delete_part.append(table_name);
          update_sql_delete_part.append("` ");
          update_sql_delete_part.append(sql, st.scanner->where_pos - 1,
                                        strlen(sql));
          update_sql_delete_part.append(" limit 1");

          new_schema_tmp.clear();
          adjust_shard_schema(par_table->join->schema_name, schema,
                              new_schema_tmp,
                              dataspace_set_list->get_virtual_machine_id(),
                              dataspace_set_list->get_partition_id());
          int set_start_pos = st.sql->update_oper->update_set_start_pos;
          int set_end_pos = st.sql->update_oper->update_set_end_pos;
          update_sql_insert_part = "insert into `";
          update_sql_insert_part.append(new_schema_tmp);
          update_sql_insert_part.append("`.`");
          update_sql_insert_part.append(table_name);
          update_sql_insert_part.append("` ");
          update_sql_insert_part.append(sql, set_start_pos - 1,
                                        set_end_pos - (set_start_pos - 1));
          if (!auto_inc_key_name.empty() &&
              !set_col_names.count(auto_inc_key_name)) {
            update_sql_insert_part.append(", `");
            update_sql_insert_part.append(auto_inc_key_name);
            update_sql_insert_part.append("`=");
            int64_t auto_inc_val = get_auto_increment_value(
                par_space, plan, schema_name, table_name, NULL, 1, false);
            char tmp[21];
            sprintf(tmp, "%ld", auto_inc_val);
            update_sql_insert_part.append(tmp);
          }

          update_sql_insert_part_revert = "delete from `";
          update_sql_insert_part_revert.append(new_schema_tmp);
          update_sql_insert_part_revert.append("`.`");
          update_sql_insert_part_revert.append(table_name);
          update_sql_insert_part_revert.append("` where ");
          ListExpression *expr_list = (ListExpression *)set_list;
          expr_list_item *head = expr_list->expr_list_head;
          expr_list_item *tmp = head;
          do {
            Expression *expr = tmp->expr;  // like a=b
            update_sql_insert_part_revert.append(
                sql, expr->start_pos - 1, expr->end_pos - expr->start_pos + 1);
            tmp = tmp->next;
            if (tmp != head) {
              update_sql_insert_part_revert.append(" and ");
            }
          } while (tmp != head);
          update_sql_insert_part_revert.append(" limit 1");

          LOG_DEBUG(
              "for update_del_quick_limit, the UPDATE sql of "
              "primary/unique-key table is split into 1 delele and 1 insert "
              "sql:\n");
          LOG_DEBUG("delete sql [%s]\n", update_sql_delete_part.c_str());
          LOG_DEBUG("insert sql [%s]\n", update_sql_insert_part.c_str());
          LOG_DEBUG("revert sql [%s]\n", update_sql_insert_part_revert.c_str());

          map<DataSpace *, const char *> spaces_map;
          spaces_map[dataspace_where_clause] = update_sql_delete_part.c_str();
          spaces_map[dataspace_set_list] = update_sql_insert_part.c_str();
          ExecuteNode *node = NULL;
          node = plan->get_mul_modify_node(spaces_map);
          plan->set_start_node(node);
          is_update_via_delete_insert_revert = true;
          update_via_delete_insert_space_insert = dataspace_set_list;
          plan->session->set_affected_servers(AFFETCED_MUL_SERVER);
          plan->session->set_execute_plan_touch_partition_nums(2);
          return;
        } else {
          int set_col_count = set_col_names.size();
          map<string, TableColumnInfo *, strcasecomp> *column_info_map;
          int table_column_count = 0;
          TableInfoCollection *tic = TableInfoCollection::instance();
          TableInfo *ti = tic->get_table_info_for_read(schema_name, table_name);
          try {
            column_info_map =
                ti->element_table_column
                    ->get_element_map_columnname_table_column_info(
                        stmt_session);
            table_column_count = column_info_map->size();
          } catch (...) {
            LOG_ERROR(
                "Error occured when try to get table info column "
                "info(map_table_column_info) of table [%s.%s]\n",
                schema_name, table_name);
            ti->release_table_info_lock();
            throw;
          }

          int fetch_row_count =
              0;  // TODO add config item to control total-update-rows
          vector<vector<string> > result_vec;
          vector<vector<bool> > result_null_vec;
          string fetch_sql;
          vector<string> fetch_col_names;
          DataSpace *dataspace_where_clause = NULL;
          string new_schema_tmp;
          try {
            dataspace_where_clause =
                par_space->get_partition(part_id_where_clause);
            adjust_shard_schema(
                par_table->join->schema_name, schema, new_schema_tmp,
                dataspace_where_clause->get_virtual_machine_id(),
                dataspace_where_clause->get_partition_id());
          } catch (...) {
            ti->release_table_info_lock();
            throw;
          }
          try {  // try-catch to make sure ti->release_table_info_lock()
            build_fetch_sql_for_update_sql(fetch_sql, set_col_count,
                                           table_column_count, column_info_map,
                                           set_col_names, fetch_col_names,
                                           new_schema_tmp, table_name, sql, st);
            fetch_rows_to_update(dataspace_where_clause, fetch_sql, result_vec,
                                 result_null_vec, fetch_row_count,
                                 set_col_count == table_column_count);
            if (fetch_row_count == 0) {
              // 0 row fetched, this means 0 rows will be updated, so return ok
              // packet directly
              ti->release_table_info_lock();
              ExecuteNode *node = plan->get_return_ok_node();
              plan->set_start_node(node);
              return;
            }

            // then build one delete sql and one(or more) insert sql
            update_sql_delete_part = "delete from `";
            update_sql_delete_part.append(new_schema_tmp);
            update_sql_delete_part.append("`.`");
            update_sql_delete_part.append(table_name);
            update_sql_delete_part.append("` ");
            update_sql_delete_part.append(sql, st.scanner->where_pos - 1,
                                          strlen(sql));

            new_schema_tmp.clear();
            adjust_shard_schema(par_table->join->schema_name, schema,
                                new_schema_tmp,
                                dataspace_set_list->get_virtual_machine_id(),
                                dataspace_set_list->get_partition_id());
            int set_start_pos = st.sql->update_oper->update_set_start_pos;
            int set_end_pos = st.sql->update_oper->update_set_end_pos;
            string insert_head = "insert into `";
            insert_head.append(new_schema_tmp);
            insert_head.append("`.`");
            insert_head.append(table_name);
            insert_head.append("` ");
            insert_head.append(sql, set_start_pos - 1,
                               set_end_pos - (set_start_pos - 1));
            string charset;
            if (get_client_charset_type() == CHARSET_TYPE_GBK)
              charset.assign("gbk");
            else if (get_client_charset_type() == CHARSET_TYPE_GB18030)
              charset.assign("gb18030");
            build_update_sql_insert_part(
                insert_head, update_sql_insert_part, fetch_col_names.size(),
                fetch_row_count, column_info_map, result_vec, result_null_vec,
                fetch_col_names, charset.c_str());
            ti->release_table_info_lock();
          } catch (...) {
            ti->release_table_info_lock();
            throw;
          }

          LOG_DEBUG(
              "after split, the UPDATE sql is split into 1 delete sql and %d "
              "insert sqls:\n",
              fetch_row_count);
          LOG_DEBUG("delete sql [%s]\n", update_sql_delete_part.c_str());
          LOG_DEBUG("insert sql [%s]\n", update_sql_insert_part.c_str());

          map<DataSpace *, const char *> spaces_map;
          map<DataSpace *, int> sql_count_map;
          spaces_map[dataspace_where_clause] = update_sql_delete_part.c_str();
          spaces_map[dataspace_set_list] = update_sql_insert_part.c_str();
          sql_count_map[dataspace_where_clause] = 1;
          sql_count_map[dataspace_set_list] = fetch_row_count;
          ExecuteNode *node = NULL;
          if (fetch_row_count > 1) {
            node = plan->get_mul_modify_node(spaces_map, true, &sql_count_map);
          } else {
            node = plan->get_mul_modify_node(spaces_map);
          }
          plan->set_start_node(node);
          is_update_via_delete_insert = true;
          plan->session->set_affected_servers(AFFETCED_MUL_SERVER);
          plan->session->set_execute_plan_touch_partition_nums(2);
        }
        return;
      }
    }
    case STMT_DELETE: {
      /*If the modify table is small table or modify table is (or related to,
       * such as "DELETE FROM t2 USING par_table,t2") a partition table with
       * the limit condition, we need to seperated this sql into one modify
       * sql + one select sql.*/
      if ((st.scanner->first_table->next &&
           st.scanner->first_table->next->join->cur_rec_scan == st.scanner &&
           !update_on_one_server && !delete_on_one_server &&
           !(st.type == STMT_UPDATE && table_contains_set_list(par_table))) ||
          (rs->limit && !stmt_with_limit_using_quick_limit)) {
        handle_one_par_table_mul_spaces(plan);
        break;
      }
    }
    case STMT_SELECT: {
      vector<unsigned int> par_ids;
      unsigned int partition_num = par_space->get_partition_num();
      PartitionMethod *method = par_space->get_partition_method();
      get_partitons_one_record_scan(rs, method, partition_num, &par_ids);
      par_space->replace_real_par_id_vec_from_virtual_ids(&par_ids);
      unsigned int par_ids_size = par_ids.size();
      DataSpace *dataspace = NULL;
      // in multiple_mode, when session delete part table auto_increment value,
      // record value. then this session can insert the value to the part table
      if (multiple_mode && type == STMT_DELETE &&
          instance_option_value["record_auto_increment_delete_value"]
              .uint_val) {
        string full_table_name;
        splice_full_table_name(schema_name, table_name, full_table_name);
        Backend *backend = Backend::instance();
        // check whether table has set auto_increment info
        if (backend->get_alter_table_flag(full_table_name,
                                          ALTER_TABLE_FLAG_AUTO_INCREMENT) ||
            !backend->has_set_auto_increment_info(full_table_name)) {
          init_auto_increment_params(plan, schema_name, table_name, par_space);
        }
        // check whether table is Auto_increment table
        if (backend->has_auto_increment_field(full_table_name)) {
          string auto_inc_key_name =
              par_space->get_auto_increment_key(full_table_name);
          string part_key = key_names->at(0);
          // check whether partition key is auto increment key
          if (part_key == auto_inc_key_name) {
            vector<const char *> *vec = get_key_values(rs, key_names->at(0));
            plan->session->record_auto_increment_delete_values(full_table_name,
                                                               vec);
          }
        }
      }
      if (par_ids_size == 1) {
        dataspace = par_space->get_partition(par_ids[0]);
        assemble_one_partition_plan(plan, dataspace);
        plan->session->set_execute_plan_touch_partition_nums(1);
        return;
      } else if (par_ids_size > 1 &&
                 par_ids_size <= par_space->get_real_partition_num()) {
        if (type == STMT_SELECT) {
          if (!is_acceptable_join_par_table(par_table->join)) {
            LOG_ERROR(
                "Unsupport join type for partition table with sql [%s].\n",
                sql);
            throw UnSupportPartitionSQL(
                "Unsupport join type for partition table");
          }
          bool need_check_distinct = true;
          if (check_and_replace_par_distinct_with_groupby(par_table)) {
            // sql has update by replacing DISTINCT with GROUP BY, so need
            // reparse.
            Statement *new_stmt = NULL;
            sql = no_distinct_sql.c_str();
            re_parser_stmt(&(st.scanner), &new_stmt, sql);
            need_check_distinct = false;
          }
          assemble_mul_par_result_set_plan(plan, &par_ids, par_space,
                                           need_check_distinct);
        } else {
          if (close_cross_node_transaction) {
            LOG_ERROR(
                "Refuse to execute cross node transaction when "
                "close_cross_node_transaction != 0\n");
            throw Error(
                "Refuse to execute cross node transaction when "
                "close_cross_node_transaction != 0");
          }
          assemble_mul_par_modify_plan(plan, &par_ids, par_space);
        }
        plan->session->set_execute_plan_touch_partition_nums(par_ids_size);
        return;
      }
      break;
    }
    case STMT_DBSCALE_ESTIMATE: {
      vector<unsigned int> *par_ids = new vector<unsigned int>();
      Statement *stmt = plan->statement;
      record_scan *rs = stmt->get_stmt_node()->cur_rec_scan;
      unsigned int partition_num = par_space->get_partition_num();
      PartitionMethod *method = par_space->get_partition_method();
      stmt->get_partitons_one_record_scan(rs, method, partition_num, par_ids);
      par_space->replace_real_par_id_vec_from_virtual_ids(par_ids);
      assemble_dbscale_estimate_select_partition_plan(plan, par_ids, par_space,
                                                      sql, this);
    } break;
    case STMT_PREPARE: {
      // TODO: implement it in the future.
      break;
    }
    case STMT_TRUNCATE: {
      if (st.table_list_num > 1) {
        LOG_ERROR(
            "Drop multiple tables, including partiton table, is not "
            "supported!");
        throw UnSupportPartitionSQL(
            "Drop multiple tables, including partiton table, is not supported");
      }
      st.enable_alter_table_sync_stmt = enable_alter_table_sync;
      table_link *table = st.table_list_head;
      schema_name =
          table->join->schema_name ? table->join->schema_name : schema;
      if (dbscale_safe_sql_mode > 0 &&
          Backend::instance()->is_system_schema(schema_name)) {
        throw Error(
            "Refuse truncate table of system schema for "
            "dbscale_safe_sql_mode>0");
      }
      if (multiple_mode && st.enable_alter_table_sync_stmt) {
        if (MultipleManager::instance()->get_is_cluster_master()) {
          plan->session->set_alter_table_name(full_table_name);
        } else {
          assemble_forward_master_role_plan(plan, true);
          break;
        }
      }
      if (enable_table_recycle_bin)
        assemble_move_table_to_recycle_bin_plan(plan);
      else
        assemble_modify_all_partition_plan(plan, par_space);
      break;
    }
    case STMT_CREATE_TB: {
      if (par_space->get_key_names()->size() == 1) {
        const char *par_key = par_space->get_key_names()->front();
        if (!is_partition_table_contain_partition_key(par_key)) {
          LOG_ERROR("Fail to find the partition key [%s] from table %s.%s\n",
                    par_key, schema_name, table_name);
          throw Error("Fail to find partition key from table.");
        }
      } else {
        LOG_ERROR("Not specify partition key for table %s.%s\n", schema_name,
                  table_name);
        throw Error("Not specify partition key for table.");
      }

      if (st.sql->create_tb_oper->has_foreign_key) {
        LOG_INFO(
            "Create table defination contains foreign key restriction, "
            "please make sure these keys are partitioned appropriately.\n");
      }
      if (st.sql->create_tb_oper->part_table_def_start_pos > 0) {
        no_part_table_def_sql.assign(
            sql, st.sql->create_tb_oper->part_table_def_start_pos - 1);
        no_part_table_def_sql.append(
            sql + st.sql->create_tb_oper->part_table_def_end_pos);
        sql = no_part_table_def_sql.c_str();
        Statement *new_stmt = NULL;
        re_parser_stmt(&(st.scanner), &new_stmt, sql);
      }
    }
    case STMT_ALTER_TABLE: {
      if ((is_partial_parsed() && type == STMT_ALTER_TABLE)) {
        st.modify_column = true;
      }
      if (type == STMT_CREATE_TB) {
        string full_table_name;
        splice_full_table_name(schema_name, table_name, full_table_name);
        if (Backend::instance()->has_auto_increment_field(full_table_name)) {
          LOG_DEBUG("DBScale contains auto_increment info for table [%s]\n",
                    full_table_name.c_str());
          st.modify_column = true;
        }
      }
      st.enable_alter_table_sync_stmt = enable_alter_table_sync;
      if (!multiple_mode)
        Backend::instance()->alter_table_flag_on(full_table_name);
      if (multiple_mode &&
          (st.enable_alter_table_sync_stmt || st.modify_column) &&
          !(type == STMT_CREATE_TB &&
            st.sql->create_tb_oper->part_table_def_start_pos > 0) &&
          !(type == STMT_CREATE_TB &&
            plan->session->get_session_option("auto_space_level").uint_val ==
                AUTO_SPACE_TABLE)) {
        if (MultipleManager::instance()->get_is_cluster_master()) {
          plan->session->set_alter_table_name(full_table_name);
        } else {
          LOG_DEBUG("CREATE TB or ALTER TB be execute on slave dbscale\n");
          assemble_forward_master_role_plan(plan, true);
          break;
        }
      }
      assemble_modify_all_partition_plan(plan, par_space);
      break;
    }
    case STMT_LOAD:
      par_space->set_auto_increment_info(plan->handler, this, schema_name,
                                         table_name, true, true);
      if (!support_load_set) {
        if (st.sql->load_oper->has_ignore ||
            st.sql->load_oper->has_set_columns) {
          LOG_ERROR("unsupport load data with ignore or set columns.\n");
          throw UnSupportPartitionSQL(
              "unsupport load data with ignore or set columns.");
        }
      }
      if (st.sql->load_oper->local) {
        assemble_partition_load_local_plan(plan, par_space, schema_name,
                                           table_name);
      } else {
        assemble_partition_load_data_infile_plan(plan, par_space, schema_name,
                                                 table_name);
      }
      break;

    case STMT_TABLE_MAINTENANCE: {
      vector<unsigned int> par_ids;
      unsigned int partition_num = par_space->get_real_partition_num();
      par_ids.clear();
      unsigned int i = 0;
      for (; i < partition_num; ++i) par_ids.push_back(i);

      ExecuteNode *send_node = plan->get_send_node();
      ExecuteNode *fetch_node = NULL;
      DataSpace *dataspace = NULL;

      unsigned int id;
      for (i = 0; i < par_ids.size(); ++i) {
        id = par_ids[i];
        dataspace = par_space->get_partition(id);
        const char *used_sql = adjust_stmt_sql_for_shard(dataspace, sql);
        fetch_node = plan->get_fetch_node(dataspace, used_sql);
        send_node->add_child(fetch_node);
      }
      plan->session->set_execute_plan_touch_partition_nums(-1);
      if (max_fetch_node_threads == 1) plan->set_fetch_node_no_thread(true);
      plan->set_start_node(send_node);
      break;
    }
    case STMT_SHOW_INDEX: {
      DataSpace *meta = Backend::instance()->get_metadata_data_space();
      if (!meta) {
        throw Error("fail to get metadata dataspace");
      }
      assemble_direct_exec_plan(plan, meta);
      break;
    }
    case STMT_SHOW_FIELDS:
    case STMT_SHOW_CREATE_TABLE: {
      DataSpace *dataspace = Backend::instance()->get_metadata_data_space();
      if (!dataspace) {
        dataspace = par_space->get_partition(0);
      }
      assemble_one_partition_plan(plan, dataspace);
      break;
    }
    case STMT_DBSCALE_SHOW_PARTITIONS: {
      assemble_dbscale_show_partition_plan(plan);
      break;
    }
    case STMT_REPLACE_SELECT:
    case STMT_INSERT_SELECT: {
      LOG_ERROR(
          "Unsupport INSERT SELECT no table, please use plain INSERT statement "
          "instead, the sql is [%s].\n",
          sql);
      throw UnSupportPartitionSQL(
          "Unsupport INSERT SELECT no table, please use plain INSERT statement "
          "instead");
    } break;
    default:
      LOG_ERROR("Unsupport one partition sql type, the sql is [%s].\n", sql);
      throw UnSupportPartitionSQL("Unsupport one partition sql type");
      break;
  }
}

SPExecuteFlag Statement::check_procedure_execution_flag(
    const char *sp_schema, const char *cur_schema, Statement *stmt,
    DataSpace **data_space) {
  stmt_node *sp_stmt = stmt->get_stmt_node();
  *data_space = NULL;
  Backend *backend = Backend::instance();
  DataSpace *schema_space =
      Backend::instance()->get_data_space_for_table(sp_schema, NULL);
  DataSpace *tmp_space = NULL;
  name_item *modify_schema_list = sp_stmt->modify_schema_list;
  name_item *tmp = modify_schema_list;
  const char *tmp_schema = NULL;

  DataSpace *standard_space = NULL;

  /*For modify schema, such as drop_db, it must be exec on the same server of
   * schema space of store procedure.*/
  if (tmp) {
    if (!tmp->name) {
#ifdef DEBUG
      ACE_ASSERT(0);
#endif
    }
    standard_space = backend->get_data_space_for_table(tmp->name, NULL);
    tmp = tmp->next;
    while (tmp != modify_schema_list) {
      tmp_schema = tmp->name;
      if (!tmp_schema) {
#ifdef DEBUG
        ACE_ASSERT(0);
#endif
        continue;
      }
      tmp_space = backend->get_data_space_for_table(tmp_schema, NULL);
      if (!is_share_same_server(standard_space, tmp_space))
        return EXECUTE_ON_DBSCALE;
      tmp = tmp->next;
    }
  }

  /*For used tables, if it will be modify, it also must be exec on the same
   * server of schema space of store procedure. */

  table_link *table = sp_stmt->table_list_head;
  const char *schema_name = NULL;
  const char *table_name = NULL;

  while (table) {
    const char *tmp_schema_name = sp_schema ? sp_schema : cur_schema;
    schema_name =
        table->join->schema_name ? table->join->schema_name : tmp_schema_name;
    table_name = table->join->table_name;

    tmp_space = backend->get_data_space_for_table(schema_name, table_name);
    if (!(tmp_space->get_data_source())) {
      return EXECUTE_ON_DBSCALE;
    }

    if (!standard_space) {
      standard_space = tmp_space;
      continue;
    }

    if (table->join->is_modify_table) {
      // for modify table, must be the same server
      if (!is_share_same_server(standard_space, tmp_space))
        return EXECUTE_ON_DBSCALE;
      standard_space = tmp_space;
    } else {
      // for non-modify table, dataspace cover is enough
      if (!stmt_session->is_dataspace_cover_session_level(standard_space,
                                                          tmp_space))
        return EXECUTE_ON_DBSCALE;
    }
    table = table->next;
  }

  if (standard_space) {
    if (!is_share_same_server(standard_space, schema_space)) {
      *data_space = standard_space;
      return EXECUTE_ON_ONE_DATASPACE;
    }
  }
  *data_space = schema_space;
  return EXECUTE_ON_SCHEMA_DATASPACE;
}

bool Statement::recursive_check_call_stmt_dataspace_info(
    Statement *stmt, const char *top_sp_schema, const char *schema_name,
    bool &has_partial_parsed, bool &has_non_single_node_stmt,
    SPExecuteFlag &sp_exe_flag, bool &stored_procedure_contain_cursor,
    DataSpace **data_space, string &sp_name) {
  bool ret = true;
#ifdef DEBUG
  LOG_DEBUG("in Statement::recursive_check_call_stmt_dataspace_info()\n");
#endif
  *data_space = NULL;
  sp_exe_flag = (SPExecuteFlag)check_procedure_execution_flag(
      top_sp_schema, schema_name, stmt, data_space);
  if (sp_exe_flag == EXECUTE_ON_DBSCALE) ret = false;

  list<Statement *> stmt_list_with_sub_call_stmt;
  list<string> stmt_sql_list;  // to avoid tmp const char* freed during re_parse
  set<string> sp_name_set;

  size_t pos = sp_name.find(DBSCALE_RESERVED_STR);
  if (pos != string::npos)
    sp_name_set.insert(string(sp_name, 0, pos));
  else
    sp_name_set.insert(sp_name);

  if (stmt->get_stmt_node()->r_call_stmt_list)
    stmt_list_with_sub_call_stmt.push_back(stmt);

  while (!stmt_list_with_sub_call_stmt.empty()) {
    Statement *curr_stmt = stmt_list_with_sub_call_stmt.front();
    recursive_call_stmt_item *call_stmt_list =
        curr_stmt->get_stmt_node()->r_call_stmt_list;
    stmt_list_with_sub_call_stmt.pop_front();

    while (call_stmt_list) {
      routine_simple_stmt *call_stmt = call_stmt_list->call_stmt;
      size_t start_pos = call_stmt->start_pos;
      size_t end_pos = call_stmt->end_pos;
      string sub_call_sql = string(curr_stmt->get_sql() + (start_pos - 1),
                                   end_pos - (start_pos - 1));
      LOG_DEBUG(
          "Statement::recursive_check_call_stmt_dataspace_info() check sub "
          "call stored procedure [%s]\n",
          sub_call_sql.c_str());
      record_scan *tmp_rs_of_sub_call = NULL;
      Statement *tmp_stmt_of_sub_call = NULL;
      re_parser_stmt(&tmp_rs_of_sub_call, &tmp_stmt_of_sub_call,
                     sub_call_sql.c_str());
      const char *sp_schema_name =
          tmp_stmt_of_sub_call->get_stmt_node()->sql->call_oper->db_name
              ? tmp_stmt_of_sub_call->get_stmt_node()->sql->call_oper->db_name
              : top_sp_schema;
      const char *procedure_name =
          tmp_stmt_of_sub_call->get_stmt_node()->sql->call_oper->procedure_name;
      DataSpace *dataspace =
          Backend::instance()->get_data_space_for_table(top_sp_schema, NULL);
      string sub_sp_name(sp_schema_name);
      sub_sp_name.append(".");
      sub_sp_name.append(procedure_name);

      string tmp_sp_name;
      size_t pos = sub_sp_name.find(DBSCALE_RESERVED_STR);
      if (pos != string::npos)
        tmp_sp_name = string(sub_sp_name, 0, pos);
      else
        tmp_sp_name = sub_sp_name;

      if (sp_name_set.count(tmp_sp_name)) {
        call_stmt_list = call_stmt_list->next;
        continue;
      }
      sp_name_set.insert(tmp_sp_name);

      string sp_name_with_enclose("`");
      sp_name_with_enclose.append(sp_schema_name);
      sp_name_with_enclose.append("`.`");
      sp_name_with_enclose.append(procedure_name);
      sp_name_with_enclose.append("`");

      ProcedureMetaData *meta_data =
          Backend::instance()->get_procedure_meta_data(sub_sp_name);
      const char *create_sp_sql = NULL;
      vector<string> sp_vec;
      if (meta_data) {
        if (meta_data->execution_flag == EXECUTE_ON_DBSCALE) {
          sp_exe_flag = EXECUTE_ON_DBSCALE;
          ret = false;
        }
        create_sp_sql = meta_data->procedure_string.c_str();
      } else {
        try {
          fetch_create_procedure_sql(sp_name_with_enclose.c_str(), dataspace,
                                     &sp_vec);
        } catch (...) {
          LOG_ERROR(
              "Statement::recursive_check_call_stmt_dataspace_info() fail to "
              "fetch create sql of stored procedure [%s]\n",
              sp_name_with_enclose.c_str());
          return false;
        }
        if (!sp_vec.empty()) {
          create_sp_sql = sp_vec[0].c_str();
          LOG_DEBUG(
              "Statement::recursive_check_call_stmt_dataspace_info() get "
              "create procedure sql [%s]\n",
              create_sp_sql);
        } else {
          LOG_ERROR(
              "Statement::recursive_check_call_stmt_dataspace_info() fail to "
              "fetch create sql of stored procedure [%s]\n",
              sp_name_with_enclose.c_str());
          return false;
        }
      }

      record_scan *tmp_rs_of_sub_create_sp_sql = NULL;
      Statement *tmp_stmt_of_sub_create_sp_sql = NULL;
      stmt_sql_list.push_back(create_sp_sql);
      create_sp_sql = stmt_sql_list.back().c_str();
      re_parser_stmt(&tmp_rs_of_sub_create_sp_sql,
                     &tmp_stmt_of_sub_create_sp_sql, create_sp_sql);
      if (tmp_stmt_of_sub_create_sp_sql->is_partial_parsed()) {
        has_partial_parsed = true;
        ret = false;
      }
      if (tmp_stmt_of_sub_create_sp_sql->get_stmt_node()
              ->stored_procedure_contain_cursor) {
        stored_procedure_contain_cursor = true;
      }
      if (tmp_stmt_of_sub_create_sp_sql->get_stmt_node()
              ->has_non_single_node_stmt) {
        has_non_single_node_stmt = true;
        ret = false;
      }

      DataSpace *data_space_tmp;
      DataSpace **p_data_space = &data_space_tmp;
      sp_exe_flag = check_procedure_execution_flag(
          sp_schema_name, schema_name, tmp_stmt_of_sub_create_sp_sql,
          p_data_space);
      if (sp_exe_flag == EXECUTE_ON_DBSCALE)
        ret = false;
      else if (!(*data_space))
        *data_space = data_space_tmp;
      else {
        if (!is_share_same_server(*data_space, data_space_tmp)) ret = false;
      }

      if (tmp_stmt_of_sub_create_sp_sql->get_stmt_node()->r_call_stmt_list)
        stmt_list_with_sub_call_stmt.push_back(tmp_stmt_of_sub_create_sp_sql);

      call_stmt_list = call_stmt_list->next;
    }
  }
  return ret;
}

void Statement::init_sp_worker_from_create_sql(
    SPWorker *spworker, ExecutePlan *plan, const char *create_sql,
    const char *routine_schema, const char *cur_schema, string &sp_name) {
  try {
    /*Here we need to reparse the create procedure sql to store the
     * parsing result stmt_node into the spworker, which will be kept
     * and used in the handling of call statement later.*/
    Parser *parser = Driver::get_driver()->get_parser();
    Statement *new_stmt = parser->parse(create_sql, stmt_allow_dot_in_ident,
                                        true, NULL, NULL, NULL, ctype);
    const char *tmp_schema =
        routine_schema ? routine_schema : plan->session->get_schema();
    new_stmt->set_default_schema(tmp_schema);
    new_stmt->set_session(stmt_session);
    spworker->set_statement(new_stmt);

    DataSpace *data_space = NULL;
    SPExecuteFlag sp_exe_flag = EXECUTE_ON_SCHEMA_DATASPACE;

    bool has_partial_parsed = new_stmt->is_partial_parsed();
    bool has_non_single_node_stmt =
        new_stmt->get_stmt_node()->has_non_single_node_stmt;
    bool stored_procedure_contain_cursor =
        new_stmt->get_stmt_node()->stored_procedure_contain_cursor;
    string tmp_create_sql = string(create_sql);
    boost::to_lower(tmp_create_sql);
    bool has_sequence_keyword =
        new_stmt->get_stmt_node()->seq_items_head ? true : false;
    bool ret = false;
    if (!has_partial_parsed && !has_non_single_node_stmt &&
        !has_sequence_keyword) {
      ret = recursive_check_call_stmt_dataspace_info(
          new_stmt, routine_schema, cur_schema, has_partial_parsed,
          has_non_single_node_stmt, sp_exe_flag,
          stored_procedure_contain_cursor, &data_space, sp_name);
    }
    if (data_space && data_space->get_data_source() &&
        data_space->get_data_source()->get_data_source_type() ==
            DATASOURCE_TYPE_ODBC)
      sp_exe_flag = EXECUTE_ON_DBSCALE;
    else if (has_sequence_keyword)
      sp_exe_flag = EXECUTE_ON_DBSCALE;
    else if (has_partial_parsed)
      sp_exe_flag = EXECUTE_ON_SCHEMA_DATASPACE;
    else if (!ret || has_non_single_node_stmt)
      sp_exe_flag = EXECUTE_ON_DBSCALE;

    if (sp_exe_flag == EXECUTE_ON_DBSCALE && stored_procedure_contain_cursor &&
        plan->session->check_for_transaction()) {
      if (plan->session->get_session_option("cursor_use_free_conn").int_val ==
          0) {
        LOG_ERROR(
            "can not call stored procedure with cursor when current session is "
            "in transaction.\n");
        throw NotSupportedError(
            "not support call stored procedure with cursor when current "
            "session is in transaction");
      }
    }

    spworker->set_execution_flag(sp_exe_flag);

    /*There is no need to do the spworker init for the situation that
     * the sp can be exec on one server.*/
    if (spworker->get_execution_flag() == EXECUTE_ON_DBSCALE) {
      LOG_DEBUG(
          "SP %s can not be exec on one server after analiyzing create sql.\n",
          spworker->get_name());
      routine_node *new_routine_d = new_stmt->get_stmt_node()->routine_d;
      spworker->init_sp_worker_by_routine_node(new_routine_d);
    } else {
      LOG_DEBUG(
          "SP %s can be exec on one server after analiyzing create sql.\n",
          spworker->get_name());
      spworker->set_data_space(data_space);
    }

  } catch (...) {
    if (spworker) {
      spworker->clean_sp_worker_for_free();
      delete spworker;
      LOG_ERROR("Get exception for the create and init of spworker.\n");
      throw;
    }
  }
}

void check_dbscale_management_acl(ExecutePlan *plan,
                                  bool is_dbscle_read_only_user = false) {
  if (is_dbscle_read_only_user) return;
  if (strcmp(supreme_admin_user, plan->session->get_username()) != 0 &&
      strcmp(normal_admin_user, plan->session->get_username()) != 0 &&
      strcmp(dbscale_internal_user, plan->session->get_username()) != 0) {
    LOG_ERROR(
        "Current User does not have the privilege to execute current command,"
        " only root/dbscale/dbscale_internal_user can.\n");
    throw Error(
        "Current User does not have the privilege to execute current command,"
        " only root/dbscale/dbscale_internal_user can.");
  }
}

void Statement::generate_execution_plan_with_no_table(ExecutePlan *plan) {
  DataSpace *dataspace = NULL;
  Backend *backend = Backend::instance();
  bool is_dbscale_read_only_user =
      !strcmp(dbscale_read_only_user, plan->session->get_username());

  switch (st.type) {
    case STMT_SHOW_TABLES: {
      assemble_show_tables_plan(plan);
    } break;
    case STMT_SHOW_DATABASES:
      assemble_show_databases_plan(plan);
      break;
    case STMT_SHOW_CREATE_EVENT: {
      assemble_show_create_event_plan(plan);
      break;
    }
    case STMT_SHOW_EVENTS: {
      assemble_show_events_plan(plan);
      break;
    }
    case STMT_SHOW_ENGINES:
    case STMT_SHOW_PLUGINS: {
      dataspace = backend->get_metadata_data_space();
      if (!dataspace) dataspace = backend->get_catalog();
      assemble_direct_exec_plan(plan, dataspace);

      break;
    }
    case STMT_PLUGIN: {
      throw NotSupportedError("Not support plugin related stmt.");
      break;
    }
    case STMT_LOCK_TB: {
      assemble_lock_tables_plan(plan);
      break;
    }
    case STMT_CREATE_TRIGGER: {
      assemble_create_trigger_plan(plan);
      break;
    }
    case STMT_DROP_TRIGGER: {
      assemble_create_trigger_plan(plan);
      break;
    }
    case STMT_DBSCALE_SHUTDOWN: {
      if (strcmp(supreme_admin_user, plan->session->get_username()) != 0 &&
          strcmp(normal_admin_user, plan->session->get_username()) != 0 &&
          strcmp(dbscale_internal_user, plan->session->get_username()) != 0) {
        LOG_ERROR("Current User does not have the privilege to SHUTDOWN.\n");
        throw Error("Current User does not have the privilege to SHUTDOWN");
      }

#ifndef CLOSE_MULTIPLE
      int cluster_id = st.sql->dbscale_shutdown_oper->cluster_id;
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      } else {
        LOG_ERROR(
            "'DBSCALE SHUTDOWN cluster_id' is only supported under Multiple "
            "DBScale mode.\n");
        throw NotSupportedError(
            "'DBSCALE SHUTDOWN cluster_id' is only supported under Multiple "
            "DBScale mode.");
      }

      if (cluster_id == ERROR_CLUSTER_ID) {
        LOG_ERROR("Cluster_id cannot be 0.\n");
        throw Error("Cluster_id cannot be 0");
      }
      // just send SHUTDOWN command when pointed cluster_id is current dbscale's
      // cluster_id
      if (cluster_id == Backend::instance()->get_cluster_id())
        return assemble_dbscale_shutdown_plan(plan);

      // else, always use master dbscale to do SHUTDOWN
      if (is_master) {
        assemble_dbscale_shutdown_plan(plan, cluster_id);
      } else {
        LOG_DEBUG("dynamic add dataspace cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#else
      LOG_ERROR(
          "'DBSCALE SHUTDOWN cluster_id' is only supported under Multiple "
          "DBScale mode.\n");
      throw NotSupportedError(
          "'DBSCALE SHUTDOWN cluster_id' is only supported under Multiple "
          "DBScale mode.");
#endif

    } break;
    case STMT_DBSCALE_CLEAN_TEMPORARY_TABLE_CACHE: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master || st.sql->clean_temp_table_cache_oper->is_internal) {
#endif
        assemble_dbscale_clean_temp_table_cache(plan);
#ifndef CLOSE_MULTIPLE
      } else {
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
    } break;
    case STMT_XA_START_TRANSACTION:
    case STMT_XA_END_TRANSACTION:
    case STMT_XA_PREPARE_TRANSACTION:
    case STMT_XA_COMMIT_TRANSACTION:
    case STMT_XA_ROLLBACK_TRANSACTION:
    case STMT_XA_COMMIT_ONE_PHASE_TRANSACTION:
    case STMT_XA_RECOVER:
      if (!enable_cluster_xa_transaction) {
        LOG_ERROR(
            "enable_cluster_xa_transaction is off, cann't execute xa stmt.\n");
        throw Error(
            "enable_cluster_xa_transaction is off, cann't execute xa stmt.");
      }
      if (!backend->is_centralized_cluster()) {
        LOG_ERROR("is not centralized group, cann't execute xa stmt.\n");
        throw Error("is not centralized group, cann't execute xa stmt.");
      }
      assemble_cluster_xatransaction_plan(plan);
      break;
    case STMT_DBSCALE_REQUEST_CLUSTER_INC_INFO:
      assemble_dbscale_request_cluster_inc_info_plan(plan);
      break;
    case STMT_DBSCALE_REQUEST_ALL_CLUSTER_INC_INFO:
      assemble_dbscale_request_all_cluster_inc_info_plan(plan);
      break;
    case STMT_DBSCALE_REQUEST_CLUSTER_ID:
      check_dbscale_management_acl(plan, is_dbscale_read_only_user);
      assemble_dbscale_request_cluster_id_plan(plan);
      break;
    case STMT_DBSCALE_REQUEST_NODE_INFO:
      assemble_dbscale_request_node_info_plan(plan);
      break;
    case STMT_DBSCALE_REQUEST_CLUSTER_INFO:
      assemble_dbscale_request_cluster_info_plan(plan);
      break;
    case STMT_DBSCALE_SHOW_ASYNC_TASK: {
      if (strcmp(dbscale_read_only_user, plan->session->get_username())) {
        if (strcmp(supreme_admin_user, plan->session->get_username()) &&
            strcmp(normal_admin_user, plan->session->get_username()) &&
            strcmp(dbscale_internal_user, plan->session->get_username())) {
          LOG_ERROR("Refuse user [%s] to execute dbscale show async task.\n",
                    plan->session->get_username());
          throw Error(
              "Only 'root' or 'dbscale' allowed to do async task management.");
        }
      }
      assemble_dbscale_show_async_task_plan(plan);
      break;
    }
    case STMT_DBSCALE_SUSPEND_ASYNC_TASK:
    case STMT_DBSCALE_CONTINUE_ASYNC_TASK:
    case STMT_DBSCALE_CANCEL_ASYNC_TASK:
    case STMT_DBSCALE_DELETE_ASYNC_TASK: {
      if (strcmp(supreme_admin_user, plan->session->get_username()) &&
          strcmp(normal_admin_user, plan->session->get_username()) &&
          strcmp(dbscale_internal_user, plan->session->get_username())) {
        LOG_ERROR("Refuse user [%s] to manage dbscale async task.\n",
                  plan->session->get_username());
        throw Error(
            "Only 'root' or 'dbscale' allowed to do async task management.");
      }

      unsigned long long id = st.sql->dbscale_async_task_oper->id;
      assemble_async_task_control_plan(plan, id);
      break;
    }
    case STMT_DBSCALE_LOAD_DATASPACE_CONFIG_FILE: {
      check_dbscale_management_acl(plan);
      const char *filename = st.sql->load_config_oper->filename;
      assemble_load_dataspace_config_file_plan(plan, filename);
      break;
    }
    case STMT_DBSCALE_REQUEST_USER_STATUS: {
      check_dbscale_management_acl(plan, is_dbscale_read_only_user);
      const char *id = st.sql->show_user_status_oper->user_id;
      bool only_show_running = st.sql->show_user_status_oper->only_show_running;
      bool show_staus_count = st.sql->show_user_status_oper->show_status_count;
      assemble_dbscale_request_cluster_user_status_plan(
          plan, id, only_show_running, show_staus_count);
      break;
    }
    case STMT_SHOW_TABLE_STATUS:
      assemble_show_table_status_plan(plan);
      break;
    case STMT_ROLLBACK: {
      if (!enable_xa_transaction ||
          plan->session->get_session_option("close_session_xa").int_val != 0) {
        RollbackType rollback_type = st.sql->rollback_point_oper->type;
        /* when session is not in transaction, we just return an error packet */
        if (rollback_type == ROLLBACK_TYPE_TO_POINT &&
            !plan->session->is_in_transaction()) {
          dataspace = backend->get_data_space_for_table(schema, NULL);
          assemble_direct_exec_plan(plan, dataspace);
        } else {
          assemble_transaction_unlock_plan(plan);
        }
      } else {
        if (plan->session->is_in_transaction_consistent()) {
          assemble_transaction_unlock_plan(plan);
        } else {
          assemble_xatransaction_plan(plan);
        }
      }
    } break;
    case STMT_COMMIT:
    case STMT_START_TRANSACTION: {
      if (!st.sql->start_tran_oper ||
          !st.sql->start_tran_oper->with_consistence) {
        if (enable_xa_transaction &&
            plan->session->get_session_option("close_session_xa").int_val ==
                0 &&
            !plan->session->is_in_transaction_consistent()) {
          assemble_xatransaction_plan(plan);
        } else {
          assemble_transaction_unlock_plan(plan);
        }
      } else {
        map<DataSpace *, const char *> spaces_map;
        Backend *backend = Backend::instance();
        plan->session->set_in_transaction_consistent(true);

        list<DataSpace *> spaces =
            backend->get_config_machine_space_for_server_level_execute();
        list<DataSpace *>::iterator it = spaces.begin();
        for (; it != spaces.end(); ++it) {
          spaces_map[*it] = sql;
        }
        ExecuteNode *node = plan->get_mul_modify_node(spaces_map, false);
        plan->set_start_node(node);
      }
      break;
    }
    case STMT_UNLOCK_TB:
    case STMT_SAVEPOINT:
      assemble_transaction_unlock_plan(plan);
      break;
    case STMT_SHUTDOWN: {
      if (strcmp(supreme_admin_user, plan->session->get_username()) != 0) {
        LOG_ERROR("Current User does not have the privilege to SHUTDOWN.\n");
        throw Error("Current User does not have the privilege to SHUTDOWN");
      }

      assemble_dbscale_shutdown_plan(plan);
    } break;
    case STMT_FLUSH_PRIVILEGES: {
      dataspace = backend->get_auth_data_space();
      assemble_direct_exec_plan(plan, dataspace);
    } break;
    case STMT_FLUSH_TABLE_WITH_READ_LOCK: {
      map<DataSpace *, const char *> spaces_map;
      Backend *backend = Backend::instance();

      list<DataSpace *> spaces =
          backend->get_config_machine_space_for_server_level_execute();
      list<DataSpace *>::iterator it = spaces.begin();
      for (; it != spaces.end(); ++it) {
        spaces_map[*it] = sql;
      }
      ExecuteNode *node = plan->get_mul_modify_node(spaces_map, false);
      plan->set_start_node(node);
    } break;
    case STMT_DBSCALE_SHOW_INNODB_LOCK_WAITING: {
      assemble_show_engine_lock_waiting_status_plan(plan, INNODB);
    } break;
    case STMT_DBSCALE_SHOW_TOKUDB_LOCK_WAITING: {
      assemble_show_engine_lock_waiting_status_plan(plan, TOKUDB);
    } break;
    case STMT_DBSCALE_SHOW_DEFAULT_SESSION_VARIABLES: {
      assemble_dbscale_show_default_session_variables(plan);
    } break;
    case STMT_DBSCALE_SHOW_VERSION: {
      assemble_dbscale_show_version_plan(plan);
    } break;
    case STMT_SHOW_VERSION: {
      if (Backend::instance()->is_grid())
        assemble_show_version_plan(plan);
      else
        assemble_dbscale_show_version_plan(plan);
    } break;
    case STMT_SHOW_COMPONENTS_VERSION: {
      assemble_show_components_version_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_HOSTNAME: {
      assemble_dbscale_show_hostname_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_ALL_FAIL_TRANSACTION: {
      assemble_dbscale_show_all_fail_transaction_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_DETAIL_FAIL_TRANSACTION: {
      const char *xid = st.sql->show_detail_fail_transaction_oper->xid;
      assemble_dbscale_show_detail_fail_transaction_plan(plan, xid);
    } break;
    case STMT_DBSCALE_SHOW_PARTITION_TABLE_STATUS: {
      const char *table_name =
          st.sql->show_partition_table_status_oper->table_name;
      assemble_dbscale_show_partition_table_status_plan(plan, table_name);
    } break;
    case STMT_DBSCALE_SHOW_SCHEMA_ACL_INFO: {
      if (!enable_acl) {
        LOG_ERROR("enable-acl=0, no schema ACL info to show\n");
        throw Error("enable-acl=0, no schema ACL info to show");
      }
      bool is_show_all = st.sql->show_schema_acl_info_oper->show_all;
      if (is_show_all &&
          strcmp(supreme_admin_user, plan->session->get_username()) != 0) {
        LOG_ERROR(
            "Current User does not have the privilege to show ALL schema "
            "ACL.\n");
        throw Error(
            "Current User does not have the privilege to show ALL schema ACL");
      }
      assemble_dbscale_show_schema_acl_info(plan, is_show_all);
    } break;
    case STMT_DBSCALE_SHOW_TABLE_ACL_INFO: {
      if (!enable_acl) {
        LOG_ERROR("enable-acl=0, no table ACL info to show\n");
        throw Error("enable-acl=0, no table ACL info to show");
      }
      bool is_show_all = st.sql->show_table_acl_info_oper->show_all;
      if (is_show_all &&
          strcmp(supreme_admin_user, plan->session->get_username()) != 0) {
        LOG_ERROR(
            "Current User does not have the privilege to show ALL table "
            "ACL.\n");
        throw Error(
            "Current User does not have the privilege to show ALL table ACL");
      }
      assemble_dbscale_show_table_acl_info(plan, is_show_all);
    } break;
    case STMT_DBSCALE_SHOW_PATH_INFO: {
      assemble_dbscale_show_path_info(plan);
    } break;
    case STMT_DBSCALE_SHOW_WARNINGS: {
      assemble_dbscale_show_warnings(plan);
    } break;
    case STMT_DBSCALE_SHOW_CRITICAL_ERRORS: {
      assemble_dbscale_show_critical_errors(plan);
    } break;
    case STMT_DBSCALE_CLEAN_FAIL_TRANSACTION: {
      const char *xid = st.sql->clean_fail_transaction_oper->xid;
      assemble_dbscale_clean_fail_transaction_plan(plan, xid);
    } break;
    case STMT_DBSCALE_SHOW_SHARD_PARTITION_TABLE: {
      const char *scheme_name =
          st.sql->show_shard_partition_tb_oper->scheme_name;
      assemble_dbscale_show_shard_partition_table_plan(plan, scheme_name);
    } break;
    case STMT_DBSCALE_SHOW_REBALANCE_WORK_LOAD: {
      const char *scheme_name =
          st.sql->show_rebalance_work_load_oper->scheme_name;
      const char *schema_name =
          st.sql->show_rebalance_work_load_oper->schema_name;
      name_item *source_head =
          st.sql->show_rebalance_work_load_oper->source_list;
      int is_remove = st.sql->show_rebalance_work_load_oper->is_remove;
      name_item *tmp = source_head;
      list<string> sources;
      while (tmp) {
        sources.push_back(string(tmp->name));
        tmp = tmp->next;
        if (tmp == source_head) break;
      }
      assemble_dbscale_show_rebalance_work_load_plan(plan, scheme_name, sources,
                                                     schema_name, is_remove);
    } break;

    case STMT_DBSCALE_ADD_DEFAULT_SESSION_VARIABLES: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        bool is_master = mul->get_is_cluster_master();
        if (!is_master) {
          LOG_DEBUG("Add default session be execute on slave dbscale\n");
          assemble_forward_master_role_plan(plan);
          break;
        }
      }
#endif
      assemble_dbscale_add_default_session_variables(plan);
    } break;
    case STMT_DBSCALE_REMOVE_DEFAULT_SESSION_VARIABLES: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        bool is_master = mul->get_is_cluster_master();
        if (!is_master) {
          LOG_DEBUG("Remove default session be execute on slave dbscale\n");
          assemble_forward_master_role_plan(plan);
          break;
        }
      }
#endif
      assemble_dbscale_remove_default_session_variables(plan);
    } break;
    case STMT_DBSCALE_SHOW_USER_SQL_COUNT: {
      check_dbscale_management_acl(plan, is_dbscale_read_only_user);
      const char *user_id = st.sql->show_user_sql_count_oper->user_id;
      assemble_dbscale_show_user_sql_count_plan(plan, user_id);
    } break;
    case STMT_DBSCALE_SHOW_DATASOURCE: {
      assemble_show_datasource_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_MIGRATE_CLEAN_TABLES: {
      check_dbscale_management_acl(plan);
      assemble_show_migrate_clean_tables_plan(plan);
    } break;
    case STMT_DBSCALE_MIGRATE_CLEAN: {
      check_dbscale_management_acl(plan);
      assemble_migrate_clean_tables_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_DATASERVER: {
      assemble_show_dataserver_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_BACKEND_THREADS: {
      check_dbscale_management_acl(plan, is_dbscale_read_only_user);
      assemble_show_backend_threads_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_PARTITION_SCHEME: {
      assemble_show_partition_scheme_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_STATUS: {
      assemble_dbscale_show_status_plan(plan);
      break;
    }
    case STMT_DBSCALE_SHOW_CATCHUP: {
      check_dbscale_management_acl(plan);
      const char *source_name = st.sql->show_catchup_oper->source_name;
      assemble_show_catchup_plan(plan, source_name);
      break;
    }
    case STMT_DBSCALE_SKIP_WAIT_CATCHUP: {
      check_dbscale_management_acl(plan);
      const char *source_name = st.sql->skip_wait_catchup_oper->source_name;
      assemble_dbscale_skip_wait_catchup_plan(plan, source_name);
    } break;
    case STMT_DBSCALE_SHOW_VIRTUAL_MAP:
    case STMT_DBSCALE_SHOW_SHARD_MAP: {
      assemble_dbscale_show_virtual_map_plan(plan);
      break;
    }
    case STMT_DBSCALE_SHOW_AUTO_INCREMENT_VALUE: {
      assemble_dbscale_show_auto_increment_info(plan);
      break;
    }
    case STMT_DBSCALE_MUL_SYNC: {
      check_dbscale_management_acl(plan);
      assemble_dbscale_mul_sync_plan(plan);
      break;
    }
    case STMT_DBSCALE_GET_GLOBAL_CONSISTENCE_POINT: {
      check_dbscale_management_acl(plan);
      assemble_dbscale_get_global_consistence_point_plan(plan);
    } break;
    case STMT_DBSCALE_MIGRATE: {
      check_dbscale_management_acl(plan);
      assemble_migrate_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_USER_MEMORY_STATUS: {
      assemble_show_user_memory_status_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_USER_STATUS: {
      check_dbscale_management_acl(plan, is_dbscale_read_only_user);
      const char *user_id = st.sql->show_user_status_oper->user_id;
      const char *user_name = st.sql->show_user_status_oper->user_name;
      bool only_show_running = st.sql->show_user_status_oper->only_show_running;
      bool instance = st.sql->show_user_status_oper->instance;
      bool show_status_count = st.sql->show_user_status_oper->show_status_count;
      assemble_show_user_status_plan(plan, user_id, user_name,
                                     only_show_running, instance,
                                     show_status_count);
    } break;
    case STMT_DBSCALE_SHOW_USER_PROCESSLIST: {
      check_dbscale_management_acl(plan, is_dbscale_read_only_user);
      const char *cluster_id = st.sql->show_user_processlist_oper->cluster_id;
      const char *user_id = st.sql->show_user_processlist_oper->user_id;
      int local = st.sql->show_user_processlist_oper->local;
      assemble_show_user_processlist_plan(plan, cluster_id, user_id, local);
    } break;
    case STMT_DBSCALE_SHOW_SESSION_ID:
    case STMT_DBSCALE_REQUEST_SESSION_ID: {
      check_dbscale_management_acl(plan);
      const char *server_name = st.sql->show_session_id_oper->server_name;
      int connection_id = st.sql->show_session_id_oper->connection_id;
      assemble_show_session_id_plan(plan, server_name, connection_id);
    } break;
    case STMT_DBSCALE_SHOW_SCHEMA: {
      const char *schema_name = st.sql->dbscale_show_schema_oper->schema;
      assemble_show_schema_plan(plan, schema_name);
    } break;
    case STMT_DBSCALE_SHOW_TABLE: {
      const char *schema_name = st.sql->dbscale_show_table_oper->schema_name;
      const char *table_name = st.sql->dbscale_show_table_oper->table_name;
      bool use_like = st.sql->dbscale_show_table_oper->use_like;
      assemble_show_table_plan(plan, schema_name, table_name, use_like);
    } break;
    case STMT_DBSCALE_BACKEND_SERVER_EXECUTE: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (!is_master) {
        LOG_DEBUG("backend_server_execute cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
      const char *stmt_sql = st.sql->backend_server_execute_oper->stmt_sql;
      assemble_backend_server_execute_plan(plan, stmt_sql);
    } break;
    case STMT_EXECUTE_ON_ALL_MASTERSERVER: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (!is_master) {
        LOG_DEBUG(
            "execute_on_all_masterserver_execute cmd be execute on slave "
            "dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
      const char *stmt_sql = st.sql->execute_on_all_masterserver_oper->stmt_sql;
      assemble_execute_on_all_masterserver_execute_plan(plan, stmt_sql);
      break;
    }
    case STMT_DBSCALE_EXECUTE_ON_DATASERVER: {
      if (strcmp(supreme_admin_user, plan->session->get_username()) != 0 &&
          strcmp(normal_admin_user, plan->session->get_username()) != 0 &&
          strcmp(dbscale_internal_user, plan->session->get_username()) != 0) {
        LOG_ERROR(
            "Only root and dbscale is allowed to run "
            "STMT_DBSCALE_EXECUTE_ON_DATASERVER.\n");
        throw Error(
            "Only root and dbscale is allowed to run "
            "STMT_DBSCALE_EXECUTE_ON_DATASERVER");
      }
      const char *dataserver_name =
          st.sql->dbscale_execute_on_dataserver_oper->dataserver_name;
      const char *stmt_sql =
          st.sql->dbscale_execute_on_dataserver_oper->stmt_sql;
      assemble_dbscale_execute_on_dataserver_plan(plan, dataserver_name,
                                                  stmt_sql);
    } break;
    case STMT_DBSCALE_SHOW_TABLE_LOCATION: {
      const char *schema_name = st.table_list_head->join->schema_name
                                    ? st.table_list_head->join->schema_name
                                    : schema;
      const char *table_name = st.table_list_head->join->table_name;
      assemble_show_table_location_plan(plan, schema_name, table_name);
    } break;
    case STMT_DBSCALE_ERASE_AUTH_INFO: {
      check_dbscale_management_acl(plan);
      name_item *username_list =
          st.sql->dbscale_erase_auth_info_oper->username_list;
      bool all_dbscale_node =
          st.sql->dbscale_erase_auth_info_oper->all_dbscale_node;
      LOG_DEBUG("Assemble dbscale erase auth info plan%s.\n",
                all_dbscale_node ? " for all dbscale node" : "");
      ExecuteNode *node = plan->get_dbscale_erase_auth_info_node(
          username_list, all_dbscale_node);
      plan->set_start_node(node);
    } break;
    case STMT_DBSCALE_UPDATE_AUDIT_USER: {
      if (strcmp(supreme_admin_user, plan->session->get_username()) != 0 &&
          strcmp(dbscale_internal_user, plan->session->get_username()) != 0) {
        LOG_ERROR(
            "Current User does not have the privilege of dynamic "
            "configuration.\n");
        throw Error(
            "Current User does not have the privilege of dynamic "
            "configuration.");
      }
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (!is_master) {
        LOG_DEBUG("update audit user cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
      const char *username = st.sql->dbscale_update_audit_user_oper->username;
      bool is_add = st.sql->dbscale_update_audit_user_oper->is_add;
      LOG_DEBUG("assemble_dbscale_update_audit_user plan.\n");
      ExecuteNode *node =
          plan->get_dbscale_update_audit_user_node(username, is_add);
      plan->set_start_node(node);
    } break;
    case STMT_DBSCALE_DYNAMIC_UPDATE_WHITE: {
      if (!enable_acl) {
        LOG_ERROR("enable-acl=0, can not set ACL\n");
        throw Error("enable-acl=0, can not set ACL");
      }
      if (strcmp(supreme_admin_user, plan->session->get_username()) != 0 &&
          strcmp(dbscale_internal_user, plan->session->get_username()) != 0) {
        LOG_ERROR(
            "Current User does not have the privilege of dynamic "
            "configuration.\n");
        throw Error(
            "Current User does not have the privilege of dynamic "
            "configuration.");
      }
      const char *comment = st.sql->dynamic_update_white_oper->comment;
      if (comment && strlen(comment) > 512) {
        throw Error("comment length sould no larger than 512 bytes");
      }
      ssl_option_struct ssl_option_value;
      ssl_option_value.ssl_option =
          st.sql->dynamic_update_white_oper->ssl_option;
      ssl_option_value.ssl_cipher =
          st.sql->dynamic_update_white_oper->ssl_cipher
              ? st.sql->dynamic_update_white_oper->ssl_cipher
              : "";
      ssl_option_value.x509_issuer =
          st.sql->dynamic_update_white_oper->x509_issuer
              ? st.sql->dynamic_update_white_oper->x509_issuer
              : "";
      ssl_option_value.x509_subject =
          st.sql->dynamic_update_white_oper->x509_subject
              ? st.sql->dynamic_update_white_oper->x509_subject
              : "";
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (!is_master) {
        LOG_DEBUG(
            "dynamic update white list cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
      bool is_add = st.sql->dynamic_update_white_oper->is_add;
      const char *user_name = st.sql->dynamic_update_white_oper->user_name;
      const char *ip = st.sql->dynamic_update_white_oper->ip;
      assemble_dbscale_dynamic_update_white_plan(
          plan, is_add, ip, ssl_option_value, comment, user_name);
    } break;
    case STMT_DBSCALE_DYNAMIC_UPDATE_FUCTION_TYPE_MAP: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master || st.sql->dynamic_update_function_type_oper->is_internal) {
#endif
        assemble_dbscale_reload_func_type_plan(plan);
#ifndef CLOSE_MULTIPLE
      } else {
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
    } break;
    case STMT_DBSCALE_DYNAMIC_ADD_DATASERVER: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        assemble_dynamic_add_data_server_plan(
            plan, st.sql->dynamic_add_data_server_oper);
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG("dynamic add server cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
    } break;
    case STMT_DBSCALE_DYNAMIC_ADD_SLAVE: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        assemble_dynamic_add_slave_plan(plan, st.sql->dynamic_add_slave_oper);
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG("dynamic add slave cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
    } break;
    case STMT_DBSCALE_DYNAMIC_ADD_PRE_DISASTER_MASTER: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        assemble_dynamic_add_pre_disaster_master_plan(
            plan, st.sql->dynamic_add_pre_disaster_master_oper);
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG("dynamic add slave cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
    } break;
    case STMT_DBSCALE_DYNAMIC_ADD_DATASOURCE: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        if (st.sql->dynamic_add_data_source_oper->type ==
                DATASOURCE_TYPE_REPLICATION &&
            !st.sql->dynamic_add_data_source_oper->semi_sync &&
            auto_master_failover_flashback) {
          throw Error(
              "For replication datasource, 'SEMI_SYNC' should be used when "
              "'auto-master-failover-flashback' is enabled");
        }
        assemble_dynamic_add_data_source_plan(
            plan, st.sql->dynamic_add_data_source_oper,
            st.sql->dynamic_add_data_source_oper->type);
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG("dynamic add datasource cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
    } break;
    case STMT_DBSCALE_DYNAMIC_ADD_DATASPACE: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        assemble_dynamic_add_data_space_plan(
            plan, st.sql->dynamic_add_data_space_oper);
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG("dynamic add dataspace cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
    } break;
    case STMT_DBSCALE_SET_SCHEMA_ACL: {
      if (!enable_acl) {
        LOG_ERROR("enable-acl=0, can not set ACL\n");
        throw Error("enable-acl=0, can not set ACL");
      }
      if (strcmp(supreme_admin_user, plan->session->get_username()) != 0 &&
          strcmp(dbscale_internal_user, plan->session->get_username()) != 0) {
        LOG_ERROR("Current User does not have the privilege to set ACL.\n");
        throw Error("Current User does not have the privilege to set ACL.");
      }
      const char *username = st.sql->set_schema_acl_oper->username;
      if (!strcmp(username, supreme_admin_user)) {
        LOG_ERROR("can not set schema ACL on target user\n");
        throw Error("can not set schema ACL on target user");
      }
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (!is_master) {
        LOG_DEBUG("dynamic set schema acl cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
      assemble_set_schema_acl_plan(plan, st.sql->set_schema_acl_oper);
    } break;
    case STMT_DBSCALE_RELOAD_USER_ALLOW_OPERATION_TIME: {
      if (!enable_acl) {
        LOG_ERROR("enable-acl=0, can not set ACL\n");
        throw Error("enable-acl=0, can not set ACL");
      }
      if (strcmp(supreme_admin_user, plan->session->get_username()) != 0 &&
          strcmp(dbscale_internal_user, plan->session->get_username()) != 0) {
        LOG_ERROR("Current User does not have the privilege to set ACL.\n");
        throw Error("Current User does not have the privilege to set ACL.");
      }
      string user_name =
          st.sql->set_user_not_allow_operation_time_oper->user_name;
      assemble_reload_user_not_allow_operation_time_plan(plan, user_name);
    } break;
    case STMT_DBSCALE_SHOW_USER_NOT_ALLOW_OPERATION_TIME: {
      if (!enable_acl) {
        LOG_ERROR("enable-acl=0, no ACL info to show\n");
        throw Error("enable-acl=0, no ACL info to show");
      }
      const char *user_name_str =
          st.sql->set_user_not_allow_operation_time_oper->user_name;
      string user_name = user_name_str ? string(user_name_str) : string("");
      if (!is_dbscale_read_only_user &&
          strcmp(supreme_admin_user, plan->session->get_username()) != 0 &&
          strcmp(dbscale_internal_user, plan->session->get_username()) != 0) {
        if (!user_name.empty() &&
            strcmp(user_name.c_str(), plan->session->get_username()) == 0) {
        } else {
          LOG_ERROR("Current User does not have the privilege to show ACL.\n");
          throw Error("Current User does not have the privilege to show ACL.");
        }
      }
      assemble_show_user_not_allow_operation_time_plan(plan, user_name);
    } break;
    case STMT_DBSCALE_SET_USER_ALLOW_OPERATION_TIME: {
      if (!enable_acl) {
        LOG_ERROR("enable-acl=0, can not set ACL\n");
        throw Error("enable-acl=0, can not set ACL");
      }
      if (strcmp(supreme_admin_user, plan->session->get_username()) != 0 &&
          strcmp(dbscale_internal_user, plan->session->get_username()) != 0) {
        LOG_ERROR("Current User does not have the privilege to set ACL.\n");
        throw Error("Current User does not have the privilege to set ACL.");
      }
      const char *username =
          st.sql->set_user_not_allow_operation_time_oper->user_name;
      if (!strcmp(username, supreme_admin_user) ||
          !strcmp(username, dbscale_internal_user)) {
        LOG_ERROR("can not set table ACL on user root or dbscale_internal\n");
        throw Error("can not set table ACL on user root or dbscale_internal");
      }
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (!is_master) {
        LOG_DEBUG("dynamic set table acl cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
      assemble_set_user_not_allow_operation_time_plan(
          plan, st.sql->set_user_not_allow_operation_time_oper);
    } break;
    case STMT_DBSCALE_SET_TABLE_ACL: {
      if (!enable_acl) {
        LOG_ERROR("enable-acl=0, can not set ACL\n");
        throw Error("enable-acl=0, can not set ACL");
      }
      if (strcmp(supreme_admin_user, plan->session->get_username()) != 0 &&
          strcmp(dbscale_internal_user, plan->session->get_username()) != 0) {
        LOG_ERROR("Current User does not have the privilege to set ACL.\n");
        throw Error("Current User does not have the privilege to set ACL.");
      }
      const char *username = st.sql->set_table_acl_oper->user_name;
      if (!strcmp(username, supreme_admin_user) ||
          !strcmp(username, dbscale_internal_user)) {
        LOG_ERROR("can not set table ACL on target user\n");
        throw Error("can not set table ACL on target user");
      }
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (!is_master) {
        LOG_DEBUG("dynamic set table acl cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
      assemble_set_table_acl_plan(plan, st.sql->set_table_acl_oper);
    } break;
    case STMT_DBSCALE_SET_SCHEMA_PUSHDOWN_PROCEDURE: {
      check_dbscale_management_acl(plan);
      int pushdown_value = st.sql->set_schema_pushdown_proc_oper->pushdown_proc;
      const char *schema_name = st.sql->set_schema_pushdown_proc_oper->schema;
      DataSpace *s = backend->get_data_space_for_table(schema_name, NULL);
      if (s == backend->get_catalog()) {
        LOG_ERROR(
            "The schema [%s] is not configed, and refuse to adjust the "
            "pushdown_procedure behavior.\n",
            schema_name);
        throw Error(
            "The schema is not configed, and refuse to adjust the "
            "pushdown_procedure behavior.");
      }
      Schema *schema = (Schema *)s;

      bool is_master = true;
#ifndef CLOSE_MULTIPLE
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
#endif
      if (!st.sql->set_schema_pushdown_proc_oper->is_real) {
#ifndef CLOSE_MULTIPLE
        if (!is_master) {
          LOG_DEBUG("dynamic set table acl cmd be execute on slave dbscale\n");
          assemble_forward_master_role_plan(plan);
          break;
        } else {
          if (multiple_mode)
            MultipleManager::instance()->adjust_schema_pushdown(
                string(schema_name), pushdown_value);
        }
#endif
      }
      int old_val = schema->get_schema_pushdown_sp_config_value();
      schema->set_schema_pushdown_stored_procedure(pushdown_value);
      if (is_master) {
        list<string> update_list;
        update_list.push_back(
            Driver::get_driver()->get_config_helper()->generate_schema_config(
                schema));
        bool ret = Driver::get_driver()->get_config_helper()->update_config(
            update_list);
        if (!ret) {
          schema->set_schema_pushdown_stored_procedure(old_val);
          throw Error("got error when update schema config in config source");
        }
#ifndef CLOSE_MULTIPLE
        if (multiple_mode) {
          Backend::instance()->flush_config_to_zoo();
        }
#endif
      }
      ExecuteNode *node = plan->get_return_ok_node();
      plan->set_start_node(node);
      break;
    }
    case STMT_DBSCALE_DYNAMIC_SET_MASTER_PRIORITY: {
      if (!strcmp(normal_admin_user, plan->session->get_username()) == 0 &&
          !strcmp(supreme_admin_user, plan->session->get_username()) == 0 &&
          !strcmp(dbscale_internal_user, plan->session->get_username()) == 0) {
        LOG_ERROR(
            "Current User does not have the priority of dynamic set master "
            "priority.\n");
        throw Error(
            "Current User does not have the priority of dynamic set master "
            "priority.");
      }

      if (multiple_mode) {
        if (st.sql->dbscale_set_master_priority_oper->is_local) {
          Backend::instance()->set_server_master_priority(
              st.sql->dbscale_set_master_priority_oper->server_name,
              st.sql->dbscale_set_master_priority_oper->master_priority);
        } else {
          MultipleManager::instance()->set_master_priority(
              string(st.sql->dbscale_set_master_priority_oper->server_name),
              st.sql->dbscale_set_master_priority_oper->master_priority);
        }
      } else
        Backend::instance()->set_server_master_priority(
            st.sql->dbscale_set_master_priority_oper->server_name,
            st.sql->dbscale_set_master_priority_oper->master_priority);

      ExecuteNode *node = plan->get_return_ok_node();
      plan->set_start_node(node);
      break;
    }
    case STMT_DBSCALE_RELOAD_MIRROR_USER: {
      check_dbscale_management_acl(plan);
      Backend::instance()->reload_mirror_user();
      ExecuteNode *node = plan->get_return_ok_node();
      plan->set_start_node(node);
      break;
    }
    case STMT_DBSCALE_RESET_TMP_TABLE: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      dbscale_reset_tmp_table_op_node *reset_oper =
          st.sql->dbscale_reset_tmp_table_oper;
      int type = reset_oper->type;
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      // type == 2 is used just for dbscale internal, the command type that
      // slave recieved from master is 2.
      if (is_master || type == RESET_SLAVE ||
          (type == RESET_DROP && reset_oper->table_pool_name)) {
#endif
        assemble_dbscale_reset_tmp_table_plan(plan);
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG("reset tmp join tables cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
    } break;
    case STMT_DBSCALE_RESET_ZOO_INFO: {
      if (strcmp(supreme_admin_user, plan->session->get_username()) == 0) {
        assenble_dbscale_reset_zoo_info_plan(plan);
      } else {
        LOG_ERROR(
            "Current User does not have the privilege of zookeeper reset "
            "info.\n");
        throw Error(
            "Current User does not have the privilege of zookeeper reset "
            "info.");
      }
    } break;
    case STMT_SET_INFO_SCHEMA_MIRROR_TB_STATUS: {
      if (strcmp(supreme_admin_user, plan->session->get_username()) != 0 &&
          strcmp(normal_admin_user, plan->session->get_username()) != 0 &&
          strcmp(dbscale_internal_user, plan->session->get_username()) != 0) {
        LOG_ERROR(
            "Current User does not have the privilege to execute "
            "STMT_SET_INFO_SCHEMA_MIRROR_TB_STATUS, only "
            "root/dbscale/dbscale_internal_user can.\n");
        throw Error(
            "Current User does not have the privilege to execute "
            "STMT_SET_INFO_SCHEMA_MIRROR_TB_STATUS, only "
            "root/dbscale/dbscale_internal_user can.");
      }
      LOG_DEBUG("Assemble dbscale set_info_mirror_tb_status_plan.\n");
      info_mirror_tb_status_node *tb_status =
          st.sql->info_mirror_tb_status_oper;
      ExecuteNode *node =
          plan->get_set_info_schema_mirror_tb_status_node(tb_status);
      plan->set_start_node(node);
      return;
    } break;
    case STMT_SET_INFO_SCHEMA_MIRROR_TB_STATUS_OUT_OF_DATE: {
      if (strcmp(supreme_admin_user, plan->session->get_username()) != 0 &&
          strcmp(normal_admin_user, plan->session->get_username()) != 0 &&
          strcmp(dbscale_internal_user, plan->session->get_username()) != 0) {
        LOG_ERROR(
            "Current User does not have the privilege to execute "
            "STMT_SET_INFO_SCHEMA_MIRROR_TB_STATUS_OUT_OF_DATE,"
            " only root/dbscale/dbscale_internal_user can.\n");
        throw Error(
            "Current User does not have the privilege to execute "
            "STMT_SET_INFO_SCHEMA_MIRROR_TB_STATUS_OUT_OF_DATE,"
            " only root/dbscale/dbscale_internal_user can.");
      }
      LOG_DEBUG(
          "Assemble dbscale set_info_mirror_tb_status_out_of_date_plan.\n");
      const char *db_tb_name =
          st.mirror_tb_out_of_date_tb_name;  // if not null, should be like
                                             // table_schema.table_name
      const char *schema_name = DBSCALE_RESERVED_STR;
      const char *table_name = DBSCALE_RESERVED_STR;
      vector<string> strs;
      if (db_tb_name) {
        boost::split(strs, db_tb_name, boost::is_any_of("."));
        if (strs.size() != 2) {
          throw Error(
              "the mirror table name should be like table_schema.table_name");
        }
        schema_name = strs[0].c_str();
        table_name = strs[1].c_str();
      }
      Backend::instance()->set_informationschema_mirror_tb_out_of_date(
          true, schema_name, table_name);
      ExecuteNode *node = plan->get_return_ok_node();
      plan->set_start_node(node);
      return;
    } break;
    case STMT_DBSCALE_DYNAMIC_CHANGE_MASTER: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        if (!mul->get_is_cluster_master()) {
          LOG_DEBUG(
              "Dynamic change master operation be execute on slave dbscale\n");
          assemble_forward_master_role_plan(plan);
          break;
        }
      }
#endif
      assemble_dynamic_change_master_plan(plan,
                                          st.sql->dynamic_change_master_oper);
    } break;
    case STMT_DBSCALE_SET_POOL_INFO: {
      check_dbscale_management_acl(plan);
      assemble_dbscale_set_pool_plan(plan, st.sql->pool_oper);
    } break;
    case STMT_DBSCALE_RESET_INFO: {
      assemble_dbscale_reset_info_plan(plan, st.sql->dbscale_reset_info_oper);
    } break;
    case STMT_DBSCALE_BLOCK: {
      check_dbscale_management_acl(plan);
      assemble_dbscale_block_plan(plan);
    } break;
    case STMT_DBSCALE_FLASHBACK_FORCE_ONLINE: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        if (!mul->get_is_cluster_master()) {
          LOG_DEBUG(
              "flashback force online operation be execute on slave dbscale\n");
          assemble_forward_master_role_plan(plan);
          break;
        }
      }
#endif
      assemble_dbscale_force_flashback_online_plan(plan);
    } break;
    case STMT_DBSCALE_XA_RECOVER_SLAVE_DBSCALE: {
      check_dbscale_management_acl(plan);
      assemble_dbscale_xa_recover_slave_dbscale_plan(plan);
    } break;
    case STMT_DBSCALE_CHECK_TABLE: {
      ExecuteNode *node = plan->get_dbscale_check_table_node();
      plan->set_start_node(node);
    } break;
    case STMT_DBSCALE_CHECK_METADATA: {
      bool is_master = true;
#ifndef CLOSE_MULTIPLE
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
#endif
      if (!is_master) {
        assemble_forward_master_role_plan(plan);
      } else {
        ExecuteNode *node = plan->get_dbscale_check_metadata();
        plan->set_start_node(node);
      }
    } break;
    case STMT_DBSCALE_CHECK_DISK_IO: {
      ExecuteNode *node = plan->get_dbscale_check_disk_io_node();
      plan->set_start_node(node);
    } break;
    case STMT_DBSCALE_DISABLE_SERVER: {
      check_dbscale_management_acl(plan);
      assemble_dbscale_disable_server_plan(plan);
    } break;
    case STMT_DBSCALE_FLUSH_POOL_VERSION: {
      check_dbscale_management_acl(plan);
      assemble_dbscale_flush_pool_plan(plan, st.sql->pool_oper);
    } break;
    case STMT_DBSCALE_PURGE_CONNECTION_POOL: {
      check_dbscale_management_acl(plan);
      LOG_DEBUG("Assemble dbscale purge connection pool plan.\n");
      ExecuteNode *node =
          plan->get_dbscale_purge_connection_pool_node(st.sql->pool_oper);
      plan->set_start_node(node);
    } break;
    case STMT_DBSCALE_FLUSH: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        if (!mul->get_is_cluster_master()) {
          LOG_DEBUG("Flush View on slave dbscale, so forword to master.\n");
          assemble_forward_master_role_plan(plan);
          break;
        }
      }
#endif
      assemble_dbscale_flush_plan(plan, st.sql->dbscale_flush_oper->type);
    } break;
    case STMT_DBSCALE_FLUSH_WEAK_PASSWORD_FILE: {
      check_dbscale_management_acl(plan);
      assemble_dbscale_flush_weak_pwd_plan(plan);
    } break;
    case STMT_DBSCALE_RELOAD_CONFIG: {
      check_dbscale_management_acl(plan);
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        if (!mul->get_is_cluster_master()) {
          assemble_forward_master_role_plan(plan);
          break;
        }
      }
      ExecuteNode *node = plan->get_dbscale_reload_config_node(sql);
      plan->set_start_node(node);
    } break;
    case STMT_DBSCALE_RELOAD_SLAVE_CONFIG: {
      check_dbscale_management_acl(plan);
      ExecuteNode *node = plan->get_dbscale_reload_config_node(sql);
      plan->set_start_node(node);
    } break;
    case STMT_DBSCALE_SET_PRIORITY: {
      check_dbscale_management_acl(plan);
      assemble_dbscale_set_priority_plan(plan);
    } break;
    case STMT_DBSCALE_DYNAMIC_CHANGE_MULTIPLE_MASTER_ACTIVE: {
      check_dbscale_management_acl(plan);
      assemble_dynamic_change_multiple_master_active_plan(
          plan, st.sql->dynamic_change_multiple_master_active_oper);
    } break;
    case STMT_DBSCALE_DYNAMIC_CHANGE_REMOTE_SSH: {
      check_dbscale_management_acl(plan);

      const char *server_name =
          st.sql->dynamic_change_dataserver_ssh_oper->server_name;
      const char *username =
          st.sql->dynamic_change_dataserver_ssh_oper->username;
      const char *pwd = st.sql->dynamic_change_dataserver_ssh_oper->pwd;
      int port = st.sql->dynamic_change_dataserver_ssh_oper->port;
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }

      bool need_forward =
          st.sql->dynamic_change_dataserver_ssh_oper->need_forward_to_master;
      if (is_master || !need_forward) {
#endif
        assemble_dynamic_change_dataserver_ssh_plan(plan, server_name, username,
                                                    pwd, port);
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG("dynamic add slave cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
      }
#endif
    } break;
    case STMT_DBSCALE_DYNAMIC_REMOVE_SLAVE: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        assemble_dynamic_remove_slave_plan(plan,
                                           st.sql->dynamic_remove_slave_oper);
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG("dynamic add slave cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
      }
#endif
    } break;
    case STMT_DBSCALE_DYNAMIC_REMOVE_SCHEMA: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        const char *schema_name =
            st.sql->dynamic_remove_schema_oper->schema_name;
        bool is_force = st.sql->dynamic_remove_schema_oper->is_force;
        assemble_dynamic_remove_schema_plan(plan, schema_name, is_force);
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG(
            "dynamic remove schema dataspace cmd be execute on slave "
            "dbscale\n");
        assemble_forward_master_role_plan(plan);
      }
#endif
    } break;
    case STMT_DBSCALE_DYNAMIC_REMOVE_TABLE: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        const char *table_name = st.sql->dynamic_remove_table_oper->table_name;
        bool is_force = st.sql->dynamic_remove_table_oper->is_force;
        assemble_dynamic_remove_table_plan(plan, table_name, is_force);
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG(
            "dynamic remove table dataspace cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
      }
#endif
    } break;
    case STMT_DBSCALE_DYNAMIC_CHANGE_TABLE_SCHEME: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        const char *table_name =
            st.sql->dynamic_remove_table_pattern_oper->table_name;
        const char *scheme_name =
            st.sql->dynamic_remove_table_pattern_oper->scheme_name;
        assemble_dynamic_change_table_scheme_plan(plan, table_name,
                                                  scheme_name);
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG(
            "dynamic remove table dataspace cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
      }
#endif
    } break;
    case STMT_DBSCALE_DYNAMIC_REMOVE_DATASERVER: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        assemble_dynamic_remove_plan(plan);
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG("dynamic add slave cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif

    } break;
    case STMT_DBSCALE_DYNAMIC_REMOVE_PARTITION_SCHEME:
    case STMT_DBSCALE_DYNAMIC_REMOVE_DATASOURCE: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        assemble_dynamic_remove_plan(plan);
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG("dynamic remove datasource on slave dbscale.\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
    } break;
    case STMT_DBSCALE_SHOW_HELP: {
      assemble_dbscale_help_plan(plan, st.sql->show_help_oper->cmd_name);
    } break;
    case STMT_DBSCALE_SHOW_SLOW_SQL_TOP_N: {
      if (slow_query_time == 0) {
        throw SlowQueryDisabledError();
      }
      LOG_DEBUG("Assemble dbscale show slow sql top n plan\n");
      ExecuteNode *node = plan->get_dbscale_show_slow_sql_top_n_node();
      plan->set_start_node(node);
    } break;
    case STMT_DBSCALE_REQUEST_SLOW_SQL_TOP_N: {
      if (slow_query_time == 0) {
        throw SlowQueryDisabledError();
      }
      LOG_DEBUG("Assemble dbscale request slow sql top n plan\n");
      ExecuteNode *node = plan->get_dbscale_request_slow_sql_top_n_node();
      plan->set_start_node(node);
    } break;
    case STMT_DBSCALE_SHOW_AUDIT_USER_LIST: {
      if (strcmp(normal_admin_user, plan->session->get_username()) == 0 ||
          strcmp(supreme_admin_user, plan->session->get_username()) == 0 ||
          strcmp(dbscale_internal_user, plan->session->get_username()) == 0) {
        LOG_DEBUG("Assemble dbscale show audit user list plan\n");
        ExecuteNode *node = plan->get_dbscale_show_audit_user_list_node();
        plan->set_start_node(node);
      } else {
        LOG_ERROR(
            "Current User does not have the privilege to show audit user "
            "list.\n");
        throw Error(
            "Current User does not have the privilege to show audit user list");
      }
    } break;
    case STMT_DBSCALE_SHOW_WHITE_LIST: {
      if (!enable_acl) {
        LOG_ERROR("enable-acl=0, no ACL info to show\n");
        throw Error("enable-acl=0, no ACL info to show");
      }
      if (strcmp(normal_admin_user, plan->session->get_username()) == 0 ||
          strcmp(supreme_admin_user, plan->session->get_username()) == 0 ||
          strcmp(dbscale_internal_user, plan->session->get_username()) == 0) {
        assemble_dbscale_show_white_list(plan);
      } else {
        LOG_ERROR(
            "Current User does not have the privilege to check white list.\n");
        throw Error(
            "Current User does not have the privilege to check white list.");
      }
    } break;
    case STMT_DBSCALE_SHOW_JOIN_STATUS: {
      assemble_dbscale_show_join_plan(plan, st.sql->show_join_oper->name);
    } break;
    case STMT_DBSCALE_SHOW_BASE_STATUS: {
      assemble_dbscale_show_base_status_plan(plan);
    } break;
    case STMT_DBSCALE_MONITOR_POINT_STATUS:
    case STMT_DBSCALE_HANDLER_MONITOR_POINT_STATUS: {
      assemble_show_monitor_point_status_plan(plan);
    } break;
    case STMT_DBSCALE_GLOBAL_MONITOR_POINT_STATUS: {
      assemble_show_global_monitor_point_status_plan(plan);
    } break;
    case STMT_DBSCALE_HISTOGRAM_MONITOR_POINT_STATUS: {
      assemble_show_histogram_monitor_point_status_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_OUTLINE_INFO: {
      assemble_dbscale_show_outline_monitor_info_plan(plan);
      break;
    }
    case STMT_DBSCALE_CREATE_OUTLINE_HINT: {
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        assemble_dbscale_create_outline_hint_plan(
            plan, st.sql->dbscale_operate_outline_hint_oper);
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG(
            "create_outline_hint cmd be execute on slave "
            "dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
      break;
    }
    case STMT_DBSCALE_FLUSH_OUTLINE_HINT: {
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        assemble_dbscale_flush_outline_hint_plan(
            plan, st.sql->dbscale_operate_outline_hint_oper);
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG(
            "flush_outline_hint cmd be execute on slave "
            "dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
      break;
    }
    case STMT_DBSCALE_SHOW_OUTLINE_HINT: {
      assemble_dbscale_show_outline_hint_plan(
          plan, st.sql->dbscale_operate_outline_hint_oper);
      break;
    }
    case STMT_DBSCALE_DELETE_OUTLINE_HINT: {
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        assemble_dbscale_delete_outline_hint_plan(
            plan, st.sql->dbscale_operate_outline_hint_oper);
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG(
            "delete_outline_hint cmd be execute on slave "
            "dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
      break;
    }
    case STMT_DBSCALE_SHOW_POOL_INFO: {
      assemble_show_pool_info_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_POOL_VERSION: {
      assemble_show_pool_version_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_LOCK_USAGE: {
      assemble_show_lock_usage_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_EXECUTION_PROFILE: {
      assemble_show_execution_profile_plan(plan);
      break;
    }
    case STMT_CHANGE_DB: {
      const char *schema_name = st.sql->change_db_oper->dbname->name;
      dataspace = backend->get_data_space_for_table(schema_name, NULL);
      assemble_direct_exec_plan(plan, dataspace);
    } break;
    case STMT_SHOW_CREATE_DB: {
      const char *schema_name = st.sql->show_create_db_oper->dbname->name;
      dataspace = backend->get_data_space_for_table(schema_name, NULL);
      assemble_direct_exec_plan(plan, dataspace);
    } break;
    // see issue #1888
    // NEW FOR REMOTE DISASTER RECOVERY
    case STMT_SHOW_BINARY_LOGS:
    case STMT_RESET_SLAVE:
    case STMT_START_SLAVE:
    case STMT_STOP_SLAVE:
    case STMT_CHANGE_MASTER: {
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        if (slave_dbscale_mode) {
          plan->session->set_read_only(false);
          dataspace = backend->get_catalog();
          assemble_direct_exec_plan(plan, dataspace);
          backend->set_need_send_stop_slave_flag(true);
        } else {
          ExecuteNode *node = plan->get_slave_dbscale_error_node();
          plan->set_start_node(node);
        }
#ifndef CLOSE_MULTIPLE
      } else {
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
    } break;
    // END NEW FOR REMOTE DISASTER RECOVERY
    case STMT_SHOW_PROCESSLIST:
    case STMT_SHOW_MASTER_STATUS:
    case STMT_SHOW_SLAVE_STATUS: {
      if (slave_dbscale_mode) plan->session->set_read_only(false);
      dataspace = backend->get_catalog();
      assemble_direct_exec_plan(plan, dataspace);
    } break;
    case STMT_SHOW_CREATE_TRIGGER: {
      dataspace = backend->get_auth_data_space();
      assemble_direct_exec_plan(plan, dataspace);
    } break;
    case STMT_SHOW_TRIGGERS: {
      const char *schema_name = st.sql->show_triggers_oper->db_name;
      if (schema_name) {
        dataspace = backend->get_data_space_for_table(schema_name, NULL);
      } else {
        dataspace = backend->get_data_space_for_table(schema, NULL);
      }
      assemble_direct_exec_plan(plan, dataspace);
    } break;
    case STMT_SHOW_CREATE_FUNCTION_OR_PROCEDURE: {
      const char *schema_name = st.sql->show_create_func_or_proc_oper->db_name;
      if (schema_name) {
        dataspace = backend->get_data_space_for_table(schema_name, NULL);
      } else {
        dataspace = backend->get_data_space_for_table(schema, NULL);
      }
      assemble_direct_exec_plan(plan, dataspace);
    } break;
    case STMT_SHOW_CREATE_VIEW: {
      if (is_partial_parsed()) {
        LOG_ERROR("partial parsed show create view sql\n");
        throw Error("fail to parse show create view sql");
      }
      if (on_view) {
        dataspace = backend->get_config_data_space();
        if (!dataspace) {
          LOG_ERROR("Can not find the metadata dataspace.\n");
          throw Error(
              "Fail to get meta dataspace, please make sure meta datasource is "
              "configured.");
        }
      } else {
        /*If dbscale no need to maintain view, the view will be created on the
         * schema dataspace.*/
        create_view_op_node *view_node = st.sql->view_oper;
        const char *view_schema =
            view_node->schema ? view_node->schema : schema;
        dataspace = backend->get_data_space_for_table(view_schema, NULL);
      }
      assemble_direct_exec_plan(plan, dataspace);
    } break;
    case STMT_SHOW_FUNCTION_OR_PROCEDURE_CODE: {
      dataspace = backend->get_metadata_data_space();
      if (!dataspace) {
        dataspace = backend->get_catalog();
      }
      assemble_direct_exec_plan(plan, dataspace);
    } break;
    case STMT_SHOW_FUNCTION_OR_PROCEDURE_STATUS: {
      dataspace = backend->get_metadata_data_space();
      if (!dataspace) {
        assemble_show_func_or_proc_status_plan(plan);
      } else {
        assemble_direct_exec_plan(plan, dataspace);
      }
    } break;
    case STMT_ALTER_FUNCTION_OR_PROCEDURE: {
      bool is_centralized_cluster = backend->is_centralized_cluster();
      if (is_centralized_cluster) {
        dataspace = backend->get_catalog();
        assemble_direct_exec_plan(plan, dataspace);
      } else {
        throw NotSupportedError(
            "Only support ALTER PROCEDURE under centralized cluster");
      }
    } break;
    case STMT_DBSCALE_CLEAN_PROCEDURE_METADATA: {
      const char *sp_schema = st.routine_d->proc_node->schema_name
                                  ? st.routine_d->proc_node->schema_name
                                  : schema;
      const char *sp_name = st.routine_d->proc_node->proc_name;

      string sp_full_name = sp_schema;
      sp_full_name.append(".").append(sp_name);

      backend->remove_one_procedure_meta_data_local(sp_full_name);
      ExecuteNode *node = plan->get_return_ok_node();
      plan->set_start_node(node);
    } break;
    case STMT_CREATE_DB:
    case STMT_DROP_DB:
    case STMT_ALTER_DB: {
#ifndef CLOSE_MULTIPLE
      /*Due to the create db may need to create new dataspace, which need to
       * sync in multiple mode, so forward to master role dbscale.*/
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        bool is_master = mul->get_is_cluster_master();

        if (!is_master) {
          LOG_DEBUG("CREATE/DROP DB statment be execute on slave dbscale\n");
          assemble_forward_master_role_plan(plan);
          break;
        }
      }
#endif
    }
      assemble_create_db_plan(plan);
      break;
    case STMT_KILL_THREAD: {
      bool is_kill_query = st.sql->kill_oper->is_kill_query;
      if (is_kill_query) {
        LOG_ERROR("Not support KILL QUERY statement, use KILL instead.\n");
        throw NotSupportedError("Not support KILL QUERY, use KILL instead");
      }

      uint32_t kid = 0;
      if (st.sql->kill_oper->kid_is_uservar) {
        map<string, string> *user_var_map = plan->session->get_user_var_map();
        string var_name(st.sql->kill_oper->kid_uservar_str);
        boost::to_upper(var_name);
        string var_value;
        if (user_var_map->count(var_name)) {
          var_value = (*user_var_map)[var_name];
          if (!is_natural_number(var_value.c_str())) {
            LOG_ERROR("Handler %@ killing with invalid thread id [%s].\n",
                      plan->handler, var_value.c_str());
            throw Error("Killing with invalid thread id");
          }
          kid = atoi(var_value.c_str());
        } else {
          LOG_ERROR(
              "Handler %@ killing with invalid thread id, uservar not exist.\n",
              plan->handler);
          throw Error("Killing with invalid thread id");
        }
      } else {
        kid = (uint32_t)st.sql->kill_oper->kid;
      }

      if (kid == 0) {
        LOG_ERROR("Handler %@ killing with invalid thread id [0].\n",
                  plan->handler);
        throw Error("Killing with invalid thread id");
      }
      const char *cluster_id = st.sql->kill_oper->cluster_id;
      int killed_cluster_id = 0;
      if (cluster_id) {
        killed_cluster_id = atoi(cluster_id);
        if (killed_cluster_id == 0)
          throw Error("Killing with invalid cluster id");
      }

      if (kid == plan->handler->get_thread_id()) {
        if (cluster_id == NULL ||
            killed_cluster_id == backend->get_cluster_id()) {
          Backend::instance()->record_latest_important_dbscale_warning(
              "Handler %p self killing with thread id %d.\n",
              (void *)(plan->handler), kid);
          throw ThreadIsKilled();
        }
      }
      if (cluster_id == NULL) {
        killed_cluster_id = backend->get_cluster_id();
      }
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (!is_master && killed_cluster_id != backend->get_cluster_id()) {
        assemble_forward_master_role_plan(plan);
        break;
      } else {
#endif
        assemble_kill_plan(plan, killed_cluster_id, kid);
#ifndef CLOSE_MULTIPLE
      }
#endif
    } break;
    case STMT_DBSCALE_SET: {
      dynamic_config_op_node *config_oper = st.sql->dynamic_config_oper;
      var_scope_type vst = config_oper->vst;

      check_disaster_relate_option(config_oper);
      if (!(strcmp(normal_admin_user, plan->session->get_username()) == 0 ||
            strcmp(supreme_admin_user, plan->session->get_username()) == 0 ||
            strcmp(dbscale_internal_user, plan->session->get_username()) == 0 ||
            vst == VAR_SCOPE_SESSION)) {
        LOG_ERROR(
            "Current User does not have the privilege of dynamic "
            "configuration.\n");
        throw Error(
            "Current User does not have the privilege of dynamic "
            "configuration.");
      }
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (!is_master && vst == VAR_SCOPE_GLOBAL) {
        LOG_DEBUG(
            "dynamic set global option cmd be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      } else {
        if (vst == VAR_SCOPE_INSTANCE && config_oper->cluster_id > 0 &&
            backend->get_cluster_id() != config_oper->cluster_id) {
          if (!is_master) {
            LOG_DEBUG(
                "dynamic set instance option cmd with other dbscale cluster id "
                "be execute on slave dbscale\n");
            assemble_forward_master_role_plan(plan);
            break;
          } else {
            MultipleManager *mul = MultipleManager::instance();
            mul->execute_master_query(sql, config_oper->cluster_id);
            mul->execute_master_query("dbscale flush config to file",
                                      config_oper->cluster_id);
            ExecuteNode *node = plan->get_return_ok_node();
            plan->set_start_node(node);
            break;
          }
        }
#endif
        assemble_dbscale_set_plan(plan);
#ifndef CLOSE_MULTIPLE
      }
#endif
    } break;
    case STMT_DBSCALE_DYNAMIC_SET_SERVER_WEIGHT: {
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        if (strcmp(normal_admin_user, plan->session->get_username()) == 0 ||
            strcmp(supreme_admin_user, plan->session->get_username()) == 0 ||
            strcmp(dbscale_internal_user, plan->session->get_username()) == 0) {
          assemble_dbscale_set_server_weight(plan);
        } else {
          LOG_ERROR(
              "Current User does not have the privilege to set server "
              "weight\n");
          throw Error(
              "Current User does not have the privilege to set server weight");
        }
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG("Set server weight be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif

    } break;
    case STMT_DBSCALE_DYNAMIC_SET_AUTO_INCREMENT_OFFSET: {
      check_dbscale_management_acl(plan);
      assemble_dbscale_set_auto_increment_offset(plan);
      break;
    } break;
    case STMT_DBSCALE_DYNAMIC_SET_REP_STRATEGY: {
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (is_master) {
#endif
        if (strcmp(normal_admin_user, plan->session->get_username()) == 0 ||
            strcmp(supreme_admin_user, plan->session->get_username()) == 0 ||
            strcmp(dbscale_internal_user, plan->session->get_username()) == 0) {
          assemble_dbscale_set_rep_strategy_plan(plan);
        } else {
          LOG_ERROR(
              "Current User does not have the privilege to change "
              " load_balance_strategy of replication data_source.\n");
          throw Error(
              "Current User does not have the privilege to change "
              "load_balance_strategy of replication data_source.");
        }
#ifndef CLOSE_MULTIPLE
      } else {
        LOG_DEBUG("Set rep strategy be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#endif
    } break;
    case STMT_DBSCALE_PURGE_MONITOR_POINT: {
      assemble_dbscale_purge_monitor_point_plan(plan);
    } break;
    case STMT_DBSCALE_CLEAN_MONITOR_POINT: {
      assemble_dbscale_clean_monitor_point_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_OPTION: {
      assemble_show_option_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_DYNAMIC_OPTION: {
      assemble_show_dynamic_option_plan(plan);
    } break;
    case STMT_DBSCALE_CHANGE_STARTUP_CONFIG: {
#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      } else {
        LOG_ERROR(
            "Change startup config is only supported under Multiple DBScale "
            "mode.\n");
        throw NotSupportedError(
            "Change startup config is only supported under Multiple DBScale "
            "mode.");
      }
      if (is_master) {
        if (strcmp(normal_admin_user, plan->session->get_username()) == 0 ||
            strcmp(supreme_admin_user, plan->session->get_username()) == 0 ||
            strcmp(dbscale_internal_user, plan->session->get_username()) == 0) {
          assemble_change_startup_config_plan(plan);
        } else {
          LOG_ERROR(
              "Current User does not have the privilege to change startup "
              "config.\n");
          throw Error(
              "Current User does not have the privilege to change startup "
              "config.");
        }
      } else {
        LOG_DEBUG("Change startup config be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
        break;
      }
#else
      LOG_ERROR(
          "Change startup config is only supported under Multiple DBScale "
          "mode.\n");
      throw NotSupportedError(
          "Change startup config is only supported under Multiple DBScale "
          "mode.");
#endif
    } break;
    case STMT_DBSCALE_SHOW_CHANGED_STARTUP_CONFIG: {
      check_dbscale_management_acl(plan);
#ifdef CLOSE_MULTIPLE
      LOG_ERROR(
          "Change startup config is only supported under Multiple DBScale "
          "mode.\n");
      throw NotSupportedError(
          "Change startup config is only supported under Multiple DBScale "
          "mode.");
#endif
      if (!multiple_mode) {
        LOG_ERROR(
            "Show change startup config is only supported under Multiple "
            "DBScale mode.\n");
        throw NotSupportedError(
            "Show change startup config is only supported under Multiple "
            "DBScale mode.");
      }
      assemble_show_changed_startup_config_plan(plan);
    } break;
    case STMT_DBSCALE_SHOW_TRANSACTION_SQLS: {
      if (enable_record_transaction_sqls) {
        assemble_show_transaction_sqls_plan(plan);
      } else {
        LOG_ERROR("Record transaction sqls has been disabled.\n");
        throw NotSupportedError("Record transaction sqls has been disabled.");
      }
    } break;
    case STMT_SHOW_GRANTS:
    case STMT_SHOW_CREATE_USER: {
      if (is_partial_parsed()) throw Error("not support partial parsed sql");

      if (plan->session->is_in_lock()) {
        LOG_ERROR("Forbid grant in lock table mode.\n");
        throw NotSupportedError("Not support grant in lock table mode.");
      }
      replace_grant_hosts();
      dataspace = backend->get_auth_data_space();
      assemble_direct_exec_plan(plan, dataspace);
    } break;
    case STMT_GRANT: {
      if (is_partial_parsed()) throw Error("not support partial parsed sql");

      if (st.sql->grant_oper != NULL) {
        user_clause_item_list *user_clause_list =
            st.sql->grant_oper->user_clause;
        if (user_clause_list->next) {
          throw NotSupportedError(
              "not support multi users in one grant statement");
        }
        if (!user_clause_list->user_name ||
            strlen(user_clause_list->user_name) == 0) {
          throw NotSupportedError("not support anonymous user");
        }
        if (!is_ip_compatible(user_clause_list->host_name)) {
          throw NotSupportedError(
              "only support IP as host value, IP format can be an single \% or "
              "{num | \%}.{num | \%}.{num | \%}.{num | \%}");
        }
        if (st.sql->grant_oper->is_grant_proxy) {
          throw NotSupportedError(
              "handle_acl_by_grant_stmt not support grant proxy");
        }
        if (st.sql->grant_oper->object_type != GRANT_OBJECT_TYPE_TABLE) {
          throw NotSupportedError(
              "only support grant for table object currently");
        }
        if (st.sql->grant_oper->priv_level == GRANT_PRIV_LEVEL_A_STAR) {
          const char *c = st.sql->grant_oper->priv_level_db_name;
          if (c && (*c == '%' || *c == '_')) {
            throw NotSupportedError(
                "not support wildcard in db name when grant database level "
                "privilege");
          }
          if (strlen(c) > 1) {
            c += 1;
            while (*c) {
              if ((*c == '%' || *c == '_') && *(c - 1) != '\\') {
                throw NotSupportedError(
                    "not support wildcard in db name when grant database level "
                    "privilege");
              }
              ++c;
            }
          }
        }
        const char *password = user_clause_list->password;
        if (password ||
            !Backend::instance()->user_existed(user_clause_list->user_name)) {
          string err_msg;
          bool is_legal = validate_password_complex(password, err_msg);
          if (!is_legal) {
            LOG_ERROR("Password is illegal, %s.\n", err_msg.c_str());
            throw Error(err_msg.c_str());
          }
        }
      }
    }
    case STMT_DROP_USER:
    case STMT_CREATE_USER:
    case STMT_ALTER_USER:
    case STMT_REVOKE: {
      if (is_partial_parsed()) throw Error("not support partial parsed sql");

      if (plan->session->is_in_lock()) {
        LOG_ERROR("Forbid grant in lock table mode.\n");
        throw NotSupportedError("Not support grant in lock table mode.");
      }
      if (st.type == STMT_REVOKE) {
        if (st.sql->revoke_oper) {
          user_clause_item_list *user_clause_list =
              st.sql->revoke_oper->user_clause;
          if (user_clause_list->next) {
            throw NotSupportedError(
                "not support multi users in one revoke statement");
          }
        }
        if (st.sql->revoke_oper->priv_level == GRANT_PRIV_LEVEL_A_STAR) {
          const char *c = st.sql->revoke_oper->priv_level_db_name;
          if (strlen(c) > 1) {
            if (c && (*c == '%' || *c == '_')) {
              throw NotSupportedError(
                  "not support wildcard in db name when grant database level "
                  "privilege");
            }
            c += 1;
            while (*c) {
              if ((*c == '%' || *c == '_') && *(c - 1) != '\\') {
                throw NotSupportedError(
                    "not support wildcard in db name when revoke database "
                    "level privilege");
              }
              ++c;
            }
          }
        }
      }
      if (st.type == STMT_DROP_USER) {
        if (st.sql->drop_user_oper) {
          user_clause_item_list *user_clause_list =
              st.sql->drop_user_oper->user_clause;
          if (user_clause_list->next) {
            throw NotSupportedError(
                "not support multi users in one drop user statement");
          }
        }
      }
      if (st.type == STMT_CREATE_USER) {
        if (st.sql->create_user_oper != NULL) {
          user_clause_item_list *user_clause_list =
              st.sql->create_user_oper->user_clause;
          if (user_clause_list->next) {
            throw NotSupportedError(
                "not support multi users in one create user statement");
          }
          if (!is_ip_compatible(user_clause_list->host_name)) {
            throw NotSupportedError("only support IP as host value");
          }
          const char *password = user_clause_list->password;
          string err_msg;
          bool is_legal = validate_password_complex(password, err_msg);
          if (!is_legal) {
            LOG_ERROR("Password is illegal, %s.\n", err_msg.c_str());
            throw Error(err_msg.c_str());
          }
          if (Backend::instance()->get_backend_server_version() ==
              MYSQL_VERSION_57) {
            if (st.create_user_default_auth_plugin &&
                strcasecmp(st.create_user_default_auth_plugin,
                           "mysql_native_password")) {
              LOG_INFO(
                  "backend dataserver is based on 5.7, create user statement "
                  "should specify authentication_plugin as "
                  "'mysql_native_password' explicitly.");
              throw Error(
                  "backend dataserver is based on 5.7, create user statement "
                  "should specify authentication_plugin as "
                  "'mysql_native_password' explicitly.");
            }
          } else if (Backend::instance()->get_backend_server_version() ==
                     MYSQL_VERSION_8) {
            if (st.create_user_default_auth_plugin &&
                strcasecmp(st.create_user_default_auth_plugin,
                           "mysql_native_password") &&
                strcasecmp(st.create_user_default_auth_plugin,
                           "caching_sha2_password")) {
              LOG_INFO(
                  "backend dataserver is based on 8.0, create user statement "
                  "should specify authentication_plugin as "
                  "'mysql_native_password' or 'caching_sha2_password' "
                  "explicitly.");
              throw Error(
                  "backend dataserver is based on 8.0, create user statement "
                  "should specify authentication_plugin as "
                  "'mysql_native_password' or 'caching_sha2_password' "
                  "explicitly.");
            }
          }
        }
      }
      // make sure that alter user is not changing auth plugin when using 8.0
      // backend cause dbscale only support mysql_native_password auth plugin
      if (st.type == STMT_ALTER_USER && st.sql->create_user_oper != NULL) {
        if (Backend::instance()->get_backend_server_version() ==
            MYSQL_VERSION_57) {
          if (st.create_user_default_auth_plugin &&
              strcasecmp(st.create_user_default_auth_plugin,
                         "mysql_native_password")) {
            LOG_INFO(
                "backend dataserver is based on 5.7 alter user should not "
                "change it's auth plugin to any plugin except "
                "'mysql_native_password'");
            throw Error(
                "backend dataserver is based on 5.7 alter user should not "
                "change it's auth plugin to any plugin except "
                "'mysql_native_password'");
          }
        } else if (Backend::instance()->get_backend_server_version() ==
                   MYSQL_VERSION_8) {
          if (st.create_user_default_auth_plugin &&
              strcasecmp(st.create_user_default_auth_plugin,
                         "mysql_native_password") &&
              strcasecmp(st.create_user_default_auth_plugin,
                         "caching_sha2_password")) {
            LOG_INFO(
                "backend dataserver is based on 8.0 alter user should not "
                "change it's auth plugin to any plugin except "
                "'mysql_native_password' or 'caching_sha2_password'");
            throw Error(
                "backend dataserver is based on 8.0 alter user should not "
                "change it's auth plugin to any plugin except "
                "'mysql_native_password' or 'caching_sha2_password'");
          }
        }
      }
      bool is_return_ok = false;
      if (st.type == STMT_GRANT || st.type == STMT_REVOKE) {
        grant_priv_type_list *priv_type = NULL;
        if (st.type == STMT_GRANT)
          priv_type = st.sql->grant_oper->grant_priv_type;
        else {
          priv_type = st.sql->revoke_oper->grant_priv_type;
        }

        ACLType acl_type = ACL_TYPE_EMPTY;
        int acl_count = 0;
        while (priv_type) {
          acl_type |= priv_type->type;
          ++acl_count;
          priv_type = priv_type->next;
        }
        if ((acl_type & ACL_TYPE_ADD_PARTITION ||
             acl_type & ACL_TYPE_DROP_PARTITION)) {
          check_dbscale_management_acl(plan);
          if (acl_count != 1) {
            string err_msg = "ADD PARTITION/DROP PARTITION need use alone";
            LOG_ERROR("%s\n", err_msg.c_str());
            throw NotSupportedError(err_msg.c_str());
          }
          add_drop_partition_acl_check(plan, acl_type);
          is_return_ok = true;
        }
      }

#ifndef CLOSE_MULTIPLE
      bool is_master = true;
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        is_master = mul->get_is_cluster_master();
      }
      if (!is_master) {
        LOG_DEBUG(
            "drop_user/create_user/revoke/grant cmd be execute on slave "
            "dbscale\n");
        assemble_forward_master_role_plan(plan);
        set_need_handle_acl_related_stmt(false);
        break;
      }
#endif
      replace_grant_hosts();
      if (is_return_ok) {
        ExecuteNode *node = plan->get_return_ok_node();
        plan->set_start_node(node);
        break;
      }
      dataspace = backend->get_auth_data_space();
      assemble_direct_exec_plan(plan, dataspace);
    } break;
    case STMT_DROP_EVENT:
    case STMT_CREATE_EVENT: {
      const char *schema_name = schema;

      if (st.sql->event_oper) {
        schema_name = st.sql->event_oper->schema_name
                          ? st.sql->event_oper->schema_name
                          : schema;
      } else {
        throw NotSupportedError(
            "Unsupport event, maybe parse fail, plz check log.");
      }
      dataspace = backend->get_data_space_for_table(schema_name, NULL);
      if (dataspace) {
        assemble_direct_exec_plan(plan, dataspace);
      } else {
#ifdef DEBUG
        ACE_ASSERT(0);
#endif
        LOG_ERROR("Can not get dataspace for schema %s\n.", schema_name);
        throw Error("Fail to get dataspace.");
      }
    } break;
    case STMT_DBSCALE_KEEPMASTER:
      assemble_keepmaster_plan(plan);
      break;
    case STMT_DBSCALE_TEST:
      assemble_dbscale_test_plan(plan);
      break;
    case STMT_DROP_VIEW: {
#ifndef CLOSE_MULTIPLE
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        bool is_master = mul->get_is_cluster_master();

        if (!is_master) {
          LOG_DEBUG("DROP VIEW statment be execute on slave dbscale\n");
          assemble_forward_master_role_plan(plan);
          break;
        }
      }
#endif
      if (on_view) {
        dataspace = backend->get_config_data_space();
        if (!dataspace) {
          LOG_ERROR("Can not find the metadata dataspace.\n");
          throw Error(
              "Fail to get meta dataspace, please make sure meta datasource is "
              "configured.");
        }
        assemble_direct_exec_plan(plan, dataspace);
      } else {
        assemble_drop_mul_table(plan, true);
      }
    } break;
    case STMT_CREATE_VIEW: {
#ifndef CLOSE_MULTIPLE
      if (multiple_mode) {
        MultipleManager *mul = MultipleManager::instance();
        bool is_master = mul->get_is_cluster_master();

        if (!is_master) {
          LOG_DEBUG("CREATE VIEW statment be execute on slave dbscale\n");
          assemble_forward_master_role_plan(plan);
          break;
        }
      }
#endif
      if (is_partial_parsed()) {
        LOG_ERROR("partial parsed create view sql\n");
        throw Error("fail to parse create view sql");
      }
      if (on_view) {
        dataspace = backend->get_config_data_space();
        if (!dataspace) {
          LOG_ERROR("Can not find the metadata dataspace.\n");
          throw Error(
              "Fail to get meta dataspace, please make sure meta datasource is "
              "configured.");
        }
      } else {
        /*If dbscale no need to maintain view, the view will be created on the
         * schema dataspace.*/
#ifdef DEBUG
        ACE_ASSERT(st.sql->select_oper);
#endif
        create_view_op_node *view_node = st.sql->select_oper->create_view_oper;
        const char *view_schema =
            view_node->schema ? view_node->schema : schema;

        dataspace = backend->get_data_space_for_table(view_schema, NULL);
      }
      assemble_direct_exec_plan(plan, dataspace);
    } break;
    case STMT_CREATE_PROCEDURE:
      if (is_partial_parsed() || st.has_unsupport_routine_element) {
        dataspace = backend->get_data_space_for_table(schema, NULL);
        assemble_direct_exec_plan(plan, dataspace);
        break;
      } else {
        const char *routine_schema = schema;
#ifndef CLOSE_PROCEDURE
        routine_node *routine_d = st.routine_d;
        store_procedure_node *proc_node = routine_d->proc_node;
        routine_schema =
            proc_node->schema_name ? proc_node->schema_name : schema;
        if (!routine_schema)
          throw dbscale::sql::SQLError(dbscale_err_msg[ERROR_NO_SCHEMA_CODE],
                                       "42000", ERROR_NO_SCHEMA_CODE);
        dataspace = backend->get_data_space_for_table(routine_schema, NULL);
        const char *proc_name = proc_node->proc_name;

        if (!enable_oracle_sequence &&
            ((dataspace->get_dataspace_type() == SCHEMA_TYPE &&
              ((Schema *)dataspace)
                  ->get_is_schema_pushdown_stored_procedure()) ||
             dataspace->get_dataspace_type() == CATALOG_TYPE)) {
#ifdef DEBUG
          LOG_DEBUG("schema pushdown stored procedure directly\n");
#endif
          assemble_direct_exec_plan(plan, dataspace);
          break;
        }

        string sp_name(routine_schema);
        sp_name.append(".");
        sp_name.append(proc_name);
        current_sp_worker_name = sp_name;
        SPWorker *run_spworker = plan->session->get_sp_worker(sp_name);
        if (!run_spworker) {
          ProcedureMetaData *meta_data =
              backend->get_procedure_meta_data(sp_name);
          if (!meta_data) {
            SPWorker *spworker = new SPWorker(sp_name, plan->session);
            init_sp_worker_from_create_sql(spworker, plan, st.handled_sql,
                                           routine_schema, schema, sp_name);
            if (spworker->get_execution_flag() == EXECUTE_ON_ONE_DATASPACE) {
              DataSpace *data_space = spworker->get_data_space();
              try {
                data_space->execute_one_modify_sql(
                    st.handled_sql, plan->handler, routine_schema);
              } catch (...) {
                spworker->clean_sp_worker_for_free();
                delete spworker;
                spworker = NULL;
              }
            }
            if (spworker) {
              spworker->clean_sp_worker_for_free();
              delete spworker;
              spworker = NULL;
            }
          }
        }
#endif

        assemble_direct_exec_plan(plan, dataspace);
      }
      break;
    case STMT_DROP_PROC: {
      if (multiple_mode &&
          !MultipleManager::instance()->get_is_cluster_master()) {
        assemble_forward_master_role_plan(plan);
        break;
      }
      if (is_partial_parsed() || st.has_unsupport_routine_element) {
        dataspace = backend->get_data_space_for_table(schema, NULL);
        assemble_direct_exec_plan(plan, dataspace);
      } else {
        routine_node *routine_d = st.routine_d;
        store_procedure_node *proc_node = routine_d->proc_node;
        const char *routine_schema =
            proc_node->schema_name ? proc_node->schema_name : schema;
        if (!routine_schema)
          throw dbscale::sql::SQLError(dbscale_err_msg[ERROR_NO_SCHEMA_CODE],
                                       "42000", ERROR_NO_SCHEMA_CODE);
        const char *proc_name = proc_node->proc_name;
        dataspace = backend->get_data_space_for_table(routine_schema, NULL);

        if (!enable_oracle_sequence &&
            ((dataspace->get_dataspace_type() == SCHEMA_TYPE &&
              ((Schema *)dataspace)
                  ->get_is_schema_pushdown_stored_procedure()) ||
             dataspace->get_dataspace_type() == CATALOG_TYPE)) {
#ifdef DEBUG
          LOG_DEBUG("schema pushdown stored procedure directly\n");
#endif
          assemble_direct_exec_plan(plan, dataspace);
          break;
        }

        string sp_name(routine_schema);
        sp_name.append(".");
        sp_name.append(proc_name);
        string sp_name_with_enclose("`");
        sp_name_with_enclose.append(routine_schema);
        sp_name_with_enclose.append("`.`");
        sp_name_with_enclose.append(proc_name);
        sp_name_with_enclose.append("`");

        ProcedureMetaData *meta_data =
            backend->get_procedure_meta_data(sp_name);
        if (meta_data) {
          if (meta_data->execution_flag == EXECUTE_ON_ONE_DATASPACE) {
            DataSpace *data_space_another = meta_data->data_space;
            try {
              data_space_another->execute_one_modify_sql(
                  st.handled_sql, plan->handler, routine_schema);
            } catch (...) {
              Backend::instance()->record_latest_important_dbscale_warning(
                  "Got error when execute Drop procedure, DataSpace: [%p], "
                  "sql: [%s]\n",
                  data_space_another, st.handled_sql);
            }
          }
        } else {
          vector<string> sp_vec;
          const char *create_sp_sql;
          try {
            fetch_create_procedure_sql(sp_name_with_enclose.c_str(), dataspace,
                                       &sp_vec);
          } catch (SQLError &e) {
            if (e.get_error_code() != ERROR_PROCEDURE_NOT_EXIST_CODE ||
                !st.if_exists)
              throw;
            Backend::instance()->record_latest_important_dbscale_warning(
                "Procedure [%s] does not exist.\n", sp_name.c_str());
          }
          SPWorker *spworker = new SPWorker(sp_name, plan->session);
          if (!sp_vec.empty()) {
            create_sp_sql = sp_vec[0].c_str();
            init_sp_worker_from_create_sql(spworker, plan, create_sp_sql,
                                           routine_schema, schema, sp_name);
            if (spworker->get_execution_flag() == EXECUTE_ON_ONE_DATASPACE) {
              DataSpace *data_space_another = spworker->get_data_space();
              try {
                data_space_another->execute_one_modify_sql(
                    st.handled_sql, plan->handler, routine_schema);
              } catch (...) {
                Backend::instance()->record_latest_important_dbscale_warning(
                    "Got error when execute Drop procedure, DataSpace: [%p], "
                    "sql: [%s]\n",
                    data_space_another, st.handled_sql);
              }
            }
          }
          spworker->clean_sp_worker_for_free();
          delete spworker;
        }
        plan->session->remove_sp_worker(sp_name);
        backend->remove_procedure_meta_data(sp_name);
        assemble_direct_exec_plan(plan, dataspace);
      }
    } break;
    case STMT_CALL: {
#ifndef DBSCALE_TEST_DISABLE
      dbscale_test_info *test_info = plan->session->get_dbscale_test_info();
      if (!strcasecmp(test_info->test_case_name.c_str(), "sp_execution") &&
          !strcasecmp(test_info->test_case_operation.c_str(),
                      "simple_sp_test_init")) {
        LOG_DEBUG(
            "Do dbscale test operation 'simple_sp_test_init' for case "
            "'sp_execution'.\n");
        SPWorker *spworker =
            new SPWorker(string("test.test_sp"), plan->session);
        plan->session->add_sp_worker(spworker);
        plan->session->set_cur_sp_worker(spworker);
        spworker->add_param("param_in", SP_PARAM_IN);
        spworker->add_param("param2", SP_PARAM_IN);
        /*
        spworker->add_param("param_inout", SP_PARAM_INOUT);
        */
        spworker->add_param("param_out", SP_PARAM_OUT);
        spworker->add_variable("var1");
        spworker->add_variable("var2");

        /* test compound statment and simple statment */
        CompondStatement *comp_s = new CompondStatement(spworker, "");
        SimpleExecStatement *se_s1 = new SimpleExecStatement(
            spworker, "set @var1=1, @var2=@param_in + 2", true, this);
        SimpleExecStatement *se_s2 = new SimpleExecStatement(
            spworker, "delete from test.t2", false, this);
        SimpleExecStatement *se_s3 = new SimpleExecStatement(
            spworker, "insert into t2 values (@var1, @var2)", true, this);
        SimpleExecStatement *se_s4 =
            new SimpleExecStatement(spworker, "select * from t2", false, this);

        /* test out parameter */
        SimpleExecStatement *se_s5 =
            new SimpleExecStatement(spworker, "set @param_out = 1", true, this);
        SimpleExecStatement *se_s10 =
            new SimpleExecStatement(spworker, "select @param2", true, this);

        /* test if */
        IntExpression *int_expr = new ((get_stmt_node())) IntExpression("1");
        VarExpression *var_expr =
            new ((get_stmt_node())) VarExpression("var1", VAR_SCOPE_USER);
        CompareExpression *com_expr = new ((get_stmt_node()))
            CompareExpression(var_expr, int_expr, EXPR_EQ, EXPR_SCOPE_NON);

        ExprConditionHandler *expr_cond = new ExprConditionHandler(com_expr);
        expr_cond->add_expr_var_map(var_expr, "var1");

        IFItemStatement *if_item = new IFItemStatement(spworker, expr_cond);
        SimpleExecStatement *se_s6 = new SimpleExecStatement(
            spworker, "insert into t2 values (6,6)", false, this);
        if_item->set_cond_stmt(se_s6);

        IFElseStatement *if_else = new IFElseStatement(spworker);

        SimpleExecStatement *se_s7 = new SimpleExecStatement(
            spworker, "insert into t2 values (8,8)", false, this);

        if_else->add_if_item(if_item);
        if_else->set_else_stmt(se_s7);

        /* test while */
        IntExpression *int_expr2 = new ((get_stmt_node())) IntExpression("5");
        VarExpression *var_expr2 =
            new ((get_stmt_node())) VarExpression("var1", VAR_SCOPE_USER);
        CompareExpression *com_expr2 = new ((get_stmt_node()))
            CompareExpression(var_expr2, int_expr2, EXPR_LESS, EXPR_SCOPE_NON);

        ExprConditionHandler *expr_cond2 = new ExprConditionHandler(com_expr2);
        expr_cond2->add_expr_var_map(var_expr2, "var1");

        SimpleExecStatement *se_s8 = new SimpleExecStatement(
            spworker, "insert into t2 values (@var1+20, @var1+20)", true, this);
        SimpleExecStatement *se_s9 =
            new SimpleExecStatement(spworker, "set @var1=@var1+1", true, this);

        CompondStatement *comp_s2 = new CompondStatement(spworker, "if_while");
        comp_s2->add_child_stmt(se_s8);
        comp_s2->add_child_stmt(se_s9);

        WhileStatement *while_s = new WhileStatement(spworker, expr_cond2, true,
                                                     "");  // while as while

        while_s->add_child_stmt(comp_s2);

        comp_s->add_child_stmt(se_s1);
        comp_s->add_child_stmt(se_s2);
        comp_s->add_child_stmt(se_s3);
        comp_s->add_child_stmt(se_s4);
        comp_s->add_child_stmt(if_else);
        comp_s->add_child_stmt(while_s);
        comp_s->add_child_stmt(se_s5);
        comp_s->add_child_stmt(se_s10);
        spworker->set_sp_statement(comp_s);

        string sp_name;
        sp_name.append("test.test_sp");
        SPWorker *run_spworker = plan->session->get_sp_worker(sp_name);

        string var1("param_in");
        boost::to_upper(var1);
        string var2("param_out");
        boost::to_upper(var2);
        string var3("param2");
        boost::to_upper(var3);

        run_spworker->add_param_value_for_execution(var1, "3", EXPR_INT);
        run_spworker->add_param_value_for_execution(var3, "'abc'", EXPR_STRING);
        run_spworker->add_param_value_for_execution(var2, "out", EXPR_VAR);
        run_spworker->init_variable_for_execution();
        plan->session->set_call_store_procedure(true);
        run_spworker->exec_worker();
        run_spworker->clean_sp_worker_after_execution();
        plan->session->set_cur_sp_worker(NULL);

        ExecuteNode *node = plan->get_call_sp_return_ok_node();
        plan->set_start_node(node);
        break;
      }
#endif
      plan->session->set_is_complex_stmt(true);
      const char *schema_name =
          st.sql->call_oper->db_name ? st.sql->call_oper->db_name : schema;
      dataspace = backend->get_data_space_for_table(schema_name, NULL);

      const char *procedure_name = st.sql->call_oper->procedure_name;
      string sp_name(schema_name);
      sp_name.append(".");
      sp_name.append(procedure_name);

      if (!enable_oracle_sequence &&
          ((dataspace->get_dataspace_type() == SCHEMA_TYPE &&
            ((Schema *)dataspace)->get_is_schema_pushdown_stored_procedure()) ||
           dataspace->get_dataspace_type() == CATALOG_TYPE)) {
#ifdef DEBUG
        LOG_DEBUG("schema pushdown stored procedure directly for sp %s\n",
                  sp_name.c_str());
#endif
        plan->session->set_read_only(false);
        if (backend->get_read_only_procedures()) {
          if (backend->get_read_only_procedures()->count(sp_name)) {
            plan->session->set_read_only(true);
            LOG_DEBUG(
                "Procedure %s is configured as read-only-procedure, set "
                "statement read-only as true\n",
                sp_name.c_str());
          }
        }
        assemble_direct_exec_plan(plan, dataspace);
        break;
      }
#ifdef DEBUG
      bool is_schema_type = dataspace->get_dataspace_type() == SCHEMA_TYPE;
      LOG_DEBUG(
          "schema name is %s with type is schematype %d and pushdown procedure "
          "%d and enable-oracle-sequence %d.\n",
          schema_name, is_schema_type ? 1 : 0,
          is_schema_type ? (((Schema *)dataspace)
                                    ->get_is_schema_pushdown_stored_procedure()
                                ? 1
                                : 0)
                         : 0,
          enable_oracle_sequence);
#endif

      string sp_name_with_enclose("`");
      sp_name_with_enclose.append(schema_name);
      sp_name_with_enclose.append("`.`");
      sp_name_with_enclose.append(procedure_name);
      sp_name_with_enclose.append("`");

      string fake_sp_name = plan->session->get_next_nest_sp_name(sp_name);
      SPWorker *run_spworker = plan->session->get_sp_worker(fake_sp_name);

#ifndef CLOSE_PROCEDURE
      bool is_centralized_cluster = backend->is_centralized_cluster();
#ifdef DEBUG
      LOG_DEBUG(
          "is_centralized_cluster=%d with "
          "spworker %d\n",
          is_centralized_cluster, run_spworker ? 1 : 0);
#endif
      if (!is_centralized_cluster) {
        // Get spworker from session
        ProcedureMetaData *meta_data =
            backend->get_procedure_meta_data(sp_name);
        SPExecuteFlag execution_flag = EXECUTE_ON_DBSCALE;
        if (!run_spworker) {
          const char *create_sp_sql = NULL;
          vector<string> sp_vec;
          if (meta_data) {
            create_sp_sql = meta_data->procedure_string.c_str();
            execution_flag = meta_data->execution_flag;
          } else {
            fetch_create_procedure_sql(sp_name_with_enclose.c_str(), dataspace,
                                       &sp_vec);

            if (!sp_vec.empty()) {
              create_sp_sql = sp_vec[0].c_str();
            }
          }

          if (create_sp_sql && execution_flag == EXECUTE_ON_DBSCALE) {
            LOG_DEBUG("Get create procedure sql:[%s]\n", create_sp_sql);
            SPWorker *spworker = new SPWorker(fake_sp_name, plan->session);
            int extra_snippet_val =
                plan->session->get_extra_snippet_of_sp_var();
            spworker->set_extra_snippet_val(extra_snippet_val);
            init_sp_worker_from_create_sql(spworker, plan, create_sp_sql,
                                           schema_name, schema, sp_name);
            plan->session->add_sp_worker(spworker);
            if (!meta_data) {
              meta_data = new ProcedureMetaData(create_sp_sql,
                                                spworker->get_execution_flag(),
                                                spworker->get_data_space());
              if (!backend->add_procedure_meta_data(spworker->get_name(),
                                                    sp_name, meta_data)) {
                delete meta_data;
                meta_data = NULL;
              }
            }
            run_spworker = plan->session->get_sp_worker(fake_sp_name);
          }
        }

        if (run_spworker &&
            run_spworker->get_execution_flag() == EXECUTE_ON_DBSCALE) {
          run_spworker->set_is_for_call_store_procedure_directly(false);
#ifdef DEBUG
          LOG_DEBUG("SP %s can not be exec on one server.\n", sp_name.c_str());
#endif
          plan->session->get_status()->item_inc(
              TIMES_CALL_STMT_EXECUTE_ON_DBSCALE_SIDE);
          // try to add param value to spworker
          ListExpression *param_list =
              (ListExpression *)(st.sql->call_oper->param_list);
          list<const char *> values_list;
          list<expr_type> type_list;
          if (param_list) {
            expr_list_item *head = param_list->expr_list_head;
            expr_list_item *tmp = head;
            do {
              const char *expr_str = NULL;
              if (tmp->expr->type != EXPR_VAR)
                expr_str = expr_is_simple_value(tmp->expr);
              else if (tmp->expr->type == EXPR_VAR) {
                VarExpression *var_tmp = (VarExpression *)tmp->expr;
                if (var_tmp->var_scope == VAR_SCOPE_USER) {
                  expr_str = var_tmp->str_value;
                }
              }
              if (expr_str) {
                values_list.push_back(expr_str);
                type_list.push_back(tmp->expr->type);
              } else {
                throw NotSupportedError(
                    "Unsupport param value for store procedure execution.");
              }
              tmp = tmp->next;
            } while (tmp != head);
          }
          if (values_list.size() != run_spworker->get_param_list()->size()) {
            // TODO: thow sql error
            throw Error("Incorrect number of arguments for PROCEDURE.");
          }

          list<string> *sp_param_list = run_spworker->get_param_list();
          list<string>::iterator it = sp_param_list->begin();
          list<const char *>::iterator it2 = values_list.begin();
          list<expr_type>::iterator it3 = type_list.begin();
          for (; it != sp_param_list->end();) {
            run_spworker->add_param_value_for_execution(*it, *it2, *it3);
            ++it;
            ++it2;
            ++it3;
          }
          run_spworker->init_variable_for_execution();

          plan->session->set_call_store_procedure(true);
          try {
            plan->session->set_cur_sp_worker(run_spworker);
            plan->session->increase_call_sp_nest();
            plan->session->set_schema(schema_name);
            run_spworker->exec_worker();
            run_spworker->clean_sp_worker_after_execution();
            plan->session->set_schema(schema);
          } catch (...) {
            plan->session->decrease_call_sp_nest();
            run_spworker->clean_sp_worker_after_execution();
            plan->session->pop_cur_sp_worker_one_recurse();
            plan->session->set_schema(schema);
            throw;
          }

          ExecuteNode *node = plan->get_call_sp_return_ok_node();
          plan->set_start_node(node);

          break;
        }
        if (meta_data &&
            meta_data->execution_flag == EXECUTE_ON_ONE_DATASPACE) {
          dataspace = meta_data->data_space;
#ifdef DEBUG
          LOG_DEBUG("SP %s can be exec on one server.\n", sp_name.c_str());
#endif
        }
      }
#endif
      plan->session->set_read_only(false);
      if (backend->get_read_only_procedures()) {
        if (backend->get_read_only_procedures()->count(sp_name)) {
          plan->session->set_read_only(true);
          LOG_DEBUG(
              "Procedure %s is configured as read-only-procedure, set "
              "statement read-only as true\n",
              sp_name.c_str());
        }
      }
      set_call_store_procedure_directly(true);
      plan->session->increase_call_sp_nest();
      if (!run_spworker) {
        run_spworker = new SPWorker(fake_sp_name, plan->session);
        plan->session->add_sp_worker(run_spworker);
      }
      run_spworker->set_is_for_call_store_procedure_directly(true);
      plan->session->set_cur_sp_worker(run_spworker);
      assemble_direct_exec_plan(plan, dataspace);
    } break;
    case STMT_SHOW_WARNINGS: {
      assemble_show_warnings_plan(plan);
      break;
    }
    case STMT_DBSCALE_MESSAGE_SERVICE: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_ZEROMQ
      assemble_dbscale_message_service_plan(plan);
#else
      throw NotSupportedError("Not support dbscale service command");
#endif
      break;
    }
    case STMT_DBSCALE_CREATE_BINLOG_TASK: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_ZEROMQ
      assemble_dbscale_create_binlog_task_plan(plan);
#else
      throw NotSupportedError("Not support dbscale service command");
#endif
      break;
    }
    case STMT_DBSCALE_TASK: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_ZEROMQ
      assemble_dbscale_task_plan(plan);
#else
      throw NotSupportedError("Not support dbscale service command");
#endif
      break;
    }
    case STMT_DBSCALE_BINLOG_TASK_ADD_FILTER: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_ZEROMQ
      assemble_dbscale_binlog_task_add_filter_plan(plan);
#else
      throw NotSupportedError("Now support dbscale service command");
#endif
      break;
    }
    case STMT_DBSCALE_TASK_DROP_FILTER: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_ZEROMQ
      assemble_dbscale_drop_task_filter_plan(plan);
#else
      throw NotSupportedError("Now support dbscale service command");
#endif
      break;
    }
    case STMT_DBSCALE_DROP_TASK: {
      check_dbscale_management_acl(plan);
#ifndef CLOSE_ZEROMQ
      assemble_dbscale_drop_task_plan(plan);
#else
      throw NotSupportedError("Not support dbscale drop task command");
#endif
      break;
    }
    case STMT_DBSCALE_SHOW_CLIENT_TASK_STATUS: {
      check_dbscale_management_acl(plan, is_dbscale_read_only_user);
#ifndef CLOSE_ZEROMQ
      client_task_status_op_node *node = st.sql->client_task_status_oper;
      assemble_dbscale_show_client_task_status_plan(plan, node->task_name,
                                                    node->server_task_name);
#else
      throw NotSupportedError(
          "Not support dbscale show client task status command");
#endif
      break;
    }
    case STMT_DBSCALE_SHOW_SERVER_TASK_STATUS: {
      check_dbscale_management_acl(plan, is_dbscale_read_only_user);
#ifndef CLOSE_ZEROMQ
      server_task_status_op_node *node = st.sql->server_task_status_oper;
      assemble_dbscale_show_server_task_status_plan(plan, node->task_name);
#else
      throw NotSupportedError(
          "Not support dbscale show server task status command");
#endif
      break;
    }
    case STMT_DBSCALE_RESTART_SPARK_AGENT: {
      check_dbscale_management_acl(plan);
      assemble_dbscale_restart_spark_agent_node(plan);
      break;
    }
    case STMT_DBSCALE_SHOW_CREATE_ORACLE_SEQUENCE: {
      assemble_dbscale_show_create_oracle_seq_plan(plan);
      break;
    }
    case STMT_DBSCALE_SHOW_SEQUENCE_STATUS: {
      assemble_dbscale_show_seq_status_plan(plan);
      break;
    }
    case STMT_DBSCALE_SHOW_TRANSACTION_BLOCK_INFO: {
      dbscale_show_trx_block_info_node *node =
          st.sql->dbscale_show_trx_block_info_oper;
      assemble_dbscale_show_trx_block_info_plan(plan, (bool)node->is_local);
      break;
    }
    case STMT_DBSCALE_INTERNAL_SET: {
      assemble_dbscale_internal_set_plan(plan);
    } break;
    case STMT_DBSCALE_RESTORE_TABLE: {
      join_node *join = st.table_list_head->join;
      const char *schema_name = join->schema_name ? join->schema_name : schema;
      const char *table_name = join->table_name;
      assemble_dbscale_restore_table_plan(plan, schema_name, table_name);
      break;
    }
    case STMT_DBSCALE_CLEAN_RECYCLE_TABLE: {
      join_node *join = st.table_list_head->join;
      const char *schema_name = join->schema_name ? join->schema_name : schema;
      const char *table_name = join->table_name;
      assemble_dbscale_clean_recycle_table_plan(plan, schema_name, table_name);
      break;
    }
    case STMT_DBSCALE_SHOW_PREPARE_CACHE_STATUS: {
      assemble_dbscale_show_prepare_cache_status_plan(plan);
      break;
    }
    case STMT_DBSCALE_FLUSH_PREPARE_CACHE_INFO: {
      assemble_dbscale_flush_prepare_cache_hit_plan(plan);
      break;
    }
    case STMT_DBSCALE_BACKUP_START:
    case STMT_DBSCALE_BACKUP_END:
    case STMT_DBSCALE_RECOVERY_START:
    case STMT_DBSCALE_RECOVERY_END: {
      ExecuteNode *node = plan->get_return_ok_node();
      plan->set_start_node(node);
      break;
    }
    case STMT_DBSCALE_SHOW_FETCHNODE_BUFFER_USAGE: {
      assemble_dbscale_show_fetchnode_buffer_usage_plan(plan);
      break;
    }
    default:
      if (check_is_select_version_comment()) {
        /*The login operation will execute the stmt "select @@version_comment
         * limit 1", which should be directed to auth, in case the conn pool of
         * catalog is full.*/
        dataspace = backend->get_auth_data_space();
      } else {
        dataspace = backend->get_data_space_for_table(schema, NULL);
      }
      assemble_direct_exec_plan(plan, dataspace);
      return;
  }
}

void Statement::add_drop_partition_acl_check(ExecutePlan *plan,
                                             ACLType acl_type) {
  if (st.type == STMT_REVOKE) {
    map<string, map<string, pair<ssl_option_struct, string> > >
        tmp_user_info_map;
    Backend::instance()->get_white_user_info(tmp_user_info_map);
    user_clause_item_list *user_clause = st.sql->revoke_oper->user_clause;
    while (user_clause) {
      const char *user_id = user_clause->user_name;
      if (!tmp_user_info_map.count(user_id)) {
        string err_msg = "Can't find any matching row in the user table";
        LOG_ERROR("%s user name is [%s]\n", err_msg.c_str(), user_id);
        throw NotSupportedError(err_msg.c_str());
      }
      user_clause = user_clause->next;
    }
  } else if (st.type == STMT_GRANT) {
    string schema_name = "", full_table_name = "", user_name = "";
    map<string, map<string, TableACLType, strcasecomp> > table_acl_map;
    map<string, map<string, SchemaACLType, strcasecomp> > schema_acl_map;
    Backend::instance()->get_table_acl_info_all(&table_acl_map);
    Backend::instance()->get_schema_acl_info_all(&schema_acl_map);
    TableACLType tb_acl_type = ACL_TYPE_EMPTY;
    SchemaACLType db_acl_type = ACL_TYPE_EMPTY;
    grant_node *oper = st.sql->grant_oper;
    if (oper->user_clause->password != nullptr) {
      string schema_acl_str;
      get_acl_str(schema_acl_str, acl_type);
      string err_msg = "not allow create new user execute grant sql ";
      err_msg.append(schema_acl_str);
      LOG_ERROR("%s\n", err_msg.c_str());
      throw NotSupportedError(err_msg.c_str());
    }
    switch (oper->priv_level) {
      case GRANT_PRIV_LEVEL_STAR:
      case GRANT_PRIV_LEVEL_STAR_STAR:
      case GRANT_PRIV_LEVEL_A_STAR: {
        if (oper->priv_level == GRANT_PRIV_LEVEL_STAR) {
          schema_name = string(plan->session->get_schema());
        } else if (oper->priv_level == GRANT_PRIV_LEVEL_STAR_STAR) {
          schema_name = string(DBSCALE_RESERVED_STR);
        } else if (oper->priv_level == GRANT_PRIV_LEVEL_A_STAR) {
          schema_name = string(oper->priv_level_db_name);
          boost::replace_all(schema_name, "\\_", "_");
          boost::replace_all(schema_name, "\\%", "%");
        }
        user_name = oper->user_clause->user_name;
        break;
      }
      case GRANT_PRIV_LEVEL_A:
      case GRANT_PRIV_LEVEL_A_B: {
        if (oper->priv_level == GRANT_PRIV_LEVEL_A) {
          schema_name = string(plan->session->get_schema());
        } else {
          schema_name = string(oper->priv_level_db_name);
          boost::replace_all(schema_name, "\\_", "_");
          boost::replace_all(schema_name, "\\%", "%");
        }
        const char *table_name = oper->priv_level_tb_name;
        splice_full_table_name(schema_name.c_str(), table_name,
                               full_table_name);
        user_name = oper->user_clause->user_name;
        break;
      }
    }

    if (!(table_acl_map.count(user_name) || schema_acl_map.count(user_name))) {
      string err_msg = "Can't find any matching row in the user table";
      LOG_ERROR("%s user name is [%s]\n", err_msg.c_str(), user_name.c_str());
      throw NotSupportedError(err_msg.c_str());
    }
    if (table_acl_map.count(user_name)) {
      auto it = table_acl_map[user_name].find(full_table_name);
      if (it != table_acl_map[user_name].end()) tb_acl_type = it->second;
    }
    if (schema_acl_map.count(user_name)) {
      auto it = schema_acl_map[user_name].find(schema_name);
      if (it != schema_acl_map[user_name].end()) db_acl_type = it->second;
    }

    if (acl_type & ACL_TYPE_ADD_PARTITION ||
        acl_type & ACL_TYPE_DROP_PARTITION) {
      if (!(tb_acl_type & ACL_TYPE_ALTER || db_acl_type & ACL_TYPE_ALTER)) {
        string err_msg =
            "grant add partition/drop partition need authority ALTER ";
        LOG_ERROR("%s\n", err_msg.c_str());
        throw NotSupportedError(err_msg.c_str());
      }
    }
  }
}

bool Statement::check_is_select_version_comment() {
  if (st.type == STMT_SELECT && st.session_var_list && st.scanner->limit) {
    if (!st.session_var_list->next) {
      string var_name = st.session_var_list->value;
      boost::to_upper(var_name);
      if (!strcmp(var_name.c_str(), "VERSION_COMMENT")) {
        if (!strcasecmp(sql, "select @@version_comment limit 1")) return true;
      }
    }
  }
  return false;
}

#ifndef CLOSE_ZEROMQ
void Statement::assemble_dbscale_task_plan(ExecutePlan *plan) {
  ExecuteNode *task_node = plan->get_task_node();
  plan->set_start_node(task_node);
}

void Statement::assemble_dbscale_create_binlog_task_plan(ExecutePlan *plan) {
  ExecuteNode *binlog_task_node = plan->get_binlog_task_node();
  plan->set_start_node(binlog_task_node);
}

void Statement::assemble_dbscale_message_service_plan(ExecutePlan *plan) {
  ExecuteNode *message_service_node = plan->get_message_service_node();
  plan->set_start_node(message_service_node);
}

void Statement::assemble_dbscale_binlog_task_add_filter_plan(
    ExecutePlan *plan) {
  ExecuteNode *binlog_task_add_filter_node =
      plan->get_binlog_task_add_filter_node();
  plan->set_start_node(binlog_task_add_filter_node);
}

void Statement::assemble_dbscale_drop_task_filter_plan(ExecutePlan *plan) {
  ExecuteNode *drop_task_filter_node = plan->get_drop_task_filter_node();
  plan->set_start_node(drop_task_filter_node);
}

void Statement::assemble_dbscale_drop_task_plan(ExecutePlan *plan) {
  ExecuteNode *drop_task_node = plan->get_drop_task_node();
  plan->set_start_node(drop_task_node);
}

void Statement::assemble_dbscale_show_client_task_status_plan(
    ExecutePlan *plan, const char *task_name, const char *server_task_name) {
  ExecuteNode *show_client_task_status_node =
      plan->get_show_client_task_status_node(task_name, server_task_name);
  plan->set_start_node(show_client_task_status_node);
}

void Statement::assemble_dbscale_show_server_task_status_plan(
    ExecutePlan *plan, const char *task_name) {
  ExecuteNode *show_server_task_status_node =
      plan->get_show_server_task_status_node(task_name);
  plan->set_start_node(show_server_task_status_node);
}
#endif

void Statement::assemble_dbscale_restart_spark_agent_node(ExecutePlan *plan) {
#ifndef DBSCALE_DISABLE_SPARK
  plan->session->start_executing_spark();
  SparkReturnValue return_value = restart_spark_service();
  plan->session->stop_executing_spark();

  if (!return_value.success) {
    throw Error(return_value.error_string.c_str());
  }
#endif
  ExecuteNode *node = plan->get_return_ok_node();
  plan->set_start_node(node);
}

void Statement::assemble_keepmaster_plan(ExecutePlan *plan) {
  ExecuteNode *keepmaster_node = plan->get_keepmaster_node();
  plan->set_start_node(keepmaster_node);
}

void Statement::assemble_dbscale_test_plan(ExecutePlan *plan) {
  ExecuteNode *dbscale_test_node = plan->get_dbscale_test_node();
  plan->set_start_node(dbscale_test_node);
}

void Statement::assemble_async_task_control_plan(ExecutePlan *plan,
                                                 unsigned long long id) {
  ExecuteNode *async_task_node = plan->get_async_task_control_node(id);
  plan->set_start_node(async_task_node);
}

void Statement::assemble_load_dataspace_config_file_plan(ExecutePlan *plan,
                                                         const char *filename) {
  ExecuteNode *node = plan->get_load_dataspace_file_node(filename);
  plan->set_start_node(node);
}

void Statement::assemble_create_db_plan(ExecutePlan *plan) {
  LOG_DEBUG("assemble_create_db_plan\n");
  Backend *backend = Backend::instance();
  const char *db_name = NULL;
  if (st.type == STMT_DROP_DB || st.type == STMT_ALTER_DB) {
    if (st.type == STMT_DROP_DB) {
      db_name = st.sql->drop_db_oper->dbname->name;
    } else {
      db_name = st.sql->alter_db_oper->db_name;
    }
    if (dbscale_safe_sql_mode > 0 &&
        Backend::instance()->is_system_schema(db_name)) {
      throw Error(
          "Refuse drop/alter system schema for dbscale_safe_sql_mode>0");
    }
    if (!strcasecmp(db_name, default_login_schema)) {
      LOG_ERROR("default_login_schema can not be dropped/altered.\n");
      throw Error("default_login_schema can not be dropped/altered");
    }
  } else if (st.type == STMT_CREATE_DB) {
    db_name = st.sql->create_db_oper->dbname->name;
    DataSource *schema_source = NULL;
    if (!backend->find_schema(db_name)) {
      if (plan->session->get_session_option("auto_space_level").uint_val ==
          AUTO_SPACE_SCHEMA) {
        schema_source = backend->get_one_source_from_last_scheme();
      } else {
        schema_source = backend->get_catalog()->get_data_source();
      }
      /*Here will first assign the schema dataspace, even if the create
       * dataspace operation fail in the mysql_plan.cc, the assigned
       * dataspace will not be rollback.*/
      if (schema_source) {
        try {
          LOG_INFO("Assign source %s to schema space %s.\n",
                   schema_source->get_name(), db_name);
          backend->add_schema_space_for_multiple(
              db_name, schema_source->get_name(), NULL,
              PUSHDOWN_PROCEDURE_DEPEND, false);
        } catch (...) {
          LOG_ERROR("Auto assign source to schema %s fail.\n", db_name);
          throw;
        }
      }
    }
  }
  if (db_name != NULL) {
    if (st.type == STMT_DROP_DB) {
      vector<string> tables;
      backend->get_all_table_name_for_db(db_name, tables);
      vector<string>::iterator table_it = tables.begin();
      for (; table_it != tables.end(); ++table_it) {
        plan->session->add_in_using_table(db_name, (*table_it).c_str());
      }
    }
    vector<DataSpace *> dataspaces;
    backend->get_all_dataspace_for_database(db_name, dataspaces);

    DataSpace *auth_data_space = backend->get_auth_data_space();
#ifndef DBSCALE_TEST_DISABLE
    dbscale_test_info *test_info = plan->session->get_dbscale_test_info();
    if (!strcasecmp(test_info->test_case_name.c_str(),
                    "auth_source_restrict") &&
        !strcasecmp(test_info->test_case_operation.c_str(),
                    "send_create_drop_db_to_auth")) {
      dataspaces.push_back(auth_data_space);
      LOG_DEBUG("dbscale test create_db send to auth source.\n");
    }
#else
    if (restrict_auth_source_topo && !datasource_in_one)
      dataspaces.push_back(auth_data_space);
#endif

    set<DataSpace *> merge_db;
    if (dataspaces.size() == 0) {
      throw Error("The schema name can not get a dataspace.");
    } else {
      ConfigSourceRepResource dataservers_rep_relation;
      plan->session->get_local_dataservers_rep_relation(
          dataservers_rep_relation);
#ifdef DEBUG
      map<DataSource *, set<DataSource *> > resource_dbug =
          dataservers_rep_relation.get_resource();
      map<DataSource *, set<DataSource *> >::iterator it_dbug =
          resource_dbug.begin();
      LOG_DEBUG("start print dataservers_rep_relation\n");
      for (; it_dbug != resource_dbug.end(); ++it_dbug) {
        DataSource *source = it_dbug->first;
        set<DataSource *> &slaves = it_dbug->second;
        set<DataSource *>::iterator it2 = slaves.begin();
        for (; it2 != slaves.end(); ++it2) {
          LOG_DEBUG("The datasource is [%s%@], its child is [%s%@]\n",
                    source->get_name(), source, (*it2)->get_name(), *it2);
        }
      }
      LOG_DEBUG("finish print dataservers_rep_relation\n");
      for (size_t i = 0; i < dataspaces.size(); ++i) {
        DataSource *source = dataspaces[i]->get_data_source();
        string source_name = source ? source->get_name() : "";
        LOG_DEBUG(
            "get dataspace [%s%@] with datasource name [%s] to calc merge_db\n",
            dataspaces[i]->get_name(), dataspaces[i], source_name.c_str());
      }
#endif

      set<DataSpace *> merge_db_tmp_1;
      vector<DataSpace *> merge_db_tmp_2;
      if (!merge_db_tmp_1.count(dataspaces[0])) {
        merge_db_tmp_2.push_back(dataspaces[0]);
        merge_db_tmp_1.insert(dataspaces[0]);
      }
      for (unsigned int i = 1; i < dataspaces.size(); ++i) {
        for (unsigned int j = 0; j < merge_db_tmp_2.size(); ++j) {
          if (use_same_shard_schema(dataspaces[i], merge_db_tmp_2[j],
                                    dataservers_rep_relation)) {
            merge_db_tmp_2[j] = dataspaces[i];
          } else if (!use_same_shard_schema(merge_db_tmp_2[j], dataspaces[i],
                                            dataservers_rep_relation)) {
            if (!merge_db_tmp_1.count(dataspaces[i])) {
              merge_db_tmp_2.push_back(dataspaces[i]);
              merge_db_tmp_1.insert(dataspaces[i]);
            }
          }
        }
      }
      if (merge_db_tmp_2.size() == 0) {
        throw Error("The schema name can not get a dataspace.");
      }
      merge_db_tmp_1.clear();
      for (size_t i = 0; i < merge_db_tmp_2.size(); ++i) {
        merge_db_tmp_1.insert(merge_db_tmp_2[i]);  // erase duplicated items
      }
      // check all dataspace in merge_db_tmp_1, find and use its master if
      // possible
      map<DataSource *, set<DataSource *> > resource =
          dataservers_rep_relation.get_resource();
      set<DataSpace *>::iterator it2 = merge_db_tmp_1.begin();
      set<DataSource *> used_top_source;
      for (; it2 != merge_db_tmp_1.end(); ++it2) {
        if ((*it2)->get_virtual_machine_id() > 0) {
          merge_db.insert(*it2);
        } else {
          DataSource *source = (*it2)->get_data_source();
          map<DataSource *, set<DataSource *> >::iterator it = resource.begin();
          bool got_parent = false;
          for (; it != resource.end(); ++it) {
            if (it->second.count(source)) {
              got_parent = true;
              source = dataservers_rep_relation.get_real_top_source(it->first);
              used_top_source.insert(source);
              break;
            }
          }
          if (!got_parent) {
            merge_db.insert(*it2);
          }
        }
      }
      // for all top datasource we used, add its related dataspace in merge_db
      // but we should make sure no dup dataserver between 2 datasources, which
      // means that 2 datasource in fact share same write dataserver, such as a
      // replication datasource with its inner master-source datasource
      set<DataServer *> used_servers;
      set<DataSpace *>::iterator it_merge_db = merge_db.begin();
      for (; it_merge_db != merge_db.end(); ++it_merge_db) {
        DataServer *used_server =
            (*it_merge_db)->get_data_source()->get_master_server();
        used_servers.insert(used_server);
      }
      set<DataSource *>::iterator it_used_top_source = used_top_source.begin();
      for (; it_used_top_source != used_top_source.end();
           ++it_used_top_source) {
        DataSource *source = *it_used_top_source;
        DataServer *used_server = source->get_master_server();
        if (used_servers.count(used_server)) {
          LOG_DEBUG(
              "found datasource '%s' shard same write server with other top "
              "datasource in dataservers_rep_relation, ignore it\n",
              source->get_name());
          continue;
        }
        DataSpace *space = source->get_one_space();
        if (!space) {
          string msg = "no schema/table using datasource '";
          msg.append(source->get_name());
          msg.append(
              "' yet, please dynamic add at least one schema/table using that "
              "datasource firstly");
          throw Error(msg.c_str());
        }
        if (!merge_db.count(space)) {
          merge_db.insert(space);
          used_servers.insert(used_server);
        }
      }
    }
#ifdef DEBUG
    set<DataSpace *>::iterator it_dbug = merge_db.begin();
    for (; it_dbug != merge_db.end(); ++it_dbug) {
      DataSource *source = (*it_dbug)->get_data_source();
      string source_name = source ? source->get_name() : "";
      LOG_DEBUG("get merge_db dataspace [%s%@] with datasource name [%s]\n",
                (*it_dbug)->get_name(), *it_dbug, source_name.c_str());
    }
#endif
    ExecuteNode *ok_node = plan->get_ok_node();
    ExecuteNode *ok_merge_node = plan->get_ok_merge_node();
    ExecuteNode *modify_node;
    string new_sql_tmp;
    set<DataSpace *>::iterator it = merge_db.begin();
    bool need_apply_meta = true;
    for (; it != merge_db.end(); ++it) {
      DataSpace *space = *it;
      if (need_apply_meta) {
        if (is_share_same_server(
                space, Backend::instance()->get_metadata_data_space()))
          need_apply_meta = false;
      }
      if (space->get_virtual_machine_id() > 0) {
        if (st.type == STMT_DROP_DB)
          adjust_create_drop_db_sql_for_shard(space, db_name, new_sql_tmp, 0,
                                              sql);
        else if (st.type == STMT_CREATE_DB)
          adjust_create_drop_db_sql_for_shard(
              space, db_name, new_sql_tmp,
              st.sql->create_db_oper->db_name_end_pos, sql);
        else if (st.type == STMT_ALTER_DB)
          adjust_create_drop_db_sql_for_shard(
              space, db_name, new_sql_tmp,
              st.sql->alter_db_oper->db_name_end_pos, sql);
        record_shard_sql_str(new_sql_tmp);
        modify_node = plan->get_modify_node(space, get_last_shard_sql());
      } else {
        modify_node = plan->get_modify_node(space, sql);
      }
      ok_merge_node->add_child(modify_node);
    }
    st.need_apply_metadata = need_apply_meta;

    ok_node->add_child(ok_merge_node);
    plan->set_start_node(ok_node);
  }
}

void Statement::adjust_create_drop_db_sql_for_shard(DataSpace *space,
                                                    const char *sql_schema,
                                                    string &new_sql,
                                                    int db_name_end_pos,
                                                    const char *sql) {
  string new_schema;
  unsigned int vid = space->get_virtual_machine_id();
#ifdef DEBUG
  ACE_ASSERT(vid > 0);
#endif
  unsigned int pid = space->get_partition_id();
  stmt_type type = st.type;
  adjust_shard_schema(sql_schema, schema, new_schema, vid, pid);
  switch (type) {
    case STMT_DROP_DB: {
      if (st.sql->drop_db_oper->op_exists)
        new_sql.assign("DROP DATABASE IF EXISTS `");
      else
        new_sql.assign("DROP DATABASE `");
      break;
    }
    case STMT_CREATE_DB: {
      if (st.sql->create_db_oper->op_exists)
        new_sql.assign("CREATE DATABASE IF NOT EXISTS `");
      else
        new_sql.assign("CREATE DATABASE `");
      break;
    }
    case STMT_ALTER_DB: {
      new_sql.assign("ALTER DATABASE `");
      break;
    }
    default:
      throw Error("unsupport sql type for shard table schema adjust.");
  }
  new_sql.append(new_schema.c_str());
  new_sql.append("`");
  if (db_name_end_pos > 0) new_sql.append(&sql[db_name_end_pos]);
}

void Statement::assemble_migrate_plan(ExecutePlan *plan) {
  if (multiple_mode && !MultipleManager::instance()->get_is_cluster_master()) {
    LOG_ERROR("slave dbscale do not migrate table\n");
    throw NotSupportedError("slave dbscale do not support migrate table.");
  }

  if (!enable_block_table) {
    LOG_ERROR("migrate table failed, plz set option [enable-block-table=1]\n");
    throw Error("migrate table failed, plz set option [enable-block-table=1].");
  }

  ExecuteNode *node = plan->get_migrate_node();
  plan->session->set_is_complex_stmt(true);
  plan->set_start_node(node);
}
void Statement::assemble_lock_tables_plan(ExecutePlan *plan) {
  map<DataSpace *, list<lock_table_item *> *> *lock_table_list =
      new map<DataSpace *, list<lock_table_item *> *>;
  plan->session->remove_all_lock_tables();
  try {
    build_lock_map(plan, *lock_table_list);
  } catch (...) {
    map<DataSpace *, list<lock_table_item *> *>::iterator it;
    while (!lock_table_list->empty()) {
      it = lock_table_list->begin();
      list<lock_table_item *> *table_list = it->second;
      lock_table_list->erase(it);
      table_list->clear();
      delete table_list;
    }
    delete lock_table_list;
    throw;
  }
  ExecuteNode *lock_node = plan->get_lock_node(lock_table_list);
  plan->set_start_node(lock_node);
}
void Statement::build_lock_map(
    ExecutePlan *plan,
    map<DataSpace *, list<lock_table_item *> *> &lock_table_list) {
  lock_table_item *head = st.sql->lock_tb_oper->lock_tb_list;
  lock_table_item *item = head;
  Backend *backend = Backend::instance();
  string full_view;
  string sub_sql;
  do {
    lock_table_full_item *lock_item = plan->session->add_lock_table(item);

    full_view.assign(lock_item->schema_name);
    full_view.append(".");
    full_view.append(lock_item->table_name);
    sub_sql.clear();
    plan->session->get_one_view(full_view, sub_sql);
    if (!sub_sql.empty()) {
      LOG_DEBUG("Find a view for lock table stmt, %s.\n", full_view.c_str());
      if (item->type == LOCK_TYPE_WRITE) {
        LOG_ERROR("Not support lock view %s for write.\n", full_view.c_str());
        throw NotSupportedError("Not support lock view for write.");
      }
      DataSpace *meta = Backend::instance()->get_metadata_data_space();
      if (!meta) {
        LOG_ERROR("Fail to get meta datasource.\n");
        throw Error("Fail to get meta datasource.");
      }
      add_dataspace_to_map(lock_table_list, meta, item);
    } else {
      Table *table = backend->get_table_by_name(lock_item->schema_name,
                                                lock_item->table_name);
      if (table == NULL) {
        DataSpace *space = backend->get_data_space_for_table(
            lock_item->schema_name, lock_item->table_name);
        add_dataspace_to_map(lock_table_list, space, item);
        LOG_DEBUG("Add table %s to space %@.\n", item->name, space);
      } else if (table->is_partitioned()) {
        PartitionedTable *part_table = (PartitionedTable *)table;
        for (unsigned int i = 0; i < part_table->get_real_partition_num();
             ++i) {
          if (part_table->get_partition(i)->get_virtual_machine_id() > 0) {
            string new_table_tmp;
            adjust_shard_schema(
                lock_item->schema_name, schema, new_table_tmp,
                part_table->get_partition(i)->get_virtual_machine_id(),
                part_table->get_partition(i)->get_partition_id());
            new_table_tmp.append(".");
            new_table_tmp.append(lock_item->table_name);
            record_shard_sql_str(new_table_tmp);
            lock_table_item *new_item =
                (lock_table_item *)assign_mem_for_struct(
                    &st, sizeof(lock_table_item));
            new_item->type = item->type;
            new_item->name = get_last_shard_sql();
            new_item->alias = item->alias;
            add_dataspace_to_map(lock_table_list, part_table->get_partition(i),
                                 new_item);
          } else {
            add_dataspace_to_map(lock_table_list, part_table->get_partition(i),
                                 item);
          }
        }
      } else {
        add_dataspace_to_map(lock_table_list, table, item);
      }
    }
    item = item->next;
  } while (item != head);

  ConfigSourceRepResource server_rep_resource;
  stmt_session->get_local_dataservers_rep_relation(server_rep_resource);
  map<DataSpace *, list<lock_table_item *> *>::iterator it_ds1 =
      lock_table_list.begin();
  map<DataSpace *, list<lock_table_item *> *>::iterator it_ds2 =
      lock_table_list.begin();
  for (; it_ds1 != lock_table_list.end(); ++it_ds1) {
    it_ds2 = it_ds1;
    ++it_ds2;
    for (; it_ds2 != lock_table_list.end(); ++it_ds2) {
      if (it_ds1->first->get_virtual_machine_id() ==
          it_ds2->first->get_virtual_machine_id()) {
        if (is_dataspace_contain(it_ds1->first, it_ds2->first,
                                 server_rep_resource)) {
          it_ds1->second->insert(it_ds1->second->end(), it_ds2->second->begin(),
                                 it_ds2->second->end());
        } else if (is_dataspace_contain(it_ds2->first, it_ds1->first,
                                        server_rep_resource)) {
          it_ds2->second->insert(it_ds2->second->end(), it_ds1->second->begin(),
                                 it_ds1->second->end());
        }
      }
    }
    it_ds1->second->sort();
    it_ds1->second->unique();
  }
}
void Statement::add_dataspace_to_map(
    map<DataSpace *, list<lock_table_item *> *> &lock_table_list,
    DataSpace *space, lock_table_item *item) {
  if (lock_table_list.count(space)) {
    list<lock_table_item *> *table_list = lock_table_list[space];
    table_list->push_back(item);
  } else {
    map<DataSpace *, list<lock_table_item *> *>::iterator it =
        lock_table_list.begin();
    for (; it != lock_table_list.end(); ++it) {
      if (is_share_same_server(space, it->first)) {
        list<lock_table_item *> *table_list = it->second;
        table_list->push_back(item);
        return;
      }
    }
    list<lock_table_item *> *new_table_list = new list<lock_table_item *>();
    new_table_list->push_back(item);
    lock_table_list[space] = new_table_list;
  }
}
void Statement::assemble_transaction_unlock_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble transaction or unlock plan.\n");
  ExecuteNode *node = plan->get_transaction_unlock_node(sql, true);
  plan->set_start_node(node);
}

void Statement::assemble_xatransaction_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble xa transaction.\n");
  ExecuteNode *node = plan->get_xatransaction_node(sql, true);
  plan->set_start_node(node);
}
void Statement::assemble_cluster_xatransaction_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble cluster xa transaction.\n");
  ExecuteNode *node = plan->get_cluster_xatransaction_node();
  plan->set_start_node(node);
}
void Statement::assemble_dynamic_add_data_server_plan(
    ExecutePlan *plan,
    dynamic_add_data_server_op_node *dynamic_add_data_server_oper) {
  LOG_DEBUG("Assemble dynamic add data server plan.\n");
  ExecuteNode *node =
      plan->get_dynamic_add_data_server_node(dynamic_add_data_server_oper);
  plan->set_start_node(node);
}
void Statement::assemble_dynamic_add_data_source_plan(
    ExecutePlan *plan,
    dynamic_add_data_source_op_node *dynamic_add_data_source_oper, int type) {
  LOG_DEBUG("Assemble dynamic add data source plan.\n");
  DataSourceType stype = (DataSourceType)type;
  ExecuteNode *node = plan->get_dynamic_add_data_source_node(
      dynamic_add_data_source_oper, stype);
  plan->set_start_node(node);
}
void Statement::assemble_dynamic_add_data_space_plan(
    ExecutePlan *plan,
    dynamic_add_data_space_op_node *dynamic_add_data_space_oper) {
  LOG_DEBUG("Assemble add data space.\n");
  ExecuteNode *node =
      plan->get_dynamic_add_data_space_node(dynamic_add_data_space_oper);
  plan->set_start_node(node);
}
void Statement::assemble_set_user_not_allow_operation_time_plan(
    ExecutePlan *plan, set_user_not_allow_operation_time_node *oper) {
  LOG_DEBUG("Assemble set user allow operation time plan.\n");
  ExecuteNode *node = plan->get_set_user_not_allow_operation_time_node(oper);
  plan->set_start_node(node);
}
void Statement::assemble_reload_user_not_allow_operation_time_plan(
    ExecutePlan *plan, string user_name) {
  LOG_DEBUG("Assemble reload user allow operation time plan.\n");
  ExecuteNode *node =
      plan->get_reload_user_not_allow_operation_time_node(user_name);
  plan->set_start_node(node);
}
void Statement::assemble_show_user_not_allow_operation_time_plan(
    ExecutePlan *plan, string user_name) {
  LOG_DEBUG("Assemble show user allow operation time plan.\n");
  ExecuteNode *node =
      plan->get_show_user_not_allow_operation_time_node(user_name);
  plan->set_start_node(node);
}
void Statement::assemble_set_schema_acl_plan(
    ExecutePlan *plan, set_schema_acl_op_node *set_schema_acl_oper) {
  LOG_DEBUG("Assemble set schema ACL plan.\n");
  ExecuteNode *node = plan->get_set_schema_acl_node(set_schema_acl_oper);
  plan->set_start_node(node);
}
void Statement::assemble_set_table_acl_plan(
    ExecutePlan *plan, set_table_acl_op_node *set_table_acl_oper) {
  LOG_DEBUG("Assemble set table ACL plan.\n");
  ExecuteNode *node = plan->get_set_table_acl_node(set_table_acl_oper);
  plan->set_start_node(node);
}
void Statement::assemble_dynamic_add_slave_plan(
    ExecutePlan *plan, dynamic_add_slave_op_node *dynamic_add_slave_oper) {
  LOG_DEBUG("Assemble dynamic add slave plan.\n");
  ExecuteNode *node = plan->get_dynamic_add_slave_node(dynamic_add_slave_oper);
  plan->set_start_node(node);
}
void Statement::assemble_dynamic_add_pre_disaster_master_plan(
    ExecutePlan *plan, dynamic_add_pre_disaster_master_op_node
                           *dynamic_add_pre_disaster_master_oper) {
  LOG_DEBUG("Assemble dynamic add pre_disaster_master plan.\n");
  if (!enable_disaster_mode) {
    LOG_ERROR(
        "Dynamic add pre disaster master failed because enable_disaster_mode "
        "param is 1.\n");
    throw DynamicOpFail(
        "Dynamic add pre disaster master failed because enable_disaster_mode "
        "status is error.");
  }
  if (slave_dbscale_mode == 1) {
    LOG_ERROR(
        "Dynamic add pre disaster master failed because slave_dbscale_mode "
        "param is 1.\n");
    throw DynamicOpFail(
        "Dynamic add pre disaster master failed because slave_dbscale_mode "
        "status is error.");
  }

  const char *pre_disaster_master_hosts = pre_disaster_master_info.c_str();
  if (pre_disaster_master_hosts[0] == '\0') {
    LOG_ERROR(
        "Dynamic add pre disaster master failed because "
        "pre_disaster_master_hosts param is null.\n");
    throw DynamicOpFail(
        "Dynamic add pre disaster master failed because "
        "pre_disaster_master_hosts param is null.");
  } else {
    dynamic_add_pre_disaster_master_oper->add_server_op->server_host =
        pre_disaster_master_hosts;
    LOG_DEBUG(
        "Dynamic add pre disaster master when pre_disaster_master_hosts param "
        "is [%s]\n",
        pre_disaster_master_hosts);
  }

  dynamic_add_data_server_op_node *cur_server_op =
      dynamic_add_pre_disaster_master_oper->add_server_op;
  dynamic_add_data_source_op_node *cur_source_op =
      dynamic_add_pre_disaster_master_oper->add_source_op;
  dynamic_add_slave_op_node *cur_slave_op =
      dynamic_add_pre_disaster_master_oper->add_slave_op;

#ifndef DBSCALE_TEST_DISABLE
  /*just for test pre-disaster-master*/
  dbscale_test_info *test_info = Backend::instance()->get_dbscale_test_info();
  if (!strcasecmp(test_info->test_case_name.c_str(),
                  "pre_disaster_master_test")) {
    if (!strcasecmp(test_info->test_case_operation.c_str(), "datasource")) {
      cur_source_op->server_list->server_name = "pre_disaster_server_error";
    } else if (!strcasecmp(test_info->test_case_operation.c_str(), "slave")) {
      cur_slave_op->slave_info->slave_source = "pre_disaster_source_error";
    }
  }
#endif

  LOG_DEBUG(
      "master_source[%s], slave_source[%s], server_name[%s], "
      "pool-set[%d-%d-%d-%d], "
      "server_host[%s], server_port[%d], by_add_pre_disaster_master[%d]\n",
      cur_slave_op->master_name, cur_source_op->source_name,
      cur_server_op->server_name, cur_source_op->server_list->min,
      cur_source_op->server_list->max, cur_source_op->server_list->low,
      cur_source_op->server_list->high, cur_server_op->server_host,
      cur_server_op->server_port, cur_server_op->by_add_pre_disaster_master);

  try {
    ExecuteNode *add_server_node =
        plan->get_dynamic_add_data_server_node(cur_server_op);
    plan->set_start_node(add_server_node);
    plan->execute();
    plan->clean_start_node();
    ExecuteNode *add_source_node = plan->get_dynamic_add_data_source_node(
        cur_source_op, DATASOURCE_TYPE_SERVER);
    plan->set_start_node(add_source_node);
    plan->execute();
    plan->clean_start_node();
  } catch (ErrorPacketException &e) {
    LOG_ERROR(
        "Dynamic add pre disaster master failed because dynamic add dataserver "
        "or datasource failed.\n");
    throw;
  }

  ExecuteNode *add_slave_node = plan->get_dynamic_add_slave_node(cur_slave_op);
  plan->set_start_node(add_slave_node);
}  // namespace sql
void Statement::assemble_dbscale_reset_tmp_table_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble reset tmp table plan.\n");
  ExecuteNode *node = plan->get_reset_tmp_table_node();
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_reload_func_type_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble reload function type map plan.\n");
  ExecuteNode *node = plan->get_reload_function_type_node();
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_shutdown_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble shutdown plan.\n");
  ExecuteNode *node = plan->get_shutdown_node();
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_set_pool_plan(ExecutePlan *plan,
                                               pool_node *pool_info) {
  LOG_DEBUG("Assemble dynamic set pool plan.\n");
  if (multiple_mode && !MultipleManager::instance()->get_is_cluster_master() &&
      !pool_info->is_internal) {
    assemble_forward_master_role_plan(plan);
  } else {
    ExecuteNode *node = plan->get_dbscale_set_pool_info_node(pool_info);
    plan->set_start_node(node);
  }
}

void Statement::assemble_dbscale_reset_info_plan(
    ExecutePlan *plan, dbscale_reset_info_op_node *oper) {
  LOG_DEBUG("Assemble reset dbscale info plan.\n");
  if (multiple_mode && !MultipleManager::instance()->get_is_cluster_master() &&
      !oper->is_internal) {
    assemble_forward_master_role_plan(plan);
  } else {
    ExecuteNode *node = plan->get_dbscale_reset_info_plan(oper);
    plan->set_start_node(node);
  }
}

void Statement::assemble_dbscale_disable_server_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble dbscale disbale server plan.\n");
  if (multiple_mode && !MultipleManager::instance()->get_is_cluster_master()) {
    assemble_forward_master_role_plan(plan);
  } else {
    ExecuteNode *node = plan->get_dbscale_disable_server_node();
    plan->set_start_node(node);
  }
}
void Statement::assemble_dbscale_block_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble dbscale block plan.\n");

  if (multiple_mode && !MultipleManager::instance()->get_is_cluster_master()) {
    LOG_ERROR("slave dbscale do not support block table\n");
    throw NotSupportedError("slave dbscale do not support block table.");
  }
  if (!enable_block_table) {
    LOG_ERROR("block table failed, plz set option [enable-block-table=1]\n");
    throw Error("block table failed, plz set option [enable-block-table=1].");
  }
  ExecuteNode *node = plan->get_dbscale_block_node();
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_force_flashback_online_plan(
    ExecutePlan *plan) {
  const char *name = st.sql->flashback_oper->server_name;
  ExecuteNode *node = plan->get_dbscale_force_flashback_online_node(name);
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_xa_recover_slave_dbscale_plan(
    ExecutePlan *plan) {
  const char *xa_source =
      st.sql->dbscale_xa_recover_slave_dbscale_oper->xa_recover_source_name;
  const char *top_source =
      st.sql->dbscale_xa_recover_slave_dbscale_oper->top_source_name;
  const char *ka_update_v =
      st.sql->dbscale_xa_recover_slave_dbscale_oper->ka_update_version;
  ExecuteNode *node = plan->get_dbscale_xa_recover_slave_dbscale_node(
      xa_source, top_source, ka_update_v);
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_flush_pool_plan(ExecutePlan *plan,
                                                 pool_node *pool_info) {
  LOG_DEBUG("Assemble dbscale flush pool plan.\n");
  ExecuteNode *node = plan->get_dbscale_flush_pool_info_node(pool_info);
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_flush_plan(ExecutePlan *plan,
                                            dbscale_flush_type type) {
  LOG_DEBUG("Assemble dbscale flush plan.\n");
  ExecuteNode *node = plan->get_dbscale_flush_node(type);
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_flush_weak_pwd_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble dbscale flush weak password file plan.\n");
  ExecuteNode *node = plan->get_dbscale_flush_weak_pwd_node();
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_set_priority_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble dbscale set priority plan.\n");
  string user_name = get_stmt_node()->sql->priority_oper->user_name;
  int tmp_priority_value = get_stmt_node()->sql->priority_oper->priority_value;
  ExecuteNode *node =
      plan->get_dbscale_set_priority_info_node(user_name, tmp_priority_value);
  plan->set_start_node(node);
}
void Statement::assemble_dynamic_change_master_plan(
    ExecutePlan *plan,
    dynamic_change_master_op_node *dynamic_change_master_oper) {
  LOG_DEBUG("Assemble dynamic change master plan.\n");
  ExecuteNode *node =
      plan->get_dynamic_change_master_node(dynamic_change_master_oper);
  plan->set_start_node(node);
}
void Statement::assemble_dynamic_change_multiple_master_active_plan(
    ExecutePlan *plan, dynamic_change_multiple_master_active_op_node
                           *dynamic_change_multiple_master_active_oper) {
  LOG_DEBUG(
      "Assemble dynamic change multiple master datasource's active source "
      "plan.\n");
  ExecuteNode *node = plan->get_dynamic_change_multiple_master_active_node(
      dynamic_change_multiple_master_active_oper);
  plan->set_start_node(node);
}
void Statement::assemble_dynamic_change_dataserver_ssh_plan(
    ExecutePlan *plan, const char *server_name, const char *username,
    const char *pwd, int port) {
  LOG_DEBUG("Assemble dynamic change dataserver remote ssh plan.\n");
  ExecuteNode *node = plan->get_dynamic_change_dataserver_ssh_node(
      server_name, username, pwd, port);
  plan->set_start_node(node);
}
void Statement::assemble_dynamic_remove_slave_plan(
    ExecutePlan *plan,
    dynamic_remove_slave_op_node *dynamic_remove_slave_oper) {
  LOG_DEBUG("Assemble dynamic remove slave plan.\n");
  ExecuteNode *node =
      plan->get_dynamic_remove_slave_node(dynamic_remove_slave_oper);
  plan->set_start_node(node);
}
void Statement::assemble_dynamic_remove_schema_plan(ExecutePlan *plan,
                                                    const char *schema_name,
                                                    bool is_force) {
  LOG_DEBUG("Assemble dynamic remove sehema plan.\n");
  ExecuteNode *node =
      plan->get_dynamic_remove_schema_node(schema_name, is_force);
  plan->set_start_node(node);
}
void Statement::assemble_dynamic_remove_table_plan(ExecutePlan *plan,
                                                   const char *table_name,
                                                   bool is_force) {
  LOG_DEBUG("Assemble dynamic remove table plan.\n");
  ExecuteNode *node = plan->get_dynamic_remove_table_node(table_name, is_force);
  plan->set_start_node(node);
}
void Statement::assemble_dynamic_change_table_scheme_plan(
    ExecutePlan *plan, const char *table_name, const char *scheme_name) {
  LOG_DEBUG("Assemble dynamic change table scheme plan.\n");
  ExecuteNode *node =
      plan->get_dynamic_change_table_scheme_node(table_name, scheme_name);
  plan->set_start_node(node);
}
void Statement::assemble_dynamic_remove_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble dynamic remove plan.\n");
  const char *name = st.sql->dynamic_remove_oper->name;
  ExecuteNode *node = plan->get_dynamic_remove_node(name);
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_show_white_list(ExecutePlan *plan) {
  LOG_DEBUG("Assemble dbscale show white list plan\n");
  sql =
      "SELECT user_name, ip, ssl_option, ssl_cipher, x509_issuer, "
      "x509_subject, comment FROM dbscale.WHITE_USER_INFO order by user_name, "
      "ip";
  DataSpace *dataspace = Backend::instance()->get_config_data_space();
  if (!dataspace) {
    LOG_ERROR("Can not find the metadata dataspace.\n");
    throw Error(
        "Fail to get meta dataspace, please make sure meta datasource is "
        "configured.");
  }
  assemble_direct_exec_plan(plan, dataspace);
}
void Statement::assemble_dbscale_help_plan(ExecutePlan *plan,
                                           const char *name) {
  LOG_DEBUG("Assemble dbscale help plan");
  ExecuteNode *node = plan->get_dbscale_help_node(name);
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_show_join_plan(ExecutePlan *plan,
                                                const char *name) {
  LOG_DEBUG("Assemble dbscale show join status node.\n");
  ExecuteNode *node = plan->get_dbscale_show_join_node(name);
  plan->set_start_node(node);
}
void Statement::assemble_show_user_memory_status_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble show user memory status plan.\n");
  ExecuteNode *node = plan->get_show_user_memory_status_node();
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_show_base_status_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble dbscale show base status node.\n");
  ExecuteNode *node = plan->get_dbscale_show_base_status_node();
  plan->set_start_node(node);
}
void Statement::assemble_show_user_status_plan(
    ExecutePlan *plan, const char *user_id, const char *user_name,
    bool only_show_running, bool instance, bool show_status_count) {
  LOG_DEBUG("Assemble show user status plan.\n");
  ExecuteNode *node = plan->get_show_user_status_node(
      user_id, user_name, only_show_running, instance, show_status_count);
  plan->set_start_node(node);
}
void Statement::assemble_show_user_processlist_plan(ExecutePlan *plan,
                                                    const char *cluster_id,
                                                    const char *user_id,
                                                    int local) {
  LOG_DEBUG("Assemble show user processlist plan.\n");
  ExecuteNode *node =
      plan->get_show_user_processlist_node(cluster_id, user_id, local);
  plan->set_start_node(node);
}
void Statement::assemble_show_session_id_plan(ExecutePlan *plan,
                                              const char *server_name,
                                              int connection_id) {
  LOG_DEBUG("Assemble show session id by server and connection id plan.\n");
  Backend *backend = Backend::instance();
  DataServer *server = backend->find_data_server(server_name);
  if (server) {
    ExecuteNode *node =
        plan->get_show_session_id_node(server_name, connection_id);
    plan->set_start_node(node);
  } else {
    LOG_ERROR("assemble_show_session_id_plan Unknown DataServer name [%s]\n",
              server_name);
    throw Error("Unknown DataServer name.");
  }
}

void Statement::assemble_dbscale_execute_on_dataserver_plan(
    ExecutePlan *plan, const char *dataserver_name, const char *stmt_sql) {
  if (plan->session->is_in_transaction()) {
    LOG_ERROR(
        "current session is in transaction, can not execute this statement.\n");
    throw Error(
        "current session is in transaction, can not execute this statement");
  }
  Backend *backend = Backend::instance();
  Parser *parser = Driver::get_driver()->get_parser();
  Statement *stmt_tmp = NULL;
  try {
    stmt_tmp = parser->parse(stmt_sql, stmt_allow_dot_in_ident, true, NULL,
                             NULL, NULL, ctype);
  } catch (exception &e) {
    if (stmt_tmp) {
      stmt_tmp->free_resource();
      delete stmt_tmp;
      stmt_tmp = NULL;
    }
    LOG_ERROR("got error when parse dbscale execute on dataserver sql: %s\n",
              e.what());
    throw Error(e.what());
  }
  if (!stmt_tmp) {
    LOG_ERROR("Fail to parse sql [%s].\n", stmt_sql);
    throw Error("Fail to parse target sql.");
  }
  stmt_type type = stmt_tmp->get_stmt_node()->type;
  if (!(type > STMT_READ_ONLY_START && type < STMT_READ_ONLY_END) &&
      (allow_modify_server_directly == 0)) {
    stmt_tmp->free_resource();
    delete stmt_tmp;
    stmt_tmp = NULL;
    LOG_ERROR(
        "when allow_modify_server_directly=0, modify sql is not allowed via "
        "STMT_DBSCALE_EXECUTE_ON_DATASERVER, set "
        "allow_modify_server_directly=1 if you really want.\n");
    throw Error(
        "when allow_modify_server_directly=0, modify sql is not allowed via "
        "STMT_DBSCALE_EXECUTE_ON_DATASERVER, set "
        "allow_modify_server_directly=1 if you really want");
  }
  if (stmt_tmp->is_partial_parsed()) {
    stmt_tmp->free_resource();
    delete stmt_tmp;
    stmt_tmp = NULL;
    LOG_ERROR(
        "STMT_DBSCALE_EXECUTE_ON_DATASERVER not support partial parsed sql.\n");
    throw Error("not support partial parsed sql");
  }
  if (stmt_tmp->get_stmt_node()->next) {
    stmt_tmp->free_resource();
    delete stmt_tmp;
    stmt_tmp = NULL;
    LOG_ERROR("only support 1 statement at a time.\n");
    throw Error("only support 1 statement at a time");
  }
  stmt_tmp->free_resource();
  delete stmt_tmp;
  stmt_tmp = NULL;

  DataServer *server = backend->find_data_server(dataserver_name);
  if (!server) {
    LOG_ERROR("Unknown dataserver [%s]\n", dataserver_name);
    throw Error("unknown dataserver");
  }
  LOG_DEBUG("assemble_dbscale_execute_on_dataserver_plan.\n");

  list<DataSource *> server_sources;
  backend->find_data_source_by_type(DATASOURCE_TYPE_SERVER, server_sources);
  list<DataSource *>::iterator it = server_sources.begin();
  ServerDataSource *used_server_ds = NULL;
  for (; it != server_sources.end(); ++it) {
    ServerDataSource *sd = (ServerDataSource *)(*it);
    const char *server_name = sd->get_server()->get_name();
    if (!strcmp(dataserver_name, server_name)) {
      used_server_ds = sd;
      break;
    }
  }
  if (!used_server_ds) {
    LOG_ERROR("can not find server_datasource named [%s]\n", dataserver_name);
    throw Error("can not find related server datasource");
  }
  string space_name = dataserver_name;
  Schema *sch = NULL;
  do {
    space_name = DBSCALE_RESERVED_STR + space_name;
    sch = backend->find_schema(space_name.c_str());
  } while (sch != NULL);
  sch = new Schema(space_name.c_str(), used_server_ds,
                   backend->get_default_catalog());
  sch->set_need_force_conn(true);
  LOG_DEBUG(
      "Add tmp schema named [%s] to execute "
      "STMT_DBSCALE_EXECUTE_ON_DATASERVER, this schema will be desdroyed in "
      "DirectExecuteNode clean().\n",
      sch->get_name());
  plan->session->set_is_simple_direct_stmt(true);
  plan->set_need_destroy_dataspace_after_exec(true);
  ExecuteNode *node = plan->get_direct_execute_node((DataSpace *)sch, stmt_sql);
  plan->set_start_node(node);
}

void Statement::assemble_backend_server_execute_plan(ExecutePlan *plan,
                                                     const char *stmt_sql) {
  LOG_DEBUG("Assemble backend server execute plan.\n");
  ExecuteNode *node = plan->get_backend_server_execute_node(stmt_sql);
  plan->set_start_node(node);
}
void Statement::assemble_execute_on_all_masterserver_execute_plan(
    ExecutePlan *plan, const char *stmt_sql) {
  LOG_DEBUG("Assemble execute on all masterserver execute plan.\n");
  ExecuteNode *node =
      plan->get_execute_on_all_masterserver_execute_node(stmt_sql);
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_dynamic_update_white_plan(
    ExecutePlan *plan, bool is_add, const char *ip,
    const ssl_option_struct &ssl_option_value, const char *comment,
    const char *user_name) {
  LOG_DEBUG("assemble_dbscale_dynamic_update_white_plan.\n");
  ExecuteNode *node = plan->get_dynamic_update_white_node(
      is_add, ip, ssl_option_value, comment, user_name);
  plan->set_start_node(node);
}
void Statement::assemble_show_table_location_plan(ExecutePlan *plan,
                                                  const char *schema_name,
                                                  const char *table_name) {
  LOG_DEBUG("Assemble show table loction plan.\n");
  ExecuteNode *node =
      plan->get_show_table_location_node(schema_name, table_name);
  plan->set_start_node(node);
}
void Statement::assemble_show_monitor_point_status_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble show monitor point status plan.\n");
  ExecuteNode *node = plan->get_show_monitor_point_status_node();
  plan->set_start_node(node);
}
void Statement::assemble_show_global_monitor_point_status_plan(
    ExecutePlan *plan) {
  LOG_DEBUG("Assemble show global monitor point status plan.\n");
  ExecuteNode *node = plan->get_show_global_monitor_point_status_node();
  plan->set_start_node(node);
}
void Statement::assemble_show_histogram_monitor_point_status_plan(
    ExecutePlan *plan) {
  LOG_DEBUG("Assemble show histogram monitor point status plan.\n");
  ExecuteNode *node = plan->get_show_histogram_monitor_point_status_node();
  plan->set_start_node(node);
}

void Statement::assemble_dbscale_show_outline_monitor_info_plan(
    ExecutePlan *plan) {
  LOG_DEBUG("Assemble show outline monitor info.\n");
  ExecuteNode *node = plan->get_show_outline_monitor_info_node();
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_create_outline_hint_plan(
    ExecutePlan *plan, dbscale_operate_outline_hint_node *oper) {
  LOG_DEBUG("Assemble create outline hint info.\n");
  ExecuteNode *node = plan->get_dbscale_create_outline_hint_node(oper);
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_flush_outline_hint_plan(
    ExecutePlan *plan, dbscale_operate_outline_hint_node *oper) {
  LOG_DEBUG("Assemble flush outline hint info.\n");
  ExecuteNode *node = plan->get_dbscale_flush_outline_hint_node(oper);
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_show_outline_hint_plan(
    ExecutePlan *plan, dbscale_operate_outline_hint_node *oper) {
  LOG_DEBUG("Assemble show outline hint info.\n");
  ExecuteNode *node = plan->get_dbscale_show_outline_hint_node(oper);
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_delete_outline_hint_plan(
    ExecutePlan *plan, dbscale_operate_outline_hint_node *oper) {
  LOG_DEBUG("Assemble delete outline hint info.\n");
  ExecuteNode *node = plan->get_dbscale_delete_outline_hint_node(oper);
  plan->set_start_node(node);
}
void Statement::assemble_com_query_prepare_plan(ExecutePlan *plan,
                                                DataSpace *dataspace,
                                                const char *query_sql,
                                                const char *name) {
  LOG_DEBUG("Assemble COM_QUERY PREPARE plan.\n");
  ExecuteNode *node =
      plan->get_com_query_prepare_node(dataspace, query_sql, name, sql);
  plan->set_start_node(node);
}
void Statement::assemble_com_query_exec_prepare_plan(ExecutePlan *plan,
                                                     const char *name,
                                                     var_item *var_item_list) {
  LOG_DEBUG("Assemble COM_QUERY EXEC PREPARE plan.\n");
  ExecuteNode *node =
      plan->get_com_query_exec_prepare_node(name, var_item_list);
  plan->set_start_node(node);
}
void Statement::assemble_com_query_drop_prepare_plan(ExecutePlan *plan,
                                                     DataSpace *dataspace,
                                                     const char *name,
                                                     const char *sql) {
  LOG_DEBUG("Assemble COM_QUERY DROP PREPARE plan.\n");
  ExecuteNode *node =
      plan->get_com_query_drop_prepare_node(dataspace, name, sql);
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_flush_config_to_file_plan(
    ExecutePlan *plan, const char *file_name, bool flush_all) {
  LOG_DEBUG("Assemble DBSCALE FLUSH CONFIG TO FILE plan, flush_all is %d\n",
            flush_all);
  ExecuteNode *node =
      plan->get_dbscale_flush_config_to_file_node(file_name, flush_all);
  plan->set_start_node(node);
}

void Statement::assemble_dbscale_flush_acl_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble DBSCALE FLUSH ACL plan\n");
  ExecuteNode *node = plan->get_dbscale_flush_acl_node();
  plan->set_start_node(node);
}

void Statement::assemble_show_func_or_proc_status_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble function or procedure status plan.\n");
  Backend *backend = Backend::instance();
  Catalog *catalog = backend->get_default_catalog();
  map<string, Schema *, strcasecomp> schema_map_tmp;
  catalog->get_schema_map(schema_map_tmp);
  map<string, Schema *, strcasecomp> *schema_map = &schema_map_tmp;

  ExecuteNode *send_node = plan->get_send_node();
  list<const char *> *pattern1 = new list<const char *>();

  for (map<string, Schema *, strcasecomp>::iterator it = schema_map->begin();
       it != schema_map->end(); ++it) {
    pattern1->push_back(it->first.c_str());

    list<const char *> *pattern2 = new list<const char *>();
    pattern2->push_back(it->first.c_str());
    ExecuteNode *filter_node =
        plan->get_regex_filter_node(FILTER_FLAG_MATCH, 0, pattern2);
    const char *used_sql = adjust_stmt_sql_for_shard(it->second, sql);
    ExecuteNode *fetch_node = plan->get_fetch_node(it->second, used_sql);
    filter_node->add_child(fetch_node);
    send_node->add_child(filter_node);
  }

  ExecuteNode *filter_node =
      plan->get_regex_filter_node(FILTER_FLAG_UNMATCH, 0, pattern1);
  ExecuteNode *fetch_node = plan->get_fetch_node(catalog, sql);
  filter_node->add_child(fetch_node);
  send_node->add_child(filter_node);

  plan->set_start_node(send_node);
} /* assemble_show_func_or_proc_status_plan */

void Statement::assemble_dbscale_shutdown_plan(ExecutePlan *plan,
                                               int cluster_id) {
  check_is_multiple_mode();
  ExecuteNode *node = plan->get_dbscale_cluster_shutdown_node(cluster_id);
  plan->set_start_node(node);
}

void Statement::assemble_dbscale_request_cluster_id_plan(ExecutePlan *plan) {
  check_is_multiple_mode();
  if (MultipleManager::instance()->get_is_cluster_master()) {
    ExecuteNode *node = plan->get_dbscale_request_cluster_id_node();
    plan->set_start_node(node);
  } else {
    LOG_ERROR(
        "This dbscale is not cluster master, but get request get_cluster_id\n");
    throw Error(
        "This dbscale is not cluster master, but get request get_cluster_id");
  }
}

void Statement::assemble_dbscale_request_all_cluster_inc_info_plan(
    ExecutePlan *plan) {
  check_is_multiple_mode();
  ExecuteNode *node = plan->get_dbscale_request_all_cluster_inc_info_node();
  plan->set_start_node(node);
}

void Statement::assemble_dbscale_request_node_info_plan(ExecutePlan *plan) {
  check_is_multiple_mode();
  ExecuteNode *node = plan->get_dbscale_request_node_info_node();
  plan->set_start_node(node);
}

void Statement::assemble_dbscale_request_cluster_info_plan(ExecutePlan *plan) {
  check_is_multiple_mode();
  ExecuteNode *node = plan->get_dbscale_request_cluster_info_node();
  plan->set_start_node(node);
}

void Statement::assemble_dbscale_show_async_task_plan(ExecutePlan *plan) {
  ExecuteNode *node = plan->get_show_async_task_node();
  plan->set_start_node(node);
}

void Statement::assemble_dbscale_clean_temp_table_cache(ExecutePlan *plan) {
  LOG_DEBUG("Assemble clean temp table cache plan.\n");
  ExecuteNode *node = plan->get_clean_temp_table_cache_node();
  plan->set_start_node(node);
}

void Statement::assemble_dbscale_request_cluster_user_status_plan(
    ExecutePlan *plan, const char *id, bool only_show_running,
    bool show_status_count) {
  ExecuteNode *node = plan->get_dbscale_request_cluster_user_status_node(
      id, only_show_running, show_status_count);
  plan->set_start_node(node);
}

void Statement::check_is_multiple_mode() {
  if (!multiple_mode) {
    LOG_ERROR("the command is only support in multiple dbscale mode\n");
    throw Error("the command is only support in multiple dbscale mode");
  }
}

void Statement::assemble_dbscale_request_cluster_inc_info_plan(
    ExecutePlan *plan) {
  check_is_multiple_mode();
  if (!MultipleManager::instance()->get_is_cluster_master()) {
    LOG_ERROR(
        "This dbscale is not cluster master, but get request "
        "get_cluster_inc_info\n");
    throw Error(
        "This dbscale is not cluster master, but get request "
        "get_cluster_inc_info");
  }
  DataSpace *space = NULL;
  table_link *link = st.table_list_head;
  join_node *node = link->join;
  const char *schema_name = node->schema_name ? node->schema_name : schema;
  const char *table_name = node->table_name;
  Backend *backend = Backend::instance();
  space = backend->get_data_space_for_table(schema_name, table_name);
  if (space->get_dataspace_type() == TABLE_TYPE &&
      ((Table *)space)->is_partitioned()) {
    ExecuteNode *cluster_node = plan->get_dbscale_request_cluster_inc_info_node(
        (PartitionedTable *)space, schema_name, table_name);
    plan->set_start_node(cluster_node);
  } else {
    LOG_ERROR(
        "Not support get cluster Auto_increment value for normal table.\n");
    throw Error(
        "Not support get cluster Auto_increment value for normal table.");
  }
}
void Statement::assemble_show_table_status_plan(ExecutePlan *plan) {
  Backend *backend = Backend::instance();
  const char *schema_name;
  if (st.sql->show_tables_oper == NULL)
    schema_name = schema;
  else {
    schema_name = st.sql->show_tables_oper->db_name
                      ? st.sql->show_tables_oper->db_name
                      : schema;
  }

  int is_simple = st.sql->show_tables_oper->is_simple;

  if (st.sql->show_tables_oper->table_pattern) {
    string table_string(st.sql->show_tables_oper->table_pattern);
    unsigned first_dot_position = table_string.find(".");
    if (first_dot_position == (unsigned)string::npos) {
      table_string.assign(schema_name);
      table_string.append(".");
      table_string.append(st.sql->show_tables_oper->table_pattern);
    }
    string replace_str1_ori("\\_");
    string replace_str1_new("_");
    string replace_str2_ori("\\%");
    string replace_str2_new("%");

    replace_tmp_table_name(table_string, replace_str1_ori, replace_str1_new,
                           false);
    replace_tmp_table_name(table_string, replace_str2_ori, replace_str2_new,
                           false);

    LOG_DEBUG("show table status like get table name after repalce %s.\n",
              table_string.c_str());

    string sub_sql;
    plan->session->get_one_view(table_string, sub_sql);
    if (!sub_sql.empty()) {
      LOG_DEBUG("Find a view for show table status like stmt, %s.\n",
                table_string.c_str());
      DataSpace *dataspace = backend->get_metadata_data_space();
      if (!dataspace) {
        LOG_ERROR("Fail to find meta datasource.\n");
        throw Error("Fail to find meta datasource.");
      }
      assemble_direct_exec_plan(plan, dataspace);
      return;
    }
  }

  map<pair<DataSpace *, enum FilterFlag>, list<const char *> *> dataspaces;
  backend->get_dataspace_pattern_for_schema(schema_name, dataspaces);
  map<pair<DataSpace *, enum FilterFlag>, list<const char *> *>::iterator it =
      dataspaces.begin();
  if (!is_simple) {
    if (dataspaces.size() == 1) {
      assemble_direct_exec_plan(plan, (it->first).first);
      if (it->second) {
        it->second->clear();
        delete it->second;
      }
      return;
    } else {
      plan->session->set_execute_plan_touch_partition_nums(-1);
      list<AggregateDesc> aggregate_desc;
      struct AggregateDesc aggre_desc[8];
      aggre_desc[0].column_index = 4;
      aggre_desc[0].type = AGGREGATE_TYPE_SUM;
      aggre_desc[1].column_index = 6;
      aggre_desc[1].type = AGGREGATE_TYPE_SUM;
      aggre_desc[2].column_index = 7;
      aggre_desc[2].type = AGGREGATE_TYPE_MAX;
      aggre_desc[3].column_index = 8;
      aggre_desc[3].type = AGGREGATE_TYPE_SUM;
      aggre_desc[4].column_index = 9;
      aggre_desc[4].type = AGGREGATE_TYPE_SUM;
      aggre_desc[5].column_index = 11;
      aggre_desc[5].type = AGGREGATE_TYPE_MIN;
      aggre_desc[6].column_index = 12;
      aggre_desc[6].type = AGGREGATE_TYPE_MAX;
      aggre_desc[7].column_index = 13;
      aggre_desc[7].type = AGGREGATE_TYPE_MAX;
      for (int i = 0; i < 8; ++i) {
        aggregate_desc.push_back(aggre_desc[i]);
      }
      list<SortDesc> group;
      struct SortDesc sd;
      sd.column_index = 0;
      sd.sort_order = ORDER_ASC;
      sd.is_cs = false;
      sd.ctype = CHARSET_TYPE_OTHER;
      sd.is_utf8mb4_bin = false;
      group.push_back(sd);
      ExecuteNode *send_node = plan->get_send_node();
      ExecuteNode *fetch_node;
      ExecuteNode *filter_node;
      ExecuteNode *single_node;
      ExecuteNode *group_by_node = plan->get_group_node(&group, aggregate_desc);
      ExecuteNode *avg_show_table_status_node =
          plan->get_avg_show_table_status_node();
      for (; it != dataspaces.end(); ++it) {
        single_node = plan->get_single_sort_node(&group);
        filter_node =
            plan->get_regex_filter_node((it->first).second, 0, it->second);
        const char *used_sql = adjust_stmt_sql_for_shard_for_show_table_status(
            (it->first).first, sql);
        fetch_node = plan->get_fetch_node((it->first).first, used_sql);
        filter_node->add_child(fetch_node);
        single_node->add_child(filter_node);
        group_by_node->add_child(single_node);
      }
      avg_show_table_status_node->add_child(group_by_node);
      send_node->add_child(avg_show_table_status_node);
      plan->set_start_node(send_node);
    }
  } else {
    plan->session->set_execute_plan_touch_partition_nums(-1);
    list<AggregateDesc> aggregate_desc;
    struct AggregateDesc aggre_desc[3];
    aggre_desc[0].column_index = 4;
    aggre_desc[0].type = AGGREGATE_TYPE_SUM;
    aggre_desc[1].column_index = 6;
    aggre_desc[1].type = AGGREGATE_TYPE_SUM;
    aggre_desc[2].column_index = 7;
    aggre_desc[2].type = AGGREGATE_TYPE_MAX;
    for (int i = 0; i < 3; ++i) {
      aggregate_desc.push_back(aggre_desc[i]);
    }
    list<SortDesc> group;
    struct SortDesc sd;
    sd.column_index = 0;
    sd.sort_order = ORDER_ASC;
    sd.is_cs = false;
    sd.ctype = CHARSET_TYPE_OTHER;
    sd.is_utf8mb4_bin = false;
    group.push_back(sd);
    ExecuteNode *send_node = plan->get_send_node();
    ExecuteNode *fetch_node;
    ExecuteNode *filter_node;
    ExecuteNode *single_node;
    ExecuteNode *group_by_node = plan->get_group_node(&group, aggregate_desc);
    ExecuteNode *avg_show_table_status_node =
        plan->get_avg_show_table_status_node();
    string no_simple_sql(sql, st.sql->show_tables_oper->simple_start_pos - 1);
    record_statement_stored_string(no_simple_sql);
    const char *no_simple_sql_str = get_last_statement_stored_string();

    for (; it != dataspaces.end(); ++it) {
      single_node = plan->get_single_sort_node(&group);
      filter_node =
          plan->get_regex_filter_node((it->first).second, 0, it->second);
      const char *used_sql = adjust_stmt_sql_for_shard_for_show_table_status(
          (it->first).first, no_simple_sql_str);
      fetch_node = plan->get_fetch_node((it->first).first, used_sql);
      filter_node->add_child(fetch_node);
      single_node->add_child(filter_node);
      group_by_node->add_child(single_node);
    }
    avg_show_table_status_node->add_child(group_by_node);
    send_node->add_child(avg_show_table_status_node);
    plan->set_start_node(send_node);
  }
}

const char *Statement::adjust_stmt_sql_for_shard_for_show_table_status(
    DataSpace *ds, const char *old_sql) {
  string new_sql_tmp(old_sql);
  const char *used_sql = old_sql;
  if (ds->get_virtual_machine_id() > 0) {
    const char *schema_name = st.sql->show_tables_oper->db_name
                                  ? st.sql->show_tables_oper->db_name
                                  : schema;
    string new_schema_tmp;
    adjust_shard_schema(schema_name, schema_name, new_schema_tmp,
                        ds->get_virtual_machine_id(), ds->get_partition_id());
    int start_pos = 0;
    int end_pos = 0;
    if (st.sql->show_tables_oper->db_start_pos) {
      start_pos = st.sql->show_tables_oper->db_start_pos;
      start_pos = new_sql_tmp.find(" ", start_pos);
      end_pos = st.sql->show_tables_oper->db_end_pos;
      string new_schema_tmp_with_space(" `");
      new_schema_tmp_with_space.append(new_schema_tmp);
      new_schema_tmp_with_space.append("` ");
      new_sql_tmp.replace(start_pos, end_pos - start_pos + 1,
                          new_schema_tmp_with_space);
    } else if (st.sql->show_tables_oper->table_pattern_start_pos) {
      start_pos = st.sql->show_tables_oper->table_pattern_start_pos - 2;
      string new_schema_tmp_with_space(" FROM `");
      new_schema_tmp_with_space.append(new_schema_tmp);
      new_schema_tmp_with_space.append("` ");
      new_sql_tmp.replace(start_pos, 1, new_schema_tmp_with_space);
    } else if (st.scanner->where_pos) {
      start_pos = st.scanner->where_pos - 2;
      string new_schema_tmp_with_space(" FROM `");
      new_schema_tmp_with_space.append(new_schema_tmp);
      new_schema_tmp_with_space.append("` ");
      new_sql_tmp.replace(start_pos, 1, new_schema_tmp_with_space);
    } else {
      new_sql_tmp.append(" FROM `");
      new_sql_tmp.append(new_schema_tmp);
      new_sql_tmp.append("`");
    }

    LOG_DEBUG("After replace shard schema, the sql is [%s]\n",
              new_sql_tmp.c_str());
    record_shard_sql_str(new_sql_tmp);
    used_sql = get_last_shard_sql();
  }
  return used_sql;
}

void Statement::assemble_show_warnings_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble show warnings plan.\n");

  if (!(plan->session->get_load_warning_packets()->empty())) {
    // previous load data got warnings
    ExecuteNode *show_load_warning_node = plan->get_show_load_warning_node();
    plan->set_start_node(show_load_warning_node);
    return;
  }

  set<DataSpace *> warning_spaces = plan->session->get_warning_dataspaces();

  if (warning_spaces.empty()) {
    /*There is no kept warning space,
     *if session contains warnings, return warnings
     *else if session load_warning_packet_list is not empty, return those
     *warning packets else return an empty result.*/
    ExecuteNode *show_warning_node = plan->get_show_warning_node();
    plan->set_start_node(show_warning_node);
    return;
  }

  ExecuteNode *send_node = plan->get_send_node();
  set<DataSpace *>::iterator it = warning_spaces.begin();

  ExecuteNode *fetch_node = NULL;
  for (; it != warning_spaces.end(); ++it) {
    LOG_DEBUG("Use warnig space %s to assemble fetch node.\n",
              (*it)->get_name());
    fetch_node = plan->get_fetch_node(*it, sql);
    send_node->add_child(fetch_node);
  }

  plan->set_start_node(send_node);
}

void Statement::assemble_show_events_plan(ExecutePlan *plan) {
  DataSpace *dataspace = NULL;
  Backend *backend = Backend::instance();
  const char *event_schema = schema;
  if (get_stmt_node()->sql->show_event_oper &&
      get_stmt_node()->sql->show_event_oper->schema) {
    event_schema = get_stmt_node()->sql->show_event_oper->schema;
  }
  dataspace = backend->get_data_space_for_table(event_schema, NULL);
  assemble_direct_exec_plan(plan, dataspace);
}

void Statement::assemble_show_databases_plan(ExecutePlan *plan) {
  Backend *backend = Backend::instance();
  DataSpace *dataspace = backend->get_auth_data_space();
#ifdef DEBUG
  ACE_ASSERT(dataspace != NULL);
#endif
  assemble_direct_exec_plan(plan, dataspace);
}
void Statement::authenticate_db(const char *dbname, ExecutePlan *plan,
                                stmt_type st_type) {
  string dbname_string = string(dbname);
  if ((st_type != STMT_CREATE_DB && st_type != STMT_DROP_DB) &&
      !plan->session->is_contain_schema(dbname_string)) {
    Backend::instance()->record_latest_important_dbscale_warning(
        "authenticate_db does not contain [%s] for user [%s]\n", dbname,
        plan->session->get_username());
    string err_msg = "Access denied for user [";
    err_msg.append(plan->session->get_username());
    err_msg.append("] to database [");
    err_msg.append(dbname);
    err_msg.append("], session auth schema does not contain target db");
    throw dbscale::sql::SQLError(err_msg.c_str(), "42000",
                                 ERROR_AUTH_DENIED_CODE);
  }

  if (st_type == STMT_CHANGE_DB) {
    return;
  }
  SchemaACLType db_acl_type = plan->session->get_schema_acl_type(dbname);
  SchemaACLType global_acl_type =
      plan->session->get_schema_acl_type(DBSCALE_RESERVED_STR);
  vector<ACLType> acl_type_vec;
  acl_type_vec.push_back((ACLType)global_acl_type);
  acl_type_vec.push_back((ACLType)db_acl_type);
  if (!is_stmt_acl_check_ok(st_type, acl_type_vec)) {
    string err_msg = "Access denied for user [";
    err_msg.append(plan->session->get_user_name());
    err_msg.append("@");
    err_msg.append(plan->session->get_client_ip());
    err_msg.append("] to database [");
    err_msg.append(dbname);
    err_msg.append("]");
    Backend::instance()->record_latest_important_dbscale_warning(
        err_msg.c_str());
    throw dbscale::sql::SQLError(err_msg.c_str(), "42000",
                                 ERROR_AUTH_DENIED_CODE);
  }
}

void Statement::authenticate_global_priv(ExecutePlan *plan, stmt_type st_type) {
  SchemaACLType global_acl_type =
      plan->session->get_schema_acl_type(DBSCALE_RESERVED_STR);
  vector<ACLType> acl_type_vec;
  acl_type_vec.push_back((ACLType)global_acl_type);
  if (!is_stmt_acl_check_ok(st_type, acl_type_vec, true)) {
    string err_msg = "Access denied for user [";
    err_msg.append(plan->session->get_user_name());
    err_msg.append("@");
    err_msg.append(plan->session->get_client_ip());
    err_msg.append("]");
    Backend::instance()->record_latest_important_dbscale_warning(
        err_msg.c_str());
    throw dbscale::sql::SQLError(err_msg.c_str(), "42000",
                                 ERROR_AUTH_DENIED_CODE);
  }
}

void Statement::authenticate_table(ExecutePlan *plan, const char *schema_name,
                                   const char *table_name, stmt_type st_type,
                                   bool is_sub_select) {
  ACE_UNUSED_ARG(is_sub_select);
  SchemaACLType global_acl_type =
      plan->session->get_schema_acl_type(DBSCALE_RESERVED_STR);
  if ((st_type == STMT_SELECT || st_type == STMT_SUB_SELECT) &&
      !strcasecmp(schema_name, "information_schema")) {
    if (is_information_schema_table_can_always_select(table_name)) {
      return;
    }
    if (is_information_schema_table_select_priv_check_ok(global_acl_type)) {
      return;
    }
    string err_msg = "Access denied for user [";
    err_msg.append(plan->session->get_user_name());
    err_msg.append("@");
    err_msg.append(plan->session->get_client_ip());
    err_msg.append("] to table [information_schema.");
    err_msg.append(table_name);
    err_msg.append("]");
    Backend::instance()->record_latest_important_dbscale_warning(
        err_msg.c_str());
    throw dbscale::sql::SQLError(err_msg.c_str(), "42000",
                                 ERROR_AUTH_DENIED_CODE);
  }
  if ((st_type == STMT_SELECT || st_type == STMT_SUB_SELECT) &&
      !strcasecmp(schema_name, "performance_schema") &&
      (!strcasecmp(table_name, "session_variables") ||
       !strcasecmp(table_name, "session_status") ||
       !strcasecmp(table_name, "session_account_connect_attrs") ||
       !strcasecmp(table_name, "global_status") ||
       !strcasecmp(table_name, "global_variables") ||
       !strcasecmp(table_name, "persisted_variables") ||
       !strcasecmp(table_name, "processlist") ||
       !strcasecmp(table_name, "variables_info"))) {
    return;
  }
  string full_table_name;
  splice_full_table_name(schema_name, table_name, full_table_name);
  TableACLType tb_acl_type = plan->session->get_table_acl_type(full_table_name);
  SchemaACLType db_acl_type = plan->session->get_schema_acl_type(schema_name);
  vector<ACLType> acl_type_vec;
  acl_type_vec.push_back((ACLType)global_acl_type);
  acl_type_vec.push_back((ACLType)db_acl_type);
  acl_type_vec.push_back((ACLType)tb_acl_type);
  if (!is_stmt_acl_check_ok(st_type, acl_type_vec)) {
    string err_msg = "Access denied for user [";
    err_msg.append(plan->session->get_user_name());
    err_msg.append("@");
    err_msg.append(plan->session->get_client_ip());
    err_msg.append("] to table [");
    err_msg.append(full_table_name);
    err_msg.append("]");
    Backend::instance()->record_latest_important_dbscale_warning(
        err_msg.c_str());
    throw dbscale::sql::SQLError(err_msg.c_str(), "42000",
                                 ERROR_AUTH_DENIED_CODE);
  }
}

void Statement::authenticate_statement(ExecutePlan *plan) {
  const char *db_name = NULL;
  stmt_type type = st.type;

  // check user authority
  if (is_stmt_mysql_acl_type_can_always_execute(type)) {
    return;
  }
  if (is_stmt_mysql_acl_type_not_clear(type)) {
    if (dbscale_acl_strict_mode == 0) {
      return;
    } else {
      string err_msg = "Access denied for user [";
      err_msg.append(plan->session->get_user_name());
      err_msg.append("@");
      err_msg.append(plan->session->get_client_ip());
      err_msg.append(
          "] to execute this type of statement, because option "
          "dbscale-acl-strict-mode > 0");
      Backend::instance()->record_latest_important_dbscale_warning(
          err_msg.c_str());
      throw dbscale::sql::SQLError(err_msg.c_str(), "42000",
                                   ERROR_AUTH_DENIED_CODE);
    }
  }
  if (is_dbscale_command(type)) {
    return;
  }

  switch (type) {
    case STMT_CREATE_DB: {
      db_name = st.sql->create_db_oper->dbname->name;
      authenticate_db(db_name, plan, STMT_CREATE_DB);
    } break;
    case STMT_DROP_DB: {
      db_name = st.sql->drop_db_oper->dbname->name;
      authenticate_db(db_name, plan, STMT_DROP_DB);
    } break;
    case STMT_ALTER_DB: {
      db_name = st.sql->alter_db_oper->db_name;
      authenticate_db(db_name, plan, STMT_ALTER_DB);
    } break;
    case STMT_CHANGE_DB: {
      db_name = st.sql->change_db_oper->dbname->name;
      authenticate_db(db_name, plan, STMT_CHANGE_DB);
    } break;
    case STMT_REPLACE_SELECT:
    case STMT_INSERT_SELECT:
    case STMT_CREATE_SELECT:
    case STMT_CREATE_LIKE: {
      table_link *table = st.table_list_head;
      if (table) {
        const char *schema_name =
            table->join->schema_name ? table->join->schema_name : schema;
        const char *table_name = table->join->table_name;
        if (type == STMT_CREATE_SELECT || type == STMT_CREATE_LIKE) {
          authenticate_table(plan, schema_name, table_name, STMT_CREATE_TB,
                             false);
        } else {
          authenticate_table(plan, schema_name, table_name, STMT_INSERT, false);
        }
        table = table->next;
        if (table) {
          schema_name =
              table->join->schema_name ? table->join->schema_name : schema;
          table_name = table->join->table_name;
          authenticate_table(plan, schema_name, table_name, STMT_SELECT, true);
        }
      }
    } break;
    default: {
      table_link *table = st.table_list_head;
      if (table) {
        while (table) {
          const char *schema_name =
              table->join->schema_name ? table->join->schema_name : schema;
          const char *table_name = table->join->table_name;
          bool is_sub_select = table->join->cur_rec_scan->upper ? true : false;
          if (st.type == STMT_ALTER_TABLE &&
              st.sql->alter_tb_oper->alter_type == ADD_PARTITION)
            st.type = STMT_ALTER_ADD_PARTITION;
          else if (st.type == STMT_ALTER_TABLE &&
                   st.sql->alter_tb_oper->alter_type == DROP_PARTITION)
            st.type = STMT_ALTER_DROP_PARTITION;
          authenticate_table(plan, schema_name, table_name, st.type,
                             is_sub_select);
          if (st.type == STMT_ALTER_ADD_PARTITION ||
              st.type == STMT_ALTER_DROP_PARTITION)
            st.type = STMT_ALTER_TABLE;
          table = table->next;
        }
      } else {
        try {
          authenticate_db(schema, plan, st.type);
        } catch (dbscale::sql::SQLError &e) {
          authenticate_global_priv(plan, st.type);
        }
      }
    } break;
  }
}
void Statement::deal_lock_table(ExecutePlan *plan) {
  stmt_type type = st.type;
  switch (type) {
    case STMT_LOCK_TB:
    case STMT_SHOW_CREATE_TABLE:
    case STMT_SHOW_INDEX:
    case STMT_TABLE_MAINTENANCE:
    case STMT_SHOW_FIELDS:
    case STMT_DBSCALE_SHOW_PARTITIONS:
      return;
    case STMT_CREATE_DB:
    case STMT_DROP_DB:
    case STMT_CREATE_VIEW:
    case STMT_RENAME_TABLE:
      throw dbscale::sql::SQLError(
          "Can't execute the given command because "
          "you have active locked tables or an active transaction",
          "HY000", ERROR_NOT_EXECUTE_FOR_LOCK_CODE);
      break;
    default: {
      if (!st.table_list_head) break;
      table_link *link = st.table_list_head;
      join_node *node = NULL;
      plan->session->reset_lock_table_state();
      while (link) {
        node = link->join;
        if ((node->schema_name &&
             strcasecmp(node->schema_name, "INFORMATION_SCHEMA") == 0) ||
            (strlen(schema) > 0 &&
             strcasecmp(schema, "INFORMATION_SCHEMA") == 0)) {
          link = link->next;
          continue;
        }
        if (!plan->session->contains_lock_table(
                node->table_name, node->schema_name, node->alias)) {
          string err_msg = "Table '";
          err_msg += node->alias ? node->alias : node->table_name;
          err_msg += "' was not locked with LOCK TABLES";
          throw dbscale::sql::SQLError(err_msg.c_str(), "HY000",
                                       ERROR_NOT_LOCK_CODE);
        }
        link = link->next;
      }
      break;
    }
  }
}

void Statement::validate_into_outfile() {
  into_outfile_item *into_outfile = st.into_outfile;
  // for filename and the 5 fileds and lines separator,
  // the string format should be "abc" or 'abc',
  // so, here can use the first char to judge it
  char filename_c = sql[into_outfile->filename_pos - 1];
  if (filename_c != '\'' && filename_c != '\"') {
    throw NotSupportedError("Error format of filename.");
  }

  if (strlen(into_outfile->fields_enclosed) > 1 ||
      strlen(into_outfile->fields_escaped) > 1) {
    throw dbscale::sql::SQLError(
        "Field separator argument is not what is expected; "
        "check the manual",
        "42000", 1083);
  }

  if (into_outfile->fields_term_len == 0) {
    LOG_ERROR(
        "Fields separator argument of FIELDS TERMINATED BY can not be ''.\n");
    throw NotSupportedError(
        "Fields separator argument of FIELDS TERMINATED BY can not be ''.");
  }

  bool has_null = is_terminator_has_null(into_outfile->fields_term,
                                         into_outfile->fields_term_len) ||
                  is_terminator_has_null(into_outfile->fields_escaped,
                                         into_outfile->fields_escaped_len) ||
                  is_terminator_has_null(into_outfile->fields_enclosed,
                                         into_outfile->fields_enclosed_len) ||
                  is_terminator_has_null(into_outfile->lines_start,
                                         into_outfile->lines_start_len) ||
                  is_terminator_has_null(into_outfile->lines_term,
                                         into_outfile->lines_term_len);
  if (has_null) {
    LOG_ERROR("Fields separator argument can not contain '\\0'.\n");
    throw NotSupportedError("Fields separator argument can not contain '\\0'.");
  }

  if (into_outfile->is_local) {
    // in local mode, the last char of LINES TERMINATED BY should be '\n'
    if (into_outfile->lines_term[into_outfile->lines_term_len - 1] != '\n') {
      LOG_ERROR("The last character of INTO OUTFILE LOCAL should be '\\n'.\n");
      const char *err_msg =
          "The last character of INTO OUTFILE LOCAL should be '\\n'";
      throw Error(err_msg);
    }
  } else {
    // in non-local mode, it will write file on server side, it should check
    // file exists
    const char *filename = into_outfile->filename;
    fstream file(filename);
    if (file) {
      LOG_ERROR("into outfile filename already exist.\n");
      string err_msg = "File '";
      err_msg += filename;
      err_msg += "' already exists";
      throw dbscale::sql::SQLError(err_msg.c_str(), "HY000", ERROR_FILE_EXIST);
    }
    // check file can write
    ofstream file1;
    file1.open(filename);
    if (!file1.good()) {
      file1.close();
      LOG_ERROR("Failed to write to file for SELECT INTO OUTFILE.\n");
      string err_msg("Can't create/write to file '");
      err_msg += filename;
      err_msg += "'";
      throw dbscale::sql::SQLError(err_msg.c_str(), "HY000", 1);
    }
    file1.close();
  }
}

void Statement::handle_no_par_table_one_spaces(ExecutePlan *plan) {
  DataSpace *dataspace =
      one_table_node.only_one_table ? one_table_node.space : spaces[0];
  Backend *backend = Backend::instance();
  if (st.type == STMT_SELECT && st.has_where_rownum) replace_rownum_add_limit();
#ifdef DEBUG
  LOG_DEBUG("handle no partition table one space, the space name [%s]\n",
            dataspace->get_name());
#endif
  switch (st.type) {
    case STMT_LOAD: {
      DataServer *server =
          get_write_server_of_source(dataspace->get_data_source());
      if (server->get_is_external_load()) {
        if (st.sql->load_oper->local) {
          assemble_load_local_external_plan(plan, dataspace, server);
        } else {
          assemble_load_data_infile_external_plan(plan, dataspace, server);
        }
      } else {
        if (st.sql->load_oper->local) {
          assemble_load_local_plan(plan, dataspace);
        } else {
          assemble_load_data_infile_plan(plan, dataspace);
        }
      }
      break;
    }
    case STMT_DBSCALE_SHOW_PARTITIONS: {
      assemble_dbscale_show_partition_plan(plan);
      break;
    }
    case STMT_UPDATE:
    case STMT_DELETE:
    case STMT_REPLACE_SELECT:
    case STMT_INSERT_SELECT: {
      const char *schema_name, *table_name;
      DataSpace *modify_space, *select_space = dataspace;
      table_link *modify_table = get_one_modify_table();
      schema_name = modify_table->join->schema_name
                        ? modify_table->join->schema_name
                        : schema;
      table_name = modify_table->join->table_name;
      modify_space = backend->get_data_space_for_table(schema_name, table_name);

      if (!modify_space->get_data_source()) {
        /* If the partition table only has one partition, it will be treated
         * as a normal table. But here the dataspace should be adjusted to the
         * partition, rather than partition table.*/
        PartitionedTable *tmp_p = (PartitionedTable *)modify_space;
        if (tmp_p->get_real_partition_num() != 1) {
          Backend::instance()->record_latest_important_dbscale_warning(
              "Meet a mul part partition table in "
              "handle_no_par-table_one_spaces.\n");
#ifdef DEBUG
          ACE_ASSERT(0);
#endif
        }
        modify_space = tmp_p->get_partition(0);
      }

      DataServer *modify_server =
          get_write_server_of_source(modify_space->get_data_source());
      if (st.type == STMT_INSERT_SELECT && modify_server &&
          modify_server->get_is_external_load()) {
        prepare_insert_select_via_fifo(plan, modify_server, schema_name,
                                       table_name, false);
        const char *used_sql =
            adjust_stmt_sql_for_shard(dataspace, select_sub_sql.c_str());
        ExecuteNode *fetch_node = plan->get_fetch_node(dataspace, used_sql);
        ExecuteNode *send_node = plan->get_into_outfile_node();

        send_node->add_child(fetch_node);
        plan->set_start_node(send_node);
        break;
      }
      int load_insert =
          plan->session->get_session_option("use_load_data_for_insert_select")
              .int_val;
      if (st.type == STMT_INSERT_SELECT && load_insert == FIFO_INSERT_SELECT &&
          !plan->statement->is_union_table_sub() &&
          !plan->statement->is_cross_node_join() &&
          !plan->session->is_in_explain()) {
        prepare_insert_select_via_fifo(plan, modify_server, schema_name,
                                       table_name, true);
        const char *used_sql =
            adjust_stmt_sql_for_shard(dataspace, select_sub_sql.c_str());
        ExecuteNode *fetch_node = plan->get_fetch_node(dataspace, used_sql);
        ExecuteNode *send_node = plan->get_into_outfile_node();

        send_node->add_child(fetch_node);
        plan->set_start_node(send_node);
        break;
      } else if (st.type == STMT_INSERT_SELECT &&
                 load_insert == LOAD_INSERT_SELECT &&
                 !plan->statement->is_union_table_sub() &&
                 !plan->statement->is_cross_node_join() &&
                 !plan->session->is_in_explain()) {
        vector<ExecuteNode *> nodes;
        LOG_DEBUG(
            "Assemble INSERT SELECT from partition table plan via LOAD.\n");
        prepare_insert_select_via_load(plan, schema_name, table_name);
        const char *used_sql =
            adjust_stmt_sql_for_shard(dataspace, select_sub_sql.c_str());
        ExecuteNode *fetch_node = plan->get_fetch_node(dataspace, used_sql);
        ExecuteNode *load_node =
            plan->get_load_select_node(schema_name, table_name);
        load_node->add_child(fetch_node);
        plan->set_start_node(load_node);
        break;
      }

      if (modify_space != select_space) {
        /*The modify_space is a parent space of dataspace. In this case we
         * need to divide this sql into two sqls.*/
        assemble_two_phase_modify_no_partition_plan(plan, modify_space,
                                                    select_space, modify_table);
      } else {
        if (enable_last_insert_id) {
          vector<join_node *> tables;
          get_all_join_table(st.scanner->join_tables, &tables);
          for (auto table : tables) {
            schema_name = table->schema_name ? table->schema_name : schema;
            table_name = table->table_name;
            modify_space->set_auto_increment_info(
                plan->handler, this, schema_name, table_name, false, false);
          }
        }
        assemble_direct_exec_plan(plan, dataspace);
      }
      break;
    }

    case STMT_DBSCALE_ESTIMATE: {
      assemble_dbscale_estimate_select_plan(plan, dataspace, this);
    } break;
    case STMT_SHOW_INDEX: {
      DataSpace *meta = Backend::instance()->get_metadata_data_space();
      if (!meta) {
        throw Error("fail to get metadata dataspace");
      }
      assemble_direct_exec_plan(plan, meta);
      break;
    }
    case STMT_SHOW_FIELDS:
    case STMT_SHOW_CREATE_TABLE: {
      DataSpace *dspace = dataspace;
      DataSpace *meta = Backend::instance()->get_metadata_data_space();
      if (meta) dspace = meta;
      table_link *table = st.table_list_head;
      const char *schema_name =
          table->join->schema_name ? table->join->schema_name : schema;
      const char *table_name = table->join->table_name;
      if (plan->session->is_temp_table(schema_name, table_name)) {
        dspace = backend->get_data_space_for_table(schema_name, table_name);
      }
      assemble_direct_exec_plan(plan, dspace);
      break;
    }
    case STMT_TRUNCATE: {
      table_link *table = st.table_list_head;
      while (table) {
        const char *schema_name =
            table->join->schema_name ? table->join->schema_name : schema;
        if (dbscale_safe_sql_mode > 0 &&
            Backend::instance()->is_system_schema(schema_name)) {
          throw Error(
              "Refuse truncate table of system schema for "
              "dbscale_safe_sql_mode>0");
        }

        table = table->next;
      }
      if (enable_table_recycle_bin) {
        if (st.table_list_num > 1)
          throw NotSupportedError(
              "when using enable_table_recycle_bin, tables should be truncated "
              "one by one.");
        else
          assemble_move_table_to_recycle_bin_plan(plan);
      } else
        assemble_direct_exec_plan(plan, dataspace);
    } break;
    case STMT_CREATE_TB: {
      table_link *table = st.table_list_head;
      const char *schema_name =
          table->join->schema_name ? table->join->schema_name : schema;
      if (!Backend::instance()->is_centralized_cluster())
        create_tb_sql_with_create_db_if_not_exists =
            Backend::instance()->fetch_create_db_sql_from_metadata(
                plan->session, schema_name, true, NULL);
      else
        create_tb_sql_with_create_db_if_not_exists =
            "SET @_DBSCALE_RESERVED_VAR=1";  // not run create database if not
                                             // exists
      if (!is_cross_node_join())
        plan->session->reset_table_without_given_schema();
      int has_use = 0;
      if (plan->session->get_schema() &&
          strcmp(plan->session->get_schema(), "")) {
        if (!lower_case_compare(plan->session->get_schema(), schema_name)) {
          /*Do the use schema only for the current schema is the created
           * schema.*/
          create_tb_sql_with_create_db_if_not_exists.append(";USE `");
          create_tb_sql_with_create_db_if_not_exists.append(
              plan->session->get_schema());
          create_tb_sql_with_create_db_if_not_exists.append("`");
          has_use = 1;
        }
      } else {
        create_tb_sql_with_create_db_if_not_exists.append(";USE `");
        create_tb_sql_with_create_db_if_not_exists.append(DEFAULT_LOGIN_SCHEMA);
        create_tb_sql_with_create_db_if_not_exists.append("`");
        has_use = 1;
      }
      create_tb_sql_with_create_db_if_not_exists.append(";");
      create_tb_sql_with_create_db_if_not_exists.append(sql);
      map<DataSpace *, int> sql_count_map;
      map<DataSpace *, const char *> spaces_map;
      sql_count_map[dataspace] = 2 + has_use;
      spaces_map[dataspace] =
          create_tb_sql_with_create_db_if_not_exists.c_str();
      if (Backend::instance()->need_deal_with_metadata(st.type, &st))
        st.need_apply_metadata = judge_need_apply_meta_datasource(spaces_map);
      ExecuteNode *node =
          plan->get_mul_modify_node(spaces_map, true, &sql_count_map);
      plan->set_start_node(node);
    } break;
    case STMT_INSERT:
    case STMT_REPLACE: {
      int enable_last_insert_id_session =
          plan->session->get_session_option("enable_last_insert_id").int_val;
      if (enable_last_insert_id_session) {
        insert_op_node *insert = NULL;
        if (plan->session->is_call_store_procedure()) {
          insert = st.sql->insert_oper;
        } else {
          insert = get_latest_stmt_node()->sql->insert_oper;
        }
        // we do not handle last insert id with insert set
        if (!insert->assign_list) {
          table_link *table = st.table_list_head;
          const char *schema_name =
              table->join->schema_name ? table->join->schema_name : schema;
          const char *table_name = table->join->table_name;
          int alter_table = dataspace->set_auto_increment_info(
              plan->handler, this, schema_name, table_name, false, false);
          if (alter_table == 1) {
            Driver *driver = Driver::get_driver();
            driver->acquire_session_mutex();
            set<Session *, compare_session> *session_set =
                driver->get_session_set();
            set<Session *, compare_session>::iterator it = session_set->begin();
            for (; it != session_set->end(); ++it) {
              (*it)->reset_auto_increment_info(full_table_name);
            }
            driver->release_session_mutex();
          }
          if (auto_inc_status != NO_AUTO_INC_FIELD) {
            // we do not handle last insert id with insert set
            string full_table_name;
            splice_full_table_name(schema_name, table_name, full_table_name);
            TableInfoCollection *tic = TableInfoCollection::instance();
            {
              string auto_inc_name;
              TableInfo *ti = tic->get_table_info_for_read(full_table_name);
              try {
                auto_inc_name =
                    ti->element_table_auto_inc_name->get_element_string(
                        get_session());
                int auto_inc_pos =
                    ti->element_table_auto_inc_pos->get_element_int64_t(
                        get_session());
                auto_inc_key_name = auto_inc_name;
                // we check from 0
                auto_increment_key_pos = auto_inc_pos - 1;
              } catch (...) {
                LOG_ERROR(
                    "Error occured when try to get table info table auto inc "
                    "name(string) of table [%s.%s]\n",
                    schema_name, table_name);
              }
              ti->release_table_info_lock();
            }
          }
        }
      }
    }
    default:
      assemble_direct_exec_plan(plan, dataspace);
  }
}

inline bool check_xa_transaction_support_sql(stmt_node *st, Session *s) {
  // savepoint and rollback_to stmt is not support for xa transaction
  if (enable_xa_transaction &&
      s->get_session_option("close_session_xa").int_val == 0) {
    stmt_type type = st->type;
    if (type == STMT_SAVEPOINT) return false;
    if (type == STMT_ROLLBACK) {
      RollbackType rollback_type = st->sql->rollback_point_oper->type;
      if (rollback_type == ROLLBACK_TYPE_TO_POINT) return false;
    }
  }
  return true;
}

void Statement::assemble_rename_normal_table(ExecutePlan *plan,
                                             DataSpace *dataspace) {
  LOG_DEBUG("Assemble rename normal table plan.\n");
  assemble_direct_exec_plan(plan, dataspace);
}

void Statement::assemble_rename_partitioned_table(
    ExecutePlan *plan, PartitionedTable *old_table, PartitionedTable *new_table,
    const char *old_schema_name, const char *old_table_name,
    const char *new_schema_name, const char *new_table_name) {
  LOG_DEBUG("Assemble rename partitioned partition plan.\n");

  unsigned int partition_num = old_table->get_real_partition_num();

  ExecuteNode *rename_table_node = plan->get_rename_table_node(
      old_table, new_table, old_schema_name, old_table_name, new_schema_name,
      new_table_name);
  ExecuteNode *ok_merge_node = plan->get_ok_merge_node();
  ExecuteNode *modify_node = NULL;
  DataSpace *space = NULL;
  rename_table_node->add_child(ok_merge_node);

  DataSpace *meta_datasource = Backend::instance()->get_metadata_data_space();
  bool need_apply_meta = true;
  for (unsigned int i = 0; i != partition_num; ++i) {
    space = old_table->get_partition(i);
    if (is_share_same_server(meta_datasource, space)) {
      need_apply_meta = false;
    }
    modify_node = plan->get_modify_node(space, sql, true);
    ok_merge_node->add_child(modify_node);
  }

  st.need_apply_metadata = need_apply_meta;
  plan->session->add_table_info_to_delete(old_schema_name, old_table_name);
  plan->set_start_node(rename_table_node);
}

bool Statement::has_same_partitioned_key(PartitionedTable *old_part_table,
                                         PartitionedTable *new_part_table) {
  vector<const char *> *old_key_names = old_part_table->get_key_names();
  vector<const char *> *new_key_names = new_part_table->get_key_names();
  if (old_key_names->size() != new_key_names->size()) {
    return false;
  }
  // if partition-key is not the same, then the data will distribute
  // in diff data-source, so forbid it
  unsigned int key_size = old_key_names->size();
  for (unsigned int i = 0; i != key_size; ++i) {
    const char *old_key = old_key_names->at(i);
    const char *new_key = new_key_names->at(i);
    if (!strcasecmp(old_key, new_key)) {
      return false;
    }
  }

  return true;
}

void Statement::handle_rename_table(ExecutePlan *plan,
                                    rename_table_node_list *head) {
  Backend *backend = Backend::instance();

  // get the old table
  join_node *old_node = head->old_table;
  const char *old_schema_name =
      old_node->schema_name ? old_node->schema_name : schema;
  const char *old_table_name = old_node->table_name;
  DataSpace *old_table =
      backend->get_data_space_for_table(old_schema_name, old_table_name);
  DataSource *old_source = old_table->get_data_source();

  // get the new table
  join_node *new_node = head->new_table;
  const char *new_schema_name =
      new_node->schema_name ? new_node->schema_name : schema;
  const char *new_table_name = new_node->table_name;
  DataSpace *new_table =
      backend->get_data_space_for_table(new_schema_name, new_table_name);
  DataSource *new_source = new_table->get_data_source();

  // both are normal tables
  if (old_source != NULL && new_source != NULL) {
    if (old_source != new_source) {
      LOG_ERROR(
          "DBScale do not support to rename normal table "
          "with different data-source.");
      throw NotSupportedError(
          "Not supported rename normal table with different data-source.");
    }
    assemble_rename_normal_table(plan, old_table);
    return;
  }

  // one is normal table, other is partitioned table
  if ((old_source != NULL && new_source == NULL) ||
      (old_source == NULL && new_source != NULL)) {
    LOG_ERROR(
        "DBScale do not support to rename table "
        "between normal table and partitioned table.");
    throw NotSupportedError(
        "Not supported rename table between normal table and partitioned "
        "table.");
  }

  // both are partitioned table
  PartitionedTable *old_part_table = (PartitionedTable *)old_table;
  PartitionScheme *old_scheme = old_part_table->get_partition_scheme();
  PartitionedTable *new_part_table = (PartitionedTable *)new_table;
  PartitionScheme *new_scheme = new_part_table->get_partition_scheme();
  if (old_scheme != new_scheme) {
    LOG_ERROR(
        "DBScale is not support rename partitioned table"
        " with different partition-scheme.");
    throw NotSupportedError(
        "Not supported rename partitioned table with different "
        "partition-scheme.");
  }
  if (has_same_partitioned_key(old_part_table, new_part_table)) {
    LOG_ERROR(
        "DBScale do not support to rename partitioned table "
        "with different partition-key.");
    throw NotSupportedError(
        "Not supported rename partitioned table with different partition-key.");
  }
  if (old_part_table->get_partition_scheme()->is_shard() !=
      new_part_table->get_partition_scheme()->is_shard()) {
    LOG_ERROR(
        "DBScale do not support to rename shard partitioned table "
        "to non-shard, and also not support non-shard to shard.");
    throw NotSupportedError(
        "Not supported rename shard partitioned table to non-shard, and also "
        "not support non-shard to shard.");
  }
  if (old_part_table->get_partition_scheme()->is_shard() &&
      lower_case_compare(old_schema_name, new_schema_name)) {
    LOG_ERROR(
        "DBScale do not support to rename shard partitioned table "
        "to different schema.");
    throw NotSupportedError(
        "Not supported rename shard partitioned table to different schema.");
  }
  assemble_rename_partitioned_table(plan, old_part_table, new_part_table,
                                    old_schema_name, old_table_name,
                                    new_schema_name, new_table_name);
}

void Statement::assemble_set_plan(ExecutePlan *plan, DataSpace *dataspace) {
  LOG_DEBUG("Assemble set plan.\n");
  if (!dataspace)
    dataspace = Backend::instance()->get_data_space_for_table(schema, NULL);
  ExecuteNode *node = NULL;
  if (has_ignore_session_var) {
    if (!session_var_sql.length()) {
      /*All session vars are ignored or skipped. So just return OK packet*/
      node = plan->get_return_ok_node();
    } else
      node = plan->get_set_node(dataspace, session_var_sql.c_str());
  } else
    node = plan->get_set_node(dataspace, sql);
  plan->set_start_node(node);
}

const char *Statement::get_value_from_bool_expression(
    const char *column_name, BoolBinaryCaculateExpression *expr) {
  CompareExpression *expr_left = dynamic_cast<CompareExpression *>(expr->left);
  CompareExpression *expr_right =
      dynamic_cast<CompareExpression *>(expr->right);
  const char *tmp_str = NULL;
  if ((!expr_left || expr_left->type != EXPR_EQ) &&
      (!expr_right || expr_right->type != EXPR_EQ))
    return NULL;
  if (expr_left != NULL) {
    tmp_str = get_value_from_compare_expression(column_name, expr_left);
  }
  if (tmp_str == NULL && expr_right != NULL) {
    tmp_str = get_value_from_compare_expression(column_name, expr_right);
  }
  return tmp_str;
}
const char *Statement::get_value_from_compare_expression(
    const char *column_name, CompareExpression *expr) {
  StrExpression *left = dynamic_cast<StrExpression *>(expr->left);
  if (left == NULL) return NULL;
  string lower_str_value(left->str_value);
  if (boost::algorithm::to_lower_copy(lower_str_value) == column_name) {
    StrExpression *right = dynamic_cast<StrExpression *>(expr->right);
    if (right == NULL) return NULL;
    return right->str_value;
  }
  return NULL;
}

// return false if need block request
bool Statement::check_migrate_block_table(ExecutePlan *plan) {
  Backend *backend = Backend::instance();
  if (backend->need_slow_down()) {
    // TODO: only slow down the request, which will be sent to the source server
    timespec_t t =
        (timespec_t)ACE_Time_Value(0, slow_down_request_time);  // sleep 5 ms
    ACE_OS::nanosleep(&t);
  }

  // TODO: deal with partial parse sql.
  if (!st.table_list_head)  // statemet does't contains table
    return true;
  if (st.type == STMT_DBSCALE_BLOCK) return true;
  table_link *link = st.table_list_head;
  join_node *node = NULL;

  if (!backend->has_migrate_block_table()) return true;

  while (link) {
    node = link->join;
    const char *table_name = node->table_name;
    const char *alias_name = node->alias;
    const char *schema_name =
        node->schema_name ? node->schema_name : plan->session->get_schema();
    // if the handler is using the table_name, we Should wait until transaction
    // or lock end.
    if (plan->session->is_table_in_using(schema_name, table_name)) {
      link = link->next;
      continue;
    }
    MigrateBlockType block_type =
        backend->get_table_block_type(schema_name, table_name);
    if (block_type == BLOCK_ALL_TYPE)
      return false;
    else if (block_type == BLOCK_PARTITION_TYPE) {
      if (st.type == STMT_DBSCALE_MIGRATE ||
          (st.type > STMT_DDL_START && st.type < STMT_DDL_END) ||
          st.type == STMT_LOAD) {
        LOG_DEBUG(
            "the table is in partition block state, not support "
            "migrate/load/ddl operation on table.\n");
        return false;
      }
      DataSpace *space =
          backend->get_data_space_for_table(schema_name, table_name);
      if (space->get_dataspace_type() != TABLE_TYPE) {
        return true;
      } else if (!((Table *)space)->is_partitioned() || space->is_shard()) {
        // no need block normal table for migrate partition
        // now shard table not support migrate virtual partition.
        // migrate shard partition no need add mutex for BLOCK_PARTITION_TYPE
        return true;
      }

      switch (st.type) {
        case STMT_INSERT:
          break;
        case STMT_SELECT:
        case STMT_UPDATE:
        case STMT_DELETE: {
          const char *key_name =
              ((PartitionedTable *)space)->get_key_names()->at(0);
          // TODO:optimize the invoke of fullfil_par_key_equality, if it is
          // invoked here, reuse the result for the following processing.
          record_scan_par_key_equality.clear();
          record_scan_par_key_values.clear();
          fullfil_par_key_equality(st.cur_rec_scan, schema_name, table_name,
                                   alias_name, key_name);
          if (record_scan_par_key_values[st.cur_rec_scan][key_name].size() != 1)
            return false;
        } break;
        default:
          return false;
      }
    }
    link = link->next;
  }
  return true;
}

void Statement::check_and_replace_view(Session *session, table_link **table) {
  table_link *tmp_table = *table;
  map<unsigned int, pair<unsigned int, string> > view_sqls;
  map<unsigned int, string> view_alias;
  string sub_sql;
  string full_view;
  const char *view_schema = NULL;
  const char *view_name = NULL;
  bool view_is_on_top_rs = false;

  while (tmp_table) {
    join_node *join = tmp_table->join;
    view_schema = join->schema_name ? join->schema_name : schema;
    view_name = join->table_name;
    full_view.assign(view_schema);
    full_view.append(".");
    full_view.append(view_name);
    sub_sql.clear();
    session->get_one_view(full_view, sub_sql);
    if (!sub_sql.empty()) {
      view_sqls[tmp_table->start_pos] = make_pair(tmp_table->end_pos, sub_sql);
      if (!join->alias) {
        view_alias[tmp_table->start_pos] = string(view_name);
      }
      if (!view_is_on_top_rs && tmp_table->join->cur_rec_scan == st.scanner) {
        view_is_on_top_rs = true;
      }
    }
    tmp_table = tmp_table->next;
  }

  if (!view_sqls.empty()) {
    stmt_type check_type = st.type;
    if (st.type == STMT_DBSCALE_EXPLAIN || st.type == STMT_EXPLAIN) {
      if (st.sql->explain_oper) check_type = st.sql->explain_oper->explain_type;
    }
    if ((check_type == STMT_INSERT_SELECT || check_type == STMT_UPDATE ||
         check_type == STMT_DELETE) &&
        !view_is_on_top_rs) {
      // view in the select part of insert...select or update, which is ok
    } else if (check_type != STMT_SELECT) {
      LOG_ERROR("Only support do select on view now.\n");
      throw NotSupportedError("Only support do select on view.");
    }
    contains_view = true;
    string ori_sql(sql);
    view_sql.clear();
    unsigned int pos = 0;
    map<unsigned int, pair<unsigned int, string> >::iterator it =
        view_sqls.begin();
    for (; it != view_sqls.end(); it++) {
      view_sql.append(ori_sql, pos, it->first - pos - 1);
      view_sql.append("(");
      view_sql.append(it->second.second);
      view_sql.append(")");
      if (view_alias.count(it->first)) {
        view_sql.append(" AS ");
        view_sql.append(view_alias[it->first]);
      }
      pos = it->second.first;
    }
    view_sql.append(ori_sql, pos, ori_sql.length() - pos);
    sql = view_sql.c_str();
    Statement *new_stmt = NULL;
    re_parser_stmt(&(st.scanner), &new_stmt, sql);
    *table = new_stmt->get_stmt_node()->table_list_head;
    st.handled_sql = new_stmt->get_stmt_node()->handled_sql;
    LOG_DEBUG("Query sql after replace view is [%s]=[%@]\n", sql, this);
    check_and_replace_view(session, table);
  }
}

bool Statement::child_rs_has_join_tables(record_scan *root) {
  record_scan *tmp = root->children_begin;
  if (tmp) {
    if (tmp->join_tables) return true;
    for (; tmp; tmp = tmp->next) {
      if (child_rs_has_join_tables(tmp)) return true;
    }
  }
  return false;
}

bool Statement::can_use_one_table_plan(ExecutePlan *plan, DataSpace *space,
                                       stmt_node *st) {
  if (space->get_data_source()) return true;
  /*For partition table, should not has subquery with table link.
   *subquery no table_link means sql like INSERT INTO t1 SELECT 1,2,3 .*/
  if (child_rs_has_join_tables(st->scanner)) return false;
  /* For partition table, if the select stmt contains invalid function, we
   * should return false and use analysis record scan. */
  if (st->type == STMT_SELECT &&
      (check_field_list_contains_invalid_function(st->scanner) ||
       check_field_list_invalid_distinct(st->scanner)))
    return false;
  /*For partition table, the update/delete with limit can not be executed with
   * one table plan.*/
  if (st->type != STMT_UPDATE && st->type != STMT_DELETE) return true;
  if (st->scanner->limit) {
    IntExpression *limit_offset = st->scanner->limit->offset;
    IntExpression *limit_num = st->scanner->limit->num;
    long num = 0;
    if (limit_num) num = get_integer_value_from_expr(limit_num);
    if (!limit_offset && (num == 1) &&
        (st->type == STMT_UPDATE || st->type == STMT_DELETE) &&
        plan->session->get_session_option("update_delete_quick_limit")
            .int_val) {
      if (plan->session->is_in_transaction()) {
        stmt_with_limit_using_quick_limit = true;
        return true;
      } else {
        throw Error(
            "when enable 'update_delete_quick_limit', update/delete with limit "
            "1 should be executed in transaction");
      }
    } else {
      return false;
    }
  }
  return true;
}

bool Statement::get_explain_result_one_source(
    list<list<string> > *explain_result, Handler *handler, DataSpace *dataspace,
    Statement *stmt, string *sql, unsigned int row_id, string *exec_node_name,
    bool is_server_explain_stmt_always_extended) {
  bool got_error = false;
  ExecutePlan *plan = (ExecutePlan *)(handler->get_execute_plan(stmt));
  ExecuteNode *fetch_node = plan->get_fetch_node(dataspace, sql->c_str());
  ExecuteNode *fetch_explain_result_node = plan->get_fetch_explain_result_node(
      explain_result, is_server_explain_stmt_always_extended);
  fetch_explain_result_node->add_child(fetch_node);
  plan->set_start_node(fetch_explain_result_node);
  try {
    plan->execute();
  } catch (...) {
    got_error = true;
  }
  delete plan;
  plan = NULL;

  if (got_error || explain_result->empty()) {
    LOG_ERROR("explain got unexpected result.\n");
    return false;
  }

  list<list<string> >::iterator it = explain_result->begin();
  (*it).push_front(dataspace->get_data_source()->get_name());
  if (row_id == 0) {
    (*it).push_front(*exec_node_name);
  } else {
    (*it).push_front("");
  }
  ++it;
  for (; it != explain_result->end(); ++it) {
    (*it).push_front("");
    (*it).push_front("");
  }
  return true;
}

void Statement::assemble_explain_plan(ExecutePlan *plan) {
  LOG_DEBUG("Assemble explain plan.\n");
  bool is_server_explain_stmt_always_extended =
      Backend::instance()->get_is_server_explain_stmt_always_extended();
  Session *session = plan->session;
  list<ExplainElement *>::iterator it =
      session->get_explain_element_list()->begin();
  int sub_plan_total_count = (*it)->get_sub_plan_id();

  bool got_error = false;
  string error_msg;
  for (; it != session->get_explain_element_list()->end(); ++it) {
    if (got_error) break;
    ExplainElement *ei = *it;

    string exec_node_name_with_depth = "";
    if (ei->is_a_start_node()) {
      exec_node_name_with_depth.append(
          ei->get_depth() + (sub_plan_total_count - ei->get_sub_plan_id()),
          '*');
    } else {
      exec_node_name_with_depth.append(
          ei->get_depth() + (sub_plan_total_count - ei->get_sub_plan_id()),
          '-');
    }
    exec_node_name_with_depth.append(ei->get_ori_node_name());

    DataSpace *ds = ei->get_dataspace();
    if (ds) {
      string sql = string("EXPLAIN ") + ei->get_sql();
      Parser *parser = Driver::get_driver()->get_parser();
      Statement *tmp_stmt = parser->parse(sql.c_str(), stmt_allow_dot_in_ident,
                                          true, NULL, NULL, NULL, ctype);
      try {
        tmp_stmt->check_and_refuse_partial_parse();
      } catch (...) {
        tmp_stmt->free_resource();
        delete tmp_stmt;
        tmp_stmt = NULL;
        got_error = true;
        error_msg = ei->get_sql();
        break;
      }
      tmp_stmt->set_default_schema(schema);

      // FIXME: fetch explain info paralleled.
      if (!ds->get_data_source()) {  // partitioned table
        PartitionedTable *pt = (PartitionedTable *)ds;
        unsigned int partition_num = pt->get_real_partition_num();
        for (unsigned int i = 0; i < partition_num; ++i) {
          list<list<string> > explain_result_one_source;
          DataSpace *dataspace = pt->get_partition(i);
          bool ret = get_explain_result_one_source(
              &explain_result_one_source, plan->handler, dataspace, tmp_stmt,
              &sql, i, &exec_node_name_with_depth,
              is_server_explain_stmt_always_extended);
          if (!ret) {  // got error
            got_error = true;
            error_msg = ei->get_sql();
            break;
          }

          list<list<string> >::iterator it = explain_result_one_source.begin();
          for (; it != explain_result_one_source.end(); ++it) {
            explain_info.push_back(*it);
          }
        }
        if (!got_error)
          plan->session->set_execute_plan_touch_partition_nums(-1);
      } else {
        list<list<string> > explain_result_one_source;
        bool ret = get_explain_result_one_source(
            &explain_result_one_source, plan->handler, ds, tmp_stmt, &sql, 0,
            &exec_node_name_with_depth, is_server_explain_stmt_always_extended);
        if (!ret) {  // got error
          got_error = true;
          error_msg = ei->get_sql();
        } else {
          list<list<string> >::iterator it = explain_result_one_source.begin();
          for (; it != explain_result_one_source.end(); ++it) {
            explain_info.push_back(*it);
          }
        }
      }
      tmp_stmt->free_resource();
      delete tmp_stmt;
      tmp_stmt = NULL;
    } else {
      list<string> row;
      row.push_back(exec_node_name_with_depth);
      int count = is_server_explain_stmt_always_extended ? 13 : 11;
      for (int i = 0; i < count; ++i) {
        row.push_back("");
      }
      explain_info.push_back(row);
    }
  }
  session->clean_explain_element_list();

  if (got_error) {
    string msg =
        string("explain statement get exception when handle sub sql [");
    msg.append(error_msg);
    msg.append("].");
    LOG_ERROR("%s\n", msg.c_str());
    plan->set_cancel_execute(true);
  } else {
    ExecuteNode *start_node =
        plan->get_explain_node(is_server_explain_stmt_always_extended);
    plan->set_start_node(start_node);
  }
}

void Statement::refresh_norm_table_spaces() {
  SeparatedExecNode *last_node = exec_nodes[exec_nodes.size() - 1];
  if (st.type == STMT_SELECT && last_node->get_node_dataspace()) {
    par_table_num = 0;
    spaces.clear();
    par_tables.clear();
    spaces.push_back(last_node->get_node_dataspace());
    record_scan_one_space_tmp_map[st.scanner] = last_node->get_node_dataspace();
  }
}

void Statement::refresh_part_table_links() {
  stmt_node *latest_st = get_latest_stmt_node();
  vector<table_link *> tmp_par_tables;
  if (st.type == STMT_UPDATE) {
    update_on_one_server = true;
  }
  if (st.type == STMT_DELETE) {
    delete_on_one_server = true;
  }
  if (par_table_num <= 0) return;

  if (!need_reanalysis_space) {
    if (par_tables.size() == 1 &&
        (st.type == STMT_INSERT_SELECT || st.type == STMT_REPLACE_SELECT) &&
        st.scanner->children_begin &&
        st.scanner->children_begin->is_select_union) {
      return;
    }
    vector<table_link *>::iterator it = par_tables.begin();
    for (; it != par_tables.end(); ++it) {
      table_link *tmp = latest_st->table_list_head;
      while (tmp) {
        // The last part table may be a tmp table for left join. For example:
        // original sql is: select * from t1 left join t2 on t1.c1 = t2.c1.
        // current sql might be: select * from tmp_t1 left join tmp_t2 t2 on
        // tmp_t1.c1 = t2.c2. when we refresh the table links, we should compare
        // with tmp_t2 instead of t2.
        if (left_join_par_tables.count(*it)) {
          const char *schema_name1 = TMP_TABLE_SCHEMA;
          const char *schema_name2 =
              tmp->join->schema_name ? tmp->join->schema_name : schema;
          const char *table_name1 =
              left_join_table_position[left_join_par_tables[*it].first]
                                      [left_join_par_tables[*it].second]
                                          .c_str();
          const char *table_name2 = tmp->join->table_name;
          if (!lower_case_compare(table_name1, table_name2) &&
              !lower_case_compare(schema_name1, schema_name2)) {
            tmp_par_tables.push_back(tmp);
            DataSpace *space = Backend::instance()->get_data_space_for_table(
                TMP_TABLE_SCHEMA, table_name2);
            record_scan_all_table_spaces[tmp] = space;
            record_scan_par_table_map[tmp->join->cur_rec_scan] = tmp;
            break;
          }
        } else {
          const char *schema_name1 =
              (*it)->join->schema_name ? (*it)->join->schema_name : schema;
          const char *schema_name2 =
              tmp->join->schema_name ? tmp->join->schema_name : schema;
          const char *table_name1 = (*it)->join->table_name;
          const char *table_name2 = tmp->join->table_name;
          if (!lower_case_compare(table_name1, table_name2) &&
              !lower_case_compare(schema_name1, schema_name2)) {
            tmp_par_tables.push_back(tmp);
            record_scan_all_table_spaces[tmp] =
                record_scan_all_table_spaces[*it];
            record_scan_par_table_map[tmp->join->cur_rec_scan] = tmp;
            break;
          }
        }
        tmp = tmp->next;
      }
    }
#ifdef DEBUG
    ACE_ASSERT(tmp_par_tables.size() == par_tables.size());
#endif
  } else {
    if (st.type == STMT_INSERT_SELECT || st.type == STMT_REPLACE_SELECT) {
      table_link *tmp = latest_st->scanner->first_table;
      record_scan_par_table_map[tmp->join->cur_rec_scan] = tmp;
      Backend *backend = Backend::instance();
      const char *schema_name =
          tmp->join->schema_name ? tmp->join->schema_name : schema;
      DataSpace *ds =
          backend->get_data_space_for_table(schema_name, tmp->join->table_name);
      record_scan_all_table_spaces[tmp] = ds;
      bool select_table_is_part = false;
      par_table_num = 0;
      if (ds->is_partitioned()) {
        ++par_table_num;
        select_table_is_part = true;
      }

      tmp = tmp->next;
      record_scan_par_table_map[tmp->join->cur_rec_scan] = tmp;
      schema_name = tmp->join->schema_name ? tmp->join->schema_name : schema;
      ds =
          backend->get_data_space_for_table(schema_name, tmp->join->table_name);
      record_scan_all_table_spaces[tmp] = ds;
      if (ds->is_partitioned()) {
        ++par_table_num;
        tmp_par_tables.push_back(tmp);
      }

      if (select_table_is_part) {
        tmp_par_tables.push_back(latest_st->scanner->first_table);
      }
    } else {
      table_link *tmp = latest_st->table_list_head;
      Backend *backend = Backend::instance();
      DataSpace *ds = NULL;
      while (tmp) {
        const char *schema_name =
            tmp->join->schema_name ? tmp->join->schema_name : schema;
        ds = backend->get_data_space_for_table(schema_name,
                                               tmp->join->table_name);
        if (ds->is_partitioned()) {
          break;
        }
        tmp = tmp->next;
      }
      if (!tmp) {
        par_table_num = 0;
      } else {
#ifdef DEBUG
        ACE_ASSERT(tmp);
#endif
        tmp_par_tables.push_back(tmp);
        par_table_num = 1;
        record_scan_par_table_map[tmp->join->cur_rec_scan] = tmp;
        record_scan_all_table_spaces[tmp] = ds;
      }
    }
  }
  par_tables = tmp_par_tables;
}

const char *Statement::get_partition_key_for_auto_space() {
  name_item *partition_key_item = st.sql->create_tb_oper->primary_key_cols;
  if (!partition_key_item)
    partition_key_item = st.sql->create_tb_oper->all_unique_keys
                             ? st.sql->create_tb_oper->all_unique_keys->col_list
                             : NULL;
  if (!partition_key_item)
    partition_key_item = st.sql->create_tb_oper->first_key_cols;
  if (!partition_key_item)
    partition_key_item = st.sql->create_tb_oper->first_non_key_cols;
  if (!partition_key_item) {
    LOG_ERROR("Fail to determine a partition key from the sql.\n");
    throw Error("Fail to determine a partition key from the sql.");
  }

  return partition_key_item->name;
}

void Statement::add_partitioned_table_config_before_create_tb(
    table_link *table, const char *part_table_scheme,
    const char *part_table_key) {
  create_part_tb_def = NULL;
  Backend *backend = Backend::instance();
  const char *schema_name =
      table->join->schema_name ? table->join->schema_name : schema;
  const char *table_name = table->join->table_name;
  Schema *schema = backend->find_schema(schema_name);
  if (!schema) {
    string error_msg;
    error_msg.assign("Schema [");
    error_msg.append(schema_name);
    error_msg.append("] not found.");
    LOG_ERROR("%s\n", error_msg.c_str());
    throw Error(error_msg.c_str());
  }
  ACE_RW_Thread_Mutex *dynamic_add_mutex =
      backend->get_dynamic_modify_rep_mutex();
  ACE_Write_Guard<ACE_RW_Thread_Mutex> guard(*dynamic_add_mutex);
  Table *tab = schema->get_table(table_name);
  if (tab && tab->is_normal()) {
    string error_msg;
    error_msg.assign("Table [");
    error_msg.append(schema_name);
    error_msg.append(".");
    error_msg.append(table_name);
    error_msg.append("] has already configured as a normal table.");
    LOG_ERROR("%s\n", error_msg.c_str());
    throw Error(error_msg.c_str());
  }
  PartitionScheme *sch = backend->find_partition_scheme(part_table_scheme);
  if (!sch) {
    string error_msg;
    error_msg.assign("Partition scheme [");
    error_msg.append(part_table_scheme);
    error_msg.append("] not found.");
    LOG_ERROR("%s\n", error_msg.c_str());
    throw Error(error_msg.c_str());
  }
  if (tab) {  // partitioned table already configured
    LOG_ERROR(
        "Partitioned table configuration already exists, plz create it "
        "directly, or dynamic remove that"
        " configuration if you want another one.\n");
    throw Error("Partitioned table configuration already exists");
  }
  Connection *conn = NULL;
  bool has_table = false;
  try {
    conn = schema->get_connection();
    if (!conn) {
      LOG_ERROR("Failed to get connection when dynamic_add_table.\n");
      throw Error("Failed to get connection when dynamic_add_table.");
    }
    char sql[400];
    sprintf(sql,
            "SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES WHERE "
            "TABLE_SCHEMA='%s' AND TABLE_NAME='%s'",
            schema_name, table_name);
    string value;
    conn->query_for_one_value(sql, value, 0);
    conn->get_pool()->add_back_to_free(conn);
    if (value != "0") {
      has_table = true;
    }
  } catch (exception &e) {
    if (conn) {
      conn->get_pool()->add_back_to_dead(conn);
    }
    string msg =
        "got error when check whether schema has contained the target table "
        "due to : ";
    msg.append(e.what());
    LOG_ERROR("%s\n", msg.c_str());
    throw Error(msg.c_str());
  }
  if (has_table) {
    throw Error("The table already created");
  }

  size_t len_key1 = strlen(part_table_key);
  char *tmp = new char[len_key1 + 1];
  strncpy(tmp, part_table_key, len_key1 + 1);
  add_dynamic_str(tmp);
  part_table_key = tmp;
  create_part_tb_def =
      new PartitionedTable(table_name, part_table_key, sch, schema,
                           sch->get_type(), true, NULL, DEFAULT_VIRTUAL_TIMES);
  schema->add_table(create_part_tb_def);
  backend->add_data_space(create_part_tb_def);
}

bool Statement::support_navicat_profile_sql(ExecutePlan *plan) {
  if (!is_partial_parsed() && st.type == STMT_SELECT) {
    table_link *tbl = st.scanner->first_table;
    if (tbl && tbl->join) {
      if (!strcasecmp(tbl->join->table_name, "PROFILING")) {
        field_item *fi = st.scanner->field_list_head;
        if (fi && fi->next && fi->field_expr &&
            fi->field_expr->type == EXPR_STR && fi->next->field_expr &&
            fi->next->field_expr->type == EXPR_FUNC) {
          StrExpression *str_expr = (StrExpression *)fi->field_expr;
          CurrentStatementFunctionType func_type =
              fi->next->field_expr->get_cur_func_type();
          if (!strcasecmp(str_expr->str_value, "QUERY_ID") &&
              func_type == AGGREGATE_TYPE_SUM) {
            ExecuteNode *navicate_node = plan->get_navicat_profile_sql_node();
            plan->set_start_node(navicate_node);
            return true;
          }
        }
      }
    }
  }
  return false;
}

void Statement::refuse_modify_table_checking() {
  if (!is_partial_parsed() || get_stmt_node()->table_list_head) {
    if (st.type == STMT_UPDATE || st.type == STMT_DELETE) {
      table_link *tbl = st.scanner->first_table;
      if (tbl && tbl->join) {
        const char *schema_name =
            tbl->join->schema_name ? tbl->join->schema_name : schema;
        string full_table_name;
        if (tbl) {
          splice_full_table_name(schema_name, tbl->join->table_name,
                                 full_table_name);
        }
        if (stmt_session->is_refuse_modify_table(full_table_name)) {
          LOG_ERROR(
              "Not support update/delete on a table [%s] which contains no "
              "primary key or key.\n",
              full_table_name.c_str());
          throw NotSupportedError(
              "Not support update/delete on a table which contains no primary "
              "key or key.");
        }
      }
    }
  }
}

void Statement::handle_coalecse_function(table_link **table) {
  if (is_partial_parsed()) return;
  // only consider the top record_scan
  map<unsigned int, pair<unsigned int, string> > coalesce_sqls;

  field_item *fi = st.scanner->field_list_head;
  while (fi) {
    if (!fi->field_expr) {
      continue;
    }
    CurrentStatementFunctionType type = fi->field_expr->get_cur_func_type();
    if (type == AGGREGATE_TYPE_COALESCE) {
      FunctionExpression *expr = (FunctionExpression *)fi->field_expr;
      ListExpression *param_list = expr->param_list;
      expr_list_item *elh = param_list->expr_list_head;
      if (!elh) return;
      string alias;
      string case_when_sql("CASE");
      try {
        do {
          case_when_sql.append(" WHEN (");
          string para;
          elh->expr->to_string(para);
          case_when_sql.append(para);
          case_when_sql.append(" IS NOT NULL) THEN ");
          case_when_sql.append(para);
          elh = elh->next;
        } while (elh && elh != param_list->expr_list_head);
        if (fi->alias)
          alias = fi->alias;
        else
          expr->to_string(alias);
      } catch (...) {
        // ignore replace
        return;
      }
      case_when_sql.append(" ELSE NULL END AS `");
      case_when_sql.append(alias);
      case_when_sql.append("`");
      coalesce_sqls[fi->head_pos] = make_pair(fi->tail_pos, case_when_sql);
    }
    fi = fi->next;
  }

  if (!coalesce_sqls.empty()) {
    string ori_sql(sql);
    string new_sql;
    unsigned int pos = 0;
    map<unsigned int, pair<unsigned int, string> >::iterator it =
        coalesce_sqls.begin();
    for (; it != coalesce_sqls.end(); ++it) {
      new_sql.append(ori_sql, pos, it->first - pos - 1);
      new_sql.append(it->second.second);
      pos = it->second.first;
    }
    new_sql.append(ori_sql, pos, ori_sql.length() - pos);
    Statement *new_stmt = NULL;
    char *tmp;
    SAVE_STR_STMT(sql, new_sql.c_str());
    re_parser_stmt(&(st.scanner), &new_stmt, sql);
    *table = new_stmt->get_stmt_node()->table_list_head;
    LOG_DEBUG("Query sql after replace coalesce is [%s]=[%@]\n", sql, this);
  }
}

bool Statement::handle_view_related_stmt(ExecutePlan *plan,
                                         table_link **table) {
  DataSpace *dataspace = NULL;
  Backend *backend = Backend::instance();

  if (on_view && !is_partial_parsed()) {
    if (*table && st.type != STMT_CREATE_VIEW &&
        st.type != STMT_SHOW_CREATE_TABLE && st.type != STMT_SHOW_FIELDS &&
        st.type != STMT_DROP_VIEW && st.type != STMT_LOCK_TB &&
        st.type != STMT_DROP_TB && st.type != STMT_CREATE_PROCEDURE &&
        st.type != STMT_CREATE_FUNCTION && st.type != STMT_CREATE_TRIGGER &&
        st.type != STMT_SHOW_INDEX && st.type != STMT_CREATE_EVENT) {
      check_and_replace_view(plan->session, table);
    } else if (st.type == STMT_SHOW_INDEX) {
      dataspace = backend->get_metadata_data_space();
      if (!dataspace) {
        throw Error("fail to get metadata dataspace");
      }
      assemble_direct_exec_plan(plan, dataspace);
#ifndef DBSCALE_TEST_DISABLE
      plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
      return true;
    } else if (st.type == STMT_SHOW_CREATE_TABLE ||
               st.type == STMT_SHOW_FIELDS) {
      table_link *tb = *table;
      if (tb) {
        string full_view;
        string sub_sql;

        full_view.assign(tb->join->schema_name ? tb->join->schema_name
                                               : schema);
        full_view.append(".");
        full_view.append(tb->join->table_name);
        sub_sql.clear();
        plan->session->get_one_view(full_view, sub_sql);
        if (!sub_sql.empty()) {
          LOG_DEBUG("Find a view for show create table stmt, %s.\n",
                    full_view.c_str());
          dataspace = backend->get_metadata_data_space();
          if (!dataspace) {
            LOG_ERROR("Fail to get meta datasource.\n");
            throw Error("Fail to get meta datasource.");
          }
          assemble_direct_exec_plan(plan, dataspace);
#ifndef DBSCALE_TEST_DISABLE
          plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
          return true;
        }
      }
    }
    if (!(*table) && st.table_list_head) st.table_list_head = NULL;
  }

  return false;
}

void Statement::check_acl_and_safe_mode(ExecutePlan *plan) {
  if (enable_acl && !plan->session->get_has_check_acl() &&
      (!is_partial_parsed() || st.type != STMT_NON) &&
      (strcmp(supreme_admin_user, plan->session->get_username()) &&
       strcmp(dbscale_internal_user, plan->session->get_username())) &&
      !is_cross_node_join() && !plan->session->get_is_federated_session() &&
      !is_union_table_sub()) {
    plan->session->set_has_check_acl(
        true);  // only check the authenticate once for one statement
    authenticate_statement(plan);
  }

  if (dbscale_safe_sql_mode == 2) {
    if (st.type == STMT_UPDATE) {
      if (!is_partial_parsed() && st.sql->update_oper &&
          !st.sql->update_oper->condition)
        throw Error(
            "Refuse update stmt without condition for "
            "dbscale_safe_sql_mode=2.");
    } else if (st.type == STMT_DELETE) {
      if (!is_partial_parsed() && st.sql->delete_oper &&
          !st.sql->delete_oper->condition)
        throw Error(
            "Refuse delete stmt without condition for "
            "dbscale_safe_sql_mode=2.");
    }
  }
}

void Statement::row_count_and_last_insert_id_checking(ExecutePlan *plan) {
  int enable_last_insert_id_session =
      plan->handler->get_session()
          ->get_session_option("enable_last_insert_id")
          .int_val;
  if (st.last_insert_id_num && !enable_last_insert_id_session) {
    LOG_ERROR("Unsupport sql, SELECT LAST_INSERT_ID() has been disabled.\n");
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    throw NotSupportedError("SELECT LAST_INSERT_ID() has been disabled.");
  }

  if (st.row_count_num != st.scanner->row_count_num || st.row_count_num > 1) {
    LOG_ERROR(
        "Unsupport sql, the sql is [%s], only support one"
        " row_count() in top level select.\n",
        sql);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    throw NotSupportedError(
        "only support one row_count() in top level select.");
  }
  if (st.last_insert_id_num != st.scanner->last_insert_id_num ||
      st.last_insert_id_num > 1) {
    LOG_ERROR(
        "Unsupport sql, the sql is [%s], only support one"
        " last_insert_id() in top level select.\n",
        sql);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    throw NotSupportedError(
        "only support one last_insert_id() in top level select.");
  }
}

DataSpace *Statement::get_dataspace_for_explain(ExecutePlan *plan) {
  table_link *link = st.table_list_head;
  join_node *node = NULL;
  bool use_table_dataspace = true;
  DataSpace *standard_space = NULL;
  Backend *backend = Backend::instance();
  while (link) {
    node = link->join;
    const char *table_name = node->table_name;
    const char *schema_name =
        node->schema_name ? node->schema_name : plan->session->get_schema();
    DataSpace *tmp_space =
        backend->get_data_space_for_table(schema_name, table_name);
    if (!(tmp_space->get_data_source())) {
      use_table_dataspace = false;
      break;
    }
    if (!standard_space) {
      standard_space = tmp_space;
    }
    if (!is_share_same_server(standard_space, tmp_space)) {
      use_table_dataspace = false;
      break;
    }
    link = link->next;
  }
  LOG_DEBUG("explain use table dataspace %d\n", use_table_dataspace);
  if (standard_space && use_table_dataspace) {
    LOG_DEBUG("explain use data space %s\n", standard_space->get_name());
  } else {
    standard_space = backend->get_metadata_data_space();
    if (!standard_space) {
      standard_space = backend->get_catalog();
    }
    LOG_DEBUG("explain use data space %s\n", standard_space->get_name());
  }
  return standard_space;
}

void Statement::generate_plan_for_explain_related_stmt(ExecutePlan *plan) {
  DataSpace *dataspace = NULL;
  if (st.type == STMT_EXPLAIN) {
    dataspace = get_dataspace_for_explain(plan);
    assemble_direct_exec_plan(plan, dataspace);
    return;
  }
  if (st.type == STMT_DBSCALE_EXPLAIN) {
    if (!st.sql->explain_oper) {
      throw Error("no sql detected to explain");
    }
    Session *session = plan->session;
    session->set_in_explain(true);
    session->set_explain_node_sequence_id(0);
    session->reset_explain_sub_sql_list();
    session->clean_explain_element_list();

    explain_sql.clear();
    explain_sql.append(sql + st.sql->explain_oper->sql_start_pos - 1);
    LOG_DEBUG("Generate explain sql [%s]\n", explain_sql.c_str());
    Parser *parser = Driver::get_driver()->get_parser();
    Statement *explain_stmt =
        parser->parse(explain_sql.c_str(), stmt_allow_dot_in_ident, true, NULL,
                      NULL, NULL, ctype);
    explain_stmt->set_default_schema(schema);

    ExecutePlan *new_plan = NULL;
    try {
      new_plan = (ExecutePlan *)(plan->handler->get_execute_plan(explain_stmt));
      explain_stmt->generate_execution_plan(new_plan);
      if (!new_plan->start_node) {
        LOG_ERROR("generate_execution_plan for %s error\n",
                  explain_sql.c_str());
        throw Error("generate_execution_plan error");
      }
      list<ExecuteNode *> node_list;

      int seq_id = session->inc_and_get_explain_node_sequence_id();
      new_plan->start_node->set_node_id(seq_id);
      new_plan->sql = explain_stmt->get_sql();
      node_list.push_back(new_plan->start_node);
      session->update_explain_info(node_list, 0, seq_id);
    } catch (...) {
      if (new_plan) {
        delete new_plan;
        new_plan = NULL;
      }
      if (explain_stmt) {
        explain_stmt->free_resource();
        delete explain_stmt;
        explain_stmt = NULL;
      }
      session->set_in_explain(false);
      throw;
    }

    if (new_plan) {
      delete new_plan;
      new_plan = NULL;
    }
    if (explain_stmt) {
      explain_stmt->free_resource();
      delete explain_stmt;
      explain_stmt = NULL;
    }

    session->set_in_explain(false);
    assemble_explain_plan(plan);
    return;
  }
}

bool Statement::generate_plan_for_federate_session(ExecutePlan *plan,
                                                   table_link *table) {
  if (st.type != STMT_SET) plan->session->set_has_check_first_sql(true);
  if (st.type == STMT_SHOW_TABLE_STATUS && check_federated_show_status()) {
    plan->session->set_is_federated_session(true);
    assemble_federated_table_status_node(plan);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    return true;
  } else if (st.type == STMT_SELECT && check_federated_name() &&
             check_federated_select()) {
    int field_num = get_field_num();
    plan->session->set_is_federated_session(true);
    assemble_empty_set_node(plan, field_num);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    return true;
  } else if (plan->session->get_is_federated_session() &&
             st.type == STMT_SELECT && check_federated_name() &&
             (MethodType)cross_node_join_method == DATA_MOVE_READ &&
             subquery_type == SUB_SELECT_NON) {
    // If it is a subquery, then it is not a query received from federated
    // table.
    assemble_federated_thread_plan(plan, table->join->table_name);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    return true;
  }
  return false;
}

table_link *Statement::adjust_select_sql_for_funs_and_autocommit(
    ExecutePlan *plan, table_link *table) {
  if (st.type == STMT_SELECT) {
    plan->session->set_error_packet(NULL);
    if (st.session_var_list != NULL) {
      sql_tmp.assign(sql);
      bool has_autocommit = has_and_rebuild_autocommit_sql(plan);
      if (has_autocommit) {
        sql = sql_tmp.c_str();
        Statement *new_stmt = NULL;
        re_parser_stmt(&(st.scanner), &new_stmt, sql);
        table = new_stmt->get_stmt_node()->table_list_head;
        LOG_DEBUG("Query sql after replace AUTOCOMMIT is [%s]\n", sql);
      }
    }

    if (st.has_found_rows) {
      sql_tmp.assign(sql);
      rebuild_found_rows_sql(st.scanner, plan);
      sql = sql_tmp.c_str();
      Statement *new_stmt = NULL;
      re_parser_stmt(&(st.scanner), &new_stmt, sql);
      table = new_stmt->get_stmt_node()->table_list_head;
      LOG_DEBUG("Query sql after replace found_rows() is [%s]\n", sql);
    }

    if (st.row_count_num) {
      sql_tmp.assign(sql);
      replace_sql_function_field(FUNCTION_TYPE_ROW_COUNT, sql_tmp, st.scanner,
                                 plan);
      Statement *new_stmt = NULL;
      sql = sql_tmp.c_str();
      re_parser_stmt(&(st.scanner), &new_stmt, sql);
      table = new_stmt->get_stmt_node()->table_list_head;
      LOG_DEBUG("Query sql after replace row_count() is [%s]\n", sql);
    }

    if (st.last_insert_id_num) {
      set<DataSpace *> shard_table;
      DataSpace *tmp_dataspace = NULL;
      table_link *tmp_table = table;
      const char *schema_name = NULL, *table_name = NULL;
      while (tmp_table) {
        schema_name = tmp_table->join->schema_name
                          ? tmp_table->join->schema_name
                          : schema;
        table_name = tmp_table->join->table_name;
        if (schema_name && !strcmp(schema_name, "information_schema")) {
          tmp_table = tmp_table->next;
          continue;
        } else {
          tmp_dataspace = Backend::instance()->get_data_space_for_table(
              schema_name, table_name);
        }
        if (!tmp_dataspace->get_data_source()) {
          if (((Table *)tmp_dataspace)->is_partitioned()) {
            shard_table.insert(tmp_dataspace);
          }
        }
        tmp_table = tmp_table->next;
      }
      if (shard_table.size()) {
        sql_tmp.assign(sql);
        replace_sql_function_field(FUNCTION_TYPE_LAST_INSERT_ID, sql_tmp,
                                   st.scanner, plan);
        Statement *new_stmt = NULL;
        sql = sql_tmp.c_str();
        re_parser_stmt(&(st.scanner), &new_stmt, sql);
        table = new_stmt->get_stmt_node()->table_list_head;
        LOG_DEBUG("Query sql after replace last_insert_id() is [%s]\n", sql);
      }
    }

    if (st.var_item_list) deal_select_assign_uservar(st.scanner);
  }
  return table;
}

void Statement::generate_plan_for_prepare_related_stmt(ExecutePlan *plan) {
  DataSpace *dataspace = NULL;

  if (st.type == STMT_PREPARE) {
    const char *prepare_sql = st.sql->prepare_oper->preparable_stmt;
    if (st.var_item_list == NULL) {
      dataspace = get_prepare_sql_dataspace();
      assemble_com_query_prepare_plan(plan, dataspace, prepare_sql,
                                      st.sql->prepare_oper->prepare_name);
    } else {
      /*
       * process SQL like :
       *    SET @s='SELECT SQRT(POW(?,2) + POW(?,2)) AS hypotenuse';
       *    PREPARE prod FROM @s;
       *
       * Then the prepare_sql is
       *    SELECT SQRT(POW(?,2) + POW(?,2)) AS hypotenuse
       *    where equals the user var @s delete the \' lies head and the lies \'
       * tail
       *
       */
      string var_name(prepare_sql);
      boost::to_upper(var_name);
      string var_value = plan->session->get_user_var_origin_value(var_name);
      dataspace = get_prepare_sql_dataspace();
      assemble_com_query_prepare_plan(plan, dataspace, var_value.c_str(),
                                      st.sql->prepare_oper->prepare_name);
    }
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    return;
  }
  if (st.type == STMT_EXEC_PREPARE) {
    assemble_com_query_exec_prepare_plan(
        plan, st.sql->exec_prepare_oper->prepare_name, st.var_item_list);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    return;
  }
  if (st.type == STMT_DROP_PREPARE) {
    dataspace = get_prepare_sql_dataspace();
    assemble_com_query_drop_prepare_plan(
        plan, dataspace, st.sql->drop_prepare_oper->prepare_name, sql);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    return;
  }
}

void Statement::generate_plan_for_flush_config_to_file(ExecutePlan *plan) {
  const char *file_name = st.sql->dbscale_flush_config_to_file_oper->file_name;
  bool flush_all =
      st.sql->dbscale_flush_config_to_file_oper->flush_all == 0 ? false : true;
  if (!file_name) {
    file_name = OptionParser::instance()->get_config_file().c_str();
  }
  assemble_dbscale_flush_config_to_file_plan(plan, file_name, flush_all);
#ifndef DBSCALE_TEST_DISABLE
  plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
  return;
}

void Statement::generate_plan_for_flush_acl(ExecutePlan *plan) {
  assemble_dbscale_flush_acl_plan(plan);
  return;
}

void Statement::generate_plan_for_flush_table_info(ExecutePlan *plan,
                                                   const char *schema_name,
                                                   const char *table_name) {
  assemble_dbscale_flush_table_info_plan(plan, schema_name, table_name);
  return;
}

void Statement::assemble_dbscale_flush_table_info_plan(ExecutePlan *plan,
                                                       const char *schema_name,
                                                       const char *table_name) {
  LOG_DEBUG("Assemble DBSCALE FLUSH table info plan\n");
  ExecuteNode *node =
      plan->get_dbscale_flush_table_info_node(schema_name, table_name);
  plan->set_start_node(node);
}

void Statement::adjust_sql_for_into_file_or_select_into(table_link **table) {
  if (is_into_outfile()) {
    validate_into_outfile();
    into_outfile_item *into_outfile = get_latest_stmt_node()->into_outfile;
    unsigned int start_pos = into_outfile->start_pos;
    unsigned int end_pos = into_outfile->end_pos;
    outfile_sql.append(sql, start_pos - 1);
    outfile_sql.append(end_pos - start_pos + 1, ' ');
    outfile_sql.append(sql + end_pos, strlen(sql) - end_pos);
    sql = outfile_sql.c_str();
    Statement *new_stmt = NULL;
    re_parser_stmt(&(st.scanner), &new_stmt, sql);
    (*table) = new_stmt->get_stmt_node()->table_list_head;
  }

  if (is_select_into()) {
    select_into_item *select_into =
        get_latest_stmt_node()->sql->select_oper->select_into;
    name_item *into_header = select_into->into_list;
    name_item *into_tmp = into_header;
    bool all_uservar = true;
    do {
      if (into_tmp->is_uservar == false) {
        all_uservar = false;
        break;
      }
      into_tmp = into_tmp->next;
    } while (into_tmp != into_header);

    if (all_uservar) {
      unsigned int start_pos = select_into->start_pos;
      unsigned int end_pos = select_into->end_pos;
      select_into_sql.append(sql, start_pos - 1);
      select_into_sql.append(end_pos - start_pos + 1, ' ');
      select_into_sql.append(sql + end_pos, strlen(sql) - end_pos);
      sql = select_into_sql.c_str();
      Statement *new_stmt = NULL;
      re_parser_stmt(&(st.scanner), &new_stmt, sql);
      (*table) = new_stmt->get_stmt_node()->table_list_head;
    }
  }
}

void Statement::generate_plan_for_partial_parse_stmt(ExecutePlan *plan,
                                                     const char *schema_name,
                                                     const char *table_name,
                                                     table_link *table) {
  DataSpace *dataspace = NULL;
  Backend *backend = Backend::instance();

  if (st.type == STMT_CREATE_SELECT || st.type == STMT_CREATE_LIKE) {
    LOG_ERROR(
        "Not support partial parse for create select or create like stmt %s.\n",
        sql);
    string err(
        "Not support partial parse for create select or create like stmt:");
    if (st.error_message) err.append(st.error_message);
    LOG_ERROR("%s\n", err.c_str());
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    throw NotSupportedError(err.c_str());
  }
  if (table) {
    schema_name = table->join->schema_name ? table->join->schema_name : schema;
    table_name = table->join->table_name;
  } else {
    schema_name = NULL;
    table_name = NULL;
  }

  if (table) {
    if (st.type == STMT_SELECT &&
        !strcasecmp(schema_name, "information_schema")) {
      if (!strcasecmp(table_name, "events") &&
          !table->next) {  // query information_schema.events, which is not
                           // stored on meta source
        // send to the schema source, which store the event object
        dataspace = backend->get_data_space_for_table(schema, NULL);
      } else {
        if (backend->get_metadata_data_space())
          dataspace = backend->get_metadata_data_space();
        else
          dataspace = backend->get_catalog();
      }
      assemble_direct_exec_plan(plan, dataspace);
      return;
    }
  }

  dataspace = backend->get_data_space_for_table(schema_name, table_name);
  if (table && !table->join->schema_name && !is_union_table_sub() &&
      !is_cross_node_join()) {
    plan->session->add_table_without_given_schema(dataspace);
  }
  if (dataspace->is_normal()) {
    if (st.type == STMT_LOAD) {
      if (st.sql->load_oper->local) {
        assemble_load_local_plan(plan, dataspace);
      } else {
        assemble_load_data_infile_plan(plan, dataspace);
      }
    } else {
      assemble_direct_exec_plan(plan, dataspace);
    }
  } else {
    LOG_ERROR(
        "The SQL which contains partition table can not be parsed: [%s].\n",
        st.error_message);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    string error_message(
        "The SQL which contains partition table can not be parsed: ");
    error_message.append(st.error_message);
    throw UnSupportPartitionSQL(error_message.c_str());
  }
#ifndef DBSCALE_TEST_DISABLE
  plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
  return;
}

void Statement::generate_plan_for_rename_or_drop_table_stmt(ExecutePlan *plan) {
  if (st.type == STMT_RENAME_TABLE) {
    rename_table_node_list *head = st.sql->rename_table_oper->table_head;
    rename_table_node_list *tail = st.sql->rename_table_oper->table_tail;
    if (head != tail) {
      LOG_ERROR("DBScale do not support multiple-table rename.");
#ifndef DBSCALE_TEST_DISABLE
      plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
      throw NotSupportedError("Not supported multiple-table rename.");
    }

    handle_rename_table(plan, head);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    return;
  }
  if (st.type == STMT_DROP_TB) {
    if (enable_table_recycle_bin && st.sql->drop_tb_oper->op_tmp == 0 &&
        st.sql->drop_tb_oper->op_exists == 0)
      assemble_move_table_to_recycle_bin_plan(plan);
    else
      assemble_drop_mul_table(plan);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    return;
  }
}

static bool check_col_names_contain_str(name_item *col_list,
                                        const char *part_key) {
  if (!col_list) return true;
  name_item *col_name_head = col_list;
  name_item *col_name = col_list;
  bool is_cover = false;
  do {
    if (!strcasecmp(col_name->name, part_key)) {
      is_cover = true;
      break;
    };
    col_name = col_name->next;
  } while ((col_name != col_name_head));
  return is_cover;
}

void Statement::check_part_table_primary_key(const char *part_key) {
  if (!check_part_primary) return;
  if (st.type != STMT_CREATE_TB && st.type != STMT_ALTER_TABLE) return;
  name_item *primary_key = NULL;
  unique_key_list *unique_keys = NULL;

  if (st.type == STMT_CREATE_TB) {
    if (!st.sql || !st.sql->create_tb_oper) {
      string error_msg;
      error_msg.assign("sql statement node memory error.");
      throw Error(error_msg.c_str());
    }
    primary_key = st.sql->create_tb_oper->primary_key_cols;
    unique_keys = st.sql->create_tb_oper->all_unique_keys;
  } else if (st.type == STMT_ALTER_TABLE) {
    if (!st.sql || !st.sql->alter_tb_oper) {
      string error_msg;
      error_msg.assign("sql statement node memory error.");
      throw Error(error_msg.c_str());
    }
    if (st.sql->alter_tb_oper->alter_type != ADD_COLUMN &&
        st.sql->alter_tb_oper->alter_type != ADD_PRIMARY_KEY &&
        st.sql->alter_tb_oper->alter_type != MODIFY_COLUMN &&
        st.sql->alter_tb_oper->alter_type != CHANGE_COLUMN &&
        st.sql->alter_tb_oper->alter_type != ADD_UNIQUE_KEY)
      return;
    primary_key = st.sql->alter_tb_oper->primary_key_cols;
    unique_keys = st.sql->alter_tb_oper->all_unique_keys;
  }
  if (!unique_keys && !primary_key) return;

  bool is_cover = check_col_names_contain_str(primary_key, part_key);
  if (!is_cover) {
    string error_msg;
    error_msg.assign("primary key must contain partition key.");
    LOG_ERROR("%s\n", error_msg.c_str());
    throw Error(error_msg.c_str());
  }

  while (unique_keys) {
    is_cover = check_col_names_contain_str(unique_keys->col_list, part_key);
    if (!is_cover) {
      string error_msg;
      error_msg.assign("unique key must contain partition key.");
      LOG_ERROR("%s\n", error_msg.c_str());
      throw Error(error_msg.c_str());
    }
    unique_keys = unique_keys->next;
  }
  return;
}

// return true means sql be executed on master role dbscale
bool Statement::prepare_dataspace_for_extend_create_tb_stmt(ExecutePlan *plan,
                                                            table_link *table) {
  if (st.type != STMT_CREATE_TB) return false;
  if (dbscale_safe_sql_mode == 2 && st.sql->create_tb_oper->data_type_list) {
    for (data_type_node *it = st.sql->create_tb_oper->data_type_list;
         it != NULL; it = it->next) {
      if (800000 <= it->data_type && it->data_type < 1000000) {
        throw Error(
            "dbscale_safe_sql_mode is 2 CREATE TABLE disable double/float "
            "type ");
      }
    }
  }
  const char *part_table_scheme = NULL;
  const char *part_table_key = NULL;
  if (st.sql->create_tb_oper->part_table_def_start_pos > 0) {
    LOG_DEBUG("Handle create part table space.\n");
    part_table_scheme = st.sql->create_tb_oper->part_table_scheme;
    part_table_key = st.sql->create_tb_oper->part_table_key;
  } else if (plan->session->get_session_option("auto_space_level").uint_val ==
             AUTO_SPACE_TABLE) {
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) {
      if (!MultipleManager::instance()->get_is_cluster_master()) {
        LOG_ERROR(
            "When auto-space-level = 2, the create sql should only executed on "
            "master dbscale.\n");
        throw Error(
            "When auto-space-level = 2, the create sql should only executed on "
            "master dbscale.");
      }
    }
#endif
    Backend *backend = Backend::instance();
    PartitionScheme *partition_scheme = backend->get_last_scheme();
    if (!partition_scheme) {
      string error_msg;
      error_msg.assign(
          "auto_space_level is set as AUTO_SPACE_TABLE, but DBScale does not "
          "config any partition scheme.");
      LOG_ERROR("%s\n", error_msg.c_str());
      throw Error(error_msg.c_str());
    }
    part_table_scheme = partition_scheme->get_name();
    part_table_key = get_partition_key_for_auto_space();
  }
  if (part_table_scheme && part_table_key) {
    check_part_table_primary_key(part_table_key);
#ifndef CLOSE_MULTIPLE
    if (multiple_mode) {
      if (MultipleManager::instance()->get_is_cluster_master()) {
        MultipleManager::instance()->acquire_start_config_lock(
            get_session()->get_zk_start_config_lock_extra_info());
        try {
          add_partitioned_table_config_before_create_tb(
              table, part_table_scheme, part_table_key);
        } catch (...) {
          LOG_ERROR("Fail to add_partitioned_table_config_before_create_tb.\n");
          MultipleManager::instance()->release_start_config_lock(
              get_session()->get_start_config_lock_extra_info());
          throw;
        }
        MultipleManager::instance()->release_start_config_lock(
            get_session()->get_start_config_lock_extra_info());
      } else {
        LOG_DEBUG("CREATE TB or ALTER TB be execute on slave dbscale\n");
        assemble_forward_master_role_plan(plan);
#ifndef DBSCALE_TEST_DISABLE
        plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
        return true;
      }
    } else
#endif
      add_partitioned_table_config_before_create_tb(table, part_table_scheme,
                                                    part_table_key);
  }
  return false;
}

void Statement::generate_plan_for_one_table_situation(ExecutePlan *plan,
                                                      table_link *one_table,
                                                      DataSpace *one_space) {
#ifndef DBSCALE_TEST_DISABLE
  if (on_test_stmt) {
    Backend *bk = Backend::instance();
    dbscale_test_info *test_info = bk->get_dbscale_test_info();
    if (st.type == STMT_INSERT)
      while (
          !strcasecmp(test_info->test_case_name.c_str(),
                      "swap_backend_monitor") &&
          !strcasecmp(test_info->test_case_operation.c_str(), "block_insert")) {
        timespec_t t = (timespec_t)ACE_Time_Value(0, 1000000);  // sleep 1s
        ACE_OS::nanosleep(&t);
      }
  }
#endif

  if (handle_executable_comments_after_cnj(plan)) {
    return;
  }

  LOG_DEBUG("Only has one table, so skip the analysis_record_scan_tree.\n");
  one_table_node.space = one_space;
  one_table_node.only_one_table = true;

  if (!one_space->get_data_source()) {
    one_table_node.table = one_table;
    vector<const char *> *key_names =
        ((PartitionedTable *)one_space)->get_key_names();

    table_link *par_tb_tmp = one_table;
    const char *schema_name_tmp =
        par_tb_tmp->join->schema_name ? par_tb_tmp->join->schema_name : schema;

    const char *table_name_tmp = par_tb_tmp->join->table_name;
    const char *table_alias_tmp = par_tb_tmp->join->alias;

    one_table_node.rs = st.scanner;
    if (st.type != STMT_INSERT) {
      vector<const char *>::iterator it_key = key_names->begin();
      for (; it_key != key_names->end(); ++it_key) {
        fullfil_par_key_equality(one_table_node.rs, schema_name_tmp,
                                 table_name_tmp, table_alias_tmp, *it_key);
      }
    }
    check_found_rows_with_group_by(plan);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    handle_one_par_table_one_spaces(plan);
    return;

  } else {
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    handle_no_par_table_one_spaces(plan);
    return;
  }
}

void Statement::handle_separated_nodes_before_finial_execute(
    ExecutePlan *plan) {
  plan->session->add_cross_node_join_sql(sql);
  plan->session->get_status()->item_inc(TIMES_CROSS_NODE_JOIN);
  plan->session->set_is_complex_stmt(true);
  plan->add_session_traditional_num();
  /* Execute the subquery separated exection node list.
   *
   * Basic process:
   * 1. work through the exec_nodes vector, invoke the node->init_node()
   * 2. work through the exec_nodes vector, invoke the node->execute()
   * 3. work through the exec_nodes vecotr, invoke the node->clean_node()
   *
   * Note: do not invoke the node->execute() for the last node, which is done
   *       by the current statement.
   * */
  try {
    if (!lower_case_table_names) {
      const char *error_message =
          "DBScale is not supported that execute cross node join with "
          "lower-case-table-names=0.Try lower-case-table-names=1.";
      LOG_ERROR("%s\n", error_message);
      throw Error(error_message);
    }
    MethodType join_type = (MethodType)(
        plan->session->get_session_option("cross_node_join_method").int_val);
    if (exec_nodes.size() == 1 &&
        record_scan_need_cross_node_join.count(st.scanner) &&
        join_type == DATA_MOVE_READ) {
      handle_federated_join_method();
    }
    SeparatedExecNode *last_node = NULL;
    vector<SeparatedExecNode *>::iterator it = exec_nodes.begin();
    for (; it != exec_nodes.end(); ++it) {
      (*it)->init_node();
    }
    if (stmt_max_threads <= 1 || exec_nodes.size() <= 2 ||
        !sep_node_can_execute_parallel(plan)) {
      // there is meaningless for parallel execution if the exec_nodes less
      // than 3, cause the last one will not be executed parallelly
      it = exec_nodes.begin();
      for (; it + 1 != exec_nodes.end(); ++it) {
        (*it)->pre_execute();
        (*it)->execute();
        (*it)->post_execute();
      }
      last_node = (*it);
    } else {
      LOG_DEBUG(
          "Start to execute the separated node parallelly with %d nodes.\n",
          exec_nodes.size());
      is_sep_node_parallel_exec = true;
      init_parallel_separated_node_execution();
      bool has_finish_node_assignment = false;
      bool has_finish_node_execution_wait = false;
      try {
        while (1) {
          if (!has_finish_node_assignment) {
            loop_and_find_executable_sep_nodes();
            has_finish_node_assignment = assign_threads_for_work_vec();
            LOG_DEBUG(
                "After threads assignment, has_finish_node_assignment is %d,"
                " work_pointer is %d, work_vec size is %d, running_threads is "
                "%d.\n",
                has_finish_node_assignment ? 1 : 0, work_pointer,
                work_vec.size(), running_threads);
          }
          if (!has_finish_node_execution_wait) {
            has_finish_node_execution_wait = wait_threads_for_execution();
            LOG_DEBUG(
                "After node execution wait, has_finish_node_execution_wait is "
                "%d,"
                " fin_size is %d.\n",
                has_finish_node_execution_wait ? 1 : 0, fin_size);
          }
          if (has_finish_node_execution_wait) break;
        }
      } catch (...) {
        LOG_DEBUG("Get exception/error during sparated node parallelly.\n");
        clean_up_parallel_separated_node_execution();
        throw;
      }
      clean_up_parallel_separated_node_execution();
      last_node = exec_nodes[exec_nodes.size() - 1];
    }

    last_node->pre_execute();
    exec_sql_after_separated_exec.clear();
    exec_sql_after_separated_exec.append(last_node->get_execute_sql());
    sql = exec_sql_after_separated_exec.c_str();
    while (num_of_separated_exec_space--) {
      spaces.erase(spaces.begin());
    }
    if (par_table_num >= num_of_separated_par_table) {
      par_table_num = par_table_num - num_of_separated_par_table;
      while (num_of_separated_par_table-- && par_tables.size() > 1) {
        par_tables.erase(par_tables.begin());
      }
    }
    if (record_scan_need_cross_node_join.count(st.scanner) ||
        record_scan_seperated_table_subquery.count(st.scanner)) {
      if (st.type == STMT_UPDATE || st.type == STMT_DELETE)
        set_cross_node_join(false);
      else
        set_cross_node_join(true);
    }
    record_scan *child_rs = st.scanner->children_begin;
    Statement *new_stmt = NULL;
    DataSpace *tmp_space = record_scan_one_space_tmp_map[st.scanner];
    re_parser_stmt(&(st.scanner), &new_stmt, sql);
    if (st.type == STMT_UPDATE)
      *(st.sql->update_oper) = *(new_stmt->get_stmt_node()->sql->update_oper);
    if (st.type == STMT_INSERT_SELECT)
      *(st.sql->insert_oper) = *(new_stmt->get_stmt_node()->sql->insert_oper);
    record_scan_one_space_tmp_map[st.scanner] = tmp_space;
    refresh_norm_table_spaces();
    refresh_part_table_links();
    if (need_merge_subquery)
      refresh_tables_for_merged_subquery(child_rs, st.scanner->children_begin);
  } catch (...) {
    plan->session->clear_column_replace_string();
    throw;
  }
  plan->session->clear_column_replace_string();
}

void Statement::generate_plan_for_set_stmt(ExecutePlan *plan) {
  DataSpace *dataspace = NULL;
  Backend *backend = Backend::instance();
  if (is_set_password(plan)) {
    dataspace = backend->get_auth_data_space();
    assemble_direct_exec_plan(plan, dataspace);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    return;
  }

  if (!spaces.size()) {
    dataspace = backend->get_data_space_for_table(schema, NULL);
  } else if (par_table_num != 0) {
    LOG_ERROR("Unsupport set user variable select from partition table.\n");
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    throw NotSupportedError(
        "Unsupport set user variable select from partition table");
  } else if (spaces.size() == 1) {
    dataspace = spaces[0];
  } else {
    LOG_ERROR(
        "Unsupport set user variable select from table"
        " execute in different server.\n");
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    throw NotSupportedError(
        "Unsupport set user variable select from partition table");
  }

  if (st.sql->set_oper && st.sql->set_oper->names && st.sql->set_oper->value) {
    string charset_val = st.sql->set_oper->value;
    plan->handler->set_names_charset_in_session(plan->handler->get_session(),
                                                charset_val);
  }
  handle_session_var(plan);
  deal_set_var();
  assemble_set_plan(plan, dataspace);
#ifndef DBSCALE_TEST_DISABLE
  plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
  return;
}

bool Statement::generate_plan_according_to_par_table_num_and_spaces(
    ExecutePlan *plan) {
  if (!spaces.size()) {
    LOG_ERROR("Fail to find dataspace for sql [%s].\n", sql);
    throw Error("Fail to find dataspace.");
  } else if (!par_table_num && spaces.size() == 1) {
    handle_no_par_table_one_spaces(plan);
    return true;
  } else if (par_table_num == 1 && spaces.size() == 1) {
    check_found_rows_with_group_by(plan);
    handle_one_par_table_one_spaces(plan);
    return true;
  } else if (!par_table_num && spaces.size() > 1) {
    if (spaces.size() == 2) {
      if (st.type == STMT_INSERT_SELECT || st.type == STMT_REPLACE_SELECT ||
          (st.type == STMT_DBSCALE_ESTIMATE &&
           st.estimate_type == STMT_INSERT_SELECT)) {
        /*For insert...select stmt, two dataspaces is executable. Cause the
         * select part can executed independent from the insert part.*/
        handle_no_par_table_one_spaces(plan);
        return true;
      }
      if (st.type == STMT_UPDATE || st.type == STMT_DELETE) {
        if (record_scan_one_space_tmp_map[st.scanner] == spaces[0]) {
          /*For update/delete stmt with subquery, the select part dataspace
           * should cover the modify part. spaces[0] stored the merged select
           * part dataspace. record_scan_one_space_tmp_map[st.scanner] stored
           * the merged dataspace of top record scan.*/
          handle_no_par_table_one_spaces(plan);
          return true;
        }
      }
    }
    LOG_ERROR("Unsupport normal sql, the sql is [%s].\n", sql);
    throw UnSupportPartitionSQL("Unsupport normal sql");
  } else if (par_table_num == 1) {  // mul dataspace, such as insert_select stmt
    check_found_rows_with_group_by(plan);
    handle_one_par_table_mul_spaces(plan);
    return true;
  } else {  // mul par_table and mul dataspace
    // TODO: support it by execute each record_scan, which can not be merged,
    // respectively.
    check_found_rows_with_group_by(plan);
    plan->session->set_is_complex_stmt(true);
    handle_mul_par_table_mul_spaces(plan);
    return true;
    //    LOG_ERROR("Unsupport partition sql, the sql is [%s].\n", sql); throw
    //    UnSupportPartitionSQL();
  }

  return false;
}

void Statement::generate_plan_for_non_support_trx_stmt(ExecutePlan *plan) {
  if (st.type == STMT_RELEASE_SAVEPOINT) {
    LOG_WARN("Release Savepoint is not supported, sql is %s, just ignore it.\n",
             sql);
    ExecuteNode *node = plan->get_return_ok_node();
    plan->set_start_node(node);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    return;
  } else {
    /*Savepoint is not support when enable-xa-transaction is enabled, just
     * ignore it.*/
    LOG_WARN(
        "Savepoint is not support when enable-xa-transaction is enabled, sql "
        "is %s, just ignore it.\n",
        sql);
    ExecuteNode *node = plan->get_return_ok_node();
    plan->set_start_node(node);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    return;
  }
}

void Statement::check_stmt_for_lock_mode(ExecutePlan *plan) {
  try {
    deal_lock_table(plan);
  } catch (dbscale::sql::SQLError &e) {
    if (e.get_errno() == ERROR_NOT_LOCK_CODE)
      LOG_DEBUG("NotLockError for sql [%s]\n", sql);
    else if (e.get_errno() == ERROR_NOT_EXECUTE_FOR_LOCK_CODE)
      LOG_DEBUG("NotExecutableForLock sql [%s]\n", sql);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    throw;
  }
}

void Statement::generate_plan_for_information_schema(ExecutePlan *plan,
                                                     const char *table_name,
                                                     table_link *table) {
  DataSpace *dataspace = NULL;
  Backend *backend = Backend::instance();

  if (skip_informatic_reference_query &&
      !strcasecmp(table_name, "KEY_COLUMN_USAGE") && table->next &&
      !strcasecmp(table->next->join->table_name, "KEY_COLUMN_USAGE") &&
      table->next->next &&
      !strcasecmp(table->next->next->join->table_name,
                  "REFERENTIAL_CONSTRAINTS")) {
    LOG_INFO(
        "Skip the reference constrain query from informatic due to "
        "skip-informatic-reference-query.\n");
    int field_num = get_field_num();
    assemble_empty_set_node(plan, field_num);
    return;
  }

  if (support_navicat_profile_sql(plan)) {
    return;
  } else if (!strcasecmp(table_name, "events") &&
             !table->next) {  // query information_schema.events, which is not
                              // stored on meta source
    // send to the schema source, which store the event object
    dataspace = backend->get_data_space_for_table(schema, NULL);
  } else if ((!strcasecmp(table_name, "tables") ||
              !strcasecmp(table_name, "columns")) &&
             table->next) {
    const char *schema_name = table->next->join->schema_name
                                  ? table->next->join->schema_name
                                  : schema;
    if (!strcasecmp(schema_name, "information_schema")) {
      dataspace = backend->get_metadata_data_space();
      if (!dataspace) {
        LOG_ERROR("Fail to get meta datasource.\n");
        throw Error("Fail to get meta datasource.");
      }
    } else {
      dataspace = backend->get_data_space_for_table(
          schema_name, table->next->join->table_name);
    }
#ifdef DEBUG
    ACE_ASSERT(dataspace);
#endif
    if (!dataspace->is_normal()) {
      LOG_ERROR(
          "unsupport join between information_schema.tables/columns and a "
          "partitioned table.\n");
      throw NotSupportedError(
          "unsupport join between information_schema.tables/columns and a "
          "partitioned table.");
    }
    LOG_DEBUG(
        "found table join between information_schema.tables/columns and a "
        "normal table, dataspace name is [%s]\n",
        dataspace->get_name());
  } else {
    if (backend->get_metadata_data_space())
      dataspace = backend->get_metadata_data_space();
    else
      dataspace = backend->get_catalog();
  }
  assemble_direct_exec_plan(plan, dataspace);
}

void Statement::pre_work_for_non_one_table_exec_plan(ExecutePlan *plan,
                                                     bool only_one_table,
                                                     record_scan *one_rs,
                                                     DataSpace *one_space,
                                                     table_link *one_table) {
  if (st.has_where_rownum) {
    LOG_ERROR("Only support normal table to use rownum.\n");
    throw NotSupportedError("Only support normal table to use rownum.");
  }
  if (only_one_table) {  // fail to use one table mode, so need to re-fill the
                         // info to related structures, which is skipped in the
                         // previous code
    record_scan_all_table_spaces[one_table] = one_space;
    need_clean_record_scan_all_table_spaces = true;
    record_scan_all_par_tables_map[one_rs].push_back(one_table);
    need_clean_record_scan_all_par_tables_map = true;
  }
  record_scan_par_key_equality.clear();
  record_scan_par_key_values.clear();
  need_center_join = false;
#ifndef DBSCALE_TEST_DISABLE
  dbscale_test_info *test_info = plan->session->get_dbscale_test_info();
  if (!strcasecmp(test_info->test_case_name.c_str(), "cross_node_join") &&
      !strcasecmp(test_info->test_case_operation.c_str(), "use_center_join")) {
    need_center_join = true;
  }
#endif
  if (need_center_join && st.type == STMT_SELECT && !union_all_sub &&
      !contains_view && !st.scanner->is_select_union)
    analysis_center_join(plan);
  else
    analysis_record_scan_tree(st.scanner, plan);

  if ((exec_nodes.size() > 1 ||
       (exec_nodes.size() == 1 &&
        record_scan_need_cross_node_join.count(st.scanner))) &&
      st.type != STMT_DBSCALE_ESTIMATE) {
    // For a statement which contains seperated node, we should regard it as not
    // read only.
    plan->session->set_read_only(false);
    handle_separated_nodes_before_finial_execute(plan);
  } else if (st.type != STMT_DBSCALE_ESTIMATE) {
    /* For the sql which need to execute, We should not set the
     * par_table_num=1 since we need the dataspace to assemble execution
     * plan. */
    if (par_table_num > num_of_separated_par_table) {
      par_table_num = par_table_num - num_of_separated_par_table;
      while (num_of_separated_par_table-- && par_tables.size() > 1) {
        par_tables.erase(par_tables.begin());
      }
    }
  }
}

bool Statement::generate_plan_for_no_table_situation(ExecutePlan *plan) {
  if ((!st.table_list_head && st.type != STMT_LOAD && st.type != STMT_SET) ||
      st.type == STMT_LOCK_TB || st.type == STMT_CREATE_EVENT ||
      st.type == STMT_DROP_EVENT || st.type == STMT_CREATE_PROCEDURE ||
      st.type == STMT_DBSCALE_MIGRATE ||
      st.type == STMT_DBSCALE_SHOW_VIRTUAL_MAP ||
      st.type == STMT_DBSCALE_SHOW_SHARD_MAP ||
      st.type == STMT_DBSCALE_SHOW_AUTO_INCREMENT_VALUE ||
      st.type == STMT_DBSCALE_DYNAMIC_SET_AUTO_INCREMENT_OFFSET ||
      st.type == STMT_DBSCALE_REQUEST_CLUSTER_INC_INFO ||
      st.type == STMT_CREATE_VIEW || st.type == STMT_DROP_VIEW ||
      st.type == STMT_DBSCALE_SHOW_TABLE_LOCATION ||
      st.type == STMT_DBSCALE_BLOCK || st.type == STMT_DBSCALE_CHECK_TABLE ||
      st.type == STMT_CREATE_TRIGGER || st.type == STMT_DBSCALE_RESTORE_TABLE ||
      st.type == STMT_DBSCALE_CLEAN_RECYCLE_TABLE) {
    if (st.type == STMT_FLUSH) {
      /*The unsupport flush stmt, just ignore it.*/
      LOG_WARN("DBScale find unsupport flush stmt %s, just ignore it.\n", sql);
      ExecuteNode *node = plan->get_return_ok_node();
      plan->set_start_node(node);
      return true;
    }

#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    generate_execution_plan_with_no_table(plan);
    return true;
  }

  return false;
}

void Statement::generate_plan_for_create_tmp_table(ExecutePlan *plan) {
  LOG_DEBUG("generate_plan_for_create_tmp_table.\n");
  join_node *table = st.sql->create_tb_oper->table;
  const char *schema_name = table->schema_name ? table->schema_name : schema;
  const char *table_name = table->table_name;
  Backend *backend = Backend::instance();
  DataSpace *ds = backend->get_data_space_for_table(schema_name, table_name);

  if (ds->is_partitioned() || ds->is_duplicated() || ds->is_tmp_table_space()) {
    LOG_ERROR(
        "Unsupport CREATE TEMPORARY TABLE for table name [%s.%s], it should be "
        "a normal table.\n",
        schema_name, table_name);
    throw NotSupportedError("CREATE TEMPORARY TABLE should be a normal table.");
  }

  ExecuteNode *node = plan->get_direct_execute_node(ds, sql);
  plan->set_start_node(node);
  st.is_create_or_drop_temp_table = true;
}

bool Statement::is_empty_top_select_with_limit() {
  if (st.type == STMT_SELECT && st.scanner->limit &&
      !st.scanner->group_by_list && !st.scanner->having &&
      !st.scanner->condition  // no groupby or having or condition or order by
      && !st.scanner->distinct_start_pos &&
      !st.scanner->order_by_list  // no distinct or order by for top select
      && st.scanner->children_begin != NULL &&
      st.scanner->children_begin ==
          st.scanner->children_end           // only has one subquery
      && !st.scanner->children_begin->limit  // the subquery has no limit
      && st.scanner->join_tables &&
      st.scanner->join_tables->type ==
          JOIN_NODE_SUBSELECT) {  // from only has one table subquery
    field_item *fi = st.scanner->field_list_head;
    if (fi) {
      do {
        if (fi->field_expr->type == EXPR_STR ||
            fi->field_expr->type == EXPR_UNIT) {  // select * or select column
          fi = fi->next;
        } else
          return false;
      } while (fi && fi != st.scanner->field_list_tail);
    }
    return true;
  }
  return false;
}

void Statement::move_top_select_limit_to_table_subquery(table_link **table) {
  string limit_str(sql + st.scanner->limit_pos - 1,
                   st.scanner->end_pos - st.scanner->limit_pos + 1);
  string newsql(sql, st.scanner->children_begin->end_pos);
  newsql.append(" ");
  newsql.append(limit_str.c_str());
  newsql.append(
      sql + st.scanner->children_begin->end_pos,
      st.scanner->limit_pos - st.scanner->children_begin->end_pos - 1);
  sql_tmp.assign(newsql.c_str());
  LOG_DEBUG(
      "Statement::move_top_select_limit_to_table_subquery adjust the sql to "
      "[%s] from [%s].\n",
      newsql.c_str(), sql);
  sql = sql_tmp.c_str();
  Statement *new_stmt = NULL;
  re_parser_stmt(&(st.scanner), &new_stmt, sql);
  *table = new_stmt->get_stmt_node()->table_list_head;
}

string Statement::generate_full_column_name(string expr_str) {
  string str;
  size_t first_dot_pos = expr_str.find(".");
  size_t second_dot_pos = expr_str.find(".", first_dot_pos + 1);

  if (first_dot_pos == string::npos) {
    str.assign("`");
    str.append(expr_str);
    str.append("`");
  } else if (second_dot_pos == string::npos) {
    str.assign("`");
    str.append(expr_str, 0, first_dot_pos);
    str.append("`.`");
    str.append(expr_str, first_dot_pos + 1,
               expr_str.length() - first_dot_pos - 1);
    str.append("`");
  } else {
    str.assign("`");
    str.append(expr_str, 0, first_dot_pos);
    str.append("`.`");
    str.append(expr_str, first_dot_pos + 1, second_dot_pos - first_dot_pos - 1);
    str.append("`.`");
    str.append(expr_str, second_dot_pos + 1,
               expr_str.length() - second_dot_pos - 1);
    str.append("`");
  }
  return str;
}

void Statement::handle_connect_by_expr(connect_by_node *conn_node) {
  is_connect_by = true;

  string expr_str;
  if (st.scanner->condition) {
    expr_str.assign(sql, st.scanner->opt_where_expr_start_pos - 1,
                    st.scanner->opt_where_end_pos -
                        st.scanner->opt_where_expr_start_pos + 1);
    new_select_items.push_back(expr_str);
    connect_by_desc.where_index = --new_select_item_num;
  } else {
    connect_by_desc.where_index = 0;
  }

  expr_str.assign(sql, conn_node->with_start_pos - 1,
                  conn_node->with_end_pos - conn_node->with_start_pos + 1);
  new_select_items.push_back(expr_str);
  connect_by_desc.start_index = --new_select_item_num;

  expr_str.assign(sql, conn_node->prior_start_pos - 1,
                  conn_node->prior_end_pos - conn_node->prior_start_pos + 1);
  new_select_items.push_back(expr_str);
  connect_by_desc.prior_index = --new_select_item_num;

  expr_str.assign(sql, conn_node->recur_start_pos - 1,
                  conn_node->recur_end_pos - conn_node->recur_start_pos + 1);
  new_select_items.push_back(expr_str);
  connect_by_desc.recur_index = --new_select_item_num;
}

void Statement::check_and_replace_oracle_sequence(ExecutePlan *plan,
                                                  table_link **table) {
  if (!enable_oracle_sequence) return;
  if (!st.seq_items_head) return;
  if (st.type == STMT_CREATE_PROCEDURE || st.type == STMT_CREATE_FUNCTION)
    return;

  int alt_len = 0;
  oracle_sequence_item *seq_item = st.seq_items_head;
  sql_sequence_val_replaced = sql;
  while (seq_item) {
    string schemaname;
    int replace_start_pos = seq_item->seq_name_start_pos + alt_len;
    int replace_end_pos = seq_item->next_or_curr_end_pos + alt_len;
    if (seq_item->seq_schema_name_start_pos > 0) {
      schemaname = string(sql_sequence_val_replaced,
                          seq_item->seq_schema_name_start_pos - 1 + alt_len,
                          seq_item->seq_schema_name_end_pos -
                              seq_item->seq_schema_name_start_pos + 1);
      replace_start_pos = seq_item->seq_schema_name_start_pos + alt_len;
    } else {
      schemaname = plan->session->get_schema();
    }
    string seqname = string(
        sql_sequence_val_replaced, seq_item->seq_name_start_pos - 1 + alt_len,
        seq_item->seq_name_end_pos - seq_item->seq_name_start_pos + 1);
    bool is_nextval = seq_item->is_nextval;
    int64_t seqval;
    if (is_nextval) {
      seqval =
          Backend::instance()->get_oracle_sequence_nextval(seqname, schemaname);
      LOG_DEBUG("get sequence %s.%s.nextval=%Q\n", schemaname.c_str(),
                seqname.c_str(), seqval);
    } else {
      seqval =
          Backend::instance()->get_oracle_sequence_currval(seqname, schemaname);
      LOG_DEBUG("get sequence %s.%s.currval=%Q\n", schemaname.c_str(),
                seqname.c_str(), seqval);
    }
    char tmp[21];
    int val_len = sprintf(tmp, "%ld", seqval);
    string tmp_sql =
        string(sql_sequence_val_replaced, 0, replace_start_pos - 1);
    tmp_sql.append(tmp);
    tmp_sql.append(sql_sequence_val_replaced.c_str() + replace_end_pos);
    sql_sequence_val_replaced = tmp_sql;
    alt_len = (val_len - (replace_end_pos - replace_start_pos + 1)) + alt_len;
    seq_item = seq_item->next;
  }
  sql = sql_sequence_val_replaced.c_str();
  LOG_DEBUG("get sql with sequence value replaced: %s\n", sql);
  Statement *new_stmt = NULL;
  re_parser_stmt(&(st.scanner), &new_stmt, sql);
  *table = new_stmt->get_stmt_node()->table_list_head;
}

void Statement::assemble_dbscale_create_oracle_sequence_plan(
    ExecutePlan *plan) {
  create_oracle_seq_op_node *oper = st.sql->create_oracle_seq_oper;
  ExecuteNode *node = plan->get_create_oracle_sequence_node(oper);
  plan->set_start_node(node);
}

DataSpace *Statement::check_and_replace_odbc_uservar(ExecutePlan *plan) {
  if (st.var_item_list == NULL) return NULL;

  DataSpace *odbc_space = NULL;
  bool has_non_odbc = false;
  Backend *backend = Backend::instance();
  DataSpace *space = NULL;
  table_link *table = st.table_list_head;
  while (table) {
    const char *schema_name =
        table->join->schema_name ? table->join->schema_name : schema;
    const char *table_name = table->join->table_name;
    space = backend->get_data_space_for_table(schema_name, table_name);
    if (space->get_data_source() &&
        space->get_data_source()->get_data_source_type() ==
            DATASOURCE_TYPE_ODBC) {
      if (has_non_odbc) {
        LOG_ERROR("Not support odbc dataspace cross join with user var.\n");
        throw Error("Not support odbc dataspace cross join with user var.");
      }
      if (odbc_space && odbc_space != space) {
        LOG_ERROR("Not support odbc dataspace cross join with user var.\n");
        throw Error("Not support odbc dataspace cross join with user var.");
      }
      odbc_space = space;
    } else {
      if (odbc_space) {
        LOG_ERROR("Not support odbc dataspace cross join with user var.\n");
        throw Error("Not support odbc dataspace cross join with user var.");
      }
      has_non_odbc = true;
    }
    table = table->next;
  }

  if (odbc_space) {
    Session *session = plan->session;
    map<string, string> *user_var_map = session->get_user_var_map();
    string new_query_sql_string;
    var_item *var = st.var_item_list;
    list<var_item *> var_list;
    while (var) {
      var_list.push_front(var);
      var = var->next;
    }
    list<var_item *>::iterator it = var_list.begin();
    unsigned int start_pos = 0;
    for (; it != var_list.end(); ++it) {
      new_query_sql_string.append(sql, start_pos,
                                  (*it)->start_pos - start_pos - 1);

      string var_name = (*it)->value;
      boost::to_upper(var_name);
      if (user_var_map->count(var_name)) {
        new_query_sql_string.append((*user_var_map)[var_name]);
      } else {
        new_query_sql_string.append("NULL");
      }
      start_pos = (*it)->end_pos;
    }
    new_query_sql_string.append(sql, start_pos, strlen(sql) - start_pos);
    LOG_DEBUG("New sql after replace odbc user var [%s]\n",
              new_query_sql_string.c_str());
    record_statement_stored_string(new_query_sql_string);
    sql = statement_stored_string.back().c_str();
  }

  return odbc_space;
}

bool Statement::assemble_dbscale_cal_uservar_plan(ExecutePlan *plan) {
  bool ret = false;
  bool has_init_gmp_expr = false;
  bool has_init_sequence = false;
  map<const char *, oracle_sequence_item *> ora_sequence;
  map<string, pair<string, bool> > var_map;
  map<string, string> ref_var_map;
  set<string> null_vars;
  expr_list_item *head =
      ((ListExpression *)st.sql->set_oper->set_list)->expr_list_head;
  expr_list_item *tmp = head;

  CompareExpression *cmp;
  VarExpression *var_expr;
  Expression *val_expr;
  do {
    cmp = (CompareExpression *)(tmp->expr);
    var_expr = (VarExpression *)(cmp->left);
    val_expr = cmp->right;
    if (var_expr->var_scope == VAR_SCOPE_USER) {
      string var(var_expr->str_value);
      boost::to_upper(var);

      if (val_expr->type == EXPR_NULL) {
        null_vars.insert(var);
      } else if (val_expr->type == EXPR_STR) {
        if (enable_oracle_sequence && st.seq_items_head) {
          if (!has_init_sequence) {
            oracle_sequence_item *seq_item = st.seq_items_head;
            while (seq_item) {
              ora_sequence[seq_item->full_name] = seq_item;
              seq_item = seq_item->next;
            }
            has_init_sequence = true;
          }
          if (ora_sequence.count(val_expr->str_value)) {
            oracle_sequence_item *seq_item = ora_sequence[val_expr->str_value];
            const char *schema_name =
                seq_item->schema_name ? seq_item->schema_name : schema;
            const char *seq_name = seq_item->seq_name;
            string schemaname(schema_name);
            string seqname(seq_name);
            bool is_nextval = seq_item->is_nextval;
            int64_t seqval;
            if (is_nextval) {
              seqval = Backend::instance()->get_oracle_sequence_nextval(
                  seqname, schemaname);
              LOG_DEBUG("get sequence %s.%s.nextval=%Q\n", schemaname.c_str(),
                        seqname.c_str(), seqval);
            } else {
              seqval = Backend::instance()->get_oracle_sequence_currval(
                  seqname, schemaname);
              LOG_DEBUG("get sequence %s.%s.currval=%Q\n", schemaname.c_str(),
                        seqname.c_str(), seqval);
            }
            char tmp[21];
            snprintf(tmp, 21, "%ld", seqval);
            string val(tmp);
            var_map[var] = make_pair(val, false);
            continue;
          }
        }
        return false;
      } else if (val_expr->type == EXPR_STRING) {
        string val(val_expr->str_value);
        var_map[var] = make_pair(val, true);
      } else if (val_expr->type == EXPR_VAR) {
        if (((VarExpression *)val_expr)->var_scope == VAR_SCOPE_USER) {
          string val(val_expr->str_value);
          boost::to_upper(val);
          ref_var_map[var] = val;
        } else {
          return false;
        }
      } else if (val_expr->get_function_type() == FUNCTION_TYPE_CONCAT) {
        string total_val;
        ListExpression *list_expr =
            ((FunctionExpression *)val_expr)->param_list;
        expr_list_item *item = list_expr->expr_list_head;
        do {
          string val;
          Expression *value = item->expr;
          if (value->type == EXPR_VAR &&
              ((VarExpression *)value)->var_scope == VAR_SCOPE_USER) {
            val.assign(value->str_value);
            boost::to_upper(val);
            val = *(plan->session->get_user_var_value(val));
            if (val[0] == '\'') {
              boost::erase_head(val, 1);
              boost::erase_tail(val, 1);
            }
          } else {
            const char *value_str = get_str_from_simple_value(value);
            if (value_str) {
              val.assign(value_str);
            } else {
              LOG_ERROR("Fail to get concat list string value.\n");
              return false;
            }
          }
          total_val.append(val);
          item = item->next;
        } while (item != list_expr->expr_list_head);
        var_map[var] = make_pair(total_val, true);
      } else {
        map<string, string> *user_var_map = plan->session->get_user_var_map();
        if (!has_init_gmp_expr) {
          var_item *var_item_list = st.var_item_list;
          while (var_item_list != NULL) {
            string var_name = var_item_list->value;
            boost::to_upper(var_name);
            if (var_item_list->expr) {
              VarExpression *expr = (VarExpression *)var_item_list->expr;
              if (user_var_map->count(var_name)) {
                expr->set_user_var_null(false);
                expr->set_user_var_value(((*user_var_map)[var_name]).c_str());
              } else {
                expr->set_user_var_null(true);
              }
            }
            var_item_list = var_item_list->next;
          }
          has_init_gmp_expr = true;
        }
        try {
          const char *val_str = get_gmp_from_simple_value(val_expr);
          if (val_str) {
            string val(val_str);
            var_map[var] = make_pair(val, false);
            if (need_clean_simple_expression) clear_simple_expression();
          } else {
            LOG_INFO(
                "Not support handle current userver on dbscale, send it to "
                "server.\n");
            return false;
          }
        } catch (...) {
          LOG_ERROR("Get exception when get gmp from simple value.\n");
          clear_simple_expression();
          return false;
        }
      }
    } else {
      return false;
    }
    tmp = tmp->next;
  } while (tmp != head);

  if (!var_map.empty() || !ref_var_map.empty() || !null_vars.empty()) {
    ret = true;
    map<string, pair<string, bool> >::iterator it = var_map.begin();
    for (; it != var_map.end(); ++it) {
      plan->session->add_user_var_value(it->first, it->second.first,
                                        it->second.second, true);
      LOG_DEBUG("DBScale handle uservar [%s] = [%s].\n", it->first.c_str(),
                it->second.first.c_str());
    }
    map<string, string>::iterator it_ref = ref_var_map.begin();
    for (; it_ref != ref_var_map.end(); ++it_ref) {
      plan->session->set_var_by_ref(it_ref->first, it_ref->second);
      LOG_DEBUG("DBScale handle uservar [%s] with [%s].\n",
                it_ref->first.c_str(), it_ref->second.c_str());
    }
    set<string>::iterator it_set = null_vars.begin();
    for (; it_set != null_vars.end(); ++it_set) {
      plan->session->remove_user_var_by_name(*it_set);
      LOG_DEBUG("DBScale handle null uservar [%s], remove it.\n",
                it_set->c_str());
    }
  }

  if (ret) {
    ExecuteNode *node = plan->get_return_ok_node();
    plan->set_start_node(node);
  }
  return ret;
}

bool Statement::generate_forward_master_role_plan(ExecutePlan *plan) {
  if (!multiple_mode) return false;
  if (st.type == STMT_ALTER_TABLE || st.type == STMT_DROP_DB ||
      st.type == STMT_DROP_TB || st.type == STMT_RENAME_TABLE ||
      st.type == STMT_DBSCALE_CREATE_OUTLINE_HINT ||
      st.type == STMT_DBSCALE_FLUSH_OUTLINE_HINT ||
      st.type == STMT_DBSCALE_DELETE_OUTLINE_HINT) {
    check_acl_and_safe_mode(plan);
    MultipleManager *mul = MultipleManager::instance();
    bool is_master = mul->get_is_cluster_master();
    if (!is_master) {
      LOG_DEBUG("Forward sql [%s] to master.\n", sql);
      assemble_forward_master_role_plan(plan, true);
      return true;
    }
  }
  return false;
}

bool Statement::check_duplicated_table() {
  const char *schema_name = NULL;
  const char *table_name = NULL;
  DataSpace *dataspace = NULL;
  table_link *table = st.table_list_head;
  while (table) {
    schema_name = table->join->schema_name ? table->join->schema_name : schema;
    table_name = table->join->table_name;
    dataspace =
        Backend::instance()->get_data_space_for_table(schema_name, table_name);
    if (dataspace->is_duplicated()) return true;
    table = table->next;
  }
  return false;
}

void Statement::check_disaster_relate_option(
    dynamic_config_op_node *config_oper) {
  if (!config_oper || !config_oper->option_name) return;
  Backend *backend = Backend::instance();
  string option_name(config_oper->option_name);
  boost::to_lower(option_name);
  boost::replace_all(option_name, "-", "_");
  if (option_name == "enable_disaster_mode") {
    strcasecmp(config_oper->int_val, "1")
        ? backend->validate_disable_disaster_mode()
        : backend->validate_enable_disaster_mode();
  }
  if (enable_disaster_mode) return;
  if (option_name == "slave_dbscale_mode") {
    LOG_ERROR(
        "Option \"slave-dbscale-mode\" can only be set when "
        "\"enable-disaster-mode\" is open.\n");
    throw Error(
        "Option \"slave-dbscale-mode\" can only be set when "
        "\"enable-disaster-mode\" is open.\n");
  }
}

/*when create or alter table, have some restrictions.
1.restrict_create_table is 1, then can not drop primary key.
2.part table can not drop and alter part key.
3.check_part_primary is 1, then primary key must contain part key, unique key
must be part key*/
void Statement::check_create_or_alter_table_restrict(const char *schema_name,
                                                     const char *table_name) {
  if (st.type != STMT_CREATE_TB && st.type != STMT_ALTER_TABLE) return;
  if (st.type == STMT_ALTER_TABLE) {
    if (st.sql->alter_tb_oper &&
        st.sql->alter_tb_oper->alter_type == DROP_COLUMN &&
        restrict_create_table) {
      map<string, TableColumnInfo *, strcasecomp> *column_info_map;
      TableInfoCollection *tic = TableInfoCollection::instance();
      TableInfo *ti = tic->get_table_info_for_read(schema_name, table_name);
      try {
        column_info_map =
            ti->element_table_column
                ->get_element_map_columnname_table_column_info(stmt_session);
      } catch (...) {
        LOG_ERROR(
            "Error occured when try to get table info column "
            "info(map_table_column_info) of table [%s.%s]\n",
            schema_name, table_name);
        ti->release_table_info_lock();
        throw;
      }
      if (ti->primary_key_col_nums == 1) {
        // if drop column is primary col, check whether have other primary
        // key
        if (column_info_map->count(
                st.sql->alter_tb_oper->change_column->name) &&
            !strcasecmp(
                (*column_info_map)[st.sql->alter_tb_oper->change_column->name]
                    ->column_key.c_str(),
                "PRI")) {
          string error_msg;
          error_msg.assign("can't drop primary key column.");
          LOG_ERROR("%s\n", error_msg.c_str());
          ti->release_table_info_lock();
          throw Error(error_msg.c_str());
        }
      }
      ti->release_table_info_lock();
    }
  }
  DataSpace *dataspace =
      Backend::instance()->get_data_space_for_table(schema_name, table_name);
  if (dataspace->is_partitioned()) {
    vector<const char *> *key_names =
        ((PartitionedTable *)dataspace)->get_key_names();
    const char *key = key_names->at(0);
    if (st.type == STMT_ALTER_TABLE && st.sql->alter_tb_oper &&
        (st.sql->alter_tb_oper->alter_type == DROP_COLUMN ||
         st.sql->alter_tb_oper->alter_type == CHANGE_COLUMN) &&
        (!strcasecmp(st.sql->alter_tb_oper->change_column->name, key))) {
      string error_msg;
      error_msg.assign("can't drop or change part_key.");
      LOG_ERROR("%s\n", error_msg.c_str());
      throw Error(error_msg.c_str());
    }
    check_part_table_primary_key(key);
  }
}
bool Statement::is_need_parse_transparent() {
  if (use_partial_parse == NONE || use_partial_parse == PARTIAL_PARSE)
    return false;
  if (is_stmt_type_parse_transparent(get_stmt_node())) return true;
  return false;
}

void Statement::generate_execution_plan(ExecutePlan *plan) {
  if (enable_read_only &&
      (st.type >= STMT_DDL_START && st.type <= STMT_CALL &&
       st.type != STMT_SET && st.type != STMT_SHOW_WARNINGS &&
       st.type != STMT_CHANGE_DB)) {
    LOG_INFO("Read only is on,can not execute sql %s.\n", sql);
    throw ReadOnlyError();
  }

  if (reinforce_enable_read_only && ((st.type >= STMT_NORMAL_GTID_START &&
                                      st.type <= STMT_NORMAL_GTID_STOP) ||
                                     (st.type >= STMT_DBSCALE_GTID_START &&
                                      st.type <= STMT_DBSCALE_GTID_STOP))) {
    LOG_INFO("rein-force-enable-read-only is on,can not execute sql %s.\n",
             sql);
    throw ReinForceReadOnlyError();
  }
  if (is_need_parse_transparent() &&
      !plan->session->is_in_cluster_xa_transaction()) {
    st.need_apply_metadata = false;
    LOG_DEBUG("Sql [%s] parse transparent that type is [%d].\n", sql, st.type);
    plan->set_is_parse_transparent(true);
    assemble_direct_exec_plan(plan, Backend::instance()->get_catalog());
    return;
  }
#ifndef CLOSE_MULTIPLE
  if (generate_forward_master_role_plan(plan)) return;
#endif
  switch (st.type) {
    case STMT_ALTER_TABLE:
      LOG_INFO("Do alter table, the statement is [%s], user is [%s].\n", sql,
               plan->session->get_user_host().c_str());
      break;
    case STMT_DROP_TB:
      LOG_INFO("Do drop table, the statement is [%s], user is [%s].\n", sql,
               plan->session->get_user_host().c_str());
      break;
    case STMT_DROP_DB:
      LOG_INFO("Do alter database, the statement is [%s], user is [%s].\n", sql,
               plan->session->get_user_host().c_str());
      break;
    default:
      break;
  }
#ifndef DBSCALE_TEST_DISABLE
  Backend *bk = Backend::instance();
  dbscale_test_info *test_info = bk->get_dbscale_test_info();
  if (strcasecmp(test_info->test_case_name.c_str(), "duplicated_table") ||
      strcasecmp(test_info->test_case_operation.c_str(), "init_space_di")) {
#endif
    if (check_duplicated_table() && !is_cross_node_join() &&
        !is_union_all_sub()) {
      LOG_WARN(
          "Find a dup tmp table stmt for non cross node join, refuse it.\n");
      throw NotSupportedError(
          "Should not execute sql on duplicated tmp table directly");
    }
#ifndef DBSCALE_TEST_DISABLE
  }
#endif
  plan->session->set_select_lock_type(st.select_lock_type);
  bool is_info_schema_mirror_tb_reload_internal_session =
      plan->session->get_is_info_schema_mirror_tb_reload_internal_session();
  stmt_session = plan->session;
#ifndef DBSCALE_TEST_DISABLE
  if (on_test_stmt) {
    Backend *bk = Backend::instance();
    dbscale_test_info *test_info = bk->get_dbscale_test_info();
    if (!strcasecmp(test_info->test_case_name.c_str(), "generate_plan") &&
        !strcasecmp(test_info->test_case_operation.c_str(), "exception")) {
      bk->set_dbscale_test_info("", "", "");
      throw Error("generate_plan get exception.");
    }
  }
#endif

  Backend *backend = Backend::instance();
  if (is_partial_parsed() && is_into_outfile()) {
    LOG_ERROR("Cannot support partial parse for SELECT INTO OUTFILE.\n");
    throw NotSupportedError(
        "Cannot support partial parse for SELECT INTO OUTFILE");
  }
  if (is_partial_parsed() && is_user_grant_stmt()) {
    LOG_ERROR("Cannot support partial parse for USER ACCOUNT STMT.\n");
    throw NotSupportedError(
        "Cannot support partial parse for USER ACCOUNT STMT");
  }
  if (is_partial_parsed() && st.type == STMT_SET) {
    if (!force_exec_partial_set) {
      string err(
          "Not support partial parse for set stmt when close option "
          "force-execute-partial-set:");
      if (st.error_message) {
#ifndef DBSCALE_TEST_DISABLE
        string tmp(st.error_message);
        if (tmp.find("mystery character") == string::npos) {
          err.append(st.error_message);
        }
#else
        err.append(st.error_message);
#endif
      }
      LOG_ERROR("%s\n", err.c_str());
#ifndef DBSCALE_TEST_DISABLE
      plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
      throw NotSupportedError(err.c_str());
    }
    assemble_direct_exec_plan(plan, Backend::instance()->get_catalog());
    return;
  }

  table_link *table = st.table_list_head;

  if (!is_partial_parsed() &&
      plan->session->get_session_option("is_bridge_session").bool_val &&
      table) {
    ExecuteNode *node = try_assemble_bridge_mark_sql_plan(plan, table);
    if (node) {
      plan->set_start_node(node);
      return;
    }
  }

  if (st.type == STMT_SET && !table && st.sql->set_oper &&
      !st.sql->set_oper->names) {
    if (assemble_dbscale_cal_uservar_plan(plan)) return;
  }

  if (plan->session->get_session_option("restrict_create_table").int_val != 0) {
    if (st.type == STMT_CREATE_TB) {
      if (is_partial_parsed()) {
        string err("Not support partial parse for create table:");
        if (st.error_message) err.append(st.error_message);
        LOG_ERROR("%s\n", err.c_str());
        throw NotSupportedError(err.c_str());
      }
      if (backend->is_cluster_contain_mgr_datasource()) {
        string se_name;
        if (strlen(st.sql->create_tb_oper->engine_name))
          se_name = st.sql->create_tb_oper->engine_name;
        else
          se_name = backend->get_backend_server_default_storage_engine();
        if (strcasecmp(se_name.c_str(), "InnoDB")) {
          LOG_INFO("Table must use the InnoDB storage engine\n");
          throw Error(ERROR_STORAGE_ENGINE_ERROR_CODE);
        }
      }
      if (!st.sql->create_tb_oper->primary_key_cols &&
          !st.sql->create_tb_oper->all_unique_keys &&
          !st.sql->create_tb_oper->table_like) {
        LOG_INFO("Create Table must specify primary key or unique key\n");
        throw Error(ERROR_CREATE_TB_NO_PRIM_KEY_CODE);
      }
      if (!force_create_table_engine.empty() && st.engine_head) {
        LOG_DEBUG("force_create_table_engine is %s.\n",
                  force_create_table_engine.c_str());
        engine_name_item *ni = st.engine_head;
        string tmp_sql;
        const char *start_pos = sql;
        const char *copy_pos = sql;
        const char *sql_tail = sql + strlen(sql);
        while (ni) {
          if (strcasecmp(ni->engine_name, force_create_table_engine.c_str())) {
            copy_pos = sql + ni->start_pos;
            tmp_sql.append(start_pos, copy_pos - start_pos - 1);
            tmp_sql.append(" engine=");
            tmp_sql.append(force_create_table_engine.c_str());
            start_pos = sql + ni->end_pos;
          }
          ni = ni->next;
        }
        if (start_pos != sql) {
          if (start_pos <= sql_tail) tmp_sql.append(start_pos);
          LOG_DEBUG("Force adjust table engine with sql: %s.\n",
                    tmp_sql.c_str());
          record_statement_stored_string(tmp_sql);
          sql = statement_stored_string.back().c_str();
          Statement *new_stmt = NULL;
          re_parser_stmt(&(st.scanner), &new_stmt, sql);
          table = new_stmt->get_stmt_node()->table_list_head;
        }
      }
    }
    if (st.type == STMT_ALTER_TABLE) {
      if (is_partial_parsed()) {
        string err("Not support partial parse for alter table:");
        if (st.error_message) err.append(st.error_message);
        LOG_ERROR("%s\n", err.c_str());
        throw NotSupportedError(err.c_str());
      }
      if (backend->is_cluster_contain_mgr_datasource() &&
          st.sql->alter_tb_oper && st.sql->alter_tb_oper->engine_name &&
          strcasecmp(st.sql->alter_tb_oper->engine_name, "InnoDB")) {
        LOG_INFO("Table must use the InnoDB storage engine\n");
        throw Error(ERROR_STORAGE_ENGINE_ERROR_CODE);
      }
      if (st.alter_tb_modify_primary_key_counter < 0) {
        LOG_INFO("Not allow to drop table primary key\n");
        throw Error(ERROR_ALTER_TB_NOT_ALLOW_DROP_PRIM_KEY_CODE);
      }
    }
  }

  DataSpace *odbc_space = check_and_replace_odbc_uservar(plan);

  if (odbc_space) {
    assemble_direct_exec_plan(plan, odbc_space);
    return;
  }

  if (st.type == STMT_DBSCALE_CREATE_ORACLE_SEQ ||
      st.type == STMT_DBSCALE_ALTER_ORACLE_SEQ) {
    if (!enable_oracle_sequence) {
      throw Error("oracle sequence feature is disabled");
    }
#ifndef CLOSE_MULTIPLE
    bool is_master = true;
    if (multiple_mode) {
      MultipleManager *mul = MultipleManager::instance();
      is_master = mul->get_is_cluster_master();
    }
    if (st.type == STMT_DBSCALE_ALTER_ORACLE_SEQ &&
        st.sql->create_oracle_seq_oper->alter_what &
            (ALTER_SEQUENCE_MIN | ALTER_SEQUENCE_MAX) &&
        !is_master) {
      LOG_DEBUG(
          "alter sequence min or max should be execute on master dbscale\n");
      assemble_forward_master_role_plan(plan);
    } else {
#endif
      assemble_dbscale_create_oracle_sequence_plan(plan);
#ifndef CLOSE_MULTIPLE
    }
#endif
    return;
  }
  check_and_replace_oracle_sequence(plan, &table);

  if (get_latest_stmt_node()->terminater_semicolon > 0) {
    string new_query_sql_string;
    new_query_sql_string.append(
        sql, get_latest_stmt_node()->terminater_semicolon - 1);
    record_statement_stored_string(new_query_sql_string);
    sql = statement_stored_string.back().c_str();
    Statement *new_stmt = NULL;
    re_parser_stmt(&(st.scanner), &new_stmt, sql);
    table = new_stmt->get_stmt_node()->table_list_head;
  }

  if (get_latest_stmt_node()->full_column_list_head) {
    string new_query_sql_string;
    new_query_sql_string = remove_schema_from_full_columns();
    record_statement_stored_string(new_query_sql_string);
    sql = statement_stored_string.back().c_str();
    Statement *new_stmt = NULL;
    re_parser_stmt(&(st.scanner), &new_stmt, sql);
    table = new_stmt->get_stmt_node()->table_list_head;
  }

  if (st.connect_by_num) {
    bool is_centralized = backend->is_centralized_cluster();
    if (!is_centralized && enable_start_with_connect_by) {
      LOG_ERROR(
          "Only support centralized cluster open config "
          "enable_start_with_connect_by.\n");
      throw Error(
          "Only support centralized cluster open config "
          "enable_start_with_connect_by.");
    }
    if (enable_start_with_connect_by && is_centralized) {
      LOG_DEBUG("start with connect by direct send server\n");
    } else {
      plan->session->set_is_complex_stmt(true);
      if (st.connect_by_num > 1) {
        LOG_ERROR("Only support one connect by in top record scan.\n");
        throw NotSupportedError(
            "Only support one connect by in top record scan.");
      }
      if (!st.scanner->connect_by_oper) {
        LOG_ERROR("Only support connect by in top record scan.\n");
        throw NotSupportedError("Only support connect by in top record scan.");
      }
      connect_by_node *conn_node = st.scanner->connect_by_oper;
      handle_connect_by_expr(conn_node);

      string new_query_sql_string;
      if (st.scanner->condition) {
        new_query_sql_string.append(sql, st.scanner->opt_where_start_pos - 1);
        if (conn_node->cond_start_pos > 0) {
          if (conn_node->cond_start_pos >= conn_node->cond_end_pos) {
            LOG_ERROR("Fail to get connect by conditions.");
            throw Error("Fail to get connect by conditions.");
          }
          new_query_sql_string.append(" WHERE ");
          new_query_sql_string.append(
              sql, conn_node->cond_start_pos - 1,
              conn_node->cond_end_pos - conn_node->cond_start_pos + 1);
        }
        if (conn_node->start_pos - st.scanner->opt_where_end_pos > 1)
          new_query_sql_string.append(
              sql, st.scanner->opt_where_end_pos,
              conn_node->start_pos - st.scanner->opt_where_end_pos - 1);
      } else {
        new_query_sql_string.append(sql, st.scanner->opt_where_start_pos);
        if (conn_node->cond_start_pos > 0) {
          if (conn_node->cond_start_pos >= conn_node->cond_end_pos) {
            LOG_ERROR("Fail to get connect by conditions.");
            throw Error("Fail to get connect by conditions.");
          }
          new_query_sql_string.append(" WHERE ");
          new_query_sql_string.append(
              sql, conn_node->cond_start_pos - 1,
              conn_node->cond_end_pos - conn_node->cond_start_pos + 1);
        }
        if (conn_node->start_pos - st.scanner->opt_where_end_pos > 1)
          new_query_sql_string.append(
              sql, st.scanner->opt_where_end_pos,
              conn_node->start_pos - st.scanner->opt_where_end_pos - 1);
      }
      record_statement_stored_string(new_query_sql_string);
      sql = statement_stored_string.back().c_str();
      Statement *new_stmt = NULL;
      re_parser_stmt(&(st.scanner), &new_stmt, sql);
      table = new_stmt->get_stmt_node()->table_list_head;
    }
  }

  if (is_empty_top_select_with_limit()) {
    move_top_select_limit_to_table_subquery(&table);
  }

  if (handle_executable_comments_before_cnj(plan)) {
    return;
  }
  if (!is_info_schema_mirror_tb_reload_internal_session)
    refuse_modify_table_checking();

#ifndef DBSCALE_TEST_DISABLE
  plan->handler->prepare_gen_plan.start_timing();
#endif
  DataSpace *dataspace = NULL;
  const char *schema_name = NULL, *table_name = NULL;
  table_link *table_tmp = table;

  if (st.type == STMT_REPLICATION) {
    LOG_ERROR("Not support replication related stmt %s.\n", sql);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    throw NotSupportedError("Unsupport replication related stmt.");
  }

  if ((st.type == STMT_CREATE_TB || st.type == STMT_CREATE_SELECT ||
       st.type == STMT_CREATE_LIKE) &&
      st.sql->create_tb_oper->op_tmp == 1) {
    generate_plan_for_create_tmp_table(plan);
    return;
  }
  check_stmt_sql_for_dbscale_row_id(st.type, st.scanner);

  if (!is_info_schema_mirror_tb_reload_internal_session) {
    if (handle_view_related_stmt(
            plan, &table))  // return true means the plan has been generated
      return;
    handle_coalecse_function(&table);
  }

  table_tmp = table;
  if (st.type != STMT_DBSCALE_SHOW_TABLE_LOCATION &&
      st.type != STMT_DBSCALE_REQUEST_CLUSTER_INC_INFO && table) {
    if (enable_block_table) {
      unsigned int tmp_migrate_wait_timeout = migrate_wait_timeout;
      while (!check_migrate_block_table(plan)) {
        if (ACE_Reactor::instance()->reactor_event_loop_done()) {
          LOG_ERROR("generate_plan failed due to reactor_event_loop_done\n");
          throw Error("generate_plan failed due to reactor_event_loop_done");
        }
        if (tmp_migrate_wait_timeout > 0) {
          LOG_INFO("current sql [%s] is waiting for migrate lock\n", sql);
          sleep(1);
          tmp_migrate_wait_timeout--;
        } else {
          throw Error(
              "DBScale can't execute this statement cause table is blocked "
              "during migration.");
        }
      }
    }
    if (st.type != STMT_DBSCALE_MIGRATE && st.type != STMT_DBSCALE_BLOCK)
      while (table_tmp != NULL) {
        schema_name =
            table->join->schema_name ? table->join->schema_name : schema;
        plan->session->add_in_using_table(schema_name,
                                          table_tmp->join->table_name);
        table_tmp = table_tmp->next;
      }
  }
  schema_name = NULL;

#ifndef DBSCALE_TEST_DISABLE
  if (on_test_stmt && check_and_assemble_multi_send_node(plan)) {
    plan->handler->prepare_gen_plan.end_timing_and_report();
    return;
  }
#endif

  if (st.type == STMT_RELEASE_SAVEPOINT ||
      !check_xa_transaction_support_sql(&st, plan->session)) {
    generate_plan_for_non_support_trx_stmt(plan);
    return;
  }

  if (!plan->session->get_is_info_schema_mirror_tb_reload_internal_session())
    check_acl_and_safe_mode(plan);

  if (plan->session->is_in_lock()) {
    check_stmt_for_lock_mode(plan);
  }

  if (!is_info_schema_mirror_tb_reload_internal_session)
    row_count_and_last_insert_id_checking(plan);

  if (st.type == STMT_ALTER_TABLE || st.type == STMT_DROP_DB ||
      st.type == STMT_DROP_TB) {
    mark_table_info_to_delete(plan);
    if (st.type == STMT_DROP_TB) {
      const char *schema_name;
      const char *table_name;
      table_link *table = st.table_list_head;
      if (st.sql->drop_tb_oper->op_tmp == 1) {
        st.is_create_or_drop_temp_table = true;
      }

      if (!st.sql->drop_tb_oper->op_tmp) {
        int tmp_table_count = 0;
        int total_table_count = 0;
        while (table) {
          schema_name =
              table->join->schema_name ? table->join->schema_name : schema;
          table_name = table->join->table_name;
          if (plan->session->is_temp_table(schema_name, table_name)) {
            ++tmp_table_count;
          }
          table = table->next;
          ++total_table_count;
        }
        if (tmp_table_count > 0 && tmp_table_count != total_table_count) {
          throw NotSupportedError("User temp table should be dropped alone.");
        }
        if (tmp_table_count > 0) {
          st.is_create_or_drop_temp_table = true;
        } else {
          // dropping multi non-tmp tables
          if (enable_table_recycle_bin && total_table_count > 1) {
            throw NotSupportedError(
                "when using enable_table_recycle_bin, tables should be dropped "
                "one by one.");
          }
        }
      }
    }
    if (st.type == STMT_ALTER_TABLE) {
      table_link *table = st.table_list_head;
      const char *schema_name =
          table->join->schema_name ? table->join->schema_name : schema;
      const char *table_name = table->join->table_name;
      if (plan->session->is_temp_table(schema_name, table_name))
        st.is_create_or_drop_temp_table = true;
    }
  }

  if (st.type == STMT_DBSCALE_EXPLAIN || st.type == STMT_EXPLAIN) {
    generate_plan_for_explain_related_stmt(plan);
    return;
  }

  if (!is_info_schema_mirror_tb_reload_internal_session)
    if (plan->session->get_is_federated_session() ||
        !plan->session->get_has_check_first_sql())
      if (generate_plan_for_federate_session(plan, table)) return;

  if (st.var_item_list != NULL) {
    LOG_DEBUG(
        "The sql [%s] has user variable, build a set sql for the session.\n",
        sql);
    var_item *var_item_list = st.var_item_list;
    build_uservar_sql(var_item_list, plan);
    plan->session->set_var_flag(true);
  } else {
    plan->session->set_var_flag(false);
  }

  if (st.type == STMT_SELECT && st.select_plain_value &&
      !is_handle_sub_query() && !st.sql_contain_charset_str && st.scanner &&
      st.scanner->field_list_head && st.scanner->field_list_head->field_expr &&
      is_simple_expression(st.scanner->field_list_head->field_expr)) {
    const char *str_value =
        get_simple_expression_value(st.scanner->field_list_head->field_expr);
    const char *alias_name = st.scanner->field_list_head->alias;
    if (st.scanner->field_list_head->field_expr->type == EXPR_BOOL) {
      alias_name = st.scanner->field_list_head->field_expr->get_bool_value()
                       ? "TRUE"
                       : "FALSE";
    }
    assemble_select_plain_value_plan(plan, str_value, alias_name);
    return;
  }

  table = adjust_select_sql_for_funs_and_autocommit(plan, table);

  if (st.type == STMT_EXEC_PREPARE || st.type == STMT_PREPARE ||
      st.type == STMT_DROP_PREPARE) {
    generate_plan_for_prepare_related_stmt(plan);
    return;
  }

  if (st.type == STMT_DBSCALE_FLUSH_CONFIG_TO_FILE) {
    generate_plan_for_flush_config_to_file(plan);
    return;
  }

  if (st.type == STMT_DBSCALE_FLUSH_TABLE_INFO) {
    const char *schema_name =
        st.sql->dbscale_flush_table_info_oper->schema_name;
    const char *table_name = st.sql->dbscale_flush_table_info_oper->table_name;
    generate_plan_for_flush_table_info(plan, schema_name, table_name);
    return;
  }

  if (st.type == STMT_DBSCALE_FLUSH_ACL) {
    bool silently = st.sql->dbscale_flush_acl_oper->silently;
    if (!silently && !enable_acl) {
      LOG_ERROR("enable-acl=0, no ACL to flush\n");
      throw Error("enable-acl=0, no ACL to flush");
    }
    generate_plan_for_flush_acl(plan);
    return;
  }

  adjust_sql_for_into_file_or_select_into(&table);

  defined_lock_type dl_type = st.defined_lock;
  if (dl_type != DEFINED_LOCK_NON) {
    if (st.scanner->defined_lock_num > 1) {
      LOG_INFO("does not support multiple get_lock() or release_lock()\n");
      throw NotSupportedError(
          "does not support multiple get_lock() or release_lock().");
    }
    field_item *field = st.scanner->field_list_head;
    bool use_alias_name = false;
    int defined_lock_func_num = 0;
    while (field) {
      if (field->field_expr && field->field_expr->type == EXPR_FUNC) {
        FunctionExpression *func_expr =
            (FunctionExpression *)(field->field_expr);
        if (func_expr->name) {
          const char *value = func_expr->name->str_value;
          if (!strcasecmp(value, "get_lock") ||
              !strcasecmp(value, "release_lock") ||
              !strcasecmp(value, "release_all_locks")) {
            defined_lock_func_num++;
            if (field->alias) {
              use_alias_name = true;
              plan->session->set_defined_lock_field_name(field->alias);
            }
          }
        }
      }
      field = field->next;
    }
    if (defined_lock_func_num != 1) {
      LOG_INFO("does not support complex with get_lock() or release_lock()\n");
      throw NotSupportedError(
          "does not support complex with get_lock() or release_lock().");
    }
    if (!use_alias_name) {
      int head_pos = 0, tail_pos = 0;
      head_pos =
          plan->statement->get_stmt_node()->scanner->defined_lock_head_pos;
      tail_pos =
          plan->statement->get_stmt_node()->scanner->defined_lock_tail_pos;
      string str = sql;
      str = str.substr(head_pos - 1, tail_pos - head_pos + 1);
      plan->session->set_defined_lock_field_name(str.c_str());
      LOG_DEBUG("The defined lock field is %s\n", str.c_str());
    }
  }

  if (st.table_list_head &&
      (st.type == STMT_SELECT || st.type == STMT_UPDATE ||
       st.type == STMT_DELETE || st.type == STMT_INSERT)) {
  } else {
    if (generate_plan_for_no_table_situation(plan)) return;
  }

  if (is_partial_parsed() && st.type != STMT_CREATE_TB &&
      st.type != STMT_ALTER_TABLE && st.type != STMT_DBSCALE_ESTIMATE) {
    generate_plan_for_partial_parse_stmt(plan, schema_name, table_name, table);
    return;
  }

  if (st.type == STMT_RENAME_TABLE || st.type == STMT_DROP_TB) {
    generate_plan_for_rename_or_drop_table_stmt(plan);
    return;
  }
  if (st.type == STMT_CREATE_TB) {
    if (prepare_dataspace_for_extend_create_tb_stmt(plan, table)) return;
  }
  DataSpace *insert_dataspace = NULL;
  bool only_related_one_server = true;
  DataSpace *one_server_dataspace = NULL;
  DataSpace *one_space = NULL;
  record_scan *one_rs = NULL;
  bool only_one_table = false;
  table_link *one_table = NULL;
  if (table && !table->next) {
    only_one_table = true;
    one_table = table;
  }

  if (plan->session->is_in_transaction() && st.type == STMT_SELECT &&
      plan->session->get_select_lock_type()) {
    set<DataSpace *> shard_table;
    set<DataSpace *> non_shard_table;
    DataSpace *tmp_dataspace = NULL;
    table_link *tmp_table = st.table_list_head;
    while (tmp_table) {
      schema_name =
          tmp_table->join->schema_name ? tmp_table->join->schema_name : schema;
      table_name = tmp_table->join->table_name;
      if (schema_name && !strcmp(schema_name, "information_schema")) {
        tmp_table = tmp_table->next;
        continue;
      } else {
        tmp_dataspace =
            backend->get_data_space_for_table(schema_name, table_name);
      }
      if (!tmp_dataspace->get_data_source()) {
        if (!((Table *)tmp_dataspace)->is_duplicated()) {
          PartitionedTable *tmp_par_ds = (PartitionedTable *)tmp_dataspace;
          if (tmp_par_ds->get_partition_scheme()->is_shard()) {
            shard_table.insert(tmp_dataspace);
          }
        }
      } else {
        DataSource *ds = tmp_dataspace->get_data_source();
        if (ds && ds->get_data_source_type() == DATASOURCE_TYPE_REPLICATION) {
          ReplicationDataSource *rep_datasource =
              dynamic_cast<ReplicationDataSource *>(tmp_dataspace->data_source);
          if (rep_datasource && rep_datasource->has_slave_source()) {
            non_shard_table.insert(tmp_dataspace);
          }
        }
      }
      tmp_table = tmp_table->next;
    }
    if (shard_table.size() && non_shard_table.size()) {
      std::set<DataSpace *>::iterator it1;
      std::set<DataSpace *>::iterator it2;
      for (it1 = shard_table.begin(); it1 != shard_table.end(); ++it1) {
        for (it2 = non_shard_table.begin(); it2 != non_shard_table.end();
             ++it2) {
          if (stmt_session->is_dataspace_cover_session_level(*it1, *it2)) {
            LOG_ERROR(
                "Does not support SELECT sql global table JOIN shard table "
                "with FOR UPDATE or LOCK IN SHARE MODE\n");
            throw NotSupportedError(
                "Does not support SELECT sql global table JOIN shard table "
                "with FOR UPDATE or LOCK IN SHARE MODE.");
          }
        }
      }
    }
  }

  bool has_non_shard_table = false;
  bool has_shard_table = false;
  if (table && table->join) {
    schema_name = table->join->schema_name ? table->join->schema_name : schema;
    table_name = table->join->table_name;
    check_create_or_alter_table_restrict(schema_name, table_name);
  }
  Catalog *catalog = backend->get_default_catalog();
  const char *catalog_source_name = catalog->get_data_source()->get_name();
  while (table) {
    schema_name = table->join->schema_name ? table->join->schema_name : schema;
    table_name = table->join->table_name;

    if (st.type == STMT_SELECT &&
        !strcasecmp(schema_name, "information_schema")) {
      generate_plan_for_information_schema(plan, table_name, table);
      return;
    }

    record_scan *rs_table = table->join->cur_rec_scan;
    if ((st.type == STMT_INSERT_SELECT || st.type == STMT_REPLACE_SELECT) &&
        !strcasecmp(schema_name, "information_schema")) {
      dataspace = backend->get_auth_data_space();
    } else {
      dataspace = backend->get_data_space_for_table(schema_name, table_name);
    }
    if (table->join->schema_name == NULL && !is_union_table_sub() &&
        !is_cross_node_join()) {
      plan->session->add_table_without_given_schema(dataspace);
    }
    if (only_one_table)  // if only_one_table, there is no need to maintain
                         // record_scan_all_table_spaces.
      one_space = dataspace;
    else {
      record_scan_all_table_spaces[table] = dataspace;
      need_clean_record_scan_all_table_spaces = true;
    }

    if (!dataspace->get_data_source()) {
      if (DEFINED_LOCK_NON != dl_type) {
        LOG_INFO(
            "does not support get_lock() or release_lock() from partition "
            "table\n");
        throw NotSupportedError(
            "Does not support get_lock() or release_lock() from partition "
            "table.");
      }
      if (!((Table *)dataspace)->is_duplicated()) {
        PartitionedTable *tmp_par_ds = (PartitionedTable *)dataspace;
        PartitionType part_type = tmp_par_ds->get_partition_type();
        if (tmp_par_ds->get_partition_scheme()->is_shard())
          has_shard_table = true;
        if (part_type == PARTITION_TYPE_RANGE) {
          rs_table->need_range = true;
        }
        unsigned int n = tmp_par_ds->get_real_partition_num();
        if (n > 1) {
          if (!only_one_table) {  // if only has one table, there is no need to
                                  // maintain the record_scan_all_par_tables_map
            record_scan_all_par_tables_map[rs_table].push_back(table);
            need_clean_record_scan_all_par_tables_map = true;
          } else {
            one_rs =
                rs_table;  // record the record scan of one table, if can not
                           // execute on one table mode later, we should re-fill
                           // the record_scan_all_par_tables_map
          }
        } else if (n == 1) {
          record_scan_all_spaces_map[rs_table].push_back(
              tmp_par_ds->get_partition(0));
          need_clean_record_scan_all_spaces_map = true;
          dataspace = tmp_par_ds->get_partition(0);
        } else {
#ifdef DEBUG
          ACE_ASSERT(0);  // Should not be here
#endif
        }
        // TODO: for partition table, we should check whether it only operate
        // one partition.
        if (n != 1 && only_related_one_server) only_related_one_server = false;

      } else {
        if (!only_one_table) {  // if only has one table, there is no need to
                                // maintain the record_scan_all_par_tables_map
          record_scan_all_par_tables_map[rs_table].push_back(table);
          need_clean_record_scan_all_par_tables_map = true;
        } else {
          one_rs = rs_table;  // record the record scan of one table, if can not
                              // execute on one table mode later, we should
                              // re-fill the record_scan_all_par_tables_map
        }
        only_related_one_server = false;
      }
    } else {
      const char *tb_source_name = dataspace->data_source->get_name();
      if (DEFINED_LOCK_NON != dl_type &&
          strcasecmp(catalog_source_name, tb_source_name)) {
        LOG_INFO("the table on data source [%s] instead of catalog\n",
                 tb_source_name);
        throw NotSupportedError(
            "Does not support get_lock() or release_lock() from non catalog "
            "table.");
      }
      has_non_shard_table = true;

      if (!is_readonly() && dataspace->data_source->get_data_source_type() ==
                                DATASOURCE_TYPE_READ_ONLY) {
        LOG_ERROR("Unsupport execute write op on read_only datasource[%s].\n",
                  dataspace->data_source->get_name());
        string tmp("Unsupport execute write op on read_only datasources:");
        tmp.append(dataspace->data_source->get_name());
        throw ReadOnlyDatasourceWriteFail(tmp.c_str());
      }

      record_scan_all_spaces_map[rs_table].push_back(dataspace);
      need_clean_record_scan_all_spaces_map = true;
    }

    if (only_related_one_server) {
      if (!one_server_dataspace) {
        one_server_dataspace = dataspace;
        if (st.type == STMT_INSERT_SELECT) insert_dataspace = dataspace;
      } else {
        only_related_one_server = false;
        if (is_info_schema_mirror_tb_reload_internal_session) {
          // insert_dataspace is dbscale; one_server_dataspace is Auth dataspace
          only_related_one_server = true;
          one_server_dataspace = dataspace;
        } else if (is_share_same_server(one_server_dataspace, dataspace)) {
          only_related_one_server = true;
        } else if (stmt_session->is_dataspace_cover_session_level(
                       one_server_dataspace, dataspace)) {
          only_related_one_server = true;
        } else if ((st.type == STMT_SELECT || st.type == STMT_INSERT_SELECT) &&
                   stmt_session->is_dataspace_cover_session_level(
                       dataspace, one_server_dataspace)) {
          only_related_one_server = true;
          one_server_dataspace = dataspace;
        }
      }
    }

    table = table->next;
  }

  if (insert_dataspace && only_related_one_server &&
      (insert_dataspace != one_server_dataspace)) {
    only_related_one_server = false;
    spaces.clear();
    spaces.push_back(one_server_dataspace);
    need_clean_spaces = true;
    handle_no_par_table_one_spaces(plan);
    return;
  }

  if (plan->session->is_in_lock() && has_non_shard_table && has_shard_table) {
    if (st.type == STMT_SELECT || st.type == STMT_UPDATE ||
        st.type == STMT_DELETE || st.type == STMT_INSERT_SELECT ||
        st.type == STMT_REPLACE_SELECT) {
      LOG_ERROR(
          "Does not support DML sql mix with shard and non-shard table in lock "
          "mode.\n");
      throw NotSupportedError(
          "Does not support DML sql mix with shard and non-shard table in lock "
          "mode.");
    }
  }

  if (only_related_one_server && !union_table_sub) {
    par_table_num = 0;
    spaces.clear();
    spaces.push_back(one_server_dataspace);
    need_clean_spaces = true;
  } else if (st.scanner->is_select_union && st.scanner->opt_union_all &&
             union_all_sub) {
    if (!record_scan_all_spaces_map.empty()) {
      map<record_scan *, list<DataSpace *> >::iterator it_space =
          record_scan_all_spaces_map.begin();
      DataSpace *union_space = it_space->second.front();
      par_table_num = 0;
      spaces.clear();
      spaces.push_back(union_space);
      need_clean_spaces = true;
    } else {
#ifdef DEBUG
      ACE_ASSERT(0);  // Should not be here
#endif
    }

  } else if (only_one_table && can_use_one_table_plan(plan, one_space, &st)) {
    generate_plan_for_one_table_situation(plan, one_table, one_space);
    return;
  } else {
#ifndef DBSCALE_DISABLE_SPARK
    if (!only_one_table && !is_spark_one_partition_sql()) {
      bool need_spark_session =
          plan->session->get_session_option("use_spark").int_val;
      need_spark =
          need_spark_session ? need_spark_session : st.execute_on_spark;
      if (need_spark && st.type == STMT_SELECT && !union_all_sub &&
          !st.scanner->is_select_union) {
        analysis_spark_sql_join(plan);
        return;
      }
      if (need_spark && st.type == STMT_INSERT_SELECT && !union_all_sub &&
          !st.scanner->is_select_union) {
        spark_insert = true;
        table_link *modify_tb = get_one_modify_table();
        const char *schema_name = modify_tb->join->schema_name
                                      ? modify_tb->join->schema_name
                                      : schema;
        const char *table_name = modify_tb->join->table_name;
        DataSpace *modify_ds =
            backend->get_data_space_for_table(schema_name, table_name);
        DataSpace *select_ds =
            backend->get_data_space_for_table(spark_dst_schema.c_str(), NULL);

        record_scan_all_table_spaces[modify_tb] = modify_ds;
        par_table_num = 0;
        if (modify_ds->is_partitioned()) {
          ++par_table_num;
          par_tables.push_back(modify_tb);
          need_clean_par_tables = true;
          record_scan_par_table_map[st.scanner] = modify_tb;
        }
        table_link *select_tb = modify_tb->next;
        record_scan_all_table_spaces[select_tb] = select_ds;

        spaces.push_back(select_ds);
        spaces.push_back(modify_ds);
      }
    }
    if (!spark_insert)
#endif

      pre_work_for_non_one_table_exec_plan(plan, only_one_table, one_rs,
                                           one_space, one_table);
  }

  if (st.var_item_list != NULL) {
    LOG_DEBUG(
        "The sql [%s] has user variable, build a set"
        " sql for the session.\n",
        sql);
    var_item *var_item_list = st.var_item_list;
    build_uservar_sql(var_item_list, plan);
    plan->session->set_var_flag(true);
  } else {
    plan->session->set_var_flag(false);
  }

  if ((!only_related_one_server ||
       (only_related_one_server && union_table_sub)) &&
      st.scanner->is_select_union && st.scanner->opt_union_all &&
      !union_all_sub) {
    plan->session->set_is_complex_stmt(true);
    rebuild_union_all_sql_and_generate_execution_plan(plan);
#ifndef DBSCALE_TEST_DISABLE
    plan->handler->prepare_gen_plan.end_timing_and_report();
#endif
    return;
  }
  if (st.type == STMT_SET) {
    generate_plan_for_set_stmt(plan);
    return;
  }

  if (handle_executable_comments_after_cnj(plan)) {
    return;
  }

#ifndef DBSCALE_TEST_DISABLE
  plan->handler->prepare_gen_plan.end_timing_and_report();
#endif

  if (generate_plan_according_to_par_table_num_and_spaces(plan)) return;

  // Should not be here
  ACE_ASSERT(0);
}

/* Get the identify columns of one table for generate_sub_sqls(), it could be
 * primary key columns for most case. And this function also do some checks to
 * ensure the sql is executable, basing on the identify columns and sql type.
 *
 * Return   HAS_IDENTIFY_COLUMNS :
 *                 means the modify sql can be executed part by part.
 *          NO_IDENTIFY_COLUMNS :
 *                 means there is no primary key, so the modify sql only can be
 *executed as whole.
 **/
int Statement::get_identify_columns(const char *schema_name,
                                    const char *table_name,
                                    const char *table_alias,
                                    vector<string *> *columns) {
  Backend *backend = Backend::instance();
  DataSpace *space = backend->get_data_space_for_table(schema_name, table_name);
  bool is_par_table = backend->table_is_partitioned(schema_name, table_name);
  int ret = HAS_IDENTIFY_COLUMNS;

  if (is_par_table) {
    // Add partition keys into the front of column list
    PartitionedTable *par_table = (PartitionedTable *)space;
    vector<const char *> *key_names = par_table->get_key_names();
    vector<const char *>::iterator it = key_names->begin();
    for (; it != key_names->end(); ++it) {
      columns->push_back(new string(*it));
    }

    space = ((PartitionedTable *)space)->get_partition(0);
  }

  // TODO: Retry if got exception
  Connection *conn = NULL;
  try {
    conn = space->get_connection(get_session(), schema_name, true);
    if (!conn) {
      throw Error("Fail to get connection for getting identify columns.");
    }
    int r = 0;
    bool found_key = false;

    r = conn->get_identify_columns_name(schema_name, table_name, true, columns,
                                        &found_key);
    if (!found_key && !r) {
      r = conn->get_identify_columns_name(schema_name, table_name, false,
                                          columns, &found_key);

      /*For update sql, if there is no primary key(s), it must be executed as
       * a whole.*/
      if (st.type == STMT_UPDATE ||
          (st.type == STMT_DELETE && st.cur_rec_scan->order_by_list))
        ret = NO_IDENTIFY_COLUMNS;
    }
    if (r) {
      conn->get_pool()->add_back_to_dead(conn);
      conn = NULL;
      throw Error("Fail to get identify columns.");
    }
    conn->get_pool()->add_back_to_free(conn);
    conn = NULL;

    if (!found_key) {
      LOG_ERROR(
          "Unsupport sql [%s], cannot find the column info for the "
          "modify table.\n",
          sql);
      throw UnSupportPartitionSQL(
          "cannot find the column info for the modify table");
    }
  } catch (Exception &e) {
    if (conn) conn->get_pool()->add_back_to_dead(conn);
    LOG_ERROR(
        "Got exception [%s] when try to get the identify columns of table"
        " [%s.%s] in sql [%s].\n",
        e.what(), schema_name, table_name, sql);
    throw;
  }

  if (is_par_table && ret == HAS_IDENTIFY_COLUMNS && st.type == STMT_UPDATE) {
    Expression *set_list = st.sql->update_oper->update_set_list;
    string key_value = "";
    if (set_list_contain_key(set_list, columns, schema_name, table_name,
                             table_alias, key_value)) {
      LOG_ERROR(
          "Unsupport sql [%s], cannot modify the primary key for dbscale "
          "currently.\n",
          sql);
      throw UnSupportPartitionSQL(
          "cannot modify the primary key for dbscale currently");
    }
  }
  return ret;
}

void Statement::handle_federated_join_method() {
  if (record_scan_join_tables.count(st.scanner) &&
      record_scan_join_tables[st.scanner]) {
    vector<vector<TableStruct *> *> *join_table_sequence =
        record_scan_join_tables[st.scanner];
    int size = join_table_sequence->size();
    /* We can only support 2 table federated method */
    if (size == 2) {
      exec_nodes.at(0)->update_join_method(DATA_MOVE_READ);
    }
  }
}

table_link *Statement::find_table_link(const char *table_full_name,
                                       record_scan *rs, bool ignore_schema) {
  if (!table_full_name) return NULL;
  table_link *tmp = rs->first_table;
  string s1, s2;

  while (tmp && tmp->join->cur_rec_scan == rs) {
    s1.clear();
    if (tmp->join->alias) {
      s1.append(tmp->join->alias);
    } else {
      if (tmp->join->schema_name && !ignore_schema) {
        s1.append(tmp->join->schema_name);
        s1.append(".");
      }
      s1.append(tmp->join->table_name);
    }
    if (lower_case_table_names) {
      if ((strlen(table_full_name) == s1.size() &&
           !strcasecmp(table_full_name, s1.c_str()))) {
        return tmp;
      }
    } else {
      if ((strlen(table_full_name) == s1.size() &&
           !strcmp(table_full_name, s1.c_str()))) {
        return tmp;
      }
    }
    tmp = tmp->next;
  }
  return NULL;
}

unsigned int Statement::get_select_part_start_pos(record_scan *rs) {
  if (rs->where_pos) return rs->where_pos;
  if (rs->order_pos) return rs->order_pos;
  if (rs->limit_pos) return rs->limit_pos;
  LOG_ERROR(
      "Not support multiple-table update/delete without where condition.\n");
  throw NotSupportedError(
      "Not support multiple-table update/delete without where condition.");
}

/*Generate one select sql and one modify sql(partial) according to the
 * original sql.
 *
 * Only accept one modfiy table!
 *
 * Return 1. fullfil the identify column of the modify table;
 *        2. table_link of the modify table;
 *        3. return a bool to indicate whether the modify sql can be executed
 *part by part.
 **/
bool Statement::generate_two_phase_modify_sub_sqls(vector<string *> *columns,
                                                   table_link *modify_table) {
  int ret = NO_IDENTIFY_COLUMNS;
  stmt_type type = st.type;
  select_sub_sql.clear();
  modify_sub_sql.clear();
  string select_head("SELECT ");

  if (type == STMT_INSERT_SELECT || type == STMT_REPLACE_SELECT) {
    record_scan *insert_select = st.scanner->children_begin;
    unsigned int start_pos = insert_select->start_pos;
    unsigned int end_pos = insert_select->end_pos;
    unsigned int insert_end_pos = start_pos;
    while (insert_end_pos > 1 && (*(sql + insert_end_pos - 2) == ' ' ||
                                  *(sql + insert_end_pos - 2) == '('))
      insert_end_pos--;
    select_sub_sql.append(sql + start_pos - 1, end_pos - start_pos + 1);

    if (st.sql->insert_oper->insert_column_list &&
        auto_inc_status == AUTO_INC_NOT_SPECIFIED) {
      modify_sub_sql.append(sql, st.sql->insert_oper->column_list_end_pos);
      modify_sub_sql.append(", ");
      modify_sub_sql.append(auto_inc_key_name);
      modify_sub_sql.append(") VALUES ");
      LOG_DEBUG(
          "Auto_increment field is not specified in column list,"
          " append it to modify_sub_sql internally.\n");
    } else {
      modify_sub_sql.append(sql, insert_end_pos - 1);
      modify_sub_sql.append(" VALUES ");
    }
  } else {
    const char *schema_name = modify_table->join->schema_name
                                  ? modify_table->join->schema_name
                                  : schema;
    const char *table_name = modify_table->join->table_name;
    const char *table_alias = modify_table->join->alias;

    ret = get_identify_columns(schema_name, table_name, table_alias, columns);
    /*If there is no primary key, we are not able to guarantee the limit num.
     * For example: table t1 (c1 int) with data (1 , 2 ,2).
     *
     *  The sql is:
     *
     *   update t1 set c1 = 10 where c1 in (select a from t_other) limit 2.
     *
     *   We can get two c1 values (1, 2) by executing the select sql:
     *
     *   select c1 from t1 where c1 in (select a from t_other) limit 2.
     *
     *   But the generated update sql:
     *
     *    update t1 set c1=10 where c1 =1 or c1=2.
     *
     *   will update 3 records of t1.*/
    if (ret == NO_IDENTIFY_COLUMNS && modify_table->join->cur_rec_scan->limit) {
      LOG_ERROR(
          "Unsupport SQL [%s], modify SQL no identify column but"
          " with limit, which is not support by dbscale currently.\n",
          sql);
      throw UnSupportPartitionSQL(
          "modify SQL no identify column but with limit, which is not support "
          "by dbscale currently");
    }

    vector<string *>::iterator it = columns->begin();
    if (table_alias) {
      select_head += table_alias;
    } else {
      select_head += table_name;
    }
    select_head += ".";
    select_head += (*it)->c_str();
    ++it;
    for (; it != columns->end(); ++it) {
      select_head += ",";
      if (table_alias) {
        select_head += table_alias;
      } else {
        select_head += table_name;
      }
      select_head += ".";
      select_head += (*it)->c_str();
    }
    select_head += " ";

    switch (type) {
      case STMT_UPDATE: {
        if (modify_table->join->cur_rec_scan->join_tables == nullptr) {
          string err = "fail to get tables for sql ";
          LOG_ERROR("%s [%s]\n", err.c_str(), sql);
          throw Error(err.c_str());
        }
        if (modify_table->join->cur_rec_scan->join_tables->type ==
            JOIN_NODE_SINGLE) {
          Expression *update_set_list = st.sql->update_oper->update_set_list;
          ListExpression *expr_list = (ListExpression *)update_set_list;
          expr_list_item *head = expr_list->expr_list_head;
          expr_list_item *set_contain_subquery = head;
          CompareExpression *cmp_tmp = NULL;
          Expression *set_expr = NULL;
          int subquery_num = 0;
          do {
            cmp_tmp = (CompareExpression *)head->expr;
            set_expr = cmp_tmp->right;
            if (set_expr->type == EXPR_SUBSELECT) {
              if (subquery_num == 1) {
                LOG_ERROR("update set only support one subquery now. \n");
                throw Error("update set only support one subquery now.");
              }
              subquery_num += 1;
              set_contain_subquery = head;
            }
            head = head->next;
          } while (head != expr_list->expr_list_head);
          if (subquery_num == 1) {
            cmp_tmp = (CompareExpression *)set_contain_subquery->expr;
            set_expr = cmp_tmp->right;
            int start_pos = modify_table->join->cur_rec_scan->sub_start_pos;
            int end_pos = modify_table->join->cur_rec_scan->sub_end_pos;
            select_sub_sql.append(sql, start_pos, end_pos - start_pos - 1);
            modify_sub_sql.append(sql, 0, cmp_tmp->left->end_pos);
            modify_sub_sql.append(DBSCALE_TMP_COLUMN_VALUE);
            modify_sub_sql.append(sql, end_pos, strlen(sql) - end_pos + 1);
            is_update_set_subquery = true;
          } else {
            unsigned int pos = get_select_part_start_pos(st.scanner);
            select_sub_sql += select_head;
            select_sub_sql += "from ";
            select_sub_sql += schema_name;
            select_sub_sql += ".";
            select_sub_sql += table_name;
            select_sub_sql += " ";
            select_sub_sql.append(sql + pos - 1, strlen(sql) - pos + 1);

            modify_sub_sql.append(sql, pos - 1);
            modify_sub_sql += " where ";
          }
          ret = NO_IDENTIFY_COLUMNS;
        } else if (modify_table->join->cur_rec_scan->join_tables->type ==
                   JOIN_NODE_JOIN) {
          unsigned int table_pos = st.sql->update_oper->update_table_pos;
          unsigned int set_start_pos =
              st.sql->update_oper->update_set_start_pos;
          unsigned int set_end_pos = st.sql->update_oper->update_set_end_pos;
          ListExpression *set_list =
              (ListExpression *)(st.sql->update_oper->update_set_list);
          expr_list_item *item = set_list->expr_list_head;
          list<string> set_value_list;
          bool is_set_value_list_contain_field = false;
          // check whether set list contains filed expression.
          do {
            if (item->expr->type == EXPR_EQ) {
              Expression *value_expr = ((CompareExpression *)item->expr)->right;
              if (value_expr->type == EXPR_SUBSELECT) {
                throw Error(
                    "Not support cross node join update sql set list contain "
                    "select field.");
              }
              if (is_expression_contain_aggr(value_expr,
                                             modify_table->join->cur_rec_scan,
                                             false, true)) {
                is_set_value_list_contain_field = true;
              }
              string value_string;
              value_expr->to_string(value_string);
              set_value_list.push_back(value_string);
            } else {
              LOG_ERROR("Unknown update set expression for sql [%s]\n", sql);
              throw Error("Unknown update set expression for sql");
            }
            item = item->next;
          } while (item != set_list->expr_list_head);

          /*set list does not contain field, use modify select deal this sql
            deal sql like "UPDATE t2 DP INNER JOIN ( SELECT* from t1   ) X  SET
            DP.c2 = 2   WHERE DP.c1 = X.c1 AND DP.c2 = X.c2;" select_sub_sql:
            [SELECT test.DP.c1  FROM t2 DP INNER JOIN ( SELECT* from t1   ) X
                             WHERE DP.c1 = X.c1 AND DP.c2 = X.c2]
            update_sub_sql: [UPDATE test.t2 AS DP SET  DP.c2 = 2    WHERE ]
           */
          if (!is_set_value_list_contain_field) {
            select_sub_sql += select_head;
            select_sub_sql += " FROM ";
            select_sub_sql.append(sql + table_pos - 1,
                                  set_start_pos - table_pos);
            select_sub_sql.append(sql + set_end_pos, strlen(sql) - set_end_pos);
            LOG_DEBUG("select_sub_sql [%s]\n", select_sub_sql.c_str());

            modify_sub_sql = "UPDATE ";
            modify_sub_sql += schema_name;
            modify_sub_sql += ".";
            modify_sub_sql += table_name;
            if (table_alias) {
              modify_sub_sql += " AS ";
              modify_sub_sql += table_alias;
            }
            modify_sub_sql += " ";
            modify_sub_sql.append(sql + set_start_pos - 1,
                                  set_end_pos - set_start_pos + 1);
            modify_sub_sql += " WHERE ";
            LOG_DEBUG("modify_sub_sql [%s]\n", modify_sub_sql.c_str());
          } else {
            /*set list contain field, use SeparatedExecNode deal this sql.
              deal sql like "UPDATE t2 DP INNER JOIN ( SELECT* from t1   ) X SET
              DP.c2 = X.c1 * X.c2, DP.c1 = DP.c1 + 10 WHERE DP.c1 = X.c1 AND
              DP.c2 = X.c2" create_sql     [create table
              dbscale_tmp.dbscale_update_tmp (dbscale_tmp_column_key_0 int, ...,
                              dbscale_tmp_column_value_0 int, ...)]
              select_sub_sql [SELECT test.DP.c1 , X.c1 * X.c2 as
              dbscale_tmp_column_value0 , DP.c1 + 10 as
              dbscale_tmp_column_value1 FROM t2 DP INNER JOIN ( SELECT* from t1
              ) X  WHERE DP.c1 = X.c1 AND DP.c2 = X.c2] modify_sub_sql [UPDATE
              test.t2 AS DP, dbscale_tmp.dbscale_update_tmp set DP.c2 =
              dbscale_tmp_column_value0 , DP.c1 = dbscale_tmp_column_value1
                              WHERE  test.DP.c1 =
              dbscale_tmp.dbscale_update_tmp.dbscale_tmp_column_key0 ]*/

            // generate select_sub_sql
            select_sub_sql += select_head;
            list<string>::iterator it = set_value_list.begin();
            int index = 0;
            for (; it != set_value_list.end(); ++it) {
              char column_alis[100];
              size_t len = sprintf(column_alis, ", %s as %s%d ", it->c_str(),
                                   DBSCALE_TMP_COLUMN_VALUE, index);
              select_sub_sql.append(column_alis, len);
              ++index;
            }
            select_sub_sql += " FROM ";
            select_sub_sql.append(sql + table_pos - 1,
                                  set_start_pos - table_pos);
            select_sub_sql.append(sql + set_end_pos, strlen(sql) - set_end_pos);
            LOG_DEBUG("select_sub_sql [%s]\n", select_sub_sql.c_str());

            // generate modify_sub_sql
            modify_sub_sql = "UPDATE ";
            modify_sub_sql += schema_name;
            modify_sub_sql += ".";
            modify_sub_sql += table_name;
            if (table_alias) {
              modify_sub_sql += " AS ";
              modify_sub_sql += table_alias;
            }
            modify_sub_sql += ", ";
            modify_sub_sql += DBSCALE_TMP_UPDATE_TABLE;
            modify_sub_sql += " set ";
            item = set_list->expr_list_head;
            index = 0;
            do {
              Expression *key_expr = ((CompareExpression *)item->expr)->left;
              string key_str;
              key_expr->to_string(key_str);
              modify_sub_sql += key_str;

              char set_update_tmp_value[100];
              size_t len = sprintf(set_update_tmp_value, " = %s%d , ",
                                   DBSCALE_TMP_COLUMN_VALUE, index);
              modify_sub_sql.append(set_update_tmp_value, len);
              ++index;

              item = item->next;
            } while (item != set_list->expr_list_head);

            boost::erase_tail(modify_sub_sql, 2);
            modify_sub_sql += " WHERE ";

            string modify_table_name;
            modify_table_name += schema_name;
            modify_table_name += ".";
            if (table_alias) {
              modify_table_name += table_alias;
            } else {
              modify_table_name += table_name;
            }

            index = 0;
            vector<string *>::iterator it_c = columns->begin();
            for (; it_c != columns->end(); ++it_c) {
              char update_where_tmp[100];
              size_t len = sprintf(update_where_tmp, " %s.%s = %s.%s%d and ",
                                   modify_table_name.c_str(), (*it_c)->c_str(),
                                   DBSCALE_TMP_UPDATE_TABLE,
                                   DBSCALE_TMP_COLUMN_KEY, index);
              modify_sub_sql.append(update_where_tmp, len);
              ++index;
            }
            boost::erase_tail(modify_sub_sql, 4);

            LOG_DEBUG("modify_sub_sql [%s]\n", modify_sub_sql.c_str());
            throw Error(
                "Not support update sql set list contain select field.");
          }
        } else {
          LOG_ERROR("Not support this type update sql\n");
          throw Error("Not support this type update sql");
        }
        break;
      }
      case STMT_DELETE: {
        unsigned int pos = get_select_part_start_pos(st.scanner);
        select_sub_sql += select_head;
        select_sub_sql += "from ";

        table_link *head = st.scanner->first_table;
        table_link *tmp = head;

        if (tmp->join->schema_name) {
          select_sub_sql += tmp->join->schema_name;
          select_sub_sql += '.';
        }
        select_sub_sql += tmp->join->table_name;
        if (tmp->join->alias) {
          select_sub_sql += " as ";
          select_sub_sql += tmp->join->alias;
        }
        tmp = tmp->next;
        while (tmp) {
          if (tmp->join->cur_rec_scan != st.scanner) break;
          select_sub_sql += ',';
          if (tmp->join->schema_name) {
            select_sub_sql += tmp->join->schema_name;
            select_sub_sql += '.';
          }
          select_sub_sql += tmp->join->table_name;
          if (tmp->join->alias) {
            select_sub_sql += " as ";
            select_sub_sql += tmp->join->alias;
          }

          tmp = tmp->next;
        }

        select_sub_sql += " ";
        select_sub_sql.append(sql + pos - 1, strlen(sql) - pos + 1);

        modify_sub_sql += "DELETE FROM ";
        modify_sub_sql += schema_name;
        modify_sub_sql += ".";
        modify_sub_sql += table_name;
        modify_sub_sql += " WHERE";
        break;
      }
      default:
        break;
    }
  }

  LOG_DEBUG(
      "Assemble select_sub_sql [%s], modify_sub_sql [%s] for"
      " sql [%s].\n",
      select_sub_sql.c_str(), modify_sub_sql.c_str(), sql);

  return (ret == HAS_IDENTIFY_COLUMNS);
}

void Statement::set_order_by_indexs(record_scan *rs) {
  while (rs->is_select_union) rs = rs->union_select_left;
  order_item *tmp = st.scanner->order_by_list;
  order_item *start = tmp;
  Expression *expr = tmp->field->field_expr;
  int column_index;
  do {
    column_index = item_in_select_fields_pos(expr, rs);
    order_dir sort_order = tmp->order;
    if (column_index != COLUMN_INDEX_UNDEF) {
      union_order_by_index.push_back(make_pair(column_index, sort_order));
    } else {
      LOG_ERROR("The order by expr is not in select fields or contain star.\n");
      throw NotSupportedError(
          "Not support order by expression not in select fields or contain "
          "star.");
    }
  } while (tmp != start);
}

void Statement::set_limit_str(ExecutePlan *plan, record_scan *rs) {
  long offset = 0;
  long num = 0;
  IntExpression *limit_offset = rs->limit->offset;
  IntExpression *limit_num = rs->limit->num;
  offset = get_limit_clause_offset_value(limit_offset, plan, rs);
  if (limit_num)
    num = get_integer_value_from_expr(limit_num);
  else {
    LOG_ERROR("limit num is not correct for sql [%s].\n", sql);
    throw Error("limit num is not correct");
  }

  union_limit_str.assign("LIMIT");
  union_limit_str.append(" ");
  char new_num[128];
  size_t len = sprintf(new_num, "%ld", offset + num);
  union_limit_str.append(new_num, len);
}

string Statement::get_expr_or_alias_in_select_items(int column_index,
                                                    record_scan *rs,
                                                    bool is_union) {
  string ret;
  field_item *field = rs->field_list_head;
  int i = 0;

  while (i++ < column_index) {
    if (!field) {
      LOG_ERROR("Fail to find [%d] column in fields.\n", column_index);
      throw Error("Fail to find correct column in select list.");
    }
    field = field->next;
  }

  if (!field) {
    LOG_ERROR("Fail to find [%d] column in fields.\n", column_index);
    throw Error("Fail to find correct column in select list.");
  }
  if (field->alias) {
    ret.assign(field->alias);
  } else if (is_union) {
    string ret_tmp;
    field->field_expr->to_string(ret_tmp);
    size_t pos = ret_tmp.rfind(".");
    if (pos != string::npos)
      ret = ret_tmp.substr(pos + 1);
    else
      ret = ret_tmp;
  } else {
    if (field->field_expr) {
      field->field_expr->to_string(ret);
    }
  }
  return ret;
}

string Statement::append_order_by_to_union(record_scan *rs,
                                           const char *ori_sql) {
  bool is_union = false;
  while (rs->is_select_union) {
    is_union = true;
    rs = rs->union_select_left;
  }
  string original_sql(ori_sql);
  string order_str(" ORDER BY ");
  string str;
  list<pair<int, order_dir> >::iterator it = union_order_by_index.begin();
  str = get_expr_or_alias_in_select_items(it->first, rs, is_union);
  order_str.append(str);
  if (it->second == ORDER_DESC)
    order_str.append(" DESC");
  else
    order_str.append(" ASC");
  for (it++; it != union_order_by_index.end(); ++it) {
    str.clear();
    str = get_expr_or_alias_in_select_items(it->first, rs, is_union);
    order_str.append(", ");
    order_str.append(str);
    if (it->second == ORDER_DESC)
      order_str.append(" DESC");
    else
      order_str.append(" ASC");
  }
  original_sql.append(order_str);
  return original_sql;
}

string Statement::append_limit_to_union(const char *ori_sql) {
  string original_sql(ori_sql);
  original_sql.append(" ");
  original_sql.append(union_limit_str);
  return original_sql;
}

/*Get dataspaces from a execute node.*/
void Statement::get_union_all_node_spaces(ExecuteNode *node,
                                          set<DataSpace *> &spaces) {
  list<ExecuteNode *> children;
  node->get_children(children);
  if (children.empty()) {
    DataSpace *space = node->get_dataspace();
    if (space) {
      spaces.insert(space);
    } else {
      LOG_ERROR("Fail to get dataspace from node [%s].\n",
                node->get_executenode_name());
      throw Error("Fail to get dataspace from execute node");
    }
  }
  list<ExecuteNode *>::iterator it = children.begin();
  for (; it != children.end(); it++) {
    get_union_all_node_spaces(*it, spaces);
  }
}

/*If any two ExecuteNode may get the same connection from kept connections,
 *put them into different groups.
 *Use union_node_group to store the execute node group, union_space_group to
 *store dataspace for every node group. Check if the dataspaces got from a node
 *and dataspace group stored in union_spaces_group may get the same connection
 *from kept connections, put the node and it's dataspaces into the first group
 *that all dataspaces may not get the same connection, if not found the group,
 *create a new one.
 */
void Statement::build_union_node_group(ExecutePlan *plan) {
  vector<ExecuteNode *>::iterator it = union_all_nodes.begin();
  set<DataSpace *> spaces;
  list<ExecuteNode *> node_list;
  get_union_all_node_spaces(*it, spaces);
  node_list.push_back(*it);
  union_node_group.push_back(node_list);
  union_space_group.push_back(spaces);
  it++;
  for (; it != union_all_nodes.end(); it++) {
    set<DataSpace *> spaces;
    get_union_all_node_spaces(*it, spaces);
    unsigned int i = 0;
    bool node_grouped = false;
    set<DataSpace *>::iterator it_s = spaces.begin();
    while (i < union_node_group.size()) {
      bool has_space = false;
      if (union_node_group[i].size() >= max_union_all_sqls) {
        has_space = true;
      } else {
        for (; it_s != spaces.end(); it_s++) {
          if (union_space_group[i].count(*it_s)) {
            has_space = true;
            break;
          } else {
            set<DataSpace *>::iterator it_c = union_space_group[i].begin();
            for (; it_c != union_space_group[i].end(); it_c++) {
              if (plan->session->check_same_kept_connection(*it_s, *it_c)) {
                has_space = true;
                union_space_group[i].insert(*it_c);
                break;
              }
            }
            if (has_space) break;
          }
        }
      }
      if (has_space) {
        ++i;
      } else {
        union_node_group[i].push_back(*it);
        union_space_group[i].insert(spaces.begin(), spaces.end());
        node_grouped = true;
        break;
      }
    }
    if (!node_grouped) {
      list<ExecuteNode *> node_list;
      node_list.push_back(*it);
      union_node_group.push_back(node_list);
      union_space_group.push_back(spaces);
    }
  }
}

void Statement::build_max_union_node_group() {
  vector<ExecuteNode *>::iterator it = union_all_nodes.begin();
  list<ExecuteNode *> node_list;
  node_list.push_back(*it);
  union_node_group.push_back(node_list);
  it++;
  int group_size = 0;
  for (; it != union_all_nodes.end(); it++) {
    if (union_node_group[group_size].size() < max_union_all_sqls) {
      union_node_group[group_size].push_back(*it);
    } else {
      list<ExecuteNode *> node_list;
      node_list.push_back(*it);
      union_node_group.push_back(node_list);
      group_size++;
    }
  }
}

void Statement::rebuild_union_all_sql_and_generate_execution_plan(
    ExecutePlan *plan) {
  string union_all_sql(sql);
  record_scan *union_all_rs = NULL;
  if (st.type == STMT_INSERT_SELECT || st.type == STMT_REPLACE_SELECT)
    union_all_rs = st.scanner->children_begin;
  else
    union_all_rs = st.scanner;
  get_sub_sqls(union_all_rs, union_all_sql);

  if (union_all_rs->limit) set_limit_str(plan, union_all_rs);

  list<string>::iterator it = union_all_sub_sqls.begin();
  if (union_all_rs->order_by_list) {
    Parser *parser = Driver::get_driver()->get_parser();
    Statement *new_stmt = parser->parse(it->c_str(), stmt_allow_dot_in_ident,
                                        true, NULL, NULL, NULL, ctype);
    union_all_stmts.push_back(new_stmt);
    new_stmt->check_and_refuse_partial_parse();
    if (is_cross_node_join()) new_stmt->set_cross_node_join(true);
    set_order_by_indexs(new_stmt->get_stmt_node()->scanner);
  }
  generate_union_all_sub_sql_and_execution_nodes(plan, it->c_str());
  list<SortDesc> *tmp_order_list = union_all_stmts.back()->get_order_by_list();
  it++;
  for (; it != union_all_sub_sqls.end(); ++it)
    generate_union_all_sub_sql_and_execution_nodes(plan, it->c_str());

  ExecuteNode *tail = NULL;
  ExecuteNode *head = NULL;
  ExecuteNode *order_node = NULL;
  ExecuteNode *limit_node = NULL;
  if (plan->session->is_in_transaction() || plan->session->is_in_lock()) {
    build_union_node_group(plan);
    LOG_DEBUG(
        "Union all in transaction, split union all nodes to [%d] groups.\n",
        union_node_group.size());
  } else if (union_all_sub_sqls.size() > max_union_all_sqls) {
    build_max_union_node_group();
    LOG_DEBUG(
        "Reach max_union_all_sqls, split union all nodes to [%d] groups.\n",
        union_node_group.size());
  }
  if (union_node_group.size() <= 1)
    head = tail = plan->get_union_all_node(union_all_nodes);

  if (union_all_rs->limit) {
    long offset = 0;
    long num = 0;
    IntExpression *limit_offset = union_all_rs->limit->offset;
    IntExpression *limit_num = union_all_rs->limit->num;
    offset = get_limit_clause_offset_value(limit_offset, plan, union_all_rs);
    if (limit_num)
      num = get_integer_value_from_expr(limit_num);
    else {
      LOG_ERROR("limit num is not correct for sql [%s].\n", sql);
      throw Error("limit num is not correct");
    }

    limit_node = plan->get_limit_node(offset, num);
    if (!head) head = limit_node;
    if (tail) tail->add_child(limit_node);
    tail = limit_node;
  }
  if (union_all_rs->order_by_list) {
    order_node = plan->get_sort_node(tmp_order_list);
    if (!head) head = order_node;
    if (tail) tail->add_child(order_node);
    tail = order_node;
  }

  if (union_node_group.size() > 1) {
    if (tail && !strcmp(tail->get_executenode_name(), "MySQLSortNode")) {
      vector<ExecuteNode *>::iterator it = union_all_nodes.begin();
      for (; it != union_all_nodes.end(); ++it) {
        (*it)->clean();
        delete *it;
      }
      if (head) {
        head->clean();
        delete head;
      }
      LOG_ERROR(
          "Not support order by when UNION ALL query need group execute.\n");
      throw NotSupportedError(
          "Not support order by when UNION ALL need group execute.");
    }
    ExecuteNode *union_group_node =
        plan->get_union_group_node(union_node_group);
    if (!head) head = union_group_node;
    if (tail) tail->add_child(union_group_node);
    tail = union_group_node;
  }
  union_all_node = head;

  for (unsigned int i = 0; i < union_all_nodes.size(); ++i)
    tail->add_child(union_all_nodes[i]);

  if (!union_table_sub) {
    ExecuteNode *send_node = plan->get_send_node();
    send_node->add_child(union_all_node);
    plan->set_start_node(send_node);
  }
}

/* If the union query is a table subquery, we should insert the
 * result to the subquery tmp table, so we generate a insert sql,
 * treate the union subquery as a normal insert select query.
 **/
void Statement::assemble_union_table_sub(ExecutePlan *plan) {
  string insert_sql("INSERT INTO ");
  insert_sql.append(TMP_TABLE_SCHEMA);
  insert_sql.append(".");
  insert_sql.append(union_sub_table_name);
  insert_sql.append(" VALUES ");

  ExecuteNode *ok_node = plan->get_ok_node();
  ExecuteNode *ok_merge_node = plan->get_ok_merge_node();
  ExecuteNode *insert_select = NULL;
  ExecuteNode *modify_node = NULL;
  vector<ExecuteNode *> nodes;
  nodes.push_back(union_all_node);

  Backend *backend = Backend::instance();
  bool is_duplicated = false;
  DataSpace *space = backend->get_data_space_for_table(
      TMP_TABLE_SCHEMA, union_sub_table_name.c_str());
  if (space->is_duplicated()) {
    space = ((DuplicatedTable *)space)->get_partition_table();
    is_duplicated = true;
  }
  if (space->get_data_source()) {
    insert_select = plan->get_insert_select_node(insert_sql.c_str(), &nodes);
    modify_node = plan->get_modify_node(space);
    insert_select->add_child(modify_node);
  } else {
    vector<unsigned int> key_pos;
    if (!is_duplicated)
      ((PartitionedTable *)space)
          ->get_key_pos_vec(TMP_TABLE_SCHEMA, union_sub_table_name.c_str(),
                            key_pos, stmt_session);
    insert_select = plan->get_par_insert_select_node(
        insert_sql.c_str(), (PartitionedTable *)space,
        ((PartitionedTable *)space)->get_partition_method(), key_pos, &nodes,
        TMP_TABLE_SCHEMA, union_sub_table_name.c_str(), is_duplicated);
    plan->session->set_affected_servers(AFFETCED_MUL_SERVER);
  }
  ok_merge_node->add_child(insert_select);
  ok_node->add_child(ok_merge_node);
  plan->set_start_node(ok_node);
}

void Statement::get_sub_sqls(record_scan *scan, string union_all_sql) {
  string union_all_sub_sql;
  LOG_DEBUG("Get union sub sqls of %s.\n", union_all_sql.c_str());
  get_sub_sqls_poses(scan);
  map<unsigned int, unsigned int>::iterator it =
      union_all_sub_sql_poses.begin();
  for (; it != union_all_sub_sql_poses.end(); ++it) {
    union_all_sub_sql.assign(union_all_sql, it->first - 1,
                             it->second - it->first + 1);
    union_all_sub_sqls.push_back(union_all_sub_sql);
    LOG_DEBUG("UNION ALL sub query [%s]\n", union_all_sub_sql.c_str());
  }
}

DataSpace *Statement::get_sub_sqls_poses(record_scan *scan) {
  DataSpace *left_space = NULL;
  DataSpace *right_space = NULL;
  record_scan *left_scan = scan->union_select_left;
  record_scan *right_scan = scan->union_select_right;

  if (scan->is_select_union &&
      scan->opt_union_all) {  // handle union all situation
    left_space = get_sub_sqls_poses(left_scan);
    right_space = get_sub_sqls_poses(right_scan);
  } else if (scan->is_select_union) {  // handle union situation, union can be
                                       // seen as can merge here
    union_all_sub_sql_poses[scan->start_pos] = scan->end_pos;
    DataSpace *tmp_ds = record_scan_one_space_tmp_map[scan->union_select_left];
    if (!tmp_ds)
      tmp_ds = record_scan_one_space_tmp_map[scan->union_select_right];
    return tmp_ds;
  } else {  // leaf record scan.
    union_all_sub_sql_poses[scan->start_pos] = scan->end_pos;
    if (record_scan_all_par_tables_map.count(scan)) return NULL;
    DataSpace *tmp_ds = merge_dataspace_one_record_scan(scan, NULL);
    if (tmp_ds) return tmp_ds;
    return NULL;
  }

  if (left_scan->union_dataspace_can_merge) {
    if (!left_space || !right_space) {
      if (!scan->opt_union_all) {
        LOG_ERROR(
            "Unsupport sql, the sql is [%s], unsupport UNION without ALL"
            " when dataspace can not merge.\n",
            sql);
        throw NotSupportedError(
            "UNION without ALL when dataspace can not merge is not supported.");
      }
      return NULL;
    } else if (stmt_session->is_dataspace_cover_session_level(left_space,
                                                              right_space)) {
      if (union_all_sub_sql_poses.count(left_scan->start_pos)) {
        union_all_sub_sql_poses.erase(left_scan->start_pos);
      } else {
#ifdef DEBUG
        ACE_ASSERT(0);  // Should not be here
#endif
      }
      if (union_all_sub_sql_poses.count(right_scan->start_pos)) {
        union_all_sub_sql_poses.erase(right_scan->start_pos);
      } else {
#ifdef DEBUG
        ACE_ASSERT(0);  // Should not be here
#endif
      }
      union_all_sub_sql_poses[scan->start_pos] = scan->end_pos;
      scan->union_dataspace_can_merge = true;
      return left_space;
    } else if (stmt_session->is_dataspace_cover_session_level(right_space,
                                                              left_space)) {
      if (union_all_sub_sql_poses.count(left_scan->start_pos)) {
        union_all_sub_sql_poses.erase(left_scan->start_pos);
      } else {
#ifdef DEBUG
        ACE_ASSERT(0);  // Should not be here
#endif
      }
      if (union_all_sub_sql_poses.count(right_scan->start_pos)) {
        union_all_sub_sql_poses.erase(right_scan->start_pos);
      } else {
#ifdef DEBUG
        ACE_ASSERT(0);  // Should not be here
#endif
      }
      union_all_sub_sql_poses[scan->start_pos] = scan->end_pos;
      scan->union_dataspace_can_merge = true;
      return right_space;
    } else if (!scan->opt_union_all) {
      LOG_ERROR(
          "Unsupport sql, the sql is [%s], unsupport UNION without ALL"
          " when dataspace can not merge.\n",
          sql);
      throw NotSupportedError(
          "UNION without ALL when dataspace can not merge is not supported.");
    }
    return NULL;
  }
  if (!scan->opt_union_all) {
    LOG_ERROR(
        "Unsupport sql, the sql is [%s], unsupport UNION without ALL"
        " when dataspace can not merge.\n",
        sql);
    throw NotSupportedError(
        "UNION without ALL when dataspace can not merge is not supported.");
  }
  return NULL;
}

bool Statement::check_union_contain_star(record_scan *rs) {
  if (rs->is_select_union) {
    if (check_union_contain_star(rs->union_select_left)) return true;
    if (check_union_contain_star(rs->union_select_right)) return true;
  }
  if (rs->is_contain_star) return true;
  return false;
}

void Statement::generate_union_all_sub_sql_and_execution_nodes(
    ExecutePlan *plan, const char *sub_sql) {
  Statement *new_stmt = NULL;
  string new_sql;
  Parser *parser = Driver::get_driver()->get_parser();
  new_stmt = parser->parse(sub_sql, stmt_allow_dot_in_ident, true, NULL, NULL,
                           NULL, ctype);
  union_all_stmts.push_back(new_stmt);
  new_stmt->check_and_refuse_partial_parse();
  if (is_cross_node_join()) new_stmt->set_cross_node_join(true);

  if (st.scanner->order_by_list &&
      !new_stmt->get_stmt_node()->scanner->order_by_list) {
    if (check_union_contain_star(new_stmt->get_stmt_node()->scanner)) {
      vector<ExecuteNode *>::iterator it = union_all_nodes.begin();
      for (; it != union_all_nodes.end(); ++it) {
        (*it)->clean();
        delete *it;
      }
      LOG_ERROR("The order by expr is not in select fields or contain star.\n");
      throw NotSupportedError(
          "Not support order by expression not in select fields or contain "
          "star.");
    }
    new_sql =
        append_order_by_to_union(new_stmt->get_stmt_node()->scanner, sub_sql);
    new_stmt = parser->parse(new_sql.c_str(), stmt_allow_dot_in_ident, true,
                             NULL, NULL, NULL, ctype);
    union_all_stmts.push_back(new_stmt);
    new_stmt->check_and_refuse_partial_parse();
    if (is_cross_node_join()) new_stmt->set_cross_node_join(true);

  } else if (st.scanner->order_by_list && union_all_sub_sqls.size() > 1 &&
             new_stmt->get_stmt_node()->scanner->order_by_list) {
    LOG_ERROR(
        "Not support order by in union sub query with order by outside.\n");
    throw NotSupportedError("Not support order by in union sub query.");
  }
  if (st.scanner->limit && !new_stmt->get_stmt_node()->scanner->limit) {
    new_sql = append_limit_to_union(new_sql.c_str());
    new_stmt = parser->parse(new_sql.c_str(), stmt_allow_dot_in_ident, true,
                             NULL, NULL, NULL, ctype);
    union_all_stmts.push_back(new_stmt);
    new_stmt->check_and_refuse_partial_parse();
    if (is_cross_node_join()) new_stmt->set_cross_node_join(true);
  }
  LOG_DEBUG("New union sub query after handle limit and order by [%s].\n",
            new_sql.c_str());

  new_stmt->set_union_all_sub(true);
  new_stmt->set_default_schema(schema);
  new_stmt->generate_execution_plan(plan);
  vector<ExecuteNode *> union_all_nodes_tmp = new_stmt->get_union_all_nodes();
  for (unsigned int i = 0; i < union_all_nodes_tmp.size(); ++i) {
    union_all_nodes.push_back(union_all_nodes_tmp[i]);
  }
  /*get table vector for constrct create sql of table sub query with the first
   *union sub query that contains tables.
   */
  if (union_table_sub && !first_union_stmt &&
      new_stmt->get_stmt_node()->table_list_head) {
    record_scan *tmp_rs = new_stmt->get_stmt_node()->scanner;
    while (tmp_rs->is_select_union) {
      tmp_rs = tmp_rs->union_select_left;
    }
    first_union_stmt = new_stmt;
    vector<TableStruct> table_vector;
    union_table_vector.push_back(table_vector);
    get_table_vector(tmp_rs, &union_table_vector);
  }
}

void Statement::check_found_rows_with_group_by(ExecutePlan *plan) {
  if (st.type == STMT_SELECT) {
    if ((st.cur_rec_scan->options & SQL_OPT_SQL_CALC_FOUND_ROWS) &&
        st.scanner->group_by_list) {
      LOG_ERROR(
          "Unsupport sql, the sql is [%s], not support"
          " SQL_OPT_SQL_CALC_FOUND_ROWS with group_by for partitioned table.\n",
          sql);
      throw NotSupportedError(
          "SQL_CALC_FOUND_ROWS with GROUP BY for partitioned table is not "
          "supported.");
    }
    plan->session->set_found_rows(0);
    plan->session->set_real_fetched_rows(0);
  }
}

void Statement::assemble_drop_mul_table(ExecutePlan *plan, bool is_view) {
  Backend *backend = Backend::instance();
  table_link *table = st.table_list_head;
  const char *schema_name, *table_name;
  map<DataSpace *, vector<string *> *> table_name_space;
  map<DataSpace *, vector<string *> *>::iterator merge_it;
  DataSpace *dataspace;
  while (table) {
    schema_name = table->join->schema_name ? table->join->schema_name : schema;
    if (dbscale_safe_sql_mode > 0 &&
        Backend::instance()->is_system_schema(schema_name)) {
      throw Error(
          "Refuse drop table of system schema for dbscale_safe_sql_mode>0");
    }

    table_name = table->join->table_name;
    string *drop_full_table_name;
    if (schema_name != schema) {
      drop_full_table_name = new string("`");
      drop_full_table_name->append(schema_name);
      drop_full_table_name->append("`.`");
      drop_full_table_name->append(table_name);
      drop_full_table_name->append("`");
    } else {
      drop_full_table_name = new string("`");
      drop_full_table_name->append(table_name);
      drop_full_table_name->append("`");
    }
    dataspace = backend->get_data_space_for_table(schema_name, table_name);
    if (backend->table_is_partitioned(schema_name, table_name)) {
      PartitionedTable *part_table = (PartitionedTable *)dataspace;
      for (unsigned int i = 0; i < part_table->get_real_partition_num(); ++i) {
        string *part_full_table_name = new string(*drop_full_table_name);
        if (table_name_space.size() > 0) {
          bool insert_flag = true;
          for (merge_it = table_name_space.begin();
               merge_it != table_name_space.end(); ++merge_it) {
            if (is_share_same_server(merge_it->first,
                                     part_table->get_partition(i))) {
              merge_it->second->push_back(part_full_table_name);
              insert_flag = false;
              break;
            }
          }
          if (insert_flag) {
            vector<string *> *tmp_table_list = new vector<string *>();
            tmp_table_list->push_back(part_full_table_name);
            table_name_space[part_table->get_partition(i)] = tmp_table_list;
          }
        } else {
          vector<string *> *tmp_table_list = new vector<string *>();
          tmp_table_list->push_back(part_full_table_name);
          table_name_space[part_table->get_partition(i)] = tmp_table_list;
        }
      }
      delete drop_full_table_name;
    } else {
      if (table_name_space.size() > 0) {
        bool insert_flag = true;
        for (merge_it = table_name_space.begin();
             merge_it != table_name_space.end(); ++merge_it) {
          if (is_share_same_server(merge_it->first, dataspace)) {
            merge_it->second->push_back(drop_full_table_name);
            insert_flag = false;
            break;
          }
        }
        if (insert_flag) {
          vector<string *> *tmp_table_list = new vector<string *>();
          tmp_table_list->push_back(drop_full_table_name);
          table_name_space[dataspace] = tmp_table_list;
        }
      } else {
        vector<string *> *tmp_table_list = new vector<string *>();
        tmp_table_list->push_back(drop_full_table_name);
        table_name_space[dataspace] = tmp_table_list;
      }
    }
    table = table->next;
  }
  DataSpace *meta_datasource = Backend::instance()->get_metadata_data_space();
  map<DataSpace *, vector<string *> *>::iterator it_space =
      table_name_space.begin();
  bool need_apply_meta = true;
  for (; it_space != table_name_space.end(); ++it_space) {
    if (is_share_same_server(meta_datasource, it_space->first)) {
      need_apply_meta = false;
      break;
    }
  }
  st.need_apply_metadata = need_apply_meta;
  ExecuteNode *ok_node = plan->get_ok_node();
  ExecuteNode *drop_node = plan->get_drop_mul_table_node();
  ExecuteNode *modify_node;
  string sql("DROP");
  if (st.sql->drop_tb_oper->op_tmp == 1) {
    sql.append(" TEMPORARY");
    st.is_create_or_drop_temp_table = true;
  }
  if (is_view)
    sql.append(" VIEW");
  else
    sql.append(" TABLE");
  if (st.sql->drop_tb_oper->op_exists == 1) {
    sql.append(" IF EXISTS");
  }
  LOG_DEBUG("The generate sql prefix is %s.\n", sql.c_str());
  map<DataSpace *, vector<string *> *>::iterator it;
  for (it = table_name_space.begin(); it != table_name_space.end(); ++it) {
    string *real_sql = new string(sql);
    real_sql->append(" ");
    unsigned int i = 0;
    for (; i < it->second->size() - 1; ++i) {
      real_sql->append(*(*(it->second))[i]);
      real_sql->append(",");
    }
    real_sql->append(*(*(it->second))[i]);
    modify_node = plan->get_modify_node(it->first, real_sql->c_str(), true);
    drop_node->add_child(modify_node);
    delete real_sql;
  }
  vector<string *>::iterator itinner;
  for (it = table_name_space.begin(); it != table_name_space.end(); ++it) {
    for (itinner = it->second->begin(); itinner != it->second->end();
         ++itinner) {
      delete *itinner;
    }
    delete it->second;
  }
  ok_node->add_child(drop_node);
  plan->set_start_node(ok_node);
}

TableStruct *Statement::check_column_belongs_to(
    const char *column_name, list<TableStruct *> *table_struct_list) {
  TableStruct *ret = NULL;
  int table_nums = 0;
  list<TableStruct *>::iterator it = table_struct_list->begin();
  for (; it != table_struct_list->end(); ++it) {
    string schema_name = (*it)->schema_name;
    string table_name = (*it)->table_name;

    TableInfoCollection *tic = TableInfoCollection::instance();
    TableInfo *ti = tic->get_table_info_for_read(schema_name, table_name);
    map<string, TableColumnInfo *, strcasecomp> *column_info_map;
    try {
      column_info_map =
          ti->element_table_column
              ->get_element_map_columnname_table_column_info(stmt_session);
    } catch (...) {
      LOG_ERROR(
          "Error occured when try to get table info column "
          "info(map_columnname_table_column_info) of table [%s.%s]\n",
          schema_name.c_str(), table_name.c_str());
      ti->release_table_info_lock();
      throw;
    }

    string tmp_colname(column_name);
    boost::to_lower(tmp_colname);
    if (column_info_map != NULL) {
      if (column_info_map->count(tmp_colname)) {
        ++table_nums;
        ret = *it;
      }
    }
    ti->release_table_info_lock();
  }
  if (table_nums > 1) {
    string error_message;
    error_message.append(
        "You should add the table name to the column name since more than one "
        "table contains ");
    error_message.append(column_name);
    throw Error(error_message.c_str());
  }

  return ret;
}

TableStruct *Statement::find_column_table_from_list(
    string column_str, list<TableStruct *> *table_struct_list) {
  unsigned first_dot_position = column_str.find(".");
  unsigned second_dot_position = column_str.find(".", first_dot_position + 1);
  string column;
  string table_alias;
  string schema_name;

  TableStruct *ret = NULL;
  int table_nums = 0;
  if (first_dot_position == (unsigned)string::npos) {
    column = column_str;
    TableStruct *table_struct =
        check_column_belongs_to(column.c_str(), table_struct_list);
    if (!table_struct) {
      string error_message("Could not find columns named: ");
      error_message.append(column);
      throw Error(error_message.c_str());
    }
    table_alias = table_struct->alias;
    schema_name = table_struct->schema_name;
  } else if (second_dot_position == (unsigned)string::npos) {
    schema_name = string(schema);
    table_alias = column_str.substr(0, first_dot_position);
    column = column_str.substr(first_dot_position + 1);
  } else {
    schema_name = column_str.substr(0, first_dot_position);
    table_alias = column_str.substr(
        first_dot_position + 1, second_dot_position - first_dot_position - 1);
    column = column_str.substr(second_dot_position + 1);
  }

  list<TableStruct *>::iterator it = table_struct_list->begin();
  for (; it != table_struct_list->end(); ++it) {
    if (!strcmp((*it)->alias.c_str(), table_alias.c_str()) &&
        !strcmp((*it)->schema_name.c_str(), schema_name.c_str())) {
      ret = *it;
      ++table_nums;
    }
  }
  if (table_nums > 1) {
    string error_message;
    error_message.append("More than one table contains ");
    error_message.append(column_str);
    throw Error(error_message.c_str());
  }
  return ret;
}

void Statement::get_related_condition_full_table(
    Expression *condition, list<TableStruct *> *table_struct_list,
    map<TableStruct *, set<TableStruct *> > *table_map,
    map<string, set<TableStruct *> > *equal_values) {
  if (!condition) return;
  if (condition->type != EXPR_AND) {
    if ((condition->type == EXPR_EQ || condition->type == EXPR_EQ_N ||
         condition->type == EXPR_GE || condition->type == EXPR_GR ||
         condition->type == EXPR_LESSE || condition->type == EXPR_LESS ||
         condition->type == EXPR_NE || condition->type == EXPR_ASSIGN) &&
        ((CompareExpression *)condition)->left->type == EXPR_STR &&
        ((CompareExpression *)condition)->right->type == EXPR_STR) {
      Expression *left = ((CompareExpression *)condition)->left;
      Expression *right = ((CompareExpression *)condition)->right;
      if (is_column(left) && is_column(right)) {
        string left_str, right_str;
        left->to_string(left_str);
        right->to_string(right_str);
        TableStruct *left_ts =
            find_column_table_from_list(left_str, table_struct_list);
        TableStruct *right_ts =
            find_column_table_from_list(right_str, table_struct_list);
        if (left_ts && right_ts) {
          (*table_map)[left_ts].insert(right_ts);
          (*table_map)[right_ts].insert(left_ts);
        }
      }
    } else if (condition->type == EXPR_EQ) {
      Expression *left = ((CompareExpression *)condition)->left;
      Expression *right = ((CompareExpression *)condition)->right;
      string peer_column;
      const char *peer_value = NULL;
      if (is_column(left)) {
        peer_value = expr_is_simple_value(right);
        left->to_string(peer_column);
      } else if (is_column(right)) {
        peer_value = expr_is_simple_value(left);
        right->to_string(peer_column);
      }
      if (peer_value) {
        TableStruct *column_tb =
            find_column_table_from_list(peer_column, table_struct_list);
        if (!column_tb) return;
        (*equal_values)[peer_value].insert(column_tb);
      }
    }
  } else {
    BoolBinaryCaculateExpression *bbc_expr =
        (BoolBinaryCaculateExpression *)condition;
    get_related_condition_full_table(bbc_expr->left, table_struct_list,
                                     table_map, equal_values);
    get_related_condition_full_table(bbc_expr->right, table_struct_list,
                                     table_map, equal_values);
  }
}

TableStruct *Statement::find_table_struct_from_list(
    const char *schema_name, const char *table_name, const char *alias,
    list<TableStruct *> *table_struct_list) {
  list<TableStruct *>::iterator it = table_struct_list->begin();
  for (; it != table_struct_list->end(); ++it) {
    if (!strcasecmp(schema_name, (*it)->schema_name.c_str()) &&
        !strcasecmp(table_name, (*it)->table_name.c_str()) &&
        !strcasecmp(alias, (*it)->alias.c_str()))
      return *it;
  }
  return NULL;
}

bool Statement::check_tables_has_related_condition(
    TableStruct *table, list<TableStruct *> *merge_list,
    map<TableStruct *, set<TableStruct *> > *table_map) {
  if (!table_map->count(table)) return false;
  bool has_related_condition = false;
  list<TableStruct *>::iterator it = merge_list->begin();
  for (; it != merge_list->end(); ++it) {
    if ((*table_map)[table].count(*it)) {
      has_related_condition = true;
      break;
    }
  }
  return has_related_condition;
}

void Statement::handle_related_condition_full_table(
    record_scan *rs, list<TableStruct *> *table_struct_list,
    map<TableStruct *, set<TableStruct *> > *table_map) {
  map<string, set<TableStruct *> > equal_values;
  set<TableStruct *> tmp_set;
  equal_values["dbscale_equal_columns"] = tmp_set;

  Expression *condition = rs->condition;
  if (!condition) return;
  get_related_condition_full_table(condition, table_struct_list, table_map,
                                   &equal_values);

  table_link *tmp_tb = rs->first_table;
  while (tmp_tb) {
    if (tmp_tb->join->cur_rec_scan == rs) {
      join_node *join_tables = tmp_tb->join->upper;
      if (join_tables && join_tables->left->type == JOIN_NODE_SINGLE &&
          join_tables->right->type == JOIN_NODE_SINGLE &&
          join_tables->condition &&
          join_tables->condition->type == JOIN_COND_USING &&
          !join_tables->upper) {
        join_node *left_tb = join_tables->left;
        join_node *right_tb = join_tables->right;

        const char *schema_name =
            left_tb->schema_name ? left_tb->schema_name : schema;
        const char *table_name = left_tb->table_name;
        const char *alias = left_tb->alias ? left_tb->alias : table_name;
        TableStruct *left_ts = find_table_struct_from_list(
            schema_name, table_name, alias, table_struct_list);

        schema_name = right_tb->schema_name ? right_tb->schema_name : schema;
        table_name = right_tb->table_name;
        alias = right_tb->alias ? right_tb->alias : table_name;
        TableStruct *right_ts = find_table_struct_from_list(
            schema_name, table_name, alias, table_struct_list);

        if (left_ts && right_ts) {
          (*table_map)[left_ts].insert(right_ts);
          (*table_map)[right_ts].insert(left_ts);
        }
      } else if (join_tables && join_tables->left->type == JOIN_NODE_SINGLE &&
                 join_tables->right->type == JOIN_NODE_SINGLE &&
                 join_tables->condition &&
                 join_tables->condition->type == JOIN_COND_ON) {
        get_related_condition_full_table(join_tables->condition->expr_condition,
                                         table_struct_list, table_map,
                                         &equal_values);
      }
    }
    tmp_tb = tmp_tb->next;
  }

  set<TableStruct *>::iterator it_tb;
  set<TableStruct *>::iterator it_tb1;
  map<string, set<TableStruct *> >::iterator it_eq = equal_values.begin();
  for (; it_eq != equal_values.end(); ++it_eq) {
    set<TableStruct *> tb_set = it_eq->second;
    for (it_tb = tb_set.begin(); it_tb != tb_set.end(); ++it_tb) {
      for (it_tb1 = tb_set.begin(); it_tb1 != tb_set.end(); ++it_tb1) {
        (*table_map)[*it_tb].insert(*it_tb1);
      }
    }
  }

  // if table t1 has relation to t2, and t2 has relation to t3, we should add
  // relation t1 & t3.
  map<TableStruct *, set<TableStruct *> >::iterator it_map = table_map->begin();
  for (; it_map != table_map->end(); ++it_map) {
    set<TableStruct *> ts = it_map->second;
    set<TableStruct *>::iterator it_set = ts.begin();
    for (; it_set != ts.end(); ++it_set) {
      if (table_map->count(*it_set)) {
        set<TableStruct *> ts1 = (*table_map)[*it_set];
        set<TableStruct *>::iterator it_set1 = ts1.begin();
        for (; it_set1 != ts1.end(); ++it_set1) {
          if (!(*table_map)[it_map->first].count(*it_set1))
            (*table_map)[it_map->first].insert(*it_set1);
        }
      }
    }
  }
}

void Statement::handle_par_key_equality_full_table(record_scan *root) {
  list<table_link *> &par_tb_list = record_scan_all_par_tables_map[root];
  list<table_link *>::iterator it_tb = par_tb_list.begin();
  table_link *par_tb_tmp = NULL;
  for (; it_tb != par_tb_list.end(); ++it_tb) {
    par_tb_tmp = *it_tb;
    const char *schema_name_tmp =
        par_tb_tmp->join->schema_name ? par_tb_tmp->join->schema_name : schema;

    const char *table_name_tmp = par_tb_tmp->join->table_name;
    const char *table_alias_tmp = par_tb_tmp->join->alias;

    DataSpace *par_ds_tmp = record_scan_all_table_spaces[par_tb_tmp];
    if (!par_ds_tmp->is_partitioned()) {
      continue;
    }

    vector<const char *> *key_names =
        ((PartitionedTable *)par_ds_tmp)->get_key_names();
    vector<const char *>::iterator it_key = key_names->begin();
    for (; it_key != key_names->end(); ++it_key) {
      vector<string> added_equality_vector;
      join_node *join_tables = par_tb_tmp->join->upper;
      if (join_tables && join_tables->left->type == JOIN_NODE_SINGLE &&
          join_tables->right->type == JOIN_NODE_SINGLE &&
          join_tables->condition &&
          join_tables->condition->type == JOIN_COND_USING &&
          !join_tables->upper) {
        join_node *opponent_table = NULL;
        if (join_tables->right == par_tb_tmp->join) {
          opponent_table = join_tables->left;
        } else {
          opponent_table = join_tables->right;
        }

        const char *opponent_schema =
            opponent_table->schema_name ? opponent_table->schema_name : schema;
        string using_added_equality;
        using_added_equality.append(opponent_schema);
        using_added_equality.append(".");
        using_added_equality.append(opponent_table->table_name);
        using_added_equality.append(".");
        name_item *column_list = join_tables->condition->column_list;
        name_item *tmp = column_list;
        if (tmp) {
          do {
            if (!strcasecmp(*it_key, tmp->name)) {
              string column_name(using_added_equality);
              column_name.append(tmp->name);
              added_equality_vector.push_back(column_name);
            }
            tmp = tmp->next;
          } while (tmp != column_list);
        }
      }
      fullfil_par_key_equality_full_table(root, schema_name_tmp, table_name_tmp,
                                          table_alias_tmp, *it_key,
                                          &added_equality_vector);
    }
  }
}

/*Try to merge all partition tables inside one record_scan.
 *
 * Return: the final merged dataspace or NULL if merge fail.*/
DataSpace *Statement::merge_par_tables_one_record_scan(record_scan *root) {
  list<table_link *> &par_tb_list = record_scan_all_par_tables_map[root];
  list<table_link *>::iterator it_tb = par_tb_list.begin();

  table_link *par_tb = NULL;
  table_link *par_tb_tmp = NULL;
  table_link *first_par_tb = NULL;
  const char *schema_name_tmp = NULL;
  const char *table_name_tmp = NULL;
  const char *table_alias_tmp = NULL;
  DataSpace *par_ds_tmp = NULL, *par_ds = NULL;

  bool skip_this_par_table = false;
  bool first_is_dup = false;

  for (; it_tb != par_tb_list.end(); ++it_tb) {
    par_tb_tmp = *it_tb;
    skip_this_par_table = false;
    schema_name_tmp =
        par_tb_tmp->join->schema_name ? par_tb_tmp->join->schema_name : schema;

    table_name_tmp = par_tb_tmp->join->table_name;
    table_alias_tmp = par_tb_tmp->join->alias;

    par_ds_tmp = record_scan_all_table_spaces[par_tb_tmp];

    if (!par_tb) {
      par_tb = par_tb_tmp;
      par_ds = par_ds_tmp;
    } else {
      if (((Table *)par_ds)->is_duplicated()) {
        if (is_duplicated_table_merge(par_ds_tmp, par_ds))
          skip_this_par_table = true;
        par_ds = par_ds_tmp;
      } else if (((Table *)par_ds_tmp)->is_duplicated()) {
        if (is_duplicated_table_merge(par_ds, par_ds_tmp))
          skip_this_par_table = true;
      } else {
        if (check_two_partition_table_can_merge(
                par_tb_tmp->join->cur_rec_scan, par_tb_tmp,
                (PartitionedTable *)par_ds, (PartitionedTable *)par_ds_tmp)) {
          skip_this_par_table = true;
        } else {
          LOG_DEBUG(
              "The partition table dataspace [%s] is conflicting with"
              " partition table dataspace [%s] for sql [%s].\n",
              par_ds->get_name(), par_ds_tmp->get_name(), sql);
          return NULL;
        }
      }
    }
    if (!record_scan_par_key_equality.count(root) &&
        !((Table *)par_ds_tmp)->is_duplicated()) {
      vector<const char *> *key_names =
          ((PartitionedTable *)par_ds_tmp)->get_key_names();
      vector<const char *>::iterator it_key = key_names->begin();
      for (; it_key != key_names->end(); ++it_key) {
        vector<string> added_equality_vector;
        join_node *join_tables = par_tb_tmp->join->upper;
        if (join_tables && join_tables->left->type == JOIN_NODE_SINGLE &&
            join_tables->right->type == JOIN_NODE_SINGLE &&
            join_tables->condition &&
            join_tables->condition->type == JOIN_COND_USING &&
            !join_tables->upper) {
          join_node *opponent_table = NULL;
          if (join_tables->right == par_tb_tmp->join) {
            opponent_table = join_tables->left;
          } else {
            opponent_table = join_tables->right;
          }

          const char *opponent_schema = opponent_table->schema_name
                                            ? opponent_table->schema_name
                                            : schema;
          string using_added_equality;
          using_added_equality.append(opponent_schema);
          using_added_equality.append(".");
          using_added_equality.append(opponent_table->table_name);
          using_added_equality.append(".");
          name_item *column_list = join_tables->condition->column_list;
          name_item *tmp = column_list;
          if (tmp) {
            do {
              if (!strcasecmp(*it_key, tmp->name)) {
                string column_name(using_added_equality);
                column_name.append(tmp->name);
                added_equality_vector.push_back(column_name);
              }
              tmp = tmp->next;
            } while (tmp != column_list);
          }
        }
        fullfil_par_key_equality(root, schema_name_tmp, table_name_tmp,
                                 table_alias_tmp, *it_key,
                                 &added_equality_vector);
      }
    }
    if (!skip_this_par_table) {
      if (!first_par_tb) {
        first_par_tb = par_tb_tmp;
        if (((Table *)par_ds_tmp)->is_duplicated()) {
          first_is_dup = true;
        }
      }
    }
  }
  /*Only add to vector par_tables if merge success.*/
  if (first_par_tb && !record_scan_need_table.count(root)) {
    if (first_is_dup) {
      /*If duplicated table merged with other partition table, we will always
       * ignore the duplicated table as par_tables.*/
      if (par_tb_list.size() >= 2) {
        it_tb = par_tb_list.begin();
        ++it_tb;
        first_par_tb = *it_tb;
      }
    }
    ++par_table_num;
    par_tables.push_back(first_par_tb);
    need_clean_par_tables = true;
    record_scan_par_table_map[root] = first_par_tb;
    need_clean_record_scan_par_table_map = true;
  }

  return par_ds;
}

/*Try to merge all dataspaces inside one record scan. The merge start from the
 * only one partition dataspace (param: par_ds).
 *
 * Return: the final merged data space or NULL if merge fail.*/
DataSpace *Statement::merge_dataspace_one_record_scan(record_scan *root,
                                                      DataSpace *par_ds) {
  list<DataSpace *> &ds_list = record_scan_all_spaces_map[root];

  list<DataSpace *>::iterator it = ds_list.begin();
  DataSpace *ds = par_ds, *ds_tmp = NULL;
  for (; it != ds_list.end(); ++it) {
    ds_tmp = *it;
    if (!ds) {
      ds = ds_tmp;
    } else {
      if (stmt_session->is_dataspace_cover_session_level(ds, ds_tmp)) {
        if (enable_global_plan) {
          // this means there is global table and is merged.
          if (ds_tmp->get_data_source() &&
              ds_tmp->get_data_source()->get_data_source_type() ==
                  DATASOURCE_TYPE_REPLICATION)
            global_table_merged.insert(ds_tmp);
        }
      } else if (stmt_session->is_dataspace_cover_session_level(ds_tmp, ds)) {
        ds = ds_tmp;
      } else {
        LOG_DEBUG(
            "The dataspace [%s] is conflicting with dataspace [%s]"
            " for sql [%s].\n",
            ds->get_name(), ds_tmp->get_name(), sql);
        return NULL;
      }
    }
  }
  return ds;
}

/* Check spj situation and check the can merge situation, if can merge, we
 * should not generate SeperatedNode for this subquery. */
bool Statement::check_spj_subquery_can_merge(record_scan *parent_rs,
                                             record_scan *child_rs) {
  if (child_rs->join_belong && !child_rs->limit && child_rs->join_tables &&
      child_rs->join_tables->type == JOIN_NODE_SINGLE) {
    ;
  } else {
    return false;
  }

  if (child_rs->having) {
    if (child_rs->having->check_contains_function()) return false;
  }

  if (!record_scan_one_space_tmp_map.count(parent_rs) ||
      !record_scan_one_space_tmp_map.count(child_rs))
    return false;
  DataSpace *child_ds = record_scan_one_space_tmp_map[child_rs];
  DataSpace *parent_ds = record_scan_one_space_tmp_map[parent_rs];

  if (!child_ds || !parent_ds) return false;

  if (child_ds->get_dataspace_type() == TABLE_TYPE &&
      ((Table *)child_ds)->is_partitioned() &&
      parent_ds->get_dataspace_type() == TABLE_TYPE &&
      ((Table *)parent_ds)->is_partitioned()) {
    vector<const char *> *key_names =
        ((PartitionedTable *)child_ds)->get_key_names();
    if (child_rs->group_by_list) {
      join_node *join = child_rs->first_table->join;
      string child_table_name = join->alias ? join->alias : join->table_name;

      string child_schema_name = join->schema_name ? join->schema_name : schema;
      bool find_key = false;
      order_item *item = child_rs->group_by_list;
      do {
        if (groupby_on_par_key(item, child_schema_name.c_str(),
                               child_table_name.c_str(),
                               child_table_name.c_str(), key_names->at(0))) {
          find_key = true;
          break;
        }
        item = item->next;
      } while (item != child_rs->group_by_list);
      if (!find_key) return false;
    }
    if (check_rs_contains_partition_key(child_rs, child_ds) == -1) return false;

    /* use a fake table link to fullfil the can merge method, the can merge
     * method  will only use the table name, schema name, alias in table link.
     * */
    table_link *parent_table_link = find_partition_table_table_link(parent_rs);
    if (parent_table_link) {
      join_node tmp_node = {JOIN_NODE_SINGLE,
                            1,
                            schema,
                            child_rs->join_belong->alias,
                            child_rs->join_belong->alias,
                            NULL,
                            NULL,
                            NULL,
                            NULL,
                            NULL,
                            NULL,
                            NULL,
                            NULL,
                            0,
                            0};
      table_link fake_child_table_link = {&tmp_node, NULL, NULL, 0, 0, 0, 0};

      if (!record_scan_need_cross_node_join.count(parent_rs) ||
          !record_scan_need_cross_node_join[parent_rs])
        handle_par_key_equality_full_table(parent_rs);
      bool can_merge = check_CNJ_two_partition_table_can_merge(
          parent_rs, parent_table_link, &fake_child_table_link,
          (PartitionedTable *)parent_ds, (PartitionedTable *)child_ds);
      return can_merge;
    }
  }
  return false;
}

/*
 * Checking the target record scan, normally subquery, whether it contains the
 * partition key of the target partition table dataspace. Normally we check the
 * select field list of the target record scan, and if found, we return the pos
 * of the filed in the select list, otherwise we return -1, which means there is
 * no key in the target record scan.
 */
int Statement::check_rs_contains_partition_key(record_scan *rs, DataSpace *ds) {
  int position = 0;
  if (!rs->first_table) return -1;
  join_node *join = rs->first_table->join;
  string child_table_name = join->alias ? join->alias : join->table_name;
  string table_with_star(child_table_name);
  table_with_star.append(".*");

  string child_schema_name = join->schema_name ? join->schema_name : schema;
  string schema_table_star(child_schema_name);
  schema_table_star.append(".");
  schema_table_star.append(table_with_star);

  vector<const char *> *key_names = ((PartitionedTable *)ds)->get_key_names();
  bool contains_partition_key = false;
  field_item *fi = rs->field_list_head;
  do {
    string column_name;
    if (fi->field_expr) {
      if (fi->field_expr->type == EXPR_STR) {
        fi->field_expr->to_string(column_name);
        if (!strcasecmp(column_name.c_str(), key_names->at(0)) && !fi->alias) {
          contains_partition_key = true;
          break;
        } else if (!strcmp(column_name.c_str(), "*")) {
          contains_partition_key = true;
          break;
        } else if (!strcmp(column_name.c_str(), table_with_star.c_str())) {
          contains_partition_key = true;
          break;
        } else if (!strcmp(column_name.c_str(), schema_table_star.c_str())) {
          contains_partition_key = true;
          break;
        }
      }
    }
    ++position;
    fi = fi->next;
  } while (fi && fi != rs->field_list_tail);

  if (!contains_partition_key) return -1;
  return position;
}

table_link *Statement::find_partition_table_table_link(record_scan *rs) {
  Backend *backend = Backend::instance();
  TableStruct *table_struct = NULL;
  table_link *ret = NULL;
  if (record_scan_join_tables.count(rs) && record_scan_join_tables[rs]) {
    vector<vector<TableStruct *> *> *join_table_sequence =
        record_scan_join_tables[rs];
    vector<TableStruct *> *last_table_vector =
        join_table_sequence->at(join_table_sequence->size() - 1);
    vector<TableStruct *>::iterator it = last_table_vector->begin();
    for (; it != last_table_vector->end(); ++it) {
      DataSpace *ds = backend->get_data_space_for_table(
          (*it)->schema_name.c_str(), (*it)->table_name.c_str());
      if (ds->get_dataspace_type() == TABLE_TYPE &&
          ((Table *)ds)->is_partitioned()) {
        table_struct = *it;
        break;
      }
    }
  } else {
    table_link *table_l = rs->first_table;
    while (table_l) {
      if (table_l->join->cur_rec_scan == rs) {
        string table_name = table_l->join->table_name;
        string schema_name =
            table_l->join->schema_name ? table_l->join->schema_name : schema;
        DataSpace *ds = backend->get_data_space_for_table(schema_name.c_str(),
                                                          table_name.c_str());
        if (ds->get_dataspace_type() == TABLE_TYPE &&
            ((Table *)ds)->is_partitioned()) {
          return table_l;
        }
      }
      table_l = table_l->next;
    }
  }
  if (table_struct) {
    table_link *tmp_tb = rs->first_table;
    while (tmp_tb) {
      if (tmp_tb->join->cur_rec_scan == rs) {
        string table_name = tmp_tb->join->table_name;
        string schema_name =
            tmp_tb->join->schema_name ? tmp_tb->join->schema_name : schema;
        string alias = tmp_tb->join->alias ? tmp_tb->join->alias : table_name;
        if (!strcmp(table_struct->schema_name.c_str(), schema_name.c_str()) &&
            !strcmp(table_struct->alias.c_str(), alias.c_str())) {
          ret = tmp_tb;
          break;
        }
      }
      tmp_tb = tmp_tb->next;
    }
  }
  return ret;
}

/* For subquery in IN EXPRESSION, we can check the merge of the child
 * record_scan with the parent table. if the child is norm_table, we check the
 * cover of them. If the child is partition table and the parent is parttion
 * table, check two partition table can merge using partition key. */
bool Statement::check_column_record_scan_can_merge(record_scan *parent_rs,
                                                   record_scan *child_rs,
                                                   int record_scan_position,
                                                   ExecutePlan *plan) {
  /*

   can merge In  can merge 
  

  t1.c1 In (select t2.c2 from t2)

  c1  t1  c2  t2 t1  t2 cover

   merge t1 merge parent  final dataspace
   child mergecan_merge_record_scan_position t1
  

  */
  ACE_UNUSED_ARG(plan);
  string peer_string;              // parent string
  TableStruct table_parent;        // parent table struct
  DataSpace *space_parent = NULL;  // parent space
  string subquery_string;          // child string
  TableStruct table_child;         // child table struct
  DataSpace *space_child = NULL;   // child space
  DataSpace *child_ds = NULL;      // temporary space
  Backend *backend = Backend::instance();
  bool ret = false;
  /* We need to find out the expressions in "IN", and we should also check the
   * dataspaces can_merge using two record_scan. */
  if (subquery_peer.count(child_rs)) {
    // Find one possible key of parent dataspace
    peer_string = subquery_peer[child_rs];
    try {
      table_parent = get_table_from_record_scan(parent_rs, peer_string);
    } catch (Error &e) {
      LOG_ERROR(
          "Fail to get_table_from_record_scan in "
          "check_column_record_scan_can_merge due to %s.\n",
          e.what());
      return false;
    }
    space_parent = backend->get_data_space_for_table(
        table_parent.schema_name.c_str(), table_parent.table_name.c_str());
    Expression *child_expr = child_rs->field_list_head->field_expr;
    if (!space_parent->get_data_source()) {
      if (child_expr->type != EXPR_STR) {
        LOG_DEBUG(
            "The type of the child expression is not EXPR_STR, so can not "
            "merge.\n");
        return false;
      }
      child_expr->to_string(subquery_string);
      /* We get the exactly space of the child, we should use this space to
       * compare the key with subquery_string */
      try {
        table_child = get_table_from_record_scan(child_rs, subquery_string);
      } catch (Error &e) {
        LOG_ERROR(
            "Fail to get_table_from_record_scan in "
            "check_column_record_scan_can_merge due to %s.\n",
            e.what());
        return false;
      }

      space_child = backend->get_data_space_for_table(
          table_child.schema_name.c_str(), table_child.table_name.c_str());
    }
    child_ds = record_scan_one_space_tmp_map[child_rs];
    // For child is norm_table, If they cover, they can merge
    if (child_ds->get_data_source()) {
      if (stmt_session->is_dataspace_cover_session_level(space_parent,
                                                         child_ds))
        ret = true;
    } else if (!child_ds->get_data_source() &&
               !space_parent->get_data_source()) {
      //  subquery_string  table_child  key
      //  peer_string  table_parent  key
      //  keydataspacecovercan merge

      PartitionedTable *child_table = (PartitionedTable *)child_ds;
      PartitionedTable *parent_table = (PartitionedTable *)space_parent;
      // Find another possible key of child dataspace
      bool child_is_key = false;
      /* Findout whether the child select list is key of the child dataspace
       * */
      vector<const char *> *child_key_names =
          ((PartitionedTable *)space_child)->get_key_names();
      child_is_key = column_name_equal_key_alias_without_len(
          subquery_string.c_str(), table_child.schema_name.c_str(),
          table_child.table_name.c_str(), child_key_names->at(0),
          table_child.alias.c_str());
      bool parent_is_key = false;
      vector<const char *> *parent_key_names = parent_table->get_key_names();
      parent_is_key = column_name_equal_key_alias_without_len(
          peer_string.c_str(), table_parent.schema_name.c_str(),
          table_parent.table_name.c_str(), parent_key_names->at(0),
          table_parent.alias.c_str());

      if (parent_is_key && child_is_key &&
          is_partition_table_cover(child_table, parent_table)) {
        ret = true;
      }
    }
    if (!(child_rs->subquerytype == SUB_SELECT_ONE_COLUMN && stmt_session &&
          !stmt_session->get_session_option("use_table_for_one_column_subquery")
               .int_val)) {
      if (!ret && stmt_session &&
          (MethodType)stmt_session->get_session_option("cross_node_join_method")
                  .int_val == DATA_MOVE_WRITE &&
          !union_all_sub) {  // union and federated will be supported in issue
                             // #3365
        /*currently only support column table subquery for data move write*/
        child_rs->subquerytype = SUB_SELECT_COLUMN;
        /*mergedbscaleSUB_SELECT_COLUMNspace_parentselect
         * column from ''
         * subquery.ccSeparatedExecNode::execute_column_table_subquery*/
        DataSpace *column_table_space = space_parent;
        if (gtable_can_merge_dataspace.count(parent_rs) &&
            gtable_can_merge_dataspace[parent_rs].count(space_parent)) {
          column_table_space =
              gtable_can_merge_dataspace[parent_rs][space_parent];
        }
        column_output_dataspace[child_rs] = column_table_space;
        init_column_table_subquery_tmp_table_name(child_rs, column_table_space,
                                                  plan);
      }
    }

    // mergemerge
    // 
    // t1, t2, t3 where t2.c1 in (select c2 from)
    //  t1t2 where t2.c1 in (select c2 from ), t3
    // dataspace
    can_merge_record_scan_position[parent_rs][record_scan_position] =
        table_parent;
    // can_merge_record_scan_position
    // helper->table_position_map
  }
#ifdef DEBUG
  LOG_DEBUG("Statement::check_column_record_scan_can_merge can_merge = [%d]\n",
            ret);
#endif
  return ret;
}

void Statement::adjust_cross_node_join_rs(record_scan *rs,
                                          TableStruct table_parent) {
  TableStruct *table_struct;
  vector<TableStruct *> *table_struct_vector;
  if (record_scan_need_cross_node_join.count(rs)) {
    table_struct_vector = new vector<TableStruct *>();
    table_struct = new TableStruct();
    *table_struct = table_parent;
    table_struct_vector->push_back(table_struct);
    record_scan_join_tables[rs]->push_back(table_struct_vector);
    set_join_final_table(rs);
  }
}

TableStruct Statement::get_table_from_record_scan(record_scan *parent_rs,
                                                  string peer_string) {
  unsigned first_dot_position = peer_string.find(".");
  unsigned second_dot_position = peer_string.find(".", first_dot_position + 1);
  string column;
  string table_alias;
  string schema_name;
  string table_name;

  TableStruct ret;
  bool ret_has_value = false;
  vector<vector<TableStruct> > table_vector_vector;
  vector<TableStruct> table_vector;
  table_link *tmp_tb = parent_rs->first_table;
  while (tmp_tb) {
    if (tmp_tb->join->cur_rec_scan == parent_rs) {
      TableStruct table_struct;
      table_struct.table_name = tmp_tb->join->table_name;
      table_struct.schema_name =
          tmp_tb->join->schema_name ? tmp_tb->join->schema_name : schema;
      table_struct.alias =
          tmp_tb->join->alias ? tmp_tb->join->alias : table_struct.table_name;
      table_vector.push_back(table_struct);
    }
    tmp_tb = tmp_tb->next;
  }

  if (first_dot_position == (unsigned)string::npos) {
    column = peer_string;
    if (!strcmp(column.c_str(), "*")) {
      /*If only has one table, and the column is *, we can dirtectly return
       * the table.*/
      if (table_vector.size() == 1)
        return table_vector[0];
      else {
        string error_message(
            "Could not find table with column * with no table or more than one "
            "table in record_scan");
        throw Error(error_message.c_str());
      }
    }
    table_vector_vector.push_back(table_vector);
    TableStruct *table_struct =
        check_column_belongs_to(column.c_str(), &table_vector_vector);
    if (!table_struct) {
      string error_message("Could not find columns named: ");
      error_message.append(column);
      throw Error(error_message.c_str());
    }
    ret = *table_struct;
    ret_has_value = true;
    table_alias = table_struct->alias;
    schema_name = table_struct->schema_name;
    table_name = table_struct->table_name;
  } else if (second_dot_position == (unsigned)string::npos) {
    /* the expression is like 't1.c1' */
    string tmp_table_alias = peer_string.substr(0, first_dot_position);
    if (first_dot_position != (unsigned)string::npos) {
      for (unsigned int i = 0; i < table_vector.size(); i++) {
        TableStruct &tmp_struct = table_vector[i];
        if (tmp_struct.alias == tmp_table_alias) {
          schema_name = tmp_struct.schema_name;
          table_alias = tmp_struct.alias;
          table_name = tmp_struct.table_name;
          ret = tmp_struct;
          ret_has_value = true;
          break;
        }
      }
      if (ret_has_value == false) {
        schema_name = string(schema);
        table_alias = peer_string.substr(0, first_dot_position);
        column = peer_string.substr(first_dot_position + 1);
      }
    }
  } else {
    /* the expression is like 'test.t1.c1' */
    schema_name = peer_string.substr(0, first_dot_position);
    table_alias = peer_string.substr(
        first_dot_position + 1, second_dot_position - first_dot_position - 1);
    column = peer_string.substr(second_dot_position + 1);
  }

  if (table_name.empty()) {
    vector<TableStruct>::iterator it = table_vector.begin();
    for (; it != table_vector.end(); ++it) {
      if (!strcmp(it->alias.c_str(), table_alias.c_str()) &&
          !strcmp(it->schema_name.c_str(), schema_name.c_str())) {
        ret = *it;
        return ret;
      }
    }
  }
  if (ret_has_value) return ret;
  string error_message("Could not find table: ");
  error_message.append(schema_name);
  error_message.append(" with alias: ");
  error_message.append(table_alias);

  throw Error(error_message.c_str());
  return ret;
}

/*Try to merge the dataspace of parent record_scan and child record_scan.
 *
 * Return: The merged dataspace or NULL if fail.*/
DataSpace *Statement::merge_parent_child_record_scan(record_scan *parent_rs,
                                                     record_scan *child_rs,
                                                     int record_scan_position,
                                                     ExecutePlan *plan) {
#ifdef DEBUG
  ACE_ASSERT(child_rs);
  ACE_ASSERT(parent_rs);
  if (child_rs->upper != parent_rs) {
    LOG_ERROR(
        "Wrong params for merge_parent_child_record_scan,"
        " the child_rs should be the child of parent_rs.\n");
    ACE_ASSERT(0);
  }
#endif

  bool child_is_par_table = false;
  bool parent_is_par_table = false;

  DataSpace *parent_ds = record_scan_one_space_tmp_map.count(parent_rs)
                             ? record_scan_one_space_tmp_map[parent_rs]
                             : NULL;
  DataSpace *child_ds = record_scan_one_space_tmp_map.count(child_rs)
                            ? record_scan_one_space_tmp_map[child_rs]
                            : NULL;
  if (st.type == STMT_CREATE_SELECT) {
    if (parent_ds && child_ds &&
        stmt_session->is_dataspace_cover_session_level(parent_ds, child_ds)) {
      return parent_ds;
    }
    return NULL;
  }
  if (left_join_subquery_rs.count(child_rs)) {
    LOG_DEBUG(
        "Statement::merge_parent_child_record_scan return NULL due to "
        "left_join_subquery_rs.count.\n");
    return NULL;
  }

  if (record_scan_dependent_map.count(child_rs) &&
      !record_scan_dependent_map[child_rs].empty()) {
    // If has dependent child record scan, it need to be executed separared.
    LOG_DEBUG(
        "Statement::merge_parent_child_record_scan return NULL due to "
        "record_scan_dependent_map.count.\n");
    return NULL;
  }

  /*if the child is a cross node join, return NULL.*/
  if (record_scan_need_cross_node_join.count(child_rs)) {
    LOG_DEBUG(
        "Statement::merge_parent_child_record_scan return NULL due to "
        "record_scan_need_cross_node_join.count.\n");
    return NULL;
  }

  if (!child_ds) {
    if (child_rs->is_select_union &&
        !record_scan_has_no_table.count(child_rs)) {
      LOG_DEBUG(
          "Statement::merge_parent_child_record_scan return NULL due to "
          "child_rs->is_select_union.\n");
      return NULL;
    }
    /*The child record scan is a simple select, such as "select 1", so just
     * merged it with parent.*/
    LOG_DEBUG(
        "Statement::merge_parent_child_record_scan return parent_ds due to "
        "child_ds is NULL.\n");
    return parent_ds;
  }

  if (child_rs->first_table && child_rs->first_table->join) {
    string table_name = child_rs->first_table->join->table_name;
    if (!strcmp(table_name.c_str(), "dual")) {
      LOG_DEBUG(
          "Statement::merge_parent_child_record_scan return parent_ds due to "
          "child_rs table is dual.\n");
      return parent_ds;
    }
  }

  /*Duplicated table can not be merged cross record scan.*/
  if (parent_ds && parent_ds->get_dataspace_type() == TABLE_TYPE &&
      ((Table *)parent_ds)->is_duplicated()) {
    LOG_DEBUG(
        "Statement::merge_parent_child_record_scan return parent_ds due to "
        "parent_ds is_duplicated.\n");
    return NULL;
  }
  if (child_ds && child_ds->get_dataspace_type() == TABLE_TYPE &&
      ((Table *)child_ds)->is_duplicated()) {
    LOG_DEBUG(
        "Statement::merge_parent_child_record_scan return parent_ds due to "
        "child_ds is_duplicated.\n");
    return parent_ds;
  }

  if (child_rs->condition_belong && record_scan_need_table.count(parent_rs)) {
    LOG_DEBUG(
        "Statement::merge_parent_child_record_scan return NULL due to "
        "record_scan_need_table.count.\n");
    return NULL;
  }

  if (subquery_missing_table.count(parent_rs)) {
    /* For dependent situation: 1. if norm_table, check cover.  2. if
     * part_table, using its child record scan to check can merge.
     */
    TableStruct table = subquery_missing_table[parent_rs];
    Backend *backend = Backend::instance();
    DataSpace *ret_ds = backend->get_data_space_for_table(
        table.schema_name.c_str(), table.table_name.c_str());
    if (parent_ds != ret_ds &&
        !record_scan_need_cross_node_join.count(parent_rs)) {
      LOG_ERROR(
          "The sql [%s] is not supported, parent final table is not child "
          "missing table.\n",
          sql);
      throw Error(
          "The sql is not supported, parent final table is not child missing "
          "table.");
    }
    if (child_ds->get_data_source() && ret_ds->get_data_source()) {
      if (stmt_session->is_dataspace_cover_session_level(ret_ds, child_ds))
        return ret_ds;
    } else if (!child_ds->get_data_source() && !ret_ds->get_data_source()) {
      if (check_two_partition_table_can_merge(
              child_rs, missing_table_map[table], (PartitionedTable *)child_ds,
              (PartitionedTable *)ret_ds)) {
        return ret_ds;
      } else {
        return NULL;
      }
    } else {
      return NULL;
    }
  } else if (check_column_record_scan_can_merge(parent_rs, child_rs,
                                                record_scan_position, plan)) {
    return parent_ds;
  }

  if (check_spj_subquery_can_merge(parent_rs, child_rs)) {
    if (record_scan_par_table_map.count(child_rs)) {
      record_scan_par_table_map[parent_rs] =
          record_scan_par_table_map[child_rs];
    }
    return child_ds;
  }

  if (child_ds->get_data_source() == NULL) child_is_par_table = true;
  if (parent_ds && parent_ds->get_data_source() == NULL)
    parent_is_par_table = true;

  if (child_is_par_table && parent_is_par_table) {
    /*two partition table inside different sub-querys can not be executed
      together currently.*/
    return NULL;
  }

  if (!parent_rs->upper &&
      (st.type == STMT_INSERT_SELECT || st.type == STMT_REPLACE_SELECT ||
       st.type == STMT_SET)) {
    // Only do merge for insert...select and set stmt with normal table
    if (child_is_par_table || parent_is_par_table) return NULL;
  }

  if (child_is_par_table) {
    if (is_subquery_in_select_list(child_rs))
      /*If partition table subquery is in the select list, it can not be
       * merged.*/
      return NULL;
    table_link *par_table = record_scan_par_table_map[child_rs];
    if ((rs_contains_aggr_group_limit(child_rs, par_table) ||
         (child_rs->options & SQL_OPT_DISTINCT ||
          child_rs->options & SQL_OPT_DISTINCTROW)) &&
        (!check_rs_contain_partkey_equal_value(child_rs))) {
      /*one or merged record_scan should be executed separared if it contains
       * order_by|limit|group_by|distinct, as well as it has upper record_scan.
       */
      if (is_executed_as_table(child_rs) && !parent_ds &&
          parent_rs->children_begin == child_rs &&
          parent_rs->children_end == child_rs) {
        if (check_rs_contains_partition_key(child_rs, child_ds) != -1) {
          dataspace_partition_key[child_rs] =
              ((PartitionedTable *)child_ds)->get_key_names()->at(0);
          table_subquery_final_dataspace[child_rs] = child_ds;
        } else {
          string column_name;
          field_item *fi = child_rs->field_list_head;
          if (fi->alias) {
            dataspace_partition_key[child_rs] = fi->alias;
            table_subquery_final_dataspace[child_rs] = child_ds;
          } else if (fi->field_expr && fi->field_expr->type == EXPR_STR) {
            fi->field_expr->to_string(column_name);
            if (strcmp(column_name.c_str(), "*")) {
              string local_schema;
              string local_table;
              string local_column;
              split_value_into_schema_table_column(column_name, local_schema,
                                                   local_table, local_column);
              dataspace_partition_key[child_rs] = local_column;
              table_subquery_final_dataspace[child_rs] = child_ds;
            } else {
              Backend::instance()->record_latest_important_dbscale_warning(
                  "The first column fail to be partition key of tmp table, "
                  "this table subquery use normal table directly.\n");
            }
          } else {
            Backend::instance()->record_latest_important_dbscale_warning(
                "The first column fail to be partition key of tmp table, this "
                "table subquery use normal table directly.\n");
          }
        }

        if (!parent_rs->upper && table_subquery_final_dataspace.count(child_rs))
          need_reanalysis_space = true;
      }
      return NULL;
    }

    /*When Partition record_scan is a sub_select condition,
      If the condition type is 'exists', it can not execute without divided;
      (may allow in the future)
      If the condition type is 'in'|'=', the select list of
      partition record_scan should contain the partition key.*/
    if (child_rs->condition_belong) {
      Expression *expr = child_rs->condition_belong->parent;
      if (expr == NULL) {
        /*This subquery, with partition table, is in the select list.*/
        /*TODO: we need to separated the subquery in select list and condition
         * list!!!*/
        return NULL;
      }
      switch (expr->type) {
        case EXPR_EQ:
        case EXPR_IN: {
          vector<const char *> *key_names =
              ((PartitionedTable *)child_ds)->get_key_names();
          table_link *par_table = record_scan_par_table_map[child_rs];
          const char *schema_name = par_table->join->schema_name
                                        ? par_table->join->schema_name
                                        : schema;
          const char *table_name = par_table->join->table_name;
          const char *table_alias = par_table->join->alias;

          // select field list should contain all 'par.key'
          bool field_has_all_key = true;
          vector<const char *>::iterator it = key_names->begin();
          for (; it != key_names->end(); ++it) {
            bool field_has_key = false;
            const char *key_name = *it;
            field_item *tmp = child_rs->field_list_head;
            while (tmp) {
              Expression *expr_field = tmp->field_expr;
              if (expr_field && expr_field->type == EXPR_STR) {
                StrExpression *str_field = (StrExpression *)expr_field;
                if (is_column(str_field)) {
                  if (column_name_equal_key_alias(
                          str_field->str_value, schema_name,
                          strlen(schema_name), table_name, strlen(table_name),
                          key_name, table_alias)) {
                    field_has_key = true;
                    break;
                  }
                }
              }
              tmp = tmp->next;
            }
            if (!field_has_key) {
              field_has_all_key = false;
              break;
            }
          }
          if (!field_has_all_key) {
            LOG_DEBUG(
                "Fail to find the partition_key in the field list of"
                " partition record_scan of sql[%s].\n",
                sql);
            return NULL;
          }
          break;
        }
        default:  // NOT_IN, other compare
        {
          return NULL;
        }
      }
    }
  }

  if (!parent_ds) {
    if (child_is_par_table) {
      field_item *field = parent_rs->field_list_head;
      try {
        while (field) {
          if (field->field_expr) {
            if (is_expression_contain_aggr(field->field_expr, parent_rs,
                                           false)) {
              if (parent_rs->upper) return NULL;
            }
          }
          field = field->next;
        }
      } catch (...) {
        return NULL;
      }

      if (!record_scan_par_table_map.count(parent_rs) &&
          record_scan_par_table_map.count(child_rs)) {
        record_scan_par_table_map[parent_rs] =
            record_scan_par_table_map[child_rs];
      }
    }
    return child_ds;
  }

  if (stmt_session->is_dataspace_cover_session_level(parent_ds, child_ds)) {
    /* We defined that two record_scan can not merge. */
    join_node *join_tables = parent_rs->join_tables;
    if (join_tables && join_tables->join_bit & JT_LEFT &&
        child_rs->subquerytype == SUB_SELECT_TABLE) {
      return NULL;
    }
    return parent_ds;
  }

  if (!parent_rs->upper &&
      (st.type == STMT_INSERT_SELECT || st.type == STMT_REPLACE_SELECT)) {
    /*For insert..select stmt, should not use child_rs to merge parent_rs.*/
    return NULL;
  }

  if (stmt_session->is_dataspace_cover_session_level(child_ds, parent_ds)) {
    /* We defined that two record_scan can not merge. */
    join_node *join_tables = parent_rs->join_tables;
    if (join_tables && join_tables->join_bit & JT_LEFT &&
        child_rs->subquerytype == SUB_SELECT_TABLE) {
      return NULL;
    }
    if (record_scan_par_table_map.count(child_rs)) {
      record_scan_par_table_map[parent_rs] =
          record_scan_par_table_map[child_rs];
    }

    return child_ds;
  }

  return NULL;
}

bool Statement::check_two_partition_table_can_merge(record_scan *rs,
                                                    table_link *tb_link,
                                                    PartitionedTable *tb1,
                                                    PartitionedTable *tb2) {
  /*If the two partition table use the same deploy topo(the same
   * partition_scheme and the same partition type), we can just ignore
   * one of them. Cause now dbscale only support local join. In order
   * to ensure the correct of execution, the condition of record_scan
   * should contains the pattern {par1.key = par2.key}*/
  if (is_partition_table_cover(tb1, tb2)) {
    vector<const char *> *key_names = tb2->get_key_names();
    vector<const char *> *kept_keys = tb1->get_key_names();
    unsigned int j = 0;
    for (; j < kept_keys->size(); ++j) {
      const char *kept_key = kept_keys->at(j);
      const char *key_name = key_names->at(j);
      if (!is_all_par_table_equal(rs, tb_link, kept_key, key_name)) {
        break;
      }
    }
    if (j == kept_keys->size()) {
      return true;
    } else {
      return false;
    }
  } else {
    return false;
  }
}

bool Statement::check_CNJ_two_partition_table_can_merge(record_scan *rs,
                                                        table_link *tb_link1,
                                                        table_link *tb_link2,
                                                        PartitionedTable *tb1,
                                                        PartitionedTable *tb2) {
  /*If the two partition table use the same deploy topo(the same
   * partition_scheme and the same partition type), we can just ignore
   * one of them. Cause now dbscale only support local join. In order
   * to ensure the correct of execution, the condition of record_scan
   * should contains the pattern {par1.key = par2.key}*/
  if (is_partition_table_cover(tb1, tb2)) {
    vector<const char *> *key_names = tb2->get_key_names();
    vector<const char *> *kept_keys = tb1->get_key_names();
    unsigned int j = 0;
    for (; j < kept_keys->size(); ++j) {
      const char *kept_key = kept_keys->at(j);
      const char *key_name = key_names->at(j);
      if (!is_CNJ_all_par_table_equal(rs, tb_link1, tb_link2, kept_key,
                                      key_name)) {
        break;
      }
    }
    if (j == kept_keys->size()) {
      return true;
    } else {
      return false;
    }
  } else {
    return false;
  }
}

void Statement::get_all_table_vector(
    record_scan *rs, vector<vector<TableStruct> > *join_table_sequence) {
  join_node *tables = rs->join_tables;
  get_all_tables(tables, join_table_sequence);
}
void Statement::get_all_tables(
    join_node *tables, vector<vector<TableStruct> > *join_table_sequence) {
  const char *table_name, *schema_name, *alias;
  if (tables->left) get_all_tables(tables->left, join_table_sequence);
  if (tables->right) get_all_tables(tables->right, join_table_sequence);
  if (tables->type != JOIN_NODE_JOIN) {
    TableStruct table_struct;
    table_name = tables->table_name;
    schema_name = tables->schema_name ? tables->schema_name : schema;
    alias = tables->alias ? tables->alias : table_name;
    if (table_name) {
      table_struct.table_name = table_name;
      table_struct.alias = table_name;
    }
    if (schema_name) table_struct.schema_name = schema_name;
    if (alias) {
      table_struct.alias = alias;
      if (tables->sub_select) table_struct.table_name = alias;
    }
    (*join_table_sequence)[0].push_back(table_struct);
  }
}

void Statement::get_table_vector(
    record_scan *rs, vector<vector<TableStruct> > *join_table_sequence) {
  const char *table_name, *schema_name, *alias;
  table_link *tmp_tb = rs->first_table;
  while (tmp_tb) {
    if (tmp_tb->join->cur_rec_scan == rs) {
      TableStruct table_struct;
      table_name = tmp_tb->join->table_name;
      schema_name =
          tmp_tb->join->schema_name ? tmp_tb->join->schema_name : schema;
      alias = tmp_tb->join->alias ? tmp_tb->join->alias : table_name;
      table_struct.table_name = table_name;
      table_struct.schema_name = schema_name;
      table_struct.alias = alias;
      (*join_table_sequence)[0].push_back(table_struct);
      LOG_DEBUG(
          "Get table vector table_name = [%s], schema_name = [%s], alias = "
          "[%s]\n",
          table_struct.table_name.c_str(), table_struct.schema_name.c_str(),
          table_struct.alias.c_str());
    }
    tmp_tb = tmp_tb->next;
  }
}

vector<TableStruct *> *Statement::get_table_vector(record_scan *rs) {
  vector<TableStruct *> *vec = new vector<TableStruct *>();
  const char *table_name, *schema_name, *alias;
  table_link *tmp_tb = rs->first_table;
  while (tmp_tb) {
    if (tmp_tb->join->cur_rec_scan == rs) {
      TableStruct *table_struct = new TableStruct();
      table_name = tmp_tb->join->table_name;
      schema_name =
          tmp_tb->join->schema_name ? tmp_tb->join->schema_name : schema;
      alias = tmp_tb->join->alias ? tmp_tb->join->alias : table_name;
      table_struct->table_name = table_name;
      table_struct->schema_name = schema_name;
      table_struct->alias = alias;
      vec->push_back(table_struct);
      LOG_DEBUG(
          "Get table vector table_name = [%s], schema_name = [%s], alias = "
          "[%s]\n",
          table_struct->table_name.c_str(), table_struct->schema_name.c_str(),
          table_struct->alias.c_str());
    }
    tmp_tb = tmp_tb->next;
  }
  return vec;
}

void Statement::prepare_dataspace_for_leftjoin_subquery(
    join_node *join_tables) {
  /*rsleft joinrsleft
   * join*/

  // set the subquery dataspace and key
  DataSpace *dataspace = record_scan_one_space_tmp_map[join_tables->sub_select];
  if (!dataspace) {
    LOG_ERROR(
        "Find unexpected not support left join subquery with left join sub "
        "select space empty. the sql is [%s]",
        sql);
    throw NotSupportedError("Find unexpected unsupport left join subquery.");
  }
  if (dataspace->is_partitioned()) {
    join_condition *condition = join_tables->upper->condition;
    if (!condition || condition->type != JOIN_COND_ON) {
      LOG_ERROR("DBScale only support cross left join with ON condition.\n");
      throw NotSupportedError(
          "DBScale only support cross left join with on condition.");
    }
    const char *schema_name =
        join_tables->schema_name ? join_tables->schema_name : schema;
    const char *table_alias = join_tables->alias;
    string key = get_key_for_left_join_subquery_tmp_table(
        condition->expr_condition, schema_name, table_alias);
    /* key  left join  normal */
    if (key.empty()) {
      PartitionedTable *par_ds = (PartitionedTable *)dataspace;
      Partition *par = par_ds->get_partition(0);
      table_subquery_final_dataspace[join_tables->sub_select] = par;
    } else {
      dataspace_partition_key[join_tables->sub_select] = key;
      table_subquery_final_dataspace[join_tables->sub_select] = dataspace;
    }
  } else {
    table_subquery_final_dataspace[join_tables->sub_select] = dataspace;
  }
}

void Statement::prepare_table_vector_and_leftjoin_subquery_from_rs(
    record_scan *rs, list<TableStruct *> *table_struct_list,
    map<TableStruct *, table_link *> *table_struct_map) {
  LOG_DEBUG(
      "Statement::prepare_table_vector_and_leftjoin_subquery_from_rs for rs "
      "%@.\n",
      rs);
  /*rsjointable_struct_listtable_struct_maptable_structtable_link
    join
    1.  left_join_record_scan_position_subquery 
    2. record_scan left_join_subquery_rs
    3. dataspacedataspace_partition_key 
    table_subquery_final_dataspaceStatement::init_table_subquery_dataspacetable
    dataspacerecord_scan_one_space_tmp_map
    */
  join_node *join_tables = rs->join_tables;  // tables in the from
  vector<join_node *> join_node_vector;
  TableStruct *table_struct;

  bool record_scan_contains_left_join = rs->contains_left_join;
  int left_join_subquery_num = 0;
  join_node_vector.push_back(join_tables);
  string schema_name;
  string table_name;
  string alias;
  while (!join_node_vector.empty()) {
    join_tables = join_node_vector.back();
    while (join_tables) {
      join_tables = join_tables->left;
      join_node_vector.push_back(join_tables);
    }
    join_node_vector.pop_back();
    if (!join_node_vector.empty()) {
      join_tables = join_node_vector.back();
      join_node_vector.pop_back();
      if (join_tables->type == JOIN_NODE_SINGLE) {
        table_name = join_tables->table_name;
        schema_name =
            join_tables->schema_name ? join_tables->schema_name : schema;
        alias = join_tables->alias ? join_tables->alias : table_name;

        table_struct = new TableStruct();
        table_struct->table_name = table_name;
        table_struct->schema_name = schema_name;
        table_struct->alias = alias;
        table_struct_list->push_back(table_struct);
        (*table_struct_map)[table_struct] = join_tables->delink;
      } else if (join_tables->type == JOIN_NODE_SUBSELECT) {
        if (join_tables->upper && record_scan_contains_left_join) {
          left_join_record_scan_position_subquery[rs]
                                                 [table_struct_list->size() +
                                                  left_join_subquery_num] =
                                                     join_tables->sub_select;
          left_join_subquery_rs.insert(join_tables->sub_select);
          ++left_join_subquery_num;

          /* dataspace_partition_key 
           * table_subquery_final_dataspace left
           * joindataspace*/
          prepare_dataspace_for_leftjoin_subquery(join_tables);
        }
      }
      join_node_vector.push_back(join_tables->right);
    }
  }
}

bool Statement::only_has_one_non_global_tb_can_merge_global(record_scan *root,
                                                            DataSpace *ds) {
  ACE_ASSERT(ds);
  DataSpace *part_ds = NULL;
  vector<DataSpace *> norm_spaces;
  int part_ds_num = 0;

  if (record_scan_dependent_map.count(root) &&
      !record_scan_dependent_map[root].empty()) {
    return false;
  }

  if (ds->is_partitioned()) {
    part_ds = ds;
    ++part_ds_num;
  } else
    norm_spaces.push_back(ds);

  record_scan *tmp = root->children_begin;

  for (; tmp; tmp = tmp->next) {
    if (record_scan_dependent_map.count(tmp) &&
        !record_scan_dependent_map[tmp].empty()) {
      return false;
    }

    if (record_scan_par_table_map.count(tmp)) {
      if (++part_ds_num > 1) return false;
      table_link *par_table1 = record_scan_par_table_map[tmp];
      part_ds = (PartitionedTable *)record_scan_all_table_spaces[par_table1];
    } else {
      if (record_scan_one_space_tmp_map.count(tmp)) {
        DataSpace *global_space = record_scan_one_space_tmp_map[tmp];
        norm_spaces.push_back(global_space);
      }
    }
  }

  DataSpace *fin_space = part_ds;

  if (!norm_spaces.empty()) {
    vector<DataSpace *>::iterator it = norm_spaces.begin();
    for (; it != norm_spaces.end(); ++it) {
      if (!fin_space) {
        fin_space = *it;
        continue;
      } else if (stmt_session->is_dataspace_cover_session_level(fin_space,
                                                                *it)) {
        continue;
      } else if (stmt_session->is_dataspace_cover_session_level(*it,
                                                                fin_space)) {
        fin_space = *it;
        continue;
      } else {
        return false;
      }
    }
  }

  return true;
}

bool Statement::contains_left_join_subquery(record_scan *rs, DataSpace *ds) {
  if (rs->contains_left_join && rs->contains_subquery &&
      !only_has_one_non_global_tb_can_merge_global(
          rs,
          ds))  // If there is only one non_global_tb and can merge with other
                // global tb, not treat this rs contians left_join_subquery
    return true;

  return false;
}

void Statement::init_column_table_subquery_tmp_table_name(
    record_scan *rs, DataSpace *column_space, ExecutePlan *plan) {
  if (plan->session->is_in_lock()) {
    LOG_ERROR("Not support cross node join in lock table mode.\n");
    throw Error("Not support cross node join in lock table mode.");
  }
  string table_name(SUB_COLUMN_TABLE_NAME);
  table_name.append("_");
  table_name.append(column_space->get_name());
  table_name.append("_");
  table_name.append(SUB_COLUMN_TABLE_NAME);
  record_scan_subquery_names[rs] = table_name;
}

string Statement::get_key_for_table_subquery(record_scan *rs) {
  Backend *backend = Backend::instance();
  /* For union table subquery, the join_belong should be NULL.
   * The partition key will be handled by final dataspace.
   *
   * For the union stmt situation, dbscale may need to use table subquery cross
   * node join to move the data of one child-record_scan (such as left child) to
   * the other part (such as right child), in current implementation, the child
   * record scan of union does not set the "rs->join_belong ", so we can use it
   * to distinguish the child record scan of union which need to do table
   * subquery. For table subquery cross node join of union, the partition key is
   * specify by other logic, normally is the first filed of select list, so here
   * just return an empty key.
   */
  string key;
  if (!rs->join_belong) return key;
  const char *table_alias = rs->join_belong->alias;

  handle_par_key_equality_full_table(rs->upper);

  vector<TableStruct *> *table_vector;
  if (record_scan_need_cross_node_join.count(rs->upper)) {
    vector<vector<TableStruct *> *> *table_sequence =
        record_scan_join_tables[rs->upper];
    table_vector = (*table_sequence)[table_sequence->size() - 1];
  } else {
    table_vector = get_table_vector(rs->upper);
  }

  vector<TableStruct *>::iterator it = table_vector->begin();
  for (; it != table_vector->end(); it++) {
    string full_column((*it)->schema_name);
    full_column.append(".");
    full_column.append((*it)->alias);
    full_column.append(".");
    DataSpace *space = backend->get_data_space_for_table(
        (*it)->schema_name.c_str(), (*it)->table_name.c_str());
    if (space->get_data_source()) continue;
    const char *key_name = ((PartitionedTable *)space)->get_key_names()->at(0);
    full_column.append(key_name);
    vector<string> *vec =
        &record_scan_par_key_equality_full_table[rs->upper][full_column];
    vector<string>::iterator it_vec = vec->begin();
    if (it_vec != vec->end()) {
      it_vec++;
    }
    for (; it_vec != vec->end(); it_vec++) {
      size_t first_dot_position = it_vec->find(".");
      size_t second_dot_position = it_vec->find(".", first_dot_position + 1);
      if (first_dot_position != string::npos &&
          second_dot_position == string::npos) {
        string table_name = it_vec->substr(0, first_dot_position);
        if (lower_case_table_names) {
          if (!strcasecmp(table_name.c_str(), table_alias)) {
            key = it_vec->substr(first_dot_position + 1);
            break;
          }
        } else {
          if (!strcmp(table_name.c_str(), table_alias)) {
            key = it_vec->substr(first_dot_position + 1);
            break;
          }
        }
      }
    }
  }
  if (!record_scan_need_cross_node_join.count(rs->upper)) {
    for (it = table_vector->begin(); it != table_vector->end(); it++) {
      delete *it;
    }
    delete table_vector;
  }
  return key;
}

string Statement::get_column_in_table_without_throw(string peer_string,
                                                    string schema_name,
                                                    string table_alias) {
  string key;
  string column;
  string peer_schema;
  string peer_table;
  split_value_into_schema_table_column(peer_string, peer_schema, peer_table,
                                       column);

  if (!peer_schema.empty()) {
    if (!lower_case_compare(peer_schema.c_str(), schema_name.c_str()) &&
        !lower_case_compare(peer_table.c_str(), table_alias.c_str()))
      key = column;
  } else if (!peer_table.empty()) {
    if (!lower_case_compare(peer_table.c_str(), table_alias.c_str()))
      key = column;
  }
  return key;
}

/* Get column from one condition peer belongs to the table.
 */
string Statement::get_column_in_table(string peer_string, table_link *table) {
  string key;
  string column;
  string peer_schema;
  string peer_table;
  string schema_name =
      table->join->schema_name ? table->join->schema_name : schema;
  string table_name = table->join->table_name;
  string table_alias =
      table->join->alias ? table->join->alias : table->join->table_name;
  split_value_into_schema_table_column(peer_string, peer_schema, peer_table,
                                       column);
  if (!peer_schema.empty()) {
    if (!lower_case_compare(peer_schema.c_str(), schema_name.c_str()) &&
        !lower_case_compare(peer_table.c_str(), table_alias.c_str()))
      key = column;
  } else if (!peer_table.empty()) {
    if (!lower_case_compare(peer_table.c_str(), table_alias.c_str()))
      key = column;
  } else {
    TableInfoCollection *tic = TableInfoCollection::instance();
    TableInfo *ti = tic->get_table_info_for_read(schema_name, table_name);
    map<string, TableColumnInfo *, strcasecomp> *column_info_map;
    try {
      column_info_map =
          ti->element_table_column
              ->get_element_map_columnname_table_column_info(stmt_session);
    } catch (...) {
      LOG_ERROR("Error eccured when get table info for table [%s.%s]\n",
                schema_name.c_str(), table_name.c_str());
      ti->release_table_info_lock();
      throw;
    }
    string tmp_colname(column);
    boost::to_lower(tmp_colname);
    if (column_info_map != NULL) {
      if (column_info_map->count(tmp_colname)) key = column;
    }
    ti->release_table_info_lock();
  }
  return key;
}

string Statement::get_key_for_left_join_subquery_tmp_table(
    Expression *condition, const char *schema_name, const char *table_alias) {
  string ret;
  if (condition->type == EXPR_EQ) {
    CompareExpression *expr = (CompareExpression *)condition;
    Expression *left = expr->left;
    Expression *right = expr->right;
    if (left->type == EXPR_STR && right->type == EXPR_STR) {
      string left_peer, right_peer;
      left->to_string(left_peer);
      right->to_string(right_peer);
      string key1 = get_column_in_table_without_throw(left_peer, schema_name,
                                                      table_alias);
      string key2 = get_column_in_table_without_throw(right_peer, schema_name,
                                                      table_alias);
      if (key1.empty() && !key2.empty()) {
        ret = key2;
      } else if (!key1.empty() && key2.empty()) {
        ret = key1;
      }
    }
    return ret;
  }
  if (condition->type == EXPR_AND) {
    BoolBinaryCaculateExpression *tmp_expr =
        (BoolBinaryCaculateExpression *)condition;
    ret = get_key_for_left_join_subquery_tmp_table(tmp_expr->left, schema_name,
                                                   table_alias);
    if (ret.empty())
      ret = get_key_for_left_join_subquery_tmp_table(tmp_expr->right,
                                                     schema_name, table_alias);
  }
  return ret;
}

/* Find a equal column of the table from on condition.
 * only when the two sites of the equal condition belong to diffrent table
 * and one belongs to current table, return the column.
 */
string Statement::get_key_for_left_join_tmp_table(Expression *condition,
                                                  table_link *table) {
  string ret;
  if (condition->type == EXPR_EQ) {
    CompareExpression *expr = (CompareExpression *)condition;
    Expression *left = expr->left;
    Expression *right = expr->right;
    if (left->type == EXPR_STR && right->type == EXPR_STR) {
      string left_peer, right_peer;
      left->to_string(left_peer);
      right->to_string(right_peer);
      string key1 = get_column_in_table(left_peer, table);
      string key2 = get_column_in_table(right_peer, table);
      if (key1.empty() && !key2.empty()) {
        ret = key2;
      } else if (!key1.empty() && key2.empty()) {
        ret = key1;
      }
    }
    return ret;
  }
  if (condition->type == EXPR_AND) {
    BoolBinaryCaculateExpression *tmp_expr =
        (BoolBinaryCaculateExpression *)condition;
    ret = get_key_for_left_join_tmp_table(tmp_expr->left, table);
    if (ret.empty())
      ret = get_key_for_left_join_tmp_table(tmp_expr->right, table);
  }
  return ret;
}

bool Statement::check_column_in_peer(string peer, const char *column,
                                     table_link *table) {
  string peer_column;
  string peer_schema;
  string peer_table;
  string schema_name =
      table->join->schema_name ? table->join->schema_name : schema;
  string table_alias =
      table->join->alias ? table->join->alias : table->join->table_name;
  split_value_into_schema_table_column(peer, peer_schema, peer_table,
                                       peer_column);
  if (!peer_schema.empty()) {
    if (!lower_case_compare(peer_schema.c_str(), schema_name.c_str()) &&
        !lower_case_compare(peer_table.c_str(), table_alias.c_str()) &&
        !strcasecmp(column, peer_column.c_str()))
      return true;
  } else if (!peer_table.empty()) {
    if (!lower_case_compare(peer_table.c_str(), table_alias.c_str()) &&
        !strcasecmp(column, peer_column.c_str()))
      return true;
  } else {
    if (!strcasecmp(column, peer_column.c_str())) return true;
  }
  return false;
}

bool Statement::check_column_in_condition(Expression *condition,
                                          const char *column,
                                          table_link *table) {
  if (condition->type == EXPR_EQ) {
    CompareExpression *expr = (CompareExpression *)condition;
    Expression *left = expr->left;
    Expression *right = expr->right;
    if (left->type == EXPR_STR && right->type == EXPR_STR) {
      string left_peer, right_peer;
      left->to_string(left_peer);
      right->to_string(right_peer);
      bool lb = check_column_in_peer(left_peer, column, table);
      bool rb = check_column_in_peer(right_peer, column, table);
      if (lb && !rb) return true;
      if (!lb && rb) return true;
    }
  }
  if (condition->type == EXPR_AND) {
    BoolBinaryCaculateExpression *tmp_expr =
        (BoolBinaryCaculateExpression *)condition;
    if (check_column_in_condition(tmp_expr->left, column, table))
      return true;
    else if (check_column_in_condition(tmp_expr->right, column, table))
      return true;
  }
  return false;
}

bool Statement::check_left_join_on_par_key(table_link *table) {
  Backend *backend = Backend::instance();
  join_node *join = table->join;
  join_condition *condition = join->upper->condition;
  if (!condition || condition->type != JOIN_COND_ON) {
    LOG_ERROR("DBScale only support cross left join with ON condition.\n");
    throw NotSupportedError(
        "DBScale only support cross left join with on condition.");
  }
  const char *schema_name = join->schema_name ? join->schema_name : schema;
  const char *table_name = join->table_name;
  DataSpace *space = backend->get_data_space_for_table(schema_name, table_name);
  if (space->get_data_source()) return true;
  const char *key_name = ((PartitionedTable *)space)->get_key_names()->at(0);
  Expression *expr = condition->expr_condition;
  if (check_column_in_condition(expr, key_name, table)) return true;
  return false;
}

/* If table is right part of a left join with out equal on partition key,
 * move the data of the table to a tmp table then do cross node join with left
 * part. If find a equal column of the table from on condition, place the tmp
 * table on table scheme with equal column as partition key, if not, place the
 * tmp table on first partition of the table.
 */
string Statement::init_left_join_tmp_table_dataspace(table_link *table) {
  string table_name;
  Backend *backend = Backend::instance();
  join_node *join = table->join;
  join_condition *condition = join->upper->condition;
  if (!condition || condition->type != JOIN_COND_ON) {
    LOG_ERROR("DBScale only support cross left join with ON condition.\n");
    throw NotSupportedError(
        "DBScale only support cross left join with on condition.");
  }
  Expression *expr = condition->expr_condition;
  string key = get_key_for_left_join_tmp_table(expr, table);
  const char *local_table = join->table_name;
  const char *local_schema = join->schema_name ? join->schema_name : schema;
  LOG_DEBUG("Statement::init_left_join_tmp_table_dataspace for table %s.%s.\n",
            local_schema, local_table);
  DataSpace *local_space =
      backend->get_data_space_for_table(local_schema, local_table);
  if (key.empty()) {
    PartitionedTable *par_ds = (PartitionedTable *)local_space;
    Partition *par = par_ds->get_partition(0);
    local_space = par;
    key = "dbscale_key";
  }
  DataSpace *tab = NULL;
  local_space->acquire_tmp_table_read_mutex();
  table_name = local_space->get_tmp_table_name(key.c_str());
  local_space->release_tmp_table_mutex();
  if (table_name.empty()) {
    local_space->acquire_tmp_table_write_mutex();
    table_name = local_space->get_tmp_table_name(key.c_str());
    if (table_name.empty()) {
      JoinTableInfo table_info = {key, "", "", "", ""};
      try {
        tab = local_space->init_join_table_space(table_name, &table_info, false,
                                                 true);
      } catch (...) {
        LOG_ERROR("Got error then init left join tmp table dataspace.\n");
        local_space->release_tmp_table_mutex();
        throw;
      }
      backend->add_join_table_spaces(local_space, tab);
      local_space->set_tmp_table_name(key.c_str(), table_name);
    }
    local_space->release_tmp_table_mutex();
  }
  return table_name;
}

/*This function init the dataspace of table_subquery tables,
 *there are four situations:
 *1.parent is a normal table, locate the tmp table with parent.
 *2.parent is a partition table, if we can find a partition key, init the tmp
 *table as a partitioned table with the partition key, or init the tmp table as
 *a duplicated table. 3.parent has no table, locate the tmp table on current
 *local normal table. 4.parent has no table, locate the tmp table on current
 *local partition table's first partition. 5.otherwise, use catalog.
 *
 *We use the locate dataspace and key_name as key map to tmp table dataspace
 *name already initialized. If we can get the tmp table dataspace name with
 *key_name from local_space, use the dataspace already exists. If not, create a
 *new tmp table dataspace and store it to local_space with key_name as map key.
 *There are three kind of key_name, one is partition key for partitioned tmp
 *tables, one is "dbscale_dup" for duplicated tmp tables, one is "dbscale_key"
 *for normal tmp tables.
 *
 *Set record_scan_one_space_tmp_map of current rs as new tmp table dataspace.
 */
void Statement::init_table_subquery_dataspace(record_scan *rs,
                                              ExecutePlan *plan) {
  if (plan->session->is_in_lock()) {
    LOG_ERROR("Not support cross node join in lock table mode.\n");
    throw Error("Not support cross node join in lock table mode.");
  }

  Backend *backend = Backend::instance();
  /*In this function, current parameter rs is a child table subquery,
   *cur_dataspace is the record_scan_one_space of itself, dataspace is
   *the record_scan_one_space of it's parent record scan.
   */
  bool is_dup = false;
  string tmp_key_name;
  const char *key_name = NULL;
  string key;
  DataSpace *dataspace = record_scan_one_space_tmp_map[rs->upper];
  DataSpace *cur_dataspace = record_scan_one_space_tmp_map[rs];
  string schema_name(get_schema());
  DataSource *cur_ds = NULL;
  DataSpace *local_space = NULL;  // The final dataspace of tmp table.
  if (cur_dataspace) cur_ds = cur_dataspace->get_data_source();
  DataSource *ds = NULL;
  if (dataspace) ds = dataspace->get_data_source();
  if (ds) {
    local_space = dataspace;
    key_name = "dbscale_key";
  } else if (dataspace) {
    key = get_key_for_table_subquery(rs);
    if (key.empty()) {
      local_space = dataspace;
      key_name = "dbscale_dup";
      is_dup = true;
    } else {
      local_space = dataspace;
      key_name = key.c_str();
    }
  } else if (cur_ds) {
    local_space = cur_dataspace;
    key_name = "dbscale_key";
  } else if (cur_dataspace) {
    PartitionedTable *par_ds = (PartitionedTable *)cur_dataspace;
    Partition *par = par_ds->get_partition(0);
    local_space = par;
    key_name = "dbscale_key";
  } else {
    /*For the situation that current rs is a union all query record scan
     *and there is no table in it's parent.
     */
    local_space = backend->get_data_space_for_table(NULL, NULL);
    key_name = "dbscale_key";
  }
  if (table_subquery_final_dataspace.count(rs)) {
    local_space = table_subquery_final_dataspace[rs];
    is_dup = false;
    tmp_key_name = dataspace_partition_key[rs];
    key_name = tmp_key_name.c_str();
  }

  DataSpace *tab = get_subquery_table_dataspace(local_space, key_name, is_dup);

  local_space->acquire_tmp_table_read_mutex();
  string table_name = local_space->get_tmp_table_name(key_name);
  local_space->release_tmp_table_mutex();

  record_scan_table_subquery_space_map[rs] = record_scan_one_space_tmp_map[rs];
  record_scan_subquery_names[rs] = table_name;
  record_scan_one_space_tmp_map[rs] = tab;

  if (!dataspace) record_scan_one_space_tmp_map[rs->upper] = tab;

  need_clean_record_scan_one_space_tmp_map = true;
  /* Set the subquery tmp table idents, if the parent is a cross node join
   * the ident will be set to cross node join sequence vector, when use the
   * sequence vector to get create sql, these tmp table idents will be replaced
   * to real tmp table names.
   **/
  set_subquery_table_names_for_replace(rs, table_name);
}

TableStruct *Statement::get_subquery_table_struct(record_scan *rs) {
  const char *table_name = record_scan_subquery_names_for_replace[rs].c_str();
  const char *schema_name = get_schema();
  string tmp_table_name(table_name);
  if (tmp_table_name.find(SUB_QUERY_TABLE_NAME) != string::npos)
    schema_name = TMP_TABLE_SCHEMA;
  if (!rs->join_belong) {
    LOG_ERROR(
        "Find unecpect table subquery, maybe it is a not support sql: %s.\n",
        sql);
    throw Error("Not support SQL");
  }
  const char *alias = rs->join_belong->alias;
  TableStruct *table_struct = new TableStruct();
  table_struct->table_name = table_name;
  table_struct->schema_name = schema_name;
  table_struct->alias = alias;
  return table_struct;
}

/*This function should analysis the cross node join with table_subquery tables,
 *there are tow situations:
 *1.itself is a cross node join, add the subquery table to the end vector of
 *join_table_sequence; 2.itself is not a cross node join, and it only could be a
 *partitioned table, create a new join_table_sequence. If it is a left join and
 *the table subquery is on the left side, add the tmp table to the end of table
 *vector. Do not support left join with where condition right now.
 *
 * Return: the planed finial execution dataspace*/
DataSpace *Statement::handle_cross_node_join_with_subquery(record_scan *rs) {
  const char *table_name, *schema_name, *alias;
  TableStruct *table_struct;
  vector<TableStruct *> *table_struct_vector;

  if (left_join_record_scan_position_subquery.count(rs)) {
    /* left join
     * rstable_structpos
     * new_table_struct_vector*/
    vector<vector<TableStruct *> *> *table_struct_vector =
        record_scan_join_tables[rs];
    vector<vector<TableStruct *> *>::iterator it = table_struct_vector->begin();
    vector<vector<TableStruct *> *> *new_table_struct_vector =
        new vector<vector<TableStruct *> *>();
    int table_count = 0;
    int subquery_count = 0;
    while (left_join_record_scan_position_subquery[rs].count(table_count +
                                                             subquery_count)) {
      record_scan *subquery_rs =
          left_join_record_scan_position_subquery[rs]
                                                 [table_count + subquery_count];
      table_struct = get_subquery_table_struct(subquery_rs);
      vector<TableStruct *> *new_table_vector = new vector<TableStruct *>();
      new_table_vector->push_back(table_struct);
      new_table_struct_vector->push_back(new_table_vector);
      ++subquery_count;
    }
    for (; it != table_struct_vector->end(); ++it) {
      new_table_struct_vector->push_back(*it);
      table_count += (*it)->size();
      while (left_join_record_scan_position_subquery[rs].count(
          table_count + subquery_count)) {
        record_scan *subquery_rs =
            left_join_record_scan_position_subquery[rs][table_count +
                                                        subquery_count];
        table_struct = get_subquery_table_struct(subquery_rs);
        vector<TableStruct *> *new_table_vector = new vector<TableStruct *>();
        new_table_vector->push_back(table_struct);
        new_table_struct_vector->push_back(new_table_vector);
        ++subquery_count;
      }
    }
    /*FROMrecord_scan_one_space_tmp_map.
     * rsrecord_scan_one_space_tmp_mapdataspaceleft join rsspaceleft join
       rsspacersdataspace*/
    if (left_join_record_scan_position_subquery[rs].count(table_count +
                                                          subquery_count - 1)) {
      record_scan *subquery_rs =
          left_join_record_scan_position_subquery[rs][table_count +
                                                      subquery_count - 1];
      DataSpace *final_ds = record_scan_one_space_tmp_map[subquery_rs];
      if (final_ds->is_partitioned()) {
        need_reanalysis_space = true;
      }
      record_scan_one_space_tmp_map[rs] = final_ds;
    }
    delete table_struct_vector;
    record_scan_join_tables[rs] = new_table_struct_vector;
    set_join_final_table(rs);
  } else if (record_scan_join_tables.count(rs)) {
    // current rs is a cross node join.
    // Add the subquery tmp table to the end vector of
    // join table sequence.
    record_scan *tmp = rs->children_begin;
    for (; tmp; tmp = tmp->next) {
      if (record_scan_subquery_names.count(tmp)) {
        table_struct = get_subquery_table_struct(tmp);
        (*record_scan_join_tables[rs])[record_scan_join_tables[rs]->size() - 1]
            ->push_back(table_struct);
      }
    }
    set_join_final_table(rs);
  } else {
    // current rs is not a cross node join. Create a new join table sequence.
    if (record_scan_one_space_tmp_map[rs] &&
        !record_scan_one_space_tmp_map[rs]->get_data_source()) {
      vector<vector<TableStruct *> *> *join_table_sequence =
          new vector<vector<TableStruct *> *>();
      table_struct_vector = new vector<TableStruct *>();
      record_scan *tmp = rs->children_begin;
      for (; tmp; tmp = tmp->next) {
        if (record_scan_subquery_names.count(tmp)) {
          table_struct = get_subquery_table_struct(tmp);
          table_struct_vector->push_back(table_struct);
        }
      }
      join_table_sequence->push_back(table_struct_vector);
      table_link *tmp_tb = rs->first_table;
      table_struct_vector = NULL;
      while (tmp_tb) {
        if (tmp_tb->join->cur_rec_scan == rs) {
          if (!table_struct_vector)
            table_struct_vector = new vector<TableStruct *>();
          table_name = tmp_tb->join->table_name;
          schema_name =
              tmp_tb->join->schema_name ? tmp_tb->join->schema_name : schema;
          alias = tmp_tb->join->alias ? tmp_tb->join->alias : table_name;
          table_struct = new TableStruct();
          table_struct->table_name = table_name;
          table_struct->schema_name = schema_name;
          table_struct->alias = alias;
          table_struct_vector->push_back(table_struct);
        }
        tmp_tb = tmp_tb->next;
      }
      if (table_struct_vector) {
        join_table_sequence->push_back(table_struct_vector);
      }

      record_scan_join_tables[rs] = join_table_sequence;
      set_join_final_table(rs);
    }
  }
  record_scan_need_cross_node_join[rs] = true;

  return record_scan_one_space_tmp_map[rs];
}

bool Statement::sub_query_need_cross_node_join(DataSpace *ds_tmp,
                                               record_scan *cur_rs) {
  for (auto &table_st : can_merge_record_scan_position[cur_rs]) {
    TableStruct &table = table_st.second;
    DataSpace *ds = Backend::instance()->get_data_space_for_table(
        table.schema_name.c_str(), table.table_name.c_str());
    if (ds == ds_tmp && global_table_merged.count(ds)) {
      if (record_scan_one_space_tmp_map.count(cur_rs) &&
          !is_share_same_server(record_scan_one_space_tmp_map[cur_rs], ds))
        return true;
      else if (!record_scan_one_space_tmp_map.count(cur_rs))
        return true;
    }
  }
  return false;
}

/*This function analysis the cross node join record_scan tables, and
 * figure out the finial execution dataspace for this record_scan in
 * advance.
 *
 * Return: the planed finial execution dataspace*/
DataSpace *Statement::handle_cross_node_join(record_scan *rs, ExecutePlan *plan,
                                             bool force_no_merge) {
#ifndef CROSS_NODE_JOIN_DISABLE
  if (st.type != STMT_SELECT && st.type != STMT_INSERT_SELECT &&
      st.type != STMT_REPLACE_SELECT) {
    LOG_ERROR("Not support non select sql do cross node join.\n");
    throw Error("Not support non select sql do cross node join.");
  }
  if (plan->session->is_in_lock()) {
    LOG_ERROR("Not support cross node join in lock table mode.\n");
    throw Error("Not support cross node join in lock table mode.");
  }
  if (is_connect_by) {
    LOG_ERROR("Not support connect by for cross node join.\n");
    throw Error("Not support connect by for cross node join.");
  }
  handle_par_key_equality_full_table(rs);

  Backend *backend = Backend::instance();
  DataSpace *dataspace = NULL, *dataspace_tmp = NULL, *ret_dataspace = NULL;
  TableStruct *table_struct;
  vector<TableStruct *> *table_struct_vector;
  vector<vector<TableStruct *> *> *join_table_sequence =
      new vector<vector<TableStruct *> *>();
  record_scan_join_tables[rs] = join_table_sequence;
  list<TableStruct *> table_struct_list;
  map<TableStruct *, table_link *> table_struct_map;

  prepare_table_vector_and_leftjoin_subquery_from_rs(rs, &table_struct_list,
                                                     &table_struct_map);
  list<TableStruct *>::iterator it;

  map<TableStruct *, set<TableStruct *> >
      record_scan_related_condition_table_map;
  handle_related_condition_full_table(rs, &table_struct_list,
                                      &record_scan_related_condition_table_map);

  int left_join_subquery_count = 0;
  int table_struct_count = 0;

  if (!table_struct_list.empty()) {
    table_struct = table_struct_list.front();
    if (check_left_join(table_struct_map[table_struct])) {
      if (!check_left_join_on_par_key(table_struct_map[table_struct]))
        /*left
         * join
         * ontable_structleft_join_right_tables*/
        left_join_right_tables.insert(table_struct);
    }
  }

  vector<TableStruct *> *table_struct_vector_end = NULL;
  // Construct the join table sequence, put the tables that can merge
  // into one vector.
  bool has_skip_merge_table = false;
  while (!table_struct_list.empty()) {
    has_skip_merge_table = false;
    table_struct_vector = new vector<TableStruct *>();
    while (left_join_record_scan_position_subquery.count(rs) &&
           left_join_record_scan_position_subquery[rs].count(
               table_struct_count + left_join_subquery_count)) {
      ++left_join_subquery_count;
    }
    bool table_struct_position_end = false;
    // Example: select * from t1 inner join t2 left join t3 on t1.c1 = t3.c1 and
    // t2.c1 = t3.c1; t1t2merget1t3merge
    bool skip_left_join_merge = false;
    table_struct = table_struct_list.front();
    // If current table is a right table of left join, the dataspace should be
    // the tmp table.
    if (left_join_right_tables.count(table_struct)) {
      /*left
       * joinonon*/
      string tmp_table;
      try {
        tmp_table =
            init_left_join_tmp_table_dataspace(table_struct_map[table_struct]);
        LOG_DEBUG("Prepare to redistribute left join partition table %s.\n",
                  tmp_table.c_str());
      } catch (...) {
        for (it = table_struct_list.begin(); it != table_struct_list.end();
             ++it) {
          table_struct_vector->push_back(*it);
        }
        join_table_sequence->push_back(table_struct_vector);
        LOG_ERROR("Got exception when init left join tmp table dataspace.\n");
        throw;
      }
      /*left_join_table_positionleft_join_tables.

        left_join_table_position  left_join_position. 
        CrossNodeJoinTask  src/subquery.cc CrossNodeJoinTask
        task = {... &left_join_position, left_join_sqls};

         helper  CrossNodeJoinInfo
        left join
        merge
        DataMoveJoinInfo::handle_analysis_left_join_expr_str_expression 
        ConstructMethod::generate_left_join_sql
      */
      left_join_table_position[rs][join_table_sequence->size() +
                                   left_join_subquery_count] = tmp_table;
      left_join_tables[rs][join_table_sequence->size() +
                           left_join_subquery_count] = table_struct;
      dataspace = backend->get_data_space_for_table(TMP_TABLE_SCHEMA,
                                                    tmp_table.c_str());
      ret_dataspace = dataspace;
    } else {
      dataspace = backend->get_data_space_for_table(
          table_struct->schema_name.c_str(), table_struct->table_name.c_str());
    }
    list<TableStruct *> can_merge_table_struct_list;
    table_link *first_table = table_struct_map[table_struct];
    can_merge_table_struct_list.push_back(table_struct);
    /* We check if the child of the record_scan is missing dataspace and the
     * missing dataspace is current table.
     */
    if (subquery_missing_table.count(rs) &&
        subquery_missing_table[rs] == *table_struct) {
      /*rsrstable_structrsdataspacetable_structdataspacedataspacetable_structgroupgroupflag
       * table_struct_position_endtable_struct
       * groupgroupgrouptable_struct_vector_end*/
      table_struct_position_end = true;  // this table space must be the finial
      /*The *table_struct is different from subquery_missing_table[rs] for the
       * table name. The equal checking in the if indicate the schema.alias is
       * equal. By default subquery_missing_table[rs]'s table_name is alias,
       * here we adjust to the real table name from *table_struct.*/
      subquery_missing_table[rs] = *table_struct;
      ret_dataspace = dataspace;
    }
    ++table_struct_count;
    table_struct_list.pop_front();
    it = table_struct_list.begin();
    /*table_struct_listmergemergecan_merge_table_struct_listleft
     * joinmergejoinleft
     * joinjoinleft
     * joinmergebreak*/
    while (it != table_struct_list.end()) {
      /*left join merge*/
      if (left_join_record_scan_position_subquery.count(rs) &&
          left_join_record_scan_position_subquery[rs].count(
              table_struct_count + left_join_subquery_count)) {
        break;
      }
      if (check_left_join(table_struct_map[*it])) {
        if (subquery_missing_table.count(rs) &&
            (**it != *(table_struct_list.back()) ||
             subquery_missing_table[rs] != **it)) {
          for (it = table_struct_list.begin(); it != table_struct_list.end();
               ++it) {
            table_struct_vector->push_back(*it);
          }
          for (it = can_merge_table_struct_list.begin();
               it != can_merge_table_struct_list.end(); ++it) {
            table_struct_vector->push_back(*it);
          }
          join_table_sequence->push_back(table_struct_vector);
          LOG_ERROR("Not support left join with subquery missing table.\n");
          throw NotSupportedError(
              "Not support left join with subquery missing table.");
        }
        if (skip_left_join_merge) break;
      }
      // Now we put the left join tmp table in one table vector, maybe we can
      // merge it with other table in the future.
      if (left_join_right_tables.count(table_struct)) {
        // If current table is a right table of left join, check if next is a
        // left join too.
        if (check_left_join(table_struct_map[*it])) {
          try {
            if (!check_left_join_on_par_key(table_struct_map[*it]))
              left_join_right_tables.insert(*it);
          } catch (...) {
            for (it = table_struct_list.begin(); it != table_struct_list.end();
                 ++it) {
              table_struct_vector->push_back(*it);
            }
            for (it = can_merge_table_struct_list.begin();
                 it != can_merge_table_struct_list.end(); ++it) {
              table_struct_vector->push_back(*it);
            }
            join_table_sequence->push_back(table_struct_vector);
            LOG_ERROR("Got exception when check left join on par key 1.\n");
            throw;
          }
        }
        break;
      }
      table_struct = *it;
      dataspace_tmp = backend->get_data_space_for_table(
          table_struct->schema_name.c_str(), table_struct->table_name.c_str());
      if (!dataspace->get_data_source() && !dataspace_tmp->get_data_source()) {
        if (check_CNJ_two_partition_table_can_merge(
                table_struct_map[table_struct]->join->cur_rec_scan, first_table,
                table_struct_map[table_struct], (PartitionedTable *)dataspace,
                (PartitionedTable *)dataspace_tmp) &&
            (!has_skip_merge_table ||
             check_tables_has_related_condition(
                 table_struct, &can_merge_table_struct_list,
                 &record_scan_related_condition_table_map))) {
          can_merge_table_struct_list.push_back(table_struct);
          if (subquery_missing_table.count(rs) &&
              subquery_missing_table[rs] == *table_struct) {
            table_struct_position_end = true;
            ret_dataspace = dataspace_tmp;
            subquery_missing_table[rs] = *table_struct;
          }
          it = table_struct_list.erase(it);
          ++table_struct_count;
        } else {
          if (check_left_join(table_struct_map[table_struct])) {
            /*mergeleft
             * joinon*/
            try {
              if (!check_left_join_on_par_key(table_struct_map[table_struct]))
                left_join_right_tables.insert(table_struct);
            } catch (...) {
              for (it = table_struct_list.begin();
                   it != table_struct_list.end(); ++it) {
                table_struct_vector->push_back(*it);
              }
              for (it = can_merge_table_struct_list.begin();
                   it != can_merge_table_struct_list.end(); ++it) {
                table_struct_vector->push_back(*it);
              }
              join_table_sequence->push_back(table_struct_vector);
              LOG_ERROR("Got exception when check left join on par key 2.\n");
              throw;
            }
            break;
          }
          ++it;
          skip_left_join_merge = true;
          has_skip_merge_table = true;
        }
      } else {
        if (!dataspace_tmp->get_data_source() &&
            check_left_join(table_struct_map[table_struct])) {
          /*left join*/
          try {
            if (!check_left_join_on_par_key(table_struct_map[table_struct]))
              left_join_right_tables.insert(table_struct);
          } catch (...) {
            for (it = table_struct_list.begin(); it != table_struct_list.end();
                 ++it) {
              table_struct_vector->push_back(*it);
            }
            for (it = can_merge_table_struct_list.begin();
                 it != can_merge_table_struct_list.end(); ++it) {
              table_struct_vector->push_back(*it);
            }
            join_table_sequence->push_back(table_struct_vector);
            LOG_ERROR("Got exception when check left join on par key 3.\n");
            throw;
          }
          ++it;
          break;
        }
        /*2space(norm_table)merge.*/
        if (stmt_session->is_dataspace_cover_session_level(dataspace,
                                                           dataspace_tmp) &&
            (!force_no_merge ||
             !sub_query_need_cross_node_join(dataspace_tmp, rs)) &&
            (!has_skip_merge_table ||
             check_tables_has_related_condition(
                 table_struct, &can_merge_table_struct_list,
                 &record_scan_related_condition_table_map))) {
          can_merge_table_struct_list.push_back(table_struct);
          if (subquery_missing_table.count(rs) &&
              subquery_missing_table[rs] == *table_struct) {
            table_struct_position_end = true;
            ret_dataspace = dataspace;
            subquery_missing_table[rs] = *table_struct;
          }
          it = table_struct_list.erase(it);
          ++table_struct_count;
          if (!stmt_session->is_dataspace_cover_session_level(dataspace_tmp,
                                                              dataspace))
            gtable_can_merge_dataspace[rs][dataspace_tmp] = dataspace;
        } else if (stmt_session->is_dataspace_cover_session_level(dataspace_tmp,
                                                                  dataspace) &&
                   (!force_no_merge ||
                    !sub_query_need_cross_node_join(dataspace, rs)) &&
                   (!has_skip_merge_table ||
                    check_tables_has_related_condition(
                        table_struct, &can_merge_table_struct_list,
                        &record_scan_related_condition_table_map))) {
          can_merge_table_struct_list.push_back(table_struct);
          if (subquery_missing_table.count(rs) &&
              subquery_missing_table[rs] == *table_struct) {
            table_struct_position_end = true;
            ret_dataspace = dataspace_tmp;
            subquery_missing_table[rs] = *table_struct;
          }
          it = table_struct_list.erase(it);
          ++table_struct_count;
          dataspace = dataspace_tmp;
          if (!stmt_session->is_dataspace_cover_session_level(dataspace,
                                                              dataspace_tmp))
            gtable_can_merge_dataspace[rs][dataspace] = dataspace_tmp;
        } else {
          ++it;
          /*2norm table left joinmerge*/
          if (check_left_join(table_struct_map[table_struct])) {
            break;
          } else {
            skip_left_join_merge = true;
            has_skip_merge_table = true;
          }
        }
      }
    }
    list<TableStruct *>::iterator it_list = can_merge_table_struct_list.begin();
    for (; it_list != can_merge_table_struct_list.end(); ++it_list) {
      table_struct_vector->push_back(*it_list);
    }

    // if we have record_scan_need_table, the final dataspace must be it. so
    // ignore table_struct_position_end
    if (table_struct_position_end && !table_struct_vector_end &&
        !record_scan_need_table.count(rs)) {
      table_struct_vector_end = table_struct_vector;
      LOG_DEBUG("ADD the table in to table_struct_vector_end\n");
    } else {
      join_table_sequence->push_back(table_struct_vector);
      LOG_DEBUG("ADD the table in to join_table_sequence\n");
    }
  }

  if (subquery_missing_table.count(rs) && !table_struct_vector_end) {
    LOG_ERROR(
        "The missing table in subquery dose not exists in parent subquery.\n");
    throw Error(
        "The missing table in subquery dose not exists in parent subquery. "
        "DBScale dose not support this sql, please check. ");
  }

  /* In the situation of missing subquery
   * Example: select * from t1, t2 where t2.c1 = (select min(t3.c2) from t3
   * where t3.c1 = t1.c1); If the record_scan is father, we will change the
   * position of the joining. The output: select * from t2, t1 where t2.c1 =
   * (select min(t3.c2) from t3 where t3.c1 = t1.c1); else if the record_scan is
   * child, we would add the missing table in it. Output is: select min(t3.c2)
   * from t3, t1 where t3.c1 = t1.c1;
   */
  if (table_struct_vector_end) {
    /*  In the situation of missing subquery, normally this if cause is used for
     * parent record scan. if current record_scan should change its join order,
     * the table_struct_vector_end is not NULL if we want to set the join
     * dataspace(output dataspace), we should set the table_struct_vector_end
     */
    /*rstable_struct
     * groupgroupdataspace*/
    join_table_sequence->push_back(table_struct_vector_end);
    Backend *backend = Backend::instance();
    TableStruct table_struct_tmp;
    if (subquery_missing_table.count(rs))
      table_struct_tmp = subquery_missing_table[rs];

    DataSpace *ds_global =
        backend->get_data_space_for_table(table_struct_tmp.schema_name.c_str(),
                                          table_struct_tmp.table_name.c_str());
    if (ds_global != ret_dataspace) {
      gtable_dependent_output_dataspace[rs][ds_global] = ret_dataspace;
    }
  } else if (record_scan_need_table.count(rs)) {
    /* In the situation of missing subquery, normally this if cause is used for
     * missing subquery record scan. if current record_scan is dependent, we
     * need temporarily add the missing table to its join table sequence. and
     * remove it after construct sql.
     */
    /*rstable_struct
     * groupgroupgrouprssqldataspacesrc/sub_query.cc/SimpleCrossNodeJoinNode::do_pre_executeneed_merge_all_method/tablefinial
     * select sqlneed_merge_all_method
     * src/statement.cc/create_one_separated_exec_node*/
    TableStruct *last_table_struct = new TableStruct();
    *last_table_struct = record_scan_need_table[rs];
    vector<TableStruct *> *last_table_struct_vector =
        new vector<TableStruct *>();
    last_table_struct_vector->push_back(last_table_struct);
    join_table_sequence->push_back(last_table_struct_vector);
    ret_dataspace = backend->get_data_space_for_table(
        last_table_struct->schema_name.c_str(),
        last_table_struct->table_name.c_str());
    dataspace = ret_dataspace;
    if (join_table_sequence->size() <= 1) {
      LOG_ERROR(
          "Not support dependent subquery with no simple table in current "
          "rs.\n");
      throw NotSupportedError(
          "Not support dependent subquery with no simple table in current rs.");
    }
  }
  // TODO: rebuild the vector contains table_struct_vector and return the proper
  // dataspace.
  analysis_sequence_of_join_tables(join_table_sequence);

  if (!table_struct_vector_end) ret_dataspace = dataspace;

  set_join_final_table(NULL);
  if (!ret_dataspace->get_data_source() && !record_scan_need_table.count(rs)) {
    TableStruct *tmp_ts =
        get_join_final_table(*(record_scan_join_tables[rs]->end() - 1));
    table_link *par_table_t = table_struct_map[tmp_ts];
    if (left_join_table_position[rs].count(record_scan_join_tables[rs]->size() -
                                           1 + left_join_subquery_count))
      left_join_par_tables[par_table_t] =
          make_pair(rs, record_scan_join_tables[rs]->size() - 1 +
                            left_join_subquery_count);
    ++par_table_num;
    par_tables.push_back(par_table_t);
    need_clean_par_tables = true;
    record_scan_par_table_map[rs] = par_table_t;
    need_clean_record_scan_par_table_map = true;
  }

  record_scan_need_cross_node_join[rs] = true;
  if (rs->contains_left_join) {
    join_node *tables = rs->join_tables;
    while (tables && tables->right) {
      tables = tables->right;
    }
    if (tables && tables->type == JOIN_NODE_SUBSELECT) {
      ret_dataspace = table_subquery_final_dataspace[tables->sub_select];
    }
  }
  return ret_dataspace;
#endif

  ACE_UNUSED_ARG(rs);
  LOG_ERROR("SQL %s need cross node join.\n", sql);
  throw Error("There are two or more data space conflict.");
}

void Statement::set_vec_final_table(vector<vector<TableStruct *> *> *vec) {
  Backend *backend = Backend::instance();
  vector<vector<TableStruct *> *>::iterator it_v = vec->begin();
  for (; it_v != vec->end(); ++it_v) {
    vector<TableStruct *>::iterator it = (*it_v)->begin();
    TableStruct *tmp = *it;
    DataSpace *space1 = backend->get_data_space_for_table(
        (*it)->schema_name.c_str(), (*it)->table_name.c_str());
    if (space1->is_partitioned()) {
      (*it)->is_final = true;
      continue;
    }
    ++it;
    for (; it != (*it_v)->end(); ++it) {
      DataSpace *space2 = backend->get_data_space_for_table(
          (*it)->schema_name.c_str(), (*it)->table_name.c_str());
      if (space2->is_partitioned()) {
        tmp = *it;
        break;
      }
      if (stmt_session->is_dataspace_cover_session_level(space1, space2)) {
        // do nothing
      } else if (stmt_session->is_dataspace_cover_session_level(space2,
                                                                space1)) {
        space1 = space2;
        tmp = *it;
      }
    }
    tmp->is_final = true;
  }
}

void Statement::set_join_final_table(record_scan *rs) {
  if (rs) {
    if (!record_scan_join_tables.count(rs)) {
      LOG_ERROR("Fail to get join tables when set final join table.\n");
      throw Error(
          "Not support sql, fail to get join tables when set final join "
          "table.");
    }
    set_vec_final_table(record_scan_join_tables[rs]);
  } else {
    map<record_scan *, vector<vector<TableStruct *> *> *>::iterator it_m =
        record_scan_join_tables.begin();
    for (; it_m != record_scan_join_tables.end(); ++it_m) {
      set_vec_final_table(it_m->second);
    }
  }
}

TableStruct *Statement::get_join_final_table(vector<TableStruct *> *vec) {
  if (vec->empty()) {
    LOG_ERROR("Empty join vector.\n");
    throw Error("Not support sql, empty join vector.");
  }
  vector<TableStruct *>::iterator it = vec->begin();
  TableStruct *ret = *it;
  ++it;
  for (; it != vec->end(); ++it) {
    if ((*it)->is_final) {
      ret = *it;
      break;
    }
  }
  return ret;
}

void Statement::analysis_sequence_of_join_tables(
    vector<vector<TableStruct *> *> *join_table_sequence) {
  // TODO: rebuild the vector contains table_struct_vector used for cross node
  // join.
  ACE_UNUSED_ARG(join_table_sequence);
}

bool Statement::check_rs_contain_partkey_equal_value(record_scan *rs) {
  if (!rs->join_tables || rs->join_tables->type != JOIN_NODE_SINGLE)
    return false;
  const char *schema_name = rs->join_tables->schema_name
                                ? rs->join_tables->schema_name
                                : get_schema();
  const char *table_name = rs->join_tables->table_name;
  DataSpace *ds =
      Backend::instance()->get_data_space_for_table(schema_name, table_name);
  if (!ds || !ds->is_partitioned()) return false;
  PartitionedTable *partition_table = (PartitionedTable *)ds;
  const char *part_key = partition_table->get_key_names()->at(0);
  return check_express_contain_partkey_equal_value(
      rs->condition, schema_name, table_name, part_key, rs->join_tables->alias);
}

bool Statement::check_express_contain_partkey_equal_value(
    Expression *where_condition, const char *schema_name,
    const char *table_name, const char *key_name, const char *alias_name) {
  if (!where_condition) return false;
  if (where_condition->type == EXPR_AND) {
    BoolBinaryCaculateExpression *and_expr =
        (BoolBinaryCaculateExpression *)where_condition;
    bool ret1 = check_express_contain_partkey_equal_value(
        and_expr->left, schema_name, table_name, key_name, alias_name);
    bool ret2 = check_express_contain_partkey_equal_value(
        and_expr->right, schema_name, table_name, key_name, alias_name);
    return ret1 || ret2;
  } else if (where_condition->type == EXPR_EQ) {
    CompareExpression *eq_expr = (CompareExpression *)where_condition;
    if (is_column(eq_expr->left) && is_simple_expression(eq_expr->right)) {
      StrExpression *str_expr = (StrExpression *)eq_expr->left;
      if (column_name_equal_key_alias_without_len(str_expr->str_value,
                                                  schema_name, table_name,
                                                  key_name, alias_name)) {
        return true;
      }
    }
    if (is_column(eq_expr->right) && is_simple_expression(eq_expr->left)) {
      StrExpression *str_expr = (StrExpression *)eq_expr->right;
      if (column_name_equal_key_alias_without_len(str_expr->str_value,
                                                  schema_name, table_name,
                                                  key_name, alias_name)) {
        return true;
      }
    }
  }
  return false;
}

void Statement::handle_column_subquery(record_scan *root) {
  /*column subquerycolumn
   * a in (select ...)rs subquery_peer[rs] = a*/
  if (root->upper && root->condition_belong && root->condition_belong->parent &&
      (root->condition_belong->parent->type == EXPR_IN ||
       root->condition_belong->parent->type == EXPR_NOT_IN) &&
      root->subquerytype == SUB_SELECT_ONE_COLUMN) {
    //  In  Expr EXPR_STR 
    Expression *peer = get_bool_binary_peer(root->condition_belong);
    if (peer->type != EXPR_STR) {
      return;
    }
    //  And AND
    Expression *parent = root->condition_belong->parent;
    ConditionAndOr and_or = is_and_or_condition(parent, root->upper);
    if (and_or != CONDITION_AND) {
      return;
    }
    // dual
    if (root->first_table) {
      table_link *ltable = root->first_table;
      const char *table_name = ltable->join->table_name;
      if (!strcasecmp(table_name, "dual")) return;
    }
    string peer_string;
    peer->to_string(peer_string);
    /*subquery_peer
     * Statement::merge_parent_child_record_scan 
     * check_column_record_scan_can_merge */
    // here we do not handle the column, just store it.
    subquery_peer[root] = peer_string;
  }
}

#ifndef DBSCALE_DISABLE_SPARK

map<string, unsigned int> Statement::generate_table_partition_map(
    const char *spark_partition) {
  map<string, unsigned int> table_partition_map;
  vector<string> split_name;
  boost::split(split_name, spark_partition, boost::is_any_of(":"));
  unsigned int item_size = split_name.size();
  if (item_size % 3 != 0) {
    LOG_ERROR(
        "The executable comments of SPARK_PARTITION_NUM should be like "
        "[schema1:table1:num1:schema2:table2:num2]\n");
    throw Error(
        "The executable comments of SPARK_PARTITION_NUM should be like "
        "[schema1:table1:num1:schema2:table2:num2]");
  }

  for (unsigned int i = 0; i < item_size; i += 3) {
    string schema_name = split_name.at(i);
    string table_name = split_name.at(i + 1);
    string partition_num_str = split_name.at(i + 2);
    unsigned int partition_num = atoi(partition_num_str.c_str());
    string full_table_name(schema_name);
    full_table_name.append(".");
    full_table_name.append(table_name);
    table_partition_map[full_table_name] = partition_num;
  }
  return table_partition_map;
}

void Statement::analysis_spark_sql_join(ExecutePlan *plan) {
  Backend *backend = Backend::instance();
  bool is_insert_select = false;
  if (spark_insert) {
    is_insert_select = true;

    sql = select_sub_sql.c_str();
    Statement *new_stmt = NULL;
    re_parser_stmt(&(st.scanner), &new_stmt, sql);
  }

  LOG_DEBUG("Execute the sql using Spark SQL.\n");
  record_scan *rs = st.scanner;
  map<string, unsigned int> table_partition_map;

  if (st.exe_comment_spark_partition_num) {
    table_partition_map =
        generate_table_partition_map(st.exe_comment_spark_partition_num);
  }

  cj_garbage_collector.set_handler(plan->handler);
  ReconstructSQL *reconstruct_sql = new ReconstructSQL(
      schema, &st, "dbscale_tmp", stmt_session, DATA_MOVE_CENTER);
  reconstruct_sql->handler = plan->handler;
  cj_garbage_collector.set_reconstruct_sql(reconstruct_sql);

  vector<vector<TableStruct> > *table_vector_query =
      new vector<vector<TableStruct> >();
  cj_garbage_collector.set_table_vector_query(table_vector_query);

  vector<string> *generated_sql = new vector<string>();
  cj_garbage_collector.set_generated_sql(generated_sql);

  reconstruct_sql->set_orignal_sql(sql);

  set<string> table_name_set;
  table_link *ltable = rs->first_table;
  while (ltable) {
    const char *schema_name =
        ltable->join->schema_name ? ltable->join->schema_name : schema;
    const char *table_name = ltable->join->table_name;
    const char *alias =
        ltable->join->alias ? ltable->join->alias : ltable->join->table_name;

    // if the table is added previously, ignore it
    string full_table_name;
    splice_full_table_name(schema_name, table_name, full_table_name);
    if (table_name_set.count(full_table_name)) {
      ltable = ltable->next;
      continue;
    } else {
      table_name_set.insert(full_table_name);
    }
    TableStruct table_struct;
    table_struct.schema_name = schema_name;
    table_struct.table_name = table_name;
    table_struct.alias = alias;
    vector<TableStruct> table_vector;
    table_vector.push_back(table_struct);
    table_vector_query->push_back(table_vector);
    ltable = ltable->next;
  }

  CrossNodeJoinTask task = {
      rs, table_vector_query, generated_sql, NULL, NULL, NULL, NULL};
  bool use_spark_sql_method = true;
  reconstruct_sql->set_is_use_spark(use_spark_sql_method);
  reconstruct_sql->construct_mul_record_scan_sqls(task);
  vector<string> tmp_tables = reconstruct_sql->get_sorted_tmp_tables();
  DataSpace *ds =
      backend->get_data_space_for_table(spark_dst_schema.c_str(), NULL);
  if (ds->is_normal()) {
    if (!spark_without_insert) init_spark_schema(ds);

    string spark_dst_url = get_spark_dst_jdbc_url(ds);
    string final_sql = generated_sql->at(2 * tmp_tables.size());

    string target_name = spark_dst_schema;
    if (!spark_without_insert) {
      target_name.append(".spark_table");
      int cluster_id = backend->get_cluster_id();
      target_name.append("_");
      target_name.append(SSTR(cluster_id));

      string target_name_base = target_name;
      int loops_num = 0;
      target_name.append("_");
      target_name.append(SSTR(loops_num));
      while (!backend->try_add_spark_sql_table_name(target_name)) {
        ++loops_num;
        target_name = target_name_base;
        target_name.append("_");
        target_name.append(SSTR(loops_num));
      }
      cj_garbage_collector.set_spark_sql_table_name(target_name);
      string drop_sql = "DROP TABLE if exists ";
      drop_sql.append(target_name);
      cj_garbage_collector.set_spark_sql_query(drop_sql);
    }
    string master;
    string url = spark_dbscale_url;
    string dst_url = spark_dst_url;
    string user = spark_user;
    string pwd = spark_password;
    string job_id = plan->session->get_spark_job_id();
    plan->session->set_cur_spark_job_id(job_id);
    string executor_memory;
    int executor_cores;
    OptionParser::instance()->get_option_value(&master, "spark_master");
    OptionParser::instance()->get_option_value(&url, "spark_dbscale_url");
    OptionParser::instance()->get_option_value(&dst_url, "spark_dst_url");
    OptionParser::instance()->get_option_value(&user, "spark_user");
    OptionParser::instance()->get_option_value(&pwd, "spark_password");
    OptionParser::instance()->get_option_value(&executor_memory,
                                               "spark_executor_memory");
    OptionParser::instance()->get_option_value(&executor_cores,
                                               "spark_executor_cores");
    vector<SparkTableInfo> spark_table_infos;
    vector<string>::iterator it_sql = generated_sql->begin();
    vector<string>::iterator it = tmp_tables.begin();

    int loop_num = 0;
    for (; it != tmp_tables.end(); ++it) {
      // string create_sql = *it_sql; create sql ignore
      ++it_sql;
      string select_sql = *it_sql;
      ++it_sql;
      string select_first_part(select_sql, 0, 14);
      bool is_no_column_sql = false;
      if (!strcmp(select_first_part.c_str(), " SELECT 1 from"))
        is_no_column_sql = true;

      TableStruct ts = table_vector_query->at(loop_num).at(0);
      DataSpace *ds = backend->get_data_space_for_table(ts.schema_name.c_str(),
                                                        ts.table_name.c_str());
      int real_partition_num = 0;
      if (ds->is_partitioned()) {
        int par_num = ((PartitionedTable *)ds)->get_real_partition_num();
        if (is_no_column_sql)
          real_partition_num = par_num;
        else {
          string full_table_name;
          splice_full_table_name(ts.schema_name, ts.table_name,
                                 full_table_name);
          if (table_partition_map.count(full_table_name)) {
            real_partition_num = table_partition_map[full_table_name];
          } else if (st.exe_comment_default_partition_num) {
            real_partition_num = atoi(st.exe_comment_default_partition_num);
          } else {
            real_partition_num =
                ((PartitionedTable *)ds)->get_real_partition_num();
          }

          if (real_partition_num % par_num) {
            real_partition_num =
                real_partition_num + par_num - real_partition_num % par_num;
          }
        }

        LOG_DEBUG("The partition number is [%d] with sql [%s]\n",
                  real_partition_num, select_sql.c_str());
      }
      SparkTableInfo spark_table_info = {*it, ts.schema_name, ts.table_name,
                                         select_sql, real_partition_num};
      spark_table_infos.push_back(spark_table_info);
      ++loop_num;
      LOG_DEBUG("Spark parameter name = [%s], sql = [%s]\n", (*it).c_str(),
                select_sql.c_str());
    }

    LOG_DEBUG(
        "Spark parameter url=[%s], spark_dst_url=[%s], user=[%s], "
        "password=[%s], sql=[%s], dest_table=[%s], spark_master=[%s], "
        "job_id=[%s]\n",
        url.c_str(), spark_dst_url.c_str(), user.c_str(), pwd.c_str(),
        final_sql.c_str(), target_name.c_str(), spark_master.c_str(),
        job_id.c_str());

    SparkConfigParameter config_parameter;
    config_parameter.url = url;
    config_parameter.dst_url = dst_url;
    config_parameter.user = user;
    config_parameter.password = pwd;
    config_parameter.dbtable = spark_table_infos;
    config_parameter.sql = final_sql;
    config_parameter.dst_table = target_name;
    config_parameter.spark_master = spark_master;
    config_parameter.memory_limit = executor_memory;
    config_parameter.core_num = executor_cores;
    config_parameter.job_id = job_id;
    config_parameter.trans_method = spark_without_insert;

    if (spark_without_insert) {
      if (is_insert_select) {
        spark_node = plan->get_spark_node(config_parameter, true);
      } else {
        ExecuteNode *spark_node = plan->get_spark_node(config_parameter, false);
        plan->set_start_node(spark_node);
        set_cross_node_join(true);
      }
      return;
    }
    plan->session->start_executing_spark();
    SparkReturnValue return_value = run_spark_service(config_parameter);
    plan->session->stop_executing_spark();

    if (!return_value.success) {
      throw Error(return_value.error_string.c_str());
    }

    string planed_sql = "SELECT * FROM ";
    planed_sql.append(target_name);
    order_item *order_by_list = st.scanner->order_by_list
                                    ? st.scanner->order_by_list
                                    : st.scanner->group_by_list;
    if (order_by_list) {
      planed_sql.append(" ORDER BY ");
      do {
        string order_by_str;
        if (order_by_list->field->alias) {
          order_by_str = order_by_list->field->alias;
        } else {
          Expression *expr = order_by_list->field->field_expr;
          string schema_name, table_name, column_name;
          expr->to_string(order_by_str);
          split_value_into_schema_table_column(order_by_str, schema_name,
                                               table_name, column_name);
          order_by_str = column_name;
        }
        if (order_by_list != st.scanner->order_by_list) planed_sql.append(", ");
        planed_sql.append(order_by_str);
        order_by_list = order_by_list->next;
      } while (order_by_list != st.scanner->order_by_list);
    }

    string *tmp_sql = &(generated_sql->at(2 * tmp_tables.size()));
    *tmp_sql = planed_sql;
    sql = generated_sql->at(2 * tmp_tables.size()).c_str();
    ExecuteNode *node = plan->get_direct_execute_node(ds, sql);
    plan->set_start_node(node);
    set_cross_node_join(true);
  } else {
    throw Error("Not support partition table as final dataspace.\n");
  }
}
#endif

void Statement::analysis_center_join(ExecutePlan *plan) {
  LOG_DEBUG("Execute the sql using Center Join.\n");
  record_scan *rs = st.scanner;

  cj_garbage_collector.set_handler(plan->handler);
  ReconstructSQL *reconstruct_sql = new ReconstructSQL(
      schema, &st, "dbscale_tmp", stmt_session, DATA_MOVE_CENTER);
  cj_garbage_collector.set_reconstruct_sql(reconstruct_sql);

  vector<vector<TableStruct> > *table_vector_query =
      new vector<vector<TableStruct> >();
  cj_garbage_collector.set_table_vector_query(table_vector_query);

  vector<string> *generated_sql = new vector<string>();
  cj_garbage_collector.set_generated_sql(generated_sql);

  reconstruct_sql->set_orignal_sql(sql);

  set<string> table_name_set;
  table_link *ltable = rs->first_table;
  while (ltable) {
    const char *schema_name =
        ltable->join->schema_name ? ltable->join->schema_name : schema;
    const char *table_name = ltable->join->table_name;
    const char *alias =
        ltable->join->alias ? ltable->join->alias : ltable->join->table_name;

    // if the table is added previously, ignore it
    string full_table_name;
    splice_full_table_name(schema_name, table_name, full_table_name);
    if (table_name_set.count(full_table_name)) {
      ltable = ltable->next;
      continue;
    } else {
      table_name_set.insert(full_table_name);
    }

    TableStruct table_struct;
    table_struct.schema_name = schema_name;
    table_struct.table_name = table_name;
    table_struct.alias = alias;
    vector<TableStruct> table_vector;
    table_vector.push_back(table_struct);
    table_vector_query->push_back(table_vector);
    ltable = ltable->next;
  }

  CrossNodeJoinTask task = {
      rs, table_vector_query, generated_sql, NULL, NULL, NULL, NULL};
  reconstruct_sql->construct_mul_record_scan_sqls(task);
  set<string> tmp_tables = reconstruct_sql->get_tmp_tables();

  Backend *backend = Backend::instance();
  DataSpace *ds =
      backend->get_data_space_for_table("mytest", "center_join_dataspace");
  if (ds->get_data_source()) {
    string last_select_sql = generated_sql->at(2 * tmp_tables.size());
    LOG_DEBUG("The final center join sql is [%s]\n", last_select_sql.c_str());
    SeparatedExecNode *final_node =
        new LocalExecNode(this, st.scanner, plan->handler);
    ((LocalExecNode *)final_node)->set_new_sql(last_select_sql);

    set<string>::iterator it = tmp_tables.begin();
    vector<string>::iterator it_sql = generated_sql->begin();
    for (; it != tmp_tables.end(); ++it) {
      DataSpace *ds_tmp = ds->init_center_join_space(*it);
      cj_garbage_collector.add_data_space(ds_tmp);
      string create_sql = *it_sql;
      ++it_sql;
      string insert_sql = *it_sql;
      ++it_sql;

      record_scan *fake_rs = new record_scan();
      cj_garbage_collector.add_fake_record_scan(fake_rs);

      CenterJoinParameter center_join_parameter = {create_sql, insert_sql};
      SeparatedExecNode *sep_node = new CenterJoinNode(
          this, fake_rs, plan->handler, center_join_parameter);

      record_scan_dependent_map[st.scanner].push_back(fake_rs);
      exec_nodes.push_back(sep_node);
      need_clean_exec_nodes = true;
      record_scan_exec_node_map[fake_rs] = sep_node;
    }
    need_clean_exec_nodes = true;
    exec_nodes.push_back(final_node);
    record_scan_exec_node_map[st.scanner] = final_node;

    par_table_num = 0;
    num_of_separated_par_table = 0;
    par_tables.clear();
    num_of_separated_exec_space = 0;
    spaces.clear();
    spaces.push_back(ds);
  } else {
    throw Error("Not support partition table as final dataspace.\n");
  }
}

record_scan *Statement::find_root_union_record_scan(record_scan *rs,
                                                    record_scan **last_union) {
  *last_union = NULL;
  if (!rs->opt_union_all) *last_union = rs;
  while (rs->upper && rs->upper->is_select_union) {
    if (!rs->upper->opt_union_all) *last_union = rs->upper;
    rs = rs->upper;
  }
  return rs;
}
/*
 * This function is handling the leaf record scan in union statement, except the
 * last union left and right record scan if they are leaf. If the record scan is
 * cross node join, generate a new seperated node for it. Otherwise, ignore.
 */
void Statement::handle_union_all_record_scan_tree(record_scan *root,
                                                  record_scan *top_union_rs,
                                                  ExecutePlan *plan) {
  if (root->union_select_left && root->union_select_left->is_select_union) {
    ;
  } else if (root->union_select_left) {
    if (record_scan_need_cross_node_join.count(root->union_select_left)) {
      record_scan_dependent_map[top_union_rs].push_back(
          root->union_select_left);
      SeparatedExecNode *exec_node =
          create_one_separated_exec_node(root->union_select_left, plan);
      exec_nodes.push_back(exec_node);
      need_clean_exec_nodes = true;
      record_scan_exec_node_map[root->union_select_left] = exec_node;
    } else {
      list<record_scan *>::iterator it_rs =
          record_scan_dependent_map[root->union_select_left].begin();
      for (; it_rs != record_scan_dependent_map[root->union_select_left].end();
           ++it_rs) {
        record_scan_dependent_map[top_union_rs].push_back(*it_rs);
      }
    }
    DataSpace *ds_left = record_scan_one_space_tmp_map[root->union_select_left];
    if (ds_left && ds_left->is_partitioned()) {
      table_link *delete_par =
          record_scan_par_table_map[root->union_select_left];
      vector<table_link *>::iterator it2 = par_tables.begin();
      for (; it2 != par_tables.end(); ++it2) {
        table_link *delete_tmp = *it2;
        if (delete_tmp == delete_par) {
          par_tables.erase(it2);
          par_table_num--;
          break;
        }
      }
    }
  }

  if (root->union_select_right && root->union_select_right->is_select_union)
    ;
  else if (root->union_select_right) {
    if (record_scan_need_cross_node_join.count(root->union_select_right)) {
      record_scan_dependent_map[top_union_rs].push_back(
          root->union_select_right);
      SeparatedExecNode *exec_node =
          create_one_separated_exec_node(root->union_select_right, plan);
      exec_nodes.push_back(exec_node);
      need_clean_exec_nodes = true;
      record_scan_exec_node_map[root->union_select_right] = exec_node;
    } else {
      list<record_scan *>::iterator it_rs =
          record_scan_dependent_map[root->union_select_right].begin();
      for (; it_rs != record_scan_dependent_map[root->union_select_right].end();
           ++it_rs) {
        record_scan_dependent_map[top_union_rs].push_back(*it_rs);
      }
    }
    DataSpace *ds_right =
        record_scan_one_space_tmp_map[root->union_select_right];
    if (ds_right && ds_right->is_partitioned()) {
      table_link *delete_par =
          record_scan_par_table_map[root->union_select_right];
      vector<table_link *>::iterator it2 = par_tables.begin();
      for (; it2 != par_tables.end(); ++it2) {
        table_link *delete_tmp = *it2;
        if (delete_tmp == delete_par) {
          par_tables.erase(it2);
          par_table_num--;
          break;
        }
      }
    }
  }
}

map<string, string, strcasecomp> Statement::get_alias_table_map(
    record_scan *rs) {
  map<string, string, strcasecomp> ret;
  table_link *tl = rs->first_table;
  while (tl) {
    if (tl->join->cur_rec_scan == rs) {
      if (tl->join->alias) ret[tl->join->alias] = tl->join->table_name;
    }
    tl = tl->next;
  }
  return ret;
}

void Statement::get_all_join_table(join_node *jtable,
                                   vector<join_node *> *jtables) {
  if (jtable) {
    if (jtable->table_name) jtables->push_back(jtable);
    if (jtable->left) {
      get_all_join_table(jtable->left, jtables);
    }
    if (jtable->right) {
      get_all_join_table(jtable->right, jtables);
    }
  }
}
join_node *Statement::get_first_join_table(record_scan *rs) {
  join_node *join_table = rs->join_tables;
  while (join_table->type == JOIN_NODE_JOIN) {
    join_table = join_table->left;
  }
  return join_table;
}

string Statement::get_first_column(record_scan *rs) {
  string left_key =
      rs->field_list_head->alias ? rs->field_list_head->alias : "";
  if (!left_key.empty()) return left_key;
  if (rs->field_list_head->field_expr &&
      rs->field_list_head->field_expr->type == EXPR_STR) {
    rs->field_list_head->field_expr->to_string(left_key);
    if (!strcmp(left_key.c_str(), "*")) {
      join_node *first_table = get_first_join_table(rs);
      if (first_table->type == JOIN_NODE_SINGLE) {
        const char *schema_name =
            first_table->schema_name ? first_table->schema_name : schema;
        const char *table_name = first_table->table_name;
        TableInfoCollection *tic = TableInfoCollection::instance();
        TableInfo *ti = tic->get_table_info_for_read(schema_name, table_name);
        vector<TableColumnInfo> *column_info_vec;
        try {
          column_info_vec =
              ti->element_table_column->get_element_vector_table_column_info(
                  stmt_session);
        } catch (...) {
          LOG_ERROR(
              "Error occured when try to get table info column "
              "info(vector_table_column_info) of table [%s.%s]\n",
              schema_name, table_name);
          ti->release_table_info_lock();
          throw;
        }
        if (!column_info_vec->empty())
          left_key = column_info_vec->at(0).column_name;
        ti->release_table_info_lock();
      } else if (first_table->type == JOIN_NODE_SUBSELECT) {
        left_key = get_first_column(first_table->sub_select);
      }
      if (!strcmp(left_key.c_str(), "*")) left_key.clear();
    }
  }
  if (left_key.empty()) {
    LOG_ERROR("Union should give an alias for first field. \n");
    throw Error("Union should give an alias for first field.");
  }
  return left_key;
}

void Statement::revise_record_scans(record_scan *end_union_rs,
                                    record_scan *top_union_rs,
                                    ExecutePlan *plan,
                                    bool &right_part_re_distribute) {
  // 1. fetch the final dataspace
  list<record_scan *> record_scan_list = union_leaf_record_scan[top_union_rs];
  record_scan *first_record_scan = record_scan_list.front();
  DataSpace *right_ds =
      record_scan_one_space_tmp_map[end_union_rs->union_select_right];
  DataSpace *final_ds = right_ds;
  if (!right_ds) {
    list<record_scan *>::iterator it = record_scan_list.begin();
    while (!record_scan_one_space_tmp_map.count(*it) ||
           !record_scan_one_space_tmp_map[*it]) {
      ++it;
    }
    final_ds = record_scan_one_space_tmp_map[*it];
  }
  if (end_union_rs == top_union_rs) {
    /* situation 2*/
    record_scan_one_space_tmp_map[end_union_rs] = final_ds;
    num_of_separated_exec_space = spaces.size();
    spaces.push_back(final_ds);
  }
  /*
   * TODO:
   * 1. we can choose the key pos of left part according to the key pos of right
   * part
   * 2. if the left part is partition table and right part is not, we should
   * move the right part to the left part
   */
  // 2. the final dataspace is partition table, get the partition key we needed
  // for left
  string left_key = first_record_scan->field_list_head->alias
                        ? first_record_scan->field_list_head->alias
                        : "";
  if (final_ds->is_partitioned()) {
    if (left_key.empty()) {
      left_key = get_first_column(first_record_scan);
    }
  } else {
    // 3. reset the key if the dataspace is normal
    left_key = "";
  }

  left_key = get_column_name(left_key);
  string right_key;
  // 4. get the partition key we needed for right, if not partitioned, empty
  // string We always use the first select field item as key, if the partition
  // dataspace's key pos is not the first, we need to use a separate node(table
  // subquery) to re-distribute the data of right child record scan.
  if (right_ds && right_ds->is_partitioned()) {
    int right_key_position = check_rs_contains_partition_key(
        end_union_rs->union_select_right, right_ds);
    if (right_key_position != 0) {
      right_key = end_union_rs->union_select_right->field_list_head->alias
                      ? end_union_rs->union_select_right->field_list_head->alias
                      : "";
      if (right_key.empty()) {
        right_key = get_first_column(end_union_rs->union_select_right);
      }
    }
  }
  right_key = get_column_name(right_key);

  // 5. regard all the record scan before end_union_rs as union all
  first_record_scan = first_record_scan->upper;
  while (first_record_scan != end_union_rs) {
    first_record_scan->opt_union_all = true;
    first_record_scan = first_record_scan->upper;
  }

  // 6. handle the left record_scan, and use table subquery cross node join to
  // move the data of left child to right part.
  /*section Asection Bfinal ds*/
  end_union_rs->union_select_left->subquerytype = SUB_SELECT_TABLE;
  record_scan_dependent_map[top_union_rs].push_back(
      end_union_rs->union_select_left);
  table_subquery_final_dataspace[end_union_rs->union_select_left] = final_ds;
  dataspace_partition_key[end_union_rs->union_select_left] = left_key;

  if (!end_union_rs->union_select_left->is_select_union &&
      record_scan_one_space_tmp_map.count(end_union_rs->union_select_left) &&
      record_scan_one_space_tmp_map[end_union_rs->union_select_left] &&
      record_scan_one_space_tmp_map[end_union_rs->union_select_left]
          ->is_partitioned()) {
    table_link *delete_par =
        record_scan_par_table_map[end_union_rs->union_select_left];
    vector<table_link *>::iterator it2 = par_tables.begin();
    for (; it2 != par_tables.end(); ++it2) {
      table_link *delete_tmp = *it2;
      if (delete_tmp == delete_par) {
        par_tables.erase(it2);
        par_table_num--;
        break;
      }
    }
  }
  init_table_subquery_dataspace(end_union_rs->union_select_left, plan);
  SeparatedExecNode *exec_node =
      create_one_separated_exec_node(end_union_rs->union_select_left, plan);
  exec_nodes.push_back(exec_node);
  need_clean_exec_nodes = true;
  record_scan_exec_node_map[end_union_rs->union_select_left] = exec_node;

  // 7. handle the right record_scan, if the right key is not empty, means we
  // need to use a table subquery to re-distribute the data of right part, so
  // that the part key of the partition table should be the first field.
  /*section
   * B,Bselectselect*/
  if (right_ds && right_ds->is_partitioned() && !right_key.empty()) {
    end_union_rs->union_select_right->subquerytype = SUB_SELECT_TABLE;
    record_scan_dependent_map[top_union_rs].push_back(
        end_union_rs->union_select_right);
    table_subquery_final_dataspace[end_union_rs->union_select_right] = final_ds;
    dataspace_partition_key[end_union_rs->union_select_right] = right_key;
    init_table_subquery_dataspace(end_union_rs->union_select_right, plan);
    SeparatedExecNode *exec_node =
        create_one_separated_exec_node(end_union_rs->union_select_right, plan);
    exec_nodes.push_back(exec_node);
    need_clean_exec_nodes = true;
    record_scan_exec_node_map[end_union_rs->union_select_right] = exec_node;
    if (end_union_rs == top_union_rs && !top_union_rs->upper)
      need_reanalysis_space = true;
    right_part_re_distribute = true;
  }
}

string Statement::get_column_name(string value) {
  unsigned first_dot_position = value.find(".");
  unsigned second_dot_position = value.find(".", first_dot_position + 1);
  string ret;
  if (first_dot_position == (unsigned)string::npos) {
    ret = value;
  } else if (second_dot_position == (unsigned)string::npos) {
    ret = value.substr(first_dot_position + 1);
  } else {
    ret = value.substr(second_dot_position + 1);
  }
  return ret;
}

/* can merge situation:
 * 1. if all the related dataspaces are normal table, check if dataspace
 * covering is enough.
 * 2. if all the related dataspace are partition table, check if dataspace is
 * covering and the pos of partition key in select list should be the same.
 */
bool Statement::check_union_record_scan_can_merge(record_scan *root,
                                                  record_scan *top_union_rs) {
  bool can_merge = true;
  DataSpace *merge_ds = NULL;
  record_scan *merged_rs = NULL;
  list<record_scan *> record_scan_list = union_leaf_record_scan[top_union_rs];
  list<record_scan *>::iterator it = record_scan_list.begin();
  for (; it != record_scan_list.end(); ++it) {
    DataSpace *current_ds = NULL;
    if (record_scan_one_space_tmp_map.count(*it))
      current_ds = record_scan_one_space_tmp_map[*it];
    if (!merge_ds) {
      merge_ds = current_ds;
      merged_rs = *it;
      continue;
    }

    if (current_ds && merge_ds) {
      bool current_is_par_table = current_ds->is_partitioned();
      bool merge_is_par_table = merge_ds->is_partitioned();
      if (current_is_par_table && merge_is_par_table) {
        if (!is_partition_table_cover((PartitionedTable *)merge_ds,
                                      (PartitionedTable *)current_ds)) {
          can_merge = false;
          break;
        } else {
          int merged_key_position =
              check_rs_contains_partition_key(merged_rs, merge_ds);
          int current_key_position =
              check_rs_contains_partition_key(*it, current_ds);

          if (merged_key_position == -1 ||
              current_key_position != merged_key_position) {
            can_merge = false;
            break;
          }
        }
        table_link *delete_par = record_scan_par_table_map[*it];
        vector<table_link *>::iterator it2 = par_tables.begin();
        for (; it2 != par_tables.end(); ++it2) {
          table_link *delete_tmp = *it2;
          if (delete_tmp == delete_par) {
            par_tables.erase(it2);
            par_table_num--;
            break;
          }
        }
      } else if (!current_is_par_table && !merge_is_par_table) {
        if (stmt_session->is_dataspace_cover_session_level(merge_ds,
                                                           current_ds)) {
          // do nothing
        } else if (stmt_session->is_dataspace_cover_session_level(current_ds,
                                                                  merge_ds)) {
          merge_ds = current_ds;
        } else {
          can_merge = false;
          break;
        }
      } else {
        can_merge = false;
        break;
      }
    }
  }
  if (can_merge) {
    record_scan_one_space_tmp_map[root] = merge_ds;
    num_of_separated_exec_space = spaces.size();
    spaces.push_back(merge_ds);
  }
  return can_merge;
}

void Statement::handle_union_record_scan_tree(record_scan *root,
                                              record_scan *top_union_rs) {
  if (root->union_select_left && !root->union_select_left->is_select_union) {
    union_leaf_record_scan[top_union_rs].push_back(root->union_select_left);
  }

  if (root->union_select_right && !root->union_select_right->is_select_union) {
    union_leaf_record_scan[top_union_rs].push_back(root->union_select_right);
  }
}

bool Statement::child_rs_is_partitioned_space(record_scan *root) {
  record_scan *tmp = root->children_begin;
  for (; tmp; tmp = tmp->next) {
    if (child_rs_is_partitioned_space(tmp)) return true;
  }
  if (record_scan_one_space_tmp_map.count(root) &&
      record_scan_one_space_tmp_map[root] &&
      record_scan_one_space_tmp_map[root]->is_partitioned())
    return true;
  return false;
}

void Statement::print_sql_for_rs(record_scan *root) {
  string str_tmp;
  stmt_node *node = get_latest_stmt_node();
  if (root->end_pos - root->start_pos < 100)
    str_tmp.assign(node->handled_sql + root->start_pos - 1,
                   root->end_pos - root->start_pos + 1);
  else {
    str_tmp.assign(node->handled_sql + root->start_pos - 1, 48);
    str_tmp.append("...to...");
    str_tmp.append(node->handled_sql + root->end_pos - 49, 48);
  }
  root->record_sql = node->handled_sql + root->start_pos - 1;
  root->record_sql_len = root->end_pos - root->start_pos + 1;
  LOG_DEBUG("Record_scan [%@] sql [%s]\n", root, str_tmp.c_str());
}

bool Statement::check_rs_has_table(record_scan *root) {
  record_scan *tmp = root->children_begin;
  for (; tmp; tmp = tmp->next) {
    if (check_rs_has_table(tmp)) return true;
  }
  if (root->first_table) return true;
  record_scan_has_no_table.insert(root);
  return false;
}

void Statement::analysis_record_scan_tree(record_scan *root, ExecutePlan *plan,
                                          unsigned int recursion_level) {
  if (st.type == STMT_CREATE_SELECT || st.type == STMT_CREATE_LIKE) {
    LOG_ERROR(
        "Not support CREATE SELECT or CREATE LIKE statement when it contains "
        "partition table "
        "or can not merge.\n");
    throw NotSupportedError(
        "Not support CREATE SELECT or CREATE LIKE statement when it contains "
        "partition table "
        "or can not merge.");
  }
  if (recursion_level > (max_union_all_sqls > MAX_RECURSION_LEVEL
                             ? max_union_all_sqls
                             : MAX_RECURSION_LEVEL)) {
    LOG_ERROR("Reach max recursion level [%d] with [%d] for sql[%s].\n",
              (max_union_all_sqls > MAX_RECURSION_LEVEL ? max_union_all_sqls
                                                        : MAX_RECURSION_LEVEL),
              recursion_level, sql);
    throw Error("Reach max recursion level.");
  }
  if (root->is_select_union && !check_rs_has_table(root)) {
    LOG_DEBUG(
        "Union with out table, ignore analysis record scan tree for this "
        "union.\n");
    return;
  }
  stmt_type type = st.type;
  if (st.type == STMT_DBSCALE_ESTIMATE) type = st.estimate_type;
  record_scan *tmp = root->children_begin;
  for (; tmp; tmp = tmp->next) {
    analysis_record_scan_tree(tmp, plan, recursion_level + 1);
  }

  print_sql_for_rs(root);

  if (root->is_select_union) {
    /*  DBScale union
     *
     *  union/union
     allrsdataspace2merge(
     Statement.cc::check_union_record_scan_can_merge)
        1. if all the related dataspaces are normal table, check if dataspace
     covering is enough.
        2. if all the related dataspace are partition table, check if dataspace
     is covering and the pos of partition key in select list should be the same.

        Unionunion/union
     all3unionunion, last_union_rs,
     statement.ccfind_root_union_record_scan
        1. union section A
        2. union rssection B
        3. unionrsunion all, union
     allsection C

        A union all B union all C union D union F union ALL G
        union ...D union F...section A  A ... D; section BF
     section C sectin A union section B union ALL G section
     Aunionunion allsection
     Cunion section A 
     Bmergeunion if Section A can not
     merge with Section B. If B is normal table, generate Separated Node for
     Section A, dataspace is B. If B is partition table, generate Separated Node
     for Section A, dataspace is B, and the partition key should be the first
     select list value. After that, we should check if the pos of partition key
     in select list of B is 0, if 0, ignore the creation of the Seperated Node,
     if 1, create Seperated Node for section B. section Asection
     Bunion

        section C
        situation 1. union/union allunion alltop union
     union all situation 2. top
     unionunion allsection Asection Btop
     unionrs if the Section A+B have no parent, it would generate
     DirectExecuteNode or SendNode-FetchNode. if the Section A+B have parent,
     and not table_subquery, use previous logic. if the Section A+B contains
     parent, and table subquery. The seperated Node of it should not be UNION
     ALL table subquery. situation 3. top unionunion
     allsection Asection Bsection Csection Crs
           if the Section C have no parent, it would generate union all node.
           if the Section C have parent, and not table_subquery, use previous
     logic. if the Section C have parent, and table_subquery, The seperated Node
     of it should be UNION ALL table subquery.
     *
     *
     * */

    record_scan *last_union_rs = NULL;
    /*uniontop_union_rslast_union_rsrsunion/union
     allrslast_union_rs
     uniontop_union_rsunion/union all
     last_union_rssituation 1; last_union_rs ==
     top_union_rs situation 2situation 3*/
    record_scan *top_union_rs =
        find_root_union_record_scan(root, &last_union_rs);
    if (last_union_rs_map.count(top_union_rs)) {
      last_union_rs = last_union_rs_map[top_union_rs];
    } else {
      last_union_rs_map[top_union_rs] = last_union_rs;
    }

    if (!last_union_rs) {  // no last union rs means union all statements.
      /*situation 1*/
      Backend *backend = Backend::instance();
      // for union, we add a catalog for the union record scan
      DataSpace *space = backend->get_data_space_for_table(NULL, NULL);
      record_scan_one_space_tmp_map[root] = space;
      handle_union_all_record_scan_tree(root, top_union_rs, plan);
    } else {  // union statement should do seperatedly when we met last union
              // rs.
      handle_union_record_scan_tree(root, top_union_rs);
      if (root == last_union_rs) {
        /* we should check the union can merge
         * if can merge, treat it as normal record scan
         * else we can generate seperated node for it.
         */
        bool can_merge = check_union_record_scan_can_merge(root, top_union_rs);
        LOG_DEBUG("Found union, the union is can merge = [%d]\n", can_merge);
        if (!can_merge) {
          /* situaiton 23*/
          bool right_part_re_distribute = false;
          revise_record_scans(last_union_rs, top_union_rs, plan,
                              right_part_re_distribute);
          if (!right_part_re_distribute) {
            if (record_scan_dependent_map.count(root->union_select_right) &&
                !record_scan_dependent_map[root->union_select_right].empty()) {
              list<record_scan *>::iterator it_rs =
                  record_scan_dependent_map[root->union_select_right].begin();
              for (; it_rs !=
                     record_scan_dependent_map[root->union_select_right].end();
                   ++it_rs) {
                record_scan_dependent_map[top_union_rs].push_back(*it_rs);
              }
            }
          }
        } else {
          if (last_union_rs == top_union_rs && top_union_rs->upper &&
              !top_union_rs->upper->upper &&
              (type == STMT_INSERT_SELECT || type == STMT_REPLACE_SELECT)) {
            if (spaces.size() > 1) {
              while (num_of_separated_exec_space--) {
                spaces.erase(spaces.begin());
              }
              num_of_separated_exec_space = 0;
            }
          }
          if (last_union_rs == top_union_rs && !top_union_rs->upper)
            need_reanalysis_space = true;
          // for union can merge situation, we should add the child
          // record_scan_dependent_map to top union
          if (record_scan_dependent_map.count(root->union_select_left) &&
              !record_scan_dependent_map[root->union_select_left].empty()) {
            list<record_scan *>::iterator it_rs =
                record_scan_dependent_map[root->union_select_left].begin();
            for (; it_rs !=
                   record_scan_dependent_map[root->union_select_left].end();
                 ++it_rs) {
              record_scan_dependent_map[top_union_rs].push_back(*it_rs);
            }
          }
          if (record_scan_dependent_map.count(root->union_select_right) &&
              !record_scan_dependent_map[root->union_select_right].empty()) {
            list<record_scan *>::iterator it_rs =
                record_scan_dependent_map[root->union_select_right].begin();
            for (; it_rs !=
                   record_scan_dependent_map[root->union_select_right].end();
                 ++it_rs) {
              record_scan_dependent_map[top_union_rs].push_back(*it_rs);
            }
          }
        }
        last_union_rs_map[top_union_rs] = NULL;
      } else {
        // Before last_union_rs, we think the union is union all sub.
        handle_union_all_record_scan_tree(
            root, last_union_rs->union_select_left, plan);
      }
    }

    if (!root->upper) {
      SeparatedExecNode *exec_node = create_one_separated_exec_node(root, plan);
      exec_nodes.push_back(exec_node);
      need_clean_exec_nodes = true;
      record_scan_exec_node_map[root] = exec_node;
      if (spaces.size() > 1) {
        while (num_of_separated_exec_space--) {
          spaces.erase(spaces.begin());
        }
        num_of_separated_exec_space = 0;
      }
    }
    return;
  }
  int record_scan_position = 0;
  if (!cross_node_join && root->upper && root->condition &&
      !(!root->upper->upper && type != STMT_SELECT)) {
    /*dbscale correlated subquery
     *
     * 1.
     analysis_record_scan_treerecord_scanrscheck_record_scan_independentrsTableStruct
       2.  rs2`record_scan_need_table
     map<record_scan, TableStruct>`  `subquery_missing_table map<record_scan,
     TableStruct>`
           record_scan_need_tablekeyrsvaluetablestructsubquery_missing_tablekeyrsvaluetablestruct
           
           a. rs b.
     rsrsc.
     rsrsrsd. e.
     rs
           if
       3.
     subquery_missing_tablersrsdataspacehandle_cross_node_joinsubquery_missing_table
       4.
     record_scan_need_tablersrsdataspacerstable_structrstable
     struct grouptable
     structsqlrstable_structdataspacehandle_cross_node_joinrecord_scan_need_table
       5.
     rsrsdataspace
       6.
     record_scan_need_tablerssrc/sub_query.cc/SimpleCrossNodeJoinNode::do_pre_executeoptimizer.construct_merge_all_sqlstable_struct2:
           a.
     CNJAnalysisUtil::check_column_belongs_totable_struct; b.
     ConstructMethod::generate_last_select_sql
     last_select_sqlfromtable_struct
     * */

    vector<vector<TableStruct> > table_vector;

    vector<TableStruct> one_table_vector;
    table_vector.push_back(one_table_vector);
    get_all_table_vector(root, &table_vector);
    set<TableStruct> table_struct;
    // Check the record scan is individual or not.
    int independent = CheckComponent::check_record_scan_independent(
        &table_vector, root, schema, &table_struct, stmt_session);
    if (INDEPENDENT_TRUE != independent) {
      if (INDEPENDENT_FALSE == independent) {
        if (table_struct.size() > 1) {
          LOG_ERROR(
              "Not support dependent subquery contains two different table.\n");
          throw NotSupportedError(
              "Not support dependent subquery contains two different table.");
        }
        set<TableStruct>::iterator table = table_struct.begin();
        TableStruct new_table = check_parent_table_alias(root->upper, *table);
        /* Currently, we do not support record scan contains two dependent
         * clidren which does not contains the same table. Currently, we do not
         * support can merge dataspace.
         * TODO: We need to handle the can merge situation.
         */
        if (subquery_missing_table.count(root->upper) &&
            subquery_missing_table[root->upper] != new_table) {
          LOG_ERROR(
              "Not support multiple subqueries dependent different tables.\n");
          throw NotSupportedError(
              "Not support multiple subqueries dependent different tables.");
        }

        record_scan *tmp = root->children_begin;
        for (; tmp; tmp = tmp->next) {
          if (record_scan_need_table.count(tmp) &&
              subquery_missing_table.count(root)) {
            LOG_ERROR(
                "Not support dependent subquery contains dependent "
                "subquery.\n");
            throw NotSupportedError(
                "Not support dependent subquery contains dependent subquery.");
          }
        }

        if (root->upper->is_select_union) {
          LOG_ERROR("Found dependent subquery in UNION sub select.\n");
          string error_info("Found dependent subquery in UNION sub select.");
          LOG_ERROR("%s\n", error_info.c_str());
          throw Error(error_info.c_str());
        }
        Backend *backend = Backend::instance();
        DataSpace *ds_dependent = backend->get_data_space_for_table(
            new_table.schema_name.c_str(), new_table.table_name.c_str());
        /* maybe the record scan contains unsupported function. so we check it.
         */
        if (ds_dependent->is_partitioned() &&
            (check_field_list_contains_invalid_function(root) ||
             check_field_list_invalid_distinct(root))) {
          LOG_ERROR(
              "We do not support dependent subquery with a not supported "
              "function. \n");
          throw Error(
              "We do not support dependent subquery with a not supported "
              "function.");
        }
        subquery_missing_table[root->upper] = new_table;
        record_scan_need_table[root] = new_table;
        root->subquerytype = SUB_SELECT_DEPENDENT;
      }
    }
  }
  /*DBScale  in/not in 

    
     1.  in/not in rs
        handle_column_subquery
     2. INmerge
         ... where t1.a in (select c ...from t2)
        t2rsfinial dataspacet1merge

        mergeStatement::merge_parent_child_record_scancheck_column_record_scan_can_merge

        mergetable_structposcan_merge_record_scan_position
        merge
          a. SUB_SELECT_ONE_COLUMN 
    use_table_for_one_column_subquery 
    0 t1.a in (1, 2,
    3)merge
          b.rscolumn_table_subqueryrsdataspacemerge
     3.
    column_table_subquerySeparatedExecNode::execute_column_table_subquery
     4.
    mergecheck_column_record_scan_can_mergemergerstable_struct
    vec
        ,expression.cc::SubSelectExpression::do_trace_expression_tree


  */
  if (!record_scan_need_table.count(root)) {
    LOG_DEBUG("Do the handle_column_subquery. \n");
    handle_column_subquery(root);
  }

  /*
    DBScalerecord_scanmerge

can
mergerecord_scanrecord_scan
2dataspace

merge

1. `record_scan`,
normalmergedbscale`is_dataspace_cover(ds1,
ds2)`
2.  ( `record_scan` ) , `partition``merge`
`partition`WHERE
ANDdbscale`check_two_partition_table_can_merge(rs, tb_link,
tb1, tb2);`
3.  ( `record_scan` ) ,
normpartitionmergenormalpartitiondbscale
Session::is_dataspace_cover_session_level

record_scan2record_scanfinial_dataspace
record_scan_one_space_tmp_mapmergeStatement::merge_parent_child_record_scan

4.
mergenormaldbscaleis_dataspace_cover(ds1,
ds2)merge
5.
mergepartitionmergepartitionpart1.key
= (select part2.key from part2), IN SOME, ANY ...
6.
mergenormalpartitionmergenormalpartition

rsrsmergestatement.cc::analysis_record_scan_tree
bool all_child_par_can_merge =
false;rsdataspace

7. mergenormal1,4
8.
mergepartitionpartition
group bygroup byexpr
9. merge
partitionnormalpartitionnormal

10. dual 3
    a. rsfinial
dataspacedualmerge (select 1
from dual) statement.cc::merge_parent_child_record_scan 15441   if
(child_rs->first_table && child_rs->first_table->join) { 15442     string
table_name = child_rs->first_table->join->table_name; 15443     if
(!table_name.compare("dual")) 15444       return parent_ds; 15445   } b.
rsdbscale_tmp.dualmergedbscale_tmp.dualdataspace
Backend::initialize()authauthspacemerge
     c.
rsdbscale_tmpdualdualdualcatalogsqldualdbscale

  */

  /*
   * 1. try to merge the par tables inside this record scan
   *
   * 2. try to merge the normal table dataspaces inside this record scan
   *
   * 3. try to merge with childrens
   *
   * 4. try to merge with the output dataspaces of separated exec table
   * subquery
   *
   * */
  DataSpace *par_ds = NULL, *ds = NULL, *merged_ds = NULL;
  DataSpace *modify_ds = NULL;
  bool need_handle_cross_node = false;
  /*Firstly merge the par tables*/
  int par_table_num_cur_rs = 0;

  bool need_use_extra_subquery = false;
  if (record_scan_all_par_tables_map.count(root) &&
      !record_scan_all_par_tables_map[root].empty()) {
    par_table_num_cur_rs = record_scan_all_par_tables_map[root].size();
    if (!cross_node_join && par_table_num_cur_rs &&
        (check_field_list_contains_invalid_function(root) ||
         check_field_list_invalid_distinct(root)) &&
        !record_scan_need_table.count(root)) {
      /*DBScale max/min/avg/sum/countgroup
        bydisctict

        1. mysqlmysql
        2. correlated
        subqueryrecord_scandataspace

        :
        1.
        check_field_list_contains_invalid_functioncheck_field_list_invalid_distinctcorrelated
        subquery
        2.  dbscale_tmp.dual  record scan Dataspace
           DataSpace`dbscale_tmp.dual`, `dbscale_tmp.dual`
         auth datasource DataSpace 
        mergenormal
           authDataSpaceDataSource
        ( SimpleCrossNodeJoinNode::handle_last_create_dataspace)*/
      if (!subquery_missing_table.count(root)) {
        TableStruct table_struct;
        table_struct.schema_name = "dbscale_tmp";
        table_struct.table_name = "dual";
        table_struct.alias = "dual";
        record_scan_need_table[root] = table_struct;
        need_handle_cross_node = true;
        need_use_extra_subquery = true;
      }
    }
  }
  if (par_table_num_cur_rs) {
    par_ds = merge_par_tables_one_record_scan(root);
    if (!par_ds || need_handle_cross_node) {
      LOG_DEBUG(
          "Do handle_cross_node_join after "
          "merge_par_tables_one_record_scan.\n");
      par_ds = handle_cross_node_join(root, plan);
    }
    if (!par_ds->get_data_source()) {
      par_table_num_cur_rs = 1;
    } else {
      par_table_num_cur_rs = 0;
    }
  }

  ds = par_ds;

  /*Then merge the normal tables*/
  if (!record_scan_need_cross_node_join.count(root)) {
    if (record_scan_all_spaces_map.count(root) &&
        !record_scan_all_spaces_map[root].empty()) {
      if (par_ds && check_left_join_with_cannot_merge_right_par(root)) {
        LOG_DEBUG(
            "Set ds to NULL cause check_left_join_with_cannot_merge_right_par "
            "is true.\n");
        ds = NULL;
      } else
        ds = merge_dataspace_one_record_scan(root, par_ds);
      if (!ds) {
        LOG_DEBUG(
            "Do handle_cross_node_join after "
            "check_left_join_with_cannot_merge_right_par or "
            "merge_dataspace_one_record_scan.\n");
        ds = handle_cross_node_join(root, plan);
      }
      if (!ds->get_data_source()) {
        par_table_num_cur_rs = 1;
      } else {
        par_table_num_cur_rs = 0;
      }
    }
  }

  if (!need_use_extra_subquery &&
      (root->options & SQL_OPT_DISTINCT ||
       root->options & SQL_OPT_DISTINCTROW) &&
      child_rs_is_partitioned_space(root)) {
    if (check_field_list_invalid_distinct(root) &&
        !record_scan_need_table.count(root)) {
      // TODO: the invalid func also need to be check here
      if (!subquery_missing_table.count(root)) {
        TableStruct table_struct;
        table_struct.schema_name = "dbscale_tmp";
        table_struct.table_name = "dual";
        table_struct.alias = "dual";
        record_scan_need_table[root] = table_struct;
        need_handle_cross_node = true;
        need_use_extra_subquery = true;
      }
    }
  }
  /*dbscaleleft join
      Left Join 
      1. A left join B left join C
      2. A left join B left join (subquery)C  left
   joincontains_left_join_subquery

     
         left joinleft join
   joinjoin
        join can merge
    A  B  left join
        1. B AB
        2. B  A  B
   
        3. B  B  B
     2 (handle_cross_node_join)
        4. B  B  B  1
   

        34
   BDataMoveJoinInfo::handle_analysis_left_join_expr_str_expressionConstructMethod::generate_left_join_sql.
        BCinsert
   selectBBSimpleCrossNodeJoinNode::replace_left_join_tmp_table

    
        1.  left join  
           A LEFT JOIN B LEFT JOIN (subquery)C  3
           A LEFT JOIN B LEFT JOIN (subquery)C LEFT JOIN D LEFT JOIN (subquery)F
    3  5
        2.  left_join_table_position 
   left_join_tables
   (Statement::handle_cross_node_joinleft_join_subquery_count)
        3.  subquery  table subquery 
   
        4. table
   subquery(Statement::handle_cross_node_join_with_subquery)
   *
   *
   * */

  if (ds && !record_scan_need_cross_node_join.count(root) &&
      contains_left_join_subquery(root, ds)) {
    LOG_DEBUG(
        "Do handle_cross_node_join due to "
        "!record_scan_need_cross_node_join.count and "
        "contains_left_join_subquery.\n");
    ds = handle_cross_node_join(root, plan);
    if (ds->is_partitioned()) par_table_num_cur_rs = 1;
  }

  if (ds) {
    record_scan_one_space_tmp_map[root] = ds;
    need_clean_record_scan_one_space_tmp_map = true;
  }

  merged_ds = ds;

  if (!root->upper &&
      (type == STMT_UPDATE || type == STMT_DELETE ||
       type == STMT_INSERT_SELECT || type == STMT_REPLACE_SELECT)) {
    modify_ds = merged_ds;
  }

  /*Start to merge with children*/
  if (root->children_begin) {
    if (!root->upper &&
        (type == STMT_INSERT_SELECT || type == STMT_REPLACE_SELECT)) {
      merged_ds = NULL;
      /*For insert...select, it will only has one direct child record_scan.*/
      if (record_scan_need_cross_node_join.count(root->children_begin) &&
          record_scan_need_cross_node_join[root->children_begin]) {
        record_scan_dependent_map[root].push_back(root->children_begin);
        SeparatedExecNode *exec_node =
            create_one_separated_exec_node(root->children_begin, plan);
        exec_nodes.push_back(exec_node);
        need_clean_exec_nodes = true;
        record_scan_exec_node_map[root->children_begin] = exec_node;
      } else if (root->children_begin->is_select_union &&
                 root->children_begin->opt_union_all) {
        // find a partition key for select stmt
        list<record_scan *> record_scan_list =
            union_leaf_record_scan[root->children_begin];
        record_scan *first_record_scan;
        if (record_scan_list.empty()) {
          record_scan *tmp_child = root->children_begin;
          while (tmp_child->is_select_union) {
            tmp_child = tmp_child->children_begin;
          }
          first_record_scan = tmp_child;
        } else
          first_record_scan = record_scan_list.front();
        string left_key = first_record_scan->field_list_head->alias
                              ? first_record_scan->field_list_head->alias
                              : "";
        if (left_key.empty()) {
          left_key = get_first_column(first_record_scan);
        }
        root->children_begin->subquerytype = SUB_SELECT_TABLE;
        record_scan_one_space_tmp_map[root->children_begin] =
            record_scan_one_space_tmp_map[root];
        table_subquery_final_dataspace[root->children_begin] =
            record_scan_one_space_tmp_map[root->children_begin];
        dataspace_partition_key[root->children_begin] = left_key;
        init_table_subquery_dataspace(root->children_begin, plan);
        record_scan_dependent_map[root].push_back(root->children_begin);
        SeparatedExecNode *exec_node =
            create_one_separated_exec_node(root->children_begin, plan);
        exec_nodes.push_back(exec_node);
        need_clean_exec_nodes = true;
        record_scan_exec_node_map[root->children_begin] = exec_node;
        DataSpace *tmp_ds = record_scan_one_space_tmp_map[root];

        if (tmp_ds->is_partitioned()) {
          merged_ds = NULL;
          need_reanalysis_space = 1;
        } else {
          merged_ds = tmp_ds;
        }
        exec_node->set_subquery_to_select_stmt(true);
      } else if (record_scan_dependent_map.count(root->children_begin) &&
                 !record_scan_dependent_map[root->children_begin].empty()) {
        list<record_scan *>::iterator it =
            record_scan_dependent_map[root->children_begin].begin();
        for (; it != record_scan_dependent_map[root->children_begin].end();
             ++it)
          record_scan_dependent_map[root].push_back((*it));
      }

      if (!merged_ds)
        merged_ds =
            merge_parent_child_record_scan(root, root->children_begin, 0, plan);

      num_of_separated_exec_space = spaces.size();
      num_of_separated_par_table = par_table_num - par_table_num_cur_rs;
      if (record_scan_one_space_tmp_map.count(root->children_begin) &&
          record_scan_one_space_tmp_map[root->children_begin] &&
          !record_scan_one_space_tmp_map[root->children_begin]
               ->get_data_source())
        num_of_separated_par_table--;
      if (!merged_ds) {
#ifdef DEBUG
        /*If insert part can not merge the select part, the select part must
         * has dataspace.*/
        ACE_ASSERT(record_scan_one_space_tmp_map.count(root->children_begin));
        ACE_ASSERT(record_scan_one_space_tmp_map[root->children_begin]);
#endif
        if (!record_scan_one_space_tmp_map.count(root->children_begin) ||
            !record_scan_one_space_tmp_map[root->children_begin])
          throw NotSupportedError("Internal handle exception.");
        spaces.push_back(record_scan_one_space_tmp_map[root->children_begin]);
        spaces.push_back(record_scan_one_space_tmp_map[root]);

        if (!record_scan_one_space_tmp_map[root]->is_tmp_table_space() &&
            record_scan_one_space_tmp_map[root->children_begin]
                ->is_tmp_table_space()) {
          need_reanalysis_space =
              1;  // always do the re-analysis for client insert...select stmt
                  // if the select dataspace is a non-dup tmp dataspace client
                  // stmt insert table should not be tmp table.
        }
        need_clean_spaces = true;
        if (need_reanalysis_space) num_of_separated_par_table = 0;

        SeparatedExecNode *exec_node =
            create_one_separated_exec_node(root, plan);
        exec_nodes.push_back(exec_node);
        need_clean_exec_nodes = true;
        record_scan_exec_node_map[root] = exec_node;
      } else {
        spaces.push_back(merged_ds);
        need_clean_spaces = true;
        SeparatedExecNode *exec_node =
            create_one_separated_exec_node(root, plan);
        exec_nodes.push_back(exec_node);
        need_clean_exec_nodes = true;
        record_scan_exec_node_map[root] = exec_node;
      }
      if (exec_nodes.size() == 1) {
        while (num_of_separated_exec_space--) {
          spaces.erase(spaces.begin());
        }
      }
      return;
    }

    tmp = root->children_begin;
    bool all_child_par_can_merge = false;
    if (!merged_ds) {
      /*If current record_scan has no table, and only have more than one
       * partition table child record_scan. DBScale should try to merge these
       * child record_scan. In some case, they can execute together.*/
      vector<record_scan *> par_table_rs_vec;
      vector<record_scan *> normal_table_rs_vec;
      for (; tmp; tmp = tmp->next) {
        if (record_scan_par_table_map.count(tmp))
          par_table_rs_vec.push_back(tmp);
        else
          normal_table_rs_vec.push_back(tmp);
      }
      size_t num = par_table_rs_vec.size();

      LOG_DEBUG("merged_ds %@, num %d, has distinct %d.\n", merged_ds, num,
                (root->options & SQL_OPT_DISTINCT ||
                 root->options & SQL_OPT_DISTINCTROW)
                    ? 1
                    : 0);
      if (num > 0 && (root->options & SQL_OPT_DISTINCT ||
                      root->options & SQL_OPT_DISTINCTROW)) {
        // If there is partition table, should check the root record scan does
        // not have distinct
        all_child_par_can_merge = false;
      } else if (num >= 2) {
        all_child_par_can_merge = true;
        vector<record_scan *>::iterator it = par_table_rs_vec.begin();
        record_scan *tmp_par_rs = *it;
        ++it;
        for (; it != par_table_rs_vec.end(); ++it) {
          if (!can_two_par_table_record_scan_merge(tmp_par_rs, *it)) {
            all_child_par_can_merge = false;
            break;
          }
        }
        if (all_child_par_can_merge) {
          vector<record_scan *>::iterator n_it = normal_table_rs_vec.begin();
          for (; n_it != normal_table_rs_vec.end(); ++n_it) {
            if (!can_par_global_table_record_scan_merge(tmp_par_rs, *n_it)) {
              all_child_par_can_merge = false;
              break;
            }
          }
        }
      } else if (num == 1 && !normal_table_rs_vec.empty()) {
        all_child_par_can_merge = true;
        record_scan *tmp_par_rs = par_table_rs_vec[0];
        vector<record_scan *>::iterator n_it = normal_table_rs_vec.begin();
        for (; n_it != normal_table_rs_vec.end(); ++n_it) {
          if (!can_par_global_table_record_scan_merge(tmp_par_rs, *n_it)) {
            all_child_par_can_merge = false;
            break;
          }
        }
      }
      if (all_child_par_can_merge) {
        merged_ds = record_scan_one_space_tmp_map[par_table_rs_vec[0]];
        record_scan_one_space_tmp_map[root] = merged_ds;
        need_clean_record_scan_one_space_tmp_map = true;
        // remove the merged par tables from vector par_tables
        vector<record_scan *>::iterator it = par_table_rs_vec.begin();
        ++it;
        table_link *delete_par = NULL, *delete_tmp = NULL;
        for (; it != par_table_rs_vec.end(); ++it) {
          delete_par = record_scan_par_table_map[*it];
          vector<table_link *>::iterator it2 = par_tables.begin();
          for (; it2 != par_tables.end(); ++it2) {
            delete_tmp = *it2;
            if (delete_tmp == delete_par) {
              par_tables.erase(it2);
              par_table_num--;
              break;
            }
          }
        }
      }
    }
    if (!all_child_par_can_merge) {
      /*Normal process for merge the parent with children.*/
      tmp = root->children_begin;
      for (; tmp; tmp = tmp->next) {
        /*Handle the situation that the subquery is a UNION query.
         *Handle the situation that table subquery
         *parent has no table, current space is a partition table,
         *and has more than one subquery.
         *
         *If the parent record_can has distinct, and the subquery has partition
         *table
         *
         *For the situation mentioned above, will not merge the child with
         *parent, treate them as seperated execute node.
         */
        if ((!record_scan_one_space_tmp_map[root] &&
             record_scan_one_space_tmp_map[tmp] &&
             !record_scan_one_space_tmp_map[tmp]->get_data_source() &&
             tmp->subquerytype == SUB_SELECT_TABLE && tmp->next) ||
            (tmp->is_select_union && !record_scan_has_no_table.count(tmp)) ||
            ((root->options & SQL_OPT_DISTINCT ||
              root->options & SQL_OPT_DISTINCTROW) &&
             record_scan_one_space_tmp_map[tmp] &&
             record_scan_one_space_tmp_map[tmp]->is_partitioned()))
          merged_ds = NULL;
        else
          merged_ds = merge_parent_child_record_scan(
              root, tmp, record_scan_position, plan);

        if (table_subquery_final_dataspace.count(tmp)) {
          if (table_subquery_final_dataspace[tmp]->get_data_source())
            par_table_num_cur_rs = 0;
          else
            par_table_num_cur_rs = 1;
        }

        LOG_DEBUG(
            "merge_parent_child_record_scan, par_table_num_cur_rs=[%d], "
            "par_table_num = [%d], has_merged_ds [%d]\n",
            par_table_num_cur_rs, par_table_num, merged_ds ? 1 : 0);

        if (merged_ds) {
          record_scan_one_space_tmp_map[root] = merged_ds;
          need_clean_record_scan_one_space_tmp_map = true;
          ++record_scan_position;
        } else {
          if (!record_scan_one_space_tmp_map[root] &&
              !record_scan_one_space_tmp_map[tmp] &&
              !(record_scan_dependent_map.count(tmp) &&
                !record_scan_dependent_map[tmp].empty()) &&
              !tmp->is_select_union) {
            /*parent and child has no table and child has no dependent
             * child_child. It means: this sql is a simple sql with table,
             * such as "select (select select(1));"*/
            continue;
          }
          if (is_executed_as_table(tmp)) {
            if (type == STMT_CREATE_SELECT) {
              if (record_scan_one_space_tmp_map[tmp] &&
                  record_scan_one_space_tmp_map[root]) {
                if (!record_scan_one_space_tmp_map[tmp]->get_data_source() ||
                    !record_scan_one_space_tmp_map[root]->get_data_source()) {
                  throw UnSupportPartitionSQL(
                      "UnSupport partition table sql for create select stmt.");
                }
                if (!stmt_session->is_dataspace_cover_session_level(
                        record_scan_one_space_tmp_map[root],
                        record_scan_one_space_tmp_map[tmp])) {
                  throw Error(
                      "Not support create select stmt needs cross node "
                      "operation.");
                }
              }
            }
            // TODO: choose a best dataspace for parent, if it has no table
            init_table_subquery_dataspace(tmp, plan);
            record_scan_seperated_table_subquery[root] = true;
            if (!record_scan_one_space_tmp_map[root])
              record_scan_one_space_tmp_map[root] =
                  record_scan_one_space_tmp_map[tmp];
          }

          if (record_scan_need_table.count(tmp)) {
            if (!record_scan_need_cross_node_join.count(tmp)) {
              LOG_DEBUG(
                  "Do handle_cross_node_join due to "
                  "!record_scan_need_cross_node_join.count and "
                  "record_scan_need_table.count.\n");
              ds = handle_cross_node_join(tmp, plan);
            }
          }

          if (record_scan_one_space_tmp_map.count(tmp) &&
              record_scan_one_space_tmp_map[tmp]) {
            spaces.push_back(record_scan_one_space_tmp_map[tmp]);
            need_clean_spaces = true;
          }
          if (is_separated_exec_result_as_sub_query(tmp))
            ++record_scan_position;
          record_scan_dependent_map[root].push_back(tmp);
          SeparatedExecNode *exec_node =
              create_one_separated_exec_node(tmp, plan);
          exec_nodes.push_back(exec_node);
          need_clean_exec_nodes = true;
          record_scan_exec_node_map[tmp] = exec_node;
        }
      }
    }
  }

  /*Try to merge the parent merged dataspace with the output dataspace of
   * separated execution table subquery.*/
  if (record_scan_dependent_map.count(root) &&
      !record_scan_dependent_map[root].empty()) {
    list<record_scan *>::iterator it = record_scan_dependent_map[root].begin();
    record_scan *dep_rs = NULL;
    DataSpace *dep_ds = NULL;
    DataSpace *cur_ds = record_scan_one_space_tmp_map[root];
    bool should_cross_node_join = false;
    if (!left_join_record_scan_position_subquery.count(root)) {
      for (; it != record_scan_dependent_map[root].end(); ++it) {
        dep_rs = *it;
        if (dep_rs->subquerytype == SUB_SELECT_TABLE) {
          /*If a record scan contains one/more separated execute table subquery,
           * dbscale should check whether the finial dataspace of separated
           * execute table subquery can be executed with this record scan.*/
          dep_ds = record_scan_one_space_tmp_map[dep_rs];
          if (!cur_ds) {
            cur_ds = dep_ds;
          } else if (!dep_ds->get_data_source()) {
            // IF subquery dataspace is a partitioned table or duplicated table,
            // then it can merge with parent.
            // do nothing
          } else if (stmt_session->is_dataspace_cover_session_level(cur_ds,
                                                                    dep_ds)) {
            // do nothing
          } else if (stmt_session->is_dataspace_cover_session_level(dep_ds,
                                                                    cur_ds)) {
            cur_ds = dep_ds;
          } else {
            should_cross_node_join = true;
            break;
          }
          if (record_scan_need_cross_node_join.count(root) &&
              record_scan_exec_node_map.count(dep_rs)) {
            should_cross_node_join = true;
          }
          record_scan_one_space_tmp_map[root] = cur_ds;
          need_clean_record_scan_one_space_tmp_map = true;
        }
      }
    }
    if (should_cross_node_join ||
        left_join_record_scan_position_subquery.count(root)) {
      /*If needs cross node join, dbscale should re-analysis the cross
       * node join path.*/
      cur_ds = handle_cross_node_join_with_subquery(root);
      record_scan_one_space_tmp_map[root] = cur_ds;
      need_clean_record_scan_one_space_tmp_map = true;
      if (cur_ds->get_data_source()) par_table_num_cur_rs = 0;
    }
  }

  /*Finially create the separated exec node for the top record_scan.*/
  if (!root->upper) {
    Backend *backend = Backend::instance();
    DataSpace *root_ds = NULL;
    if (record_scan_one_space_tmp_map.count(root) &&
        record_scan_one_space_tmp_map[root]) {
      root_ds = record_scan_one_space_tmp_map[root];
    } else
      root_ds = backend->get_data_space_for_table(schema, NULL);
    num_of_separated_exec_space = spaces.size();
    if (type == STMT_SELECT && record_scan_one_space_tmp_map.count(root) &&
        record_scan_one_space_tmp_map[root] &&
        record_scan_one_space_tmp_map[root]->is_partitioned()) {
      par_table_num_cur_rs = 1;
    } else if (type == STMT_SELECT &&
               record_scan_one_space_tmp_map.count(root) &&
               record_scan_one_space_tmp_map[root] &&
               !record_scan_one_space_tmp_map[root]->is_partitioned()) {
      par_table_num_cur_rs = 0;
    }
    LOG_DEBUG("final, par_table_num_cur_rs=[%d], par_table_num = [%d]\n",
              par_table_num_cur_rs, par_table_num);

    if (enable_global_plan && need_force_move_global(root)) {
      // `need_force_move_global` means do not merge any dataspace.
      record_scan_need_cross_node_join[root] = true;
      DataSpace *ds = handle_cross_node_join(root, plan, true);
      if (!ds->get_data_source()) {
        par_table_num_cur_rs = 1;
      } else {
        par_table_num_cur_rs = 0;
      }
      record_scan_one_space_tmp_map[root] = ds;
      root_ds = record_scan_one_space_tmp_map[root];
    }
    global_table_merged.clear();
    num_of_separated_par_table = par_table_num - par_table_num_cur_rs;

    spaces.push_back(root_ds);
    need_clean_spaces = true;
    SeparatedExecNode *exec_node = create_one_separated_exec_node(root, plan);
    exec_nodes.push_back(exec_node);
    need_clean_exec_nodes = true;
    record_scan_exec_node_map[root] = exec_node;

    /*For update and modify stmt, we should also keep the modify space which
     * will finially do the modify operation.*/
    if (type == STMT_UPDATE || type == STMT_DELETE) {
      if (record_scan_one_space_tmp_map.size() > 1) {
        num_of_separated_exec_space = spaces.size();
        if (modify_ds) {
          spaces.push_back(modify_ds);
          need_clean_spaces = true;
        }
      }
    }
  } else {
    if (enable_global_plan && need_force_move_global(root)) {
      record_scan_need_cross_node_join[root] = true;
      DataSpace *ds = handle_cross_node_join(root, plan, true);
      if (!ds->get_data_source()) {
        par_table_num_cur_rs = 1;
      } else {
        par_table_num_cur_rs = 0;
      }
      record_scan_one_space_tmp_map[root] = ds;
    }
    global_table_merged.clear();
  }
  return;
}

bool Statement::need_force_move_global(record_scan *root) {
  // `need_force_move_global` means do not merge any dataspace.
  bool need_force_move_global = false;
  for (auto &table_st : can_merge_record_scan_position[root]) {
    TableStruct &table = table_st.second;
    DataSpace *ds = Backend::instance()->get_data_space_for_table(
        table.schema_name.c_str(), table.table_name.c_str());
    if (global_table_merged.count(ds)) {
      if (record_scan_one_space_tmp_map.count(root) &&
          !is_share_same_server(record_scan_one_space_tmp_map[root], ds))
        need_force_move_global = true;
      else if (!record_scan_one_space_tmp_map.count(root))
        need_force_move_global = true;
    }
  }
  need_force_move_global =
      need_force_move_global && (!record_scan_need_cross_node_join.count(root));
  return need_force_move_global;
}

SeparatedExecNode *Statement::create_one_separated_exec_node(
    record_scan *node_rs, ExecutePlan *plan) {
  SeparatedExecNode *ret = NULL;
  if (node_rs->subquerytype == SUB_SELECT_DEPENDENT) {
    ret =
        new SimpleCrossNodeJoinNode((Statement *)this, node_rs, plan->handler);
    set_is_may_cross_node_join_related(true);
    map<DataSpace *, DataSpace *> gtable_output_dataspace;
    if (node_rs->upper &&
        gtable_dependent_output_dataspace.count(node_rs->upper))
      gtable_output_dataspace =
          gtable_dependent_output_dataspace[node_rs->upper];

    ret->set_output_dataspace(gtable_output_dataspace);
  } else if (node_rs->is_select_union && is_executed_as_table(node_rs)) {
    ret = new UnionTableSubqueryNode((Statement *)this, node_rs, plan->handler);
    set_is_may_cross_node_join_related(true);
    if (table_subquery_final_dataspace.count(node_rs))
      ret->set_final_table_subquery_dataspace(
          table_subquery_final_dataspace[node_rs]);
  } else if (record_scan_need_cross_node_join.count(node_rs)) {
#ifdef DEBUG
    Statement *top_stmt = plan->session->get_original_stmt();
    if (top_stmt && top_stmt->st.type == STMT_SELECT &&
        !top_stmt->st.scanner->is_select_union && top_stmt != this)
      ACE_ASSERT(0);
#endif
    ret =
        new SimpleCrossNodeJoinNode((Statement *)this, node_rs, plan->handler);
    if (table_subquery_final_dataspace.count(node_rs))
      ret->set_final_table_subquery_dataspace(
          table_subquery_final_dataspace[node_rs]);
    set_is_may_cross_node_join_related(true);
    if (record_scan_need_table.count(node_rs)) ret->set_need_merge_all_method();
  } else {
    ret = new LocalExecNode((Statement *)this, node_rs, plan->handler);
    if (table_subquery_final_dataspace.count(node_rs))
      ret->set_final_table_subquery_dataspace(
          table_subquery_final_dataspace[node_rs]);
  }
  return ret;
}

void Statement::assemble_dbscale_show_default_session_variables(
    ExecutePlan *plan) {
  ExecuteNode *dbscale_show_default_session_var_node =
      plan->get_dbscale_show_default_session_var_node();
  plan->set_start_node(dbscale_show_default_session_var_node);
  return;
}
void Statement::assemble_dbscale_show_version_plan(ExecutePlan *plan) {
  LOG_DEBUG("assemble dbscale_show_version plan\n");
  ExecuteNode *dbscale_show_version_node =
      plan->get_dbscale_show_version_node();
  plan->set_start_node(dbscale_show_version_node);
  return;
}
void Statement::assemble_show_components_version_plan(ExecutePlan *plan) {
  LOG_DEBUG("assemble show components version plan\n");
  ExecuteNode *show_conponents_version_node =
      plan->get_show_components_version_node();
  plan->set_start_node(show_conponents_version_node);
  return;
}
void Statement::assemble_show_version_plan(ExecutePlan *plan) {
  LOG_DEBUG("assemble show version plan\n");
  ExecuteNode *show_version_node = plan->get_show_version_node();
  plan->set_start_node(show_version_node);
  return;
}
void Statement::assemble_dbscale_show_hostname_plan(ExecutePlan *plan) {
  LOG_DEBUG("assemble dbscale_show_hostname plan\n");
  ExecuteNode *dbscale_show_hostname_node =
      plan->get_dbscale_show_hostname_node();
  plan->set_start_node(dbscale_show_hostname_node);
  return;
}
void Statement::assemble_dbscale_show_all_fail_transaction_plan(
    ExecutePlan *plan) {
  LOG_DEBUG("assemble dbscale_show_all fail transaction plan\n");
  ExecuteNode *node = plan->get_dbscale_show_all_fail_transaction_node();
  plan->set_start_node(node);
  return;
}
void Statement::assemble_dbscale_show_detail_fail_transaction_plan(
    ExecutePlan *plan, const char *xid) {
  LOG_DEBUG("assemble dbscale show detail fail transaction plan\n");
  ExecuteNode *node = plan->get_dbscale_show_detail_fail_transaction_node(xid);
  plan->set_start_node(node);
  return;
}

void Statement::assemble_dbscale_show_partition_table_status_plan(
    ExecutePlan *plan, const char *table_name) {
  LOG_DEBUG("assemble dbscale show partition table status. \n");
  ExecuteNode *node =
      plan->get_dbscale_show_partition_table_status_node(table_name);
  plan->set_start_node(node);
  return;
}

void Statement::assemble_dbscale_show_schema_acl_info(ExecutePlan *plan,
                                                      bool is_show_all) {
  LOG_DEBUG("assemble dbscale show schema ACL info plan\n");
  ExecuteNode *node = plan->get_dbscale_show_schema_acl_info_node(is_show_all);
  plan->set_start_node(node);
  return;
}

void Statement::assemble_dbscale_show_table_acl_info(ExecutePlan *plan,
                                                     bool is_show_all) {
  LOG_DEBUG("assemble dbscale show table ACL info plan\n");
  ExecuteNode *node = plan->get_dbscale_show_table_acl_info_node(is_show_all);
  plan->set_start_node(node);
  return;
}

void Statement::assemble_dbscale_show_path_info(ExecutePlan *plan) {
  LOG_DEBUG("assemble dbscale show path info.\n");
  ExecuteNode *node = plan->get_dbscale_show_path_info();
  plan->set_start_node(node);
  return;
}

void Statement::assemble_dbscale_show_warnings(ExecutePlan *plan) {
  LOG_DEBUG("assemble dbscale show warnings plan.\n");
  ExecuteNode *node = plan->get_dbscale_show_warnings_node();
  plan->set_start_node(node);
  return;
}

void Statement::assemble_dbscale_show_critical_errors(ExecutePlan *plan) {
  LOG_DEBUG("assemble dbscale show critical errors.\n");
  ExecuteNode *node = plan->get_dbscale_show_critical_errors_node();
  plan->set_start_node(node);
  return;
}

void Statement::assemble_dbscale_clean_fail_transaction_plan(ExecutePlan *plan,
                                                             const char *xid) {
  LOG_DEBUG("assemble dbscale clean fail transaction plan\n");
  ExecuteNode *node = plan->get_dbscale_clean_fail_transaction_node(xid);
  plan->set_start_node(node);
  return;
}
void Statement::assemble_dbscale_show_shard_partition_table_plan(
    ExecutePlan *plan, const char *scheme_name) {
  ExecuteNode *node = plan->get_dbscale_show_shard_partition_node(scheme_name);
  plan->set_start_node(node);
  return;
}
void Statement::assemble_dbscale_show_rebalance_work_load_plan(
    ExecutePlan *plan, const char *scheme_name, list<string> sources,
    const char *schema_name, int is_remove) {
  ExecuteNode *node = plan->get_dbscale_show_rebalance_work_load_node(
      scheme_name, sources, schema_name, is_remove);
  plan->set_start_node(node);
  return;
}

void Statement::assemble_dbscale_add_default_session_variables(
    ExecutePlan *plan) {
  DataSpace *dataspace =
      Backend::instance()->get_data_space_for_table(schema, NULL);
  ExecuteNode *dbscale_add_default_session_var_node =
      plan->get_dbscale_add_default_session_var_node(dataspace);
  plan->set_start_node(dbscale_add_default_session_var_node);
  return;
}
void Statement::assemble_dbscale_remove_default_session_variables(
    ExecutePlan *plan) {
  ExecuteNode *dbscale_remove_default_session_var_node =
      plan->get_dbscale_remove_default_session_var_node();
  plan->set_start_node(dbscale_remove_default_session_var_node);
  return;
}

void Statement::mark_table_info_to_delete(ExecutePlan *plan) {
  const char *schema_name;
  const char *table_name;
  table_link *table = st.table_list_head;

  switch (st.type) {
    case STMT_ALTER_TABLE: {
      schema_name =
          table->join->schema_name ? table->join->schema_name : schema;
      table_name = table->join->table_name;
      plan->session->add_table_info_to_delete(schema_name, table_name);
      break;
    }
    case STMT_DROP_DB: {
      schema_name = st.sql->drop_db_oper->dbname->name;
      plan->session->add_db_table_info_to_delete(schema_name);
      break;
    }
    case STMT_DROP_TB: {
      while (table) {
        schema_name =
            table->join->schema_name ? table->join->schema_name : schema;
        table_name = table->join->table_name;
        plan->session->add_table_info_to_delete(schema_name, table_name);
        table = table->next;
      }
      break;
    }
    default:
      break;
  }
}

bool Statement::check_federated_name() {
  if (!st.table_list_head || st.table_list_head != st.table_list_tail)
    return false;
  bool ret = false;
  const char *table_name = NULL;
  table_link *table = st.table_list_head;
  if (table) table_name = table->join->table_name;
  if (table_name) {
    string table_string(table_name);
    if (table_string.compare(0, strlen(TMP_TABLE_NAME), TMP_TABLE_NAME) == 0)
      ret = true;
  }
  return ret;
}

inline bool Statement::check_federated_show_status() {
  bool ret = false;
  const char *table_name = NULL;
  table_name = st.sql->show_tables_oper->table_pattern;
  if (table_name) {
    string table_string(table_name);
    // if (table_string.find(TMP_TABLE_NAME) == 0)
    if (table_string.compare(0, strlen(TMP_TABLE_NAME), TMP_TABLE_NAME) == 0) {
      ret = true;
    }
  }

  return ret;
}

void Statement::get_fe_tmp_table_name(string sql, set<string> &name_vec) {
  string tmp_table_name;
  size_t start_pos = sql.find(TMP_TABLE_NAME);
  while (start_pos != string::npos) {
    size_t end_pos1 = sql.find(".", start_pos);
    size_t end_pos2 = sql.find(",", start_pos);
    size_t end_pos3 = sql.find(" ", start_pos);
    size_t end_pos = end_pos1 < end_pos2
                         ? (end_pos1 < end_pos3 ? end_pos1 : end_pos3)
                         : (end_pos2 < end_pos3 ? end_pos2 : end_pos3);
    size_t end_pos4 = sql.find("`", start_pos);
    end_pos = end_pos < end_pos4 ? end_pos : end_pos4;
    tmp_table_name.assign(sql, start_pos, end_pos - start_pos);
    if (tmp_table_name.find("_fed_") != string::npos)
      name_vec.insert(tmp_table_name);
    start_pos = sql.find(TMP_TABLE_NAME, end_pos);
  }
}
bool Statement::check_federated_select() {
  bool ret = false;
  string sql_string(sql);
  if (sql_string.find(FEDERATED_SELECT_END_STRING) ==
      sql_string.length() - SIZE_OF_END_STRING)
    ret = true;

  return ret;
}
int Statement::get_field_num() {
  int field_num = 0;
  record_scan *rs = st.scanner;
  for (field_item *fi = rs->field_list_head; fi != NULL; fi = fi->next) {
    ++field_num;
  }
  return field_num;
}

void Statement::assemble_show_create_event_plan(ExecutePlan *plan) {
  DataSpace *dataspace = NULL;
  Backend *backend = Backend::instance();
  dataspace = backend->get_data_space_for_table(schema, NULL);
  assemble_direct_exec_plan(plan, dataspace);
}

void Statement::assemble_show_tables_plan(ExecutePlan *plan) {
  Backend *backend = Backend::instance();
  LOG_DEBUG("Assemble show tables plan.\n");
  if (!datasource_in_one) {
    const char *schema_name;
    map<DataSpace *, bool> dataspaces;
    if (st.sql->show_tables_oper == NULL)
      schema_name = schema;
    else {
      schema_name = st.sql->show_tables_oper->db_name
                        ? st.sql->show_tables_oper->db_name
                        : schema;
    }

    backend->get_all_dataspace_for_schema(schema_name, dataspaces);
    map<DataSpace *, bool>::iterator it = dataspaces.begin();

    if (dataspaces.size() == 1) {
      assemble_direct_exec_plan(plan, it->first);
    } else {
      ExecuteNode *send_node = plan->get_send_node();
      list<int> column_index;
      column_index.push_back(0);
      ExecuteNode *distinct_node = plan->get_distinct_node(column_index);
      send_node->add_child(distinct_node);
      ExecuteNode *fetch_node;

      list<DataSpace *> space_list;
      list<DataSpace *>::iterator it_s = space_list.begin();
      bool share_same_server = false;
      for (; it != dataspaces.end(); ++it) {
        share_same_server = false;
        for (it_s = space_list.begin(); it_s != space_list.end(); ++it_s) {
          if (is_share_same_server(it->first, *it_s)) {
            share_same_server = true;
            break;
          }
        }
        if (!share_same_server) {
          space_list.push_back(it->first);
          const char *used_sql = adjust_stmt_sql_for_shard(it->first, sql);
          fetch_node = plan->get_fetch_node(it->first, used_sql);
          distinct_node->add_child(fetch_node);
        }
      }

      plan->set_start_node(send_node);
    }
  } else {
    LOG_DEBUG("Show tables from metadata_data_space.\n");
    DataSpace *space = Backend::instance()->get_metadata_data_space();
    if (!space) {
      space = Backend::instance()->get_catalog();
    }
    assemble_direct_exec_plan(plan, space);
  }
}
void Statement::assemble_federated_table_status_node(ExecutePlan *plan) {
  LOG_DEBUG("Assemble federated table status node.\n");
  ExecuteNode *node = plan->get_federated_table_node();
  plan->set_start_node(node);
}

void Statement::assemble_empty_set_node(ExecutePlan *plan, int field_num) {
  LOG_DEBUG("Assemble federated table status node.\n");
  ExecuteNode *node = plan->get_empty_set_node(field_num);
  plan->set_start_node(node);
}

void Statement::handle_federated_join() {
  vector<SeparatedExecNode *>::iterator it = exec_nodes.begin();
  for (; it != exec_nodes.end(); ++it) {
    (*it)->set_tmp_tables_to_dead();
  }
}

TableStruct Statement::check_parent_table_alias(record_scan *parent_rs,
                                                TableStruct table) {
  TableStruct ret = table;
  table_link *tmp_tb = parent_rs->first_table;
  bool found_table = false;
  while (tmp_tb) {
    if (tmp_tb->join->cur_rec_scan == parent_rs) {
      const char *table_name = tmp_tb->join->table_name;
      const char *schema_name =
          tmp_tb->join->schema_name ? tmp_tb->join->schema_name : schema;
      const char *alias =
          tmp_tb->join->alias ? tmp_tb->join->alias : table_name;
      TableStruct table_struct;
      table_struct.table_name = table_name;
      table_struct.schema_name = schema_name;
      table_struct.alias = alias;
      if (table.schema_name.empty() && table.alias == string(alias)) {
        ret = table_struct;
        found_table = true;
        break;
      } else if (table_struct == table) {
        ret = table_struct;
        found_table = true;
        break;
      }
    }
    tmp_tb = tmp_tb->next;
  }
  if (!found_table) {
    LOG_ERROR(
        "Not support dependent subquery that parent contains no missing "
        "table.\n");
    throw NotSupportedError(
        "Not support dependent subquery that parent contains no missing "
        "table.");
  }
  missing_table_map[ret] = tmp_tb;
  return ret;
}

void Statement::handle_create_or_drop_view(ExecutePlan *plan) {
  if (!on_view) return;
  Backend *backend = Backend::instance();
  if (st.type == STMT_CREATE_VIEW) {
    create_view_op_node *view_node = st.sql->select_oper->create_view_oper;
    const char *view_name = view_node->view;
    const char *view_schema = view_node->schema ? view_node->schema : schema;

    DataSpace *schema_space =
        backend->get_data_space_for_table(view_schema, NULL);
    DataSpace *meta = backend->get_auth_data_space();
    if (((schema_space->get_dataspace_type() == SCHEMA_TYPE &&
          ((Schema *)schema_space)
              ->get_is_schema_pushdown_stored_procedure()) ||
         schema_space->get_dataspace_type() == CATALOG_TYPE) &&
        (!is_share_same_server(meta, schema_space))) {
      try {
        schema_space->execute_one_modify_sql(sql, plan->handler, view_schema);
      } catch (...) {
        LOG_ERROR("Fail to create view on schema dataspace [%s].\n",
                  view_schema);
        throw;
      }
    }

    string tmp_view(view_schema);
    tmp_view.append(".");
    tmp_view.append(view_name);
    vector<vector<string> > view_sqls;
    string query_sql(
        "SELECT VIEW_DEFINITION, CHARACTER_SET_CLIENT FROM "
        "INFORMATION_SCHEMA.VIEWS WHERE TABLE_SCHEMA = '");
    query_sql.append(view_schema);
    query_sql.append("' AND TABLE_NAME = '");
    query_sql.append(view_name);
    query_sql.append("';");
    DataSpace *dataspace = backend->get_config_data_space();
    if (!dataspace) {
      LOG_ERROR("Can not find the metadata dataspace.\n");
      throw Error(
          "Fail to get meta dataspace, please make sure meta datasource is "
          "configured.");
    }

    Connection *conn = NULL;
    try {
      conn = dataspace->get_connection(plan->session);
      if (!conn) {
        LOG_ERROR(
            "Fail to get connection on config source when create view.\n");
        throw Error("Fail to get config source connection.");
      }
      if (conn->get_session_var_map_md5() !=
          plan->session->get_session_var_map_md5()) {
        conn->set_session_var(plan->session->get_session_var_map());
#ifdef DEBUG
        LOG_DEBUG("conn set session var with session var map size %d.\n",
                  plan->session->get_session_var_map()->size());
#endif
      }
      conn->reset();
      conn->query_for_all_column(query_sql.c_str(), &view_sqls);
      conn->get_pool()->add_back_to_free(conn);
    } catch (...) {
      if (conn) conn->get_pool()->add_back_to_dead(conn);
      throw;
    }

    string view_sql = view_sqls[0][0];
    string view_char = view_sqls[0][1];
    if (MYSQL_VERSION_8 == Backend::instance()->get_backend_server_version())
      replace_view_select_field_alias(view_sql);

    if (view_char.empty()) view_char.assign("utf8");
    bool insert_view_ok = Driver::get_driver()->get_config_helper()->add_view(
        view_schema, view_name, view_sql.c_str(), view_char.c_str());
    if (!insert_view_ok) {
      LOG_ERROR("Fail to add view to dbscale, the sql is [%s].\n",
                view_sql.c_str());
    }
    backend->add_one_view(tmp_view, view_sql, ctype);
#ifndef CLOSE_MULTIPLE
    char param[256] = {0};
    if (multiple_mode) {
      sprintf(param, "%s %s %s %s", "1", view_schema, view_name,
              view_char.c_str());
      if (!backend->start_sync_topic(VIEW_TOPIC_NAME, param)) {
        LOG_ERROR("Fail to create view cause start VIEW_TOPIC error.\n");
        throw Error("Fail to create view cause start VIEW_TOPIC error");
      }
      try {
        MultipleManager *mul = MultipleManager::instance();
        MultipleSyncTool *sync_tool = mul->get_sync_tool();
        ViewInfoMessage *message =
            new ViewInfoMessage(view_schema, view_name, view_char);
        mul->pub_cluster_management_message(message);
        sync_tool->get_sync_topic()->set_sync_info_op_param(param);
        sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param, NULL);
        sync_tool->publish_sync_message();
        sync_tool->wait_all_children_sync();
      } catch (Exception &e) {
        LOG_ERROR("error happen in add view, due to %s.\n", e.what());
        throw Error("error hanpen in add_view sync.\n");
      }
    }
#endif
    LOG_DEBUG("Create view [%s] with sql [%s].\n", tmp_view.c_str(),
              view_sql.c_str());
  } else {
    table_link *table = st.table_list_head;
    const char *schema_name, *table_name;
    set<string> view_name_set;
    while (table) {
      schema_name =
          table->join->schema_name ? table->join->schema_name : schema;
      table_name = table->join->table_name;

      string drop_sql("DROP VIEW IF EXISTS `");
      drop_sql.append(schema_name);
      drop_sql.append("`.`");
      drop_sql.append(table_name);
      drop_sql.append("`");
      DataSpace *schema_space =
          backend->get_data_space_for_table(schema_name, NULL);
      DataSpace *meta = backend->get_auth_data_space();
      if (((schema_space->get_dataspace_type() == SCHEMA_TYPE &&
            ((Schema *)schema_space)
                ->get_is_schema_pushdown_stored_procedure()) ||
           schema_space->get_dataspace_type() == CATALOG_TYPE) &&
          (!is_share_same_server(meta, schema_space))) {
        try {
          schema_space->execute_one_modify_sql(drop_sql.c_str(), plan->handler,
                                               schema_name);
        } catch (...) {
          LOG_ERROR("Fail to drop view on schema dataspace [%s].\n",
                    schema_name);
          throw;
        }
      }

      bool insert_view_ok =
          Driver::get_driver()->get_config_helper()->remove_view(schema_name,
                                                                 table_name);
      if (!insert_view_ok) {
        LOG_ERROR("Fail to drop view to dbscale, the view is [%s.%s].\n",
                  schema_name, table_name);
      }
      string full_view(schema_name);
      full_view.append(".");
      full_view.append(table_name);
      table = table->next;
      view_name_set.insert(full_view);
    }
    backend->remove_view_set(view_name_set);
#ifndef CLOSE_MULTIPLE
    string param;
    if (multiple_mode) {
      param += "0";
      set<string>::iterator it;
      for (it = view_name_set.begin(); it != view_name_set.end(); ++it) {
        param += " ";
        param += *it;
      }
      if (!backend->start_sync_topic(VIEW_TOPIC_NAME, param.c_str())) {
        LOG_ERROR("Fail to drop view cause start VIEW_TOPIC error.\n");
        throw Error("Fail to drop view cause start VIEW_TOPIC error");
      }
      try {
        MultipleManager *mul = MultipleManager::instance();
        MultipleSyncTool *sync_tool = mul->get_sync_tool();
        ViewInfoMessage *message = new ViewInfoMessage(view_name_set);
        mul->pub_cluster_management_message(message);
        sync_tool->get_sync_topic()->set_sync_info_op_param(param.c_str());
        sync_tool->generate_sync_topic_sql(SYNC_TOPIC_STATE_FIN, param.c_str(),
                                           NULL);
        sync_tool->publish_sync_message();
        sync_tool->wait_all_children_sync();
      } catch (Exception &e) {
        LOG_ERROR("error happen in add view, due to %s.\n", e.what());
        throw Error("error hanpen in add_view sync.\n");
      }
    }
#endif
  }
  Driver *driver = Driver::get_driver();
  driver->acquire_session_mutex();
  set<Session *, compare_session> *session_set = driver->get_session_set();
  set<Session *, compare_session>::iterator it = session_set->begin();
  for (; it != session_set->end(); ++it) {
    (*it)->set_view_updated(true);
  }
  driver->release_session_mutex();
}

bool Statement::check_left_join(table_link *table) {
  bool ret = false;
  join_node *join = table->join->upper;
  if (join && join->join_bit & JT_LEFT && join->right == table->join) {
    ret = true;
  }
  return ret;
}

void Statement::replace_view_select_field_alias(string &view_select_sql) {
  Statement *new_stmt = NULL;
  Parser *parser = Driver::get_driver()->get_parser();
  try {
    new_stmt = parser->parse(view_select_sql.c_str(), stmt_allow_dot_in_ident,
                             true, NULL, NULL, NULL, ctype);
    field_item *column_it_in_select_sql = new_stmt->st.scanner->field_list_head;
    name_item *column_it_in_view_sql =
        this->st.sql->select_oper->create_view_oper->column_list;
    int size_change = 0;
    if (column_it_in_view_sql) {
      while (column_it_in_select_sql) {
        string::iterator replace_begin =
            view_select_sql.begin() +
            column_it_in_select_sql->alias_item->start_pos + size_change - 1;
        string::iterator replace_end =
            view_select_sql.begin() +
            column_it_in_select_sql->alias_item->end_pos + size_change;
        string replace_to = "`" + string(column_it_in_view_sql->name) + "`";
        size_change = replace_to.size() -
                      (column_it_in_select_sql->alias_item->end_pos -
                       column_it_in_select_sql->alias_item->start_pos + 1);
        view_select_sql.replace(replace_begin, replace_end, replace_to);
        column_it_in_view_sql = column_it_in_view_sql->next;
        column_it_in_select_sql = column_it_in_select_sql->next;
      }
    }
  } catch (exception &e) {
    string mes = "Replacing view select field alias error: ";
    mes.append(e.what());
    LOG_ERROR("%s", mes.c_str());
    new_stmt->free_resource();
    delete new_stmt;
    new_stmt = NULL;
    throw Error(mes.c_str());
  }
  new_stmt->free_resource();
  delete new_stmt;
  new_stmt = NULL;
}

bool Statement::check_left_join_with_cannot_merge_right_par(record_scan *rs) {
  Backend *backend = Backend::instance();
  join_node *join_tables = rs->join_tables;
  bool has_cannot_merge_right_par = false;
  while (join_tables) {
    join_node *table = join_tables->right;
    if (!table) table = join_tables;
    if (table->type == JOIN_NODE_SINGLE) {
      const char *schema_name =
          table->schema_name ? table->schema_name : schema;
      const char *table_name = table->table_name;
      DataSpace *space =
          backend->get_data_space_for_table(schema_name, table_name);
      if (!space->get_data_source()) {
        if (table == join_tables) {
          has_cannot_merge_right_par = false;
        } else {
          if (join_tables->join_bit & JT_LEFT)
            has_cannot_merge_right_par = true;
          else
            has_cannot_merge_right_par = false;
        }
      }
    }
    join_tables = join_tables->left;
  }
  return has_cannot_merge_right_par;
}

bool Statement::check_left_join_sub_left(record_scan *rs) {
  bool ret = false;
  join_node *join_tables = rs->join_tables;
  if (join_tables && join_tables->join_bit & JT_LEFT &&
      join_tables->left->type == JOIN_NODE_SUBSELECT &&
      join_tables->right->type != JOIN_NODE_JOIN && join_tables->condition &&
      join_tables->condition->type == JOIN_COND_ON && !join_tables->upper) {
    ret = true;
  }
  return ret;
}

bool Statement::check_mul_table_left_join_sub(record_scan *rs) {
  join_node *join_tables = rs->join_tables;
  if (join_tables && join_tables->left && join_tables->left->left &&
      join_tables->join_bit & JT_LEFT)
    return true;

  while (join_tables && join_tables->left) {
    join_tables = join_tables->left;
    if (join_tables->join_bit & JT_LEFT) return true;
  }
  return false;
}

bool Statement::sep_node_can_execute_parallel(ExecutePlan *plan) {
  /*Currently, there are following restrictions for separated node parallel
   * execution:
   *
   * 1. not support in transaction
   *
   * 2. not support in execution prepare
   *
   * 3. not support user var or session var
   *
   * 4. must be select stmt
   *
   * 5. not support found_rows*/
  Session *s = plan->session;
  if (s->is_in_transaction() || s->is_executing_prepare() ||
      s->get_var_flag() || select_uservar_flag)
    return false;
  if (st.type == STMT_SELECT) {
    if (st.sql->select_oper->options & SQL_OPT_SQL_CALC_FOUND_ROWS)
      return false;
  } else
    return false;

  return true;
}

void Statement::init_parallel_separated_node_execution() {
#ifdef DEBUG
  ACE_ASSERT(work_pointer == 0);
  ACE_ASSERT(fin_size == 0);
  ACE_ASSERT(running_threads == 0);
  ACE_ASSERT(work_vec.size() == 0);
  ACE_ASSERT(fin_vec.size() == 0);
#endif
  work_pointer = 0;
  fin_size = 0;
  running_threads = 0;
}

void Statement::clean_up_parallel_separated_node_execution() {
  size_t i = 0;
  for (; i != work_pointer; ++i) {
    /*work_pointer can used to indicate how many thread_task has been started,
      all this task will be wait for thread exit.

      Here should use work_pointer and work_vec, rather than fin_vec, in case of
      the exception situation, which there may be unfinished task.
      */
    work_vec[i]->wait_for_cond();  // ensure there is no running thread using
                                   // the nodes inside fin_vec
  }

  work_vec.clear();
  fin_vec.clear();
  work_pointer = 0;
  fin_size = 0;
  running_threads = 0;
}

void Statement::loop_and_find_executable_sep_nodes() {
  vector<SeparatedExecNode *>::iterator it = exec_nodes.begin();
  for (; it + 1 != exec_nodes.end(); ++it) {
    SeparatedExecNode *tmp = *it;
#ifdef DEBUG
    ACE_ASSERT(tmp->get_sen_state() != SEN_STATE_NON);
#endif
    if (!tmp->get_is_in_work_vec()) {  // this node has not yet be put into
                                       // work_vec
      /*here we use is_in_work_vec rather than sen_state==SEN_STATE_NON to
       * check the node is not in work_vec. Cause the sen_state may be changed
       * by the worker thread, so it may be un-safe for read.*/
#ifdef DEBUG
      ACE_ASSERT(tmp->get_sen_state() == SEN_STATE_INIT);
#endif
      if (tmp->has_no_un_executed_dependent_node()) {  // this node can be put
                                                       // into work_vec now
        tmp->set_sen_state(SEN_STATE_CAN_EXEC);
        tmp->set_is_in_work_vec(true);
        work_vec.push_back(tmp);
      }
    }
  }
}

/*If all separated execution node, except the last one, has been assigned,
 * return true, otherwise return false.*/
bool Statement::assign_threads_for_work_vec() {
  sep_node_mutex.acquire();
  while (work_pointer < work_vec.size() && running_threads < stmt_max_threads) {
    Backend *backend = Backend::instance();
    BackendThread *bthread =
        backend->get_backend_thread_pool()->get_one_from_free();
    if (!bthread) {
      // TODO: handle this exception
    }
    bthread->set_task(work_vec[work_pointer]);
    bthread->wakeup_handler_thread();
    ++work_pointer;
    ++running_threads;
  }

  if (work_pointer == (exec_nodes.size() - 1)) {
#ifdef DEBUG
    ACE_ASSERT(work_pointer == work_vec.size());
#endif
    LOG_DEBUG(
        "Coordinator thread has finished the separated node parallel"
        " execution assignment with work_pointer %d, exec_nodes.size %d.\n",
        work_pointer, work_vec.size());
    sep_node_mutex.release();
    return true;
  }

  sep_node_mutex.release();
  return false;
}

void Statement::report_finish_separated_node_exec(SeparatedExecNode *node,
                                                  bool get_exception) {
  if (!get_exception) {
    /*If got exception, we should not set the node state to fin, otherwise the
     * other separated node which dependt this node may be executed
     * unexpectedly.*/
    node->set_sen_state(SEN_STATE_FIN_EXEC);
  }
  sep_node_mutex.acquire();
  fin_vec.push_back(node);
  running_threads--;
  sep_node_cond.signal();
  sep_node_mutex.release();
  LOG_DEBUG("Node %@ finish the separated node exec by backend thread.\n",
            node);
}

void Statement::check_fin_tasks() {
  size_t new_fin_task_num = fin_vec.size() - fin_size;
  size_t pos_max = fin_vec.size();
  size_t i = fin_size;
  SeparatedExecNode *node = NULL;
  LOG_DEBUG("new fin task num is %d i %d pos_max %d.\n", new_fin_task_num, i,
            pos_max);
  for (; i < pos_max; ++i) {
    node = fin_vec[i];
    node->reproduce_exception();
  }
}

bool Statement::wait_threads_for_execution() {
  sep_node_mutex.acquire();

  LOG_DEBUG("Start to wait for node execution.\n");
  while (fin_size == fin_vec.size()) {
    sep_node_cond.wait();
  }
  LOG_DEBUG("Finish to wait for node execution.\n");

  try {
    check_fin_tasks();
  } catch (...) {
    sep_node_mutex.release();
    throw;
  }

  fin_size = fin_vec.size();
  if (fin_size == (exec_nodes.size() - 1)) {
    sep_node_mutex.release();
    return true;
  }
  sep_node_mutex.release();

  return false;
}

bool Statement::is_separated_exec_result_as_sub_query(record_scan *rs) {
  bool ret = false;
  if (rs->subquerytype == SUB_SELECT_DEPENDENT ||
      rs->subquerytype == SUB_SELECT_EXISTS ||
      rs->subquerytype == SUB_SELECT_COLUMN ||
      rs->subquerytype == SUB_SELECT_UNION)
    ret = true;
  return ret;
}

TableStruct *Statement::check_column_belongs_to(
    const char *column_name,
    vector<vector<TableStruct> > *table_vector_vector) {
  TableStruct *ret = NULL;
  int table_nums = 0;

  vector<vector<TableStruct> >::iterator table_vector_vector_iter;
  for (table_vector_vector_iter = table_vector_vector->begin();
       table_vector_vector_iter != table_vector_vector->end();
       ++table_vector_vector_iter) {
    vector<TableStruct>::iterator table_vector_iter;
    for (table_vector_iter = table_vector_vector_iter->begin();
         table_vector_iter != table_vector_vector_iter->end();
         ++table_vector_iter) {
      string schema_name = table_vector_iter->schema_name;
      string table_name = table_vector_iter->table_name;

      TableInfoCollection *tic = TableInfoCollection::instance();
      TableInfo *ti = tic->get_table_info_for_read(schema_name, table_name);
      map<string, TableColumnInfo *, strcasecomp> *column_info_map;
      try {
        column_info_map =
            ti->element_table_column
                ->get_element_map_columnname_table_column_info(stmt_session);
      } catch (...) {
        LOG_ERROR(
            "Error occured when try to get table info column "
            "info(map_columnname_table_column_info) of table [%s.%s]\n",
            schema_name.c_str(), table_name.c_str());
        ti->release_table_info_lock();
        throw;
      }

      string tmp_colname(column_name);
      boost::to_lower(tmp_colname);
      if (column_info_map != NULL) {
        if (column_info_map->count(tmp_colname)) {
          ++table_nums;
          ret = &(*table_vector_iter);
        }
      }
      ti->release_table_info_lock();
    }
  }
  if (table_nums == 0) {
    string error_message;
    error_message.append("No column in the tables named: ");
    error_message.append(column_name);
    throw Error(error_message.c_str());

  } else if (table_nums == 1) {
    ;
  } else {
    string error_message;
    error_message.append(
        "You should add the table name to the column name since more than one "
        "table contains ");
    error_message.append(column_name);
    throw Error(error_message.c_str());
  }

  return ret;
}
bool Statement::table_link_same_dataspace(table_link *modify_table,
                                          table_link *par_table) {
  Backend *backend = Backend::instance();
  const char *modify_schema = modify_table->join->schema_name
                                  ? modify_table->join->schema_name
                                  : schema;
  const char *par_schema =
      par_table->join->schema_name ? par_table->join->schema_name : schema;
  DataSpace *modify_dataspace = backend->get_data_space_for_table(
      modify_schema, modify_table->join->table_name);
  DataSpace *par_dataspace = backend->get_data_space_for_table(
      par_schema, par_table->join->table_name);

  return modify_dataspace == par_dataspace;
}
bool Statement::is_can_swap_type() {
  stmt_type type = st.type;
  if (type == STMT_CREATE_TB || type == STMT_ALTER_TABLE ||
      type == STMT_CREATE_VIEW || type == STMT_DROP_VIEW ||
      type > DBSCALE_ADMIN_COMMAND_START) {
    return false;
  }
  return true;
}

Statement *Statement::get_first_union_stmt() {
  record_scan *top_record_scan = st.scanner;
  string executed_sql(sql);
  if (!first_union_stmt) {
    string left_sql =
        executed_sql.substr(top_record_scan->union_select_left->start_pos - 1,
                            top_record_scan->union_select_left->end_pos);
    Statement *new_stmt = NULL;
    Parser *parser = Driver::get_driver()->get_parser();
    new_stmt = parser->parse(left_sql.c_str(), stmt_allow_dot_in_ident, true,
                             NULL, NULL, NULL, ctype);
    union_all_stmts.push_back(new_stmt);
    new_stmt->check_and_refuse_partial_parse();
    new_stmt->set_union_all_sub(true);
    new_stmt->set_default_schema(schema);
    first_union_stmt = new_stmt;
  }
  return first_union_stmt;
}

void Statement::set_full_table_name(const char *schema_name,
                                    const char *table_name) {
  splice_full_table_name(schema_name, table_name, full_table_name);
}

void Statement::replace_grant_hosts() {
  user_clause_item_list *user_clause_list = NULL;
  if (st.type == STMT_CREATE_USER || st.type == STMT_ALTER_USER) {
    if (st.sql->create_user_oper != NULL)
      user_clause_list = st.sql->create_user_oper->user_clause;
  } else if (st.type == STMT_DROP_USER) {
    if (st.sql->drop_user_oper != NULL)
      user_clause_list = st.sql->drop_user_oper->user_clause;
  } else if (st.type == STMT_REVOKE) {
    if (st.sql->revoke_oper != NULL)
      user_clause_list = st.sql->revoke_oper->user_clause;
  } else if (st.type == STMT_GRANT) {
    if (st.sql->grant_oper != NULL)
      user_clause_list = st.sql->grant_oper->user_clause;
  } else {
    if (st.sql->show_grants_oper != NULL)
      user_clause_list = st.sql->show_grants_oper->user_clause;
  }
  if (!user_clause_list) return;
  user_clause_item_list *tmp = user_clause_list;
  while (tmp) {
    const char *com_str = tmp->user_name;
    if (!strcmp(dbscale_internal_user, com_str) ||
        !strcmp(dbscale_read_only_user, com_str)) {
      LOG_ERROR(
          "Not support account management operation for dbscale internal "
          "user or dbscale read only user.\n");
      throw NotSupportedError(
          "Not support account management operation for dbscale internal "
          "user or dbscale read only user.");
    }
    tmp = tmp->next;
  }
  string ori_sql(sql);
  LOG_DEBUG("Orig sql before replace hosts is [%s].\n", sql);
  if (st.type == STMT_GRANT && st.sql->grant_oper &&
      st.sql->grant_oper->comment) {
    ori_sql = string(ori_sql, 0, st.sql->grant_oper->comment_start_pos - 1);
    LOG_DEBUG(
        "remove comment info in Grant statement, the sql to handle now is "
        "[%s].\n",
        ori_sql.c_str());
  }
  if (st.type == STMT_ALTER_USER && st.sql->create_user_oper &&
      st.sql->create_user_oper->ssl_option) {
    unsigned int start_pos = st.sql->create_user_oper->require_start_pos;
    unsigned int end_pos = st.sql->create_user_oper->require_end_pos;
    ori_sql.replace(start_pos - 1, end_pos - start_pos + 1, "REQUIRE NONE");
  }
  if (st.type == STMT_CREATE_USER && st.sql->create_user_oper &&
      st.sql->create_user_oper->ssl_option) {
    unsigned int start_pos = st.sql->create_user_oper->require_start_pos;
    unsigned int end_pos = st.sql->create_user_oper->require_end_pos;
    ori_sql.replace(start_pos - 1, end_pos - start_pos + 1, "REQUIRE NONE");
  }
  vector<string>::iterator host_it = dbscale_hosts_vec.begin();
  for (; host_it != dbscale_hosts_vec.end(); ++host_it) {
    string grant_sql;
    user_clause_item_list *user_clause = user_clause_list;
    unsigned int head_pos = user_clause->host_head_pos;
    unsigned int end_pos = user_clause->host_end_pos;
    if (head_pos == end_pos)
      grant_sql.assign(ori_sql, 0, head_pos);
    else
      grant_sql.assign(ori_sql, 0, head_pos - 1);
    while (user_clause) {
      grant_user_name_vec.push_back(user_clause->user_name);
      grant_sql.append("@'");
      grant_sql.append(*host_it);
      grant_sql.append("'");
      user_clause = user_clause->next;
      if (user_clause) {
        head_pos = user_clause->host_head_pos;
        unsigned int last_end_pos = end_pos;
        end_pos = user_clause->host_end_pos;
        if (head_pos == end_pos)
          grant_sql.append(ori_sql, last_end_pos, head_pos - last_end_pos);
        else
          grant_sql.append(ori_sql, last_end_pos, head_pos - last_end_pos - 1);
      }
    }
    if (end_pos < ori_sql.length())
      grant_sql.append(ori_sql, end_pos, ori_sql.length() - end_pos);
    LOG_DEBUG("Query sql after replace hosts is [%s].\n", grant_sql.c_str());
    grant_sqls.push_back(grant_sql);
  }
  sql = grant_sqls.front().c_str();
  Statement *new_stmt = NULL;
  re_parser_stmt(&(st.scanner), &new_stmt, sql);
}

void Statement::replace_set_pass_host() {
  string ori_sql(sql);
  LOG_DEBUG("Orig sql before replace hosts is [%s].\n", sql);
  set_op_node *set_oper = st.sql->set_oper;
  unsigned int head_pos = set_oper->host_head_pos;
  unsigned int end_pos = set_oper->host_end_pos;
  if (head_pos != 0) {
    vector<string>::iterator host_it = dbscale_hosts_vec.begin();
    for (; host_it != dbscale_hosts_vec.end(); ++host_it) {
      string set_sql;
      if (head_pos == end_pos)
        set_sql.assign(ori_sql, 0, head_pos);
      else
        set_sql.assign(ori_sql, 0, head_pos - 1);
      set_sql.append("@'");
      set_sql.append(*host_it);
      set_sql.append("'");
      set_sql.append(ori_sql, end_pos, ori_sql.length() - end_pos);
      LOG_DEBUG("Query sql after replace hosts is [%s].\n", set_sql.c_str());
      grant_sqls.push_back(set_sql);
    }
    sql = grant_sqls.front().c_str();
    Statement *new_stmt = NULL;
    re_parser_stmt(&(st.scanner), &new_stmt, sql);
  }
}

void Statement::get_execution_plan_for_max_count_certain(ExecutePlan *plan) {
  AggregateDesc desc;
  unsigned int column_index = 0;
  list<AggregateDesc> aggr_list;
  field_item *field = st.scanner->field_list_head;
  while (field) {
    ++column_index;
    field = field->next;
  }
  bool need_handle_one_more_max = false;
  if (plan->statement->get_stmt_node()->execute_max_count_max_certain)
    need_handle_one_more_max = true;
  if (column_index < 2) {
    throw Error("maxcountcertain should have at least two fields");
  } else if (column_index < 3 && need_handle_one_more_max) {
    throw Error(
        "maxcountcertain with last max should have at least three fields");
  }
  if (!need_handle_one_more_max) {
    desc.column_index = column_index - 2;
    desc.type = AGGREGATE_TYPE_MAX;
    aggr_list.push_back(desc);
    desc.column_index = column_index - 1;
    desc.type = AGGREGATE_TYPE_COUNT;
    aggr_list.push_back(desc);
  } else {
    desc.column_index = column_index - 3;
    desc.type = AGGREGATE_TYPE_MAX;
    aggr_list.push_back(desc);
    desc.column_index = column_index - 2;
    desc.type = AGGREGATE_TYPE_COUNT;
    aggr_list.push_back(desc);
    desc.column_index = column_index - 1;
    desc.type = AGGREGATE_TYPE_MAX;
    aggr_list.push_back(desc);
  }
  if (!st.scanner->children_begin) {
    throw Error("maxcountcertain should have one child record_scan");
  }
  list<SortDesc> group_description;
  int group_column_num = 0;
  record_scan *child_rs = st.scanner->children_begin;
  order_item *group_by_list = child_rs->group_by_list;
  if (group_by_list) {
    do {
      SortDesc sort_desc;
      group_column_num =
          get_pos_from_field_list(group_by_list->field->field_expr, st.scanner);
      sort_desc.column_index = group_column_num;
      sort_desc.sort_order = group_by_list->order;
      string collation_name =
          get_column_collation(group_by_list->field->field_expr, st.scanner);
      set_charset_and_isci_accroding_to_collation(
          collation_name, sort_desc.ctype, sort_desc.is_cs);

      group_description.push_back(sort_desc);
      group_by_list = group_by_list->next;
    } while (group_by_list != child_rs->group_by_list);
  }

  ExecuteNode *group_by_node =
      plan->get_group_node(&group_description, aggr_list);

  Backend *backend = Backend::instance();
  table_link *tl = st.scanner->first_table;
  const char *schema_name =
      tl->join->schema_name ? tl->join->schema_name : schema;
  const char *table_name = tl->join->table_name;
  DataSpace *ds = backend->get_data_space_for_table(schema_name, table_name);
  if (!ds->is_partitioned()) {
    LOG_ERROR(
        "Table %s.%s should be partition table for using maxcountcertain.\n",
        schema_name, table_name);
    throw Error("maxcountcertain can only used in partition table");
  }

  ExecuteNode *send_node = plan->get_send_node();
  PartitionedTable *partition_table = (PartitionedTable *)ds;
  unsigned int partition_num = partition_table->get_real_partition_num();
  for (unsigned int i = 0; i < partition_num; ++i) {
    DataSpace *space = partition_table->get_partition(i);
    const char *used_sql = adjust_stmt_sql_for_shard(space, sql);
    ExecuteNode *fetch_node = plan->get_fetch_node(space, used_sql);
    group_by_node->add_child(fetch_node);
  }
  send_node->add_child(group_by_node);
  plan->set_start_node(send_node);
}

void Statement::init_spark_schema(DataSpace *ds) {
  Backend *backend = Backend::instance();
  Connection *conn = NULL;
  if (!backend->get_inited_spark_schema()) {
    try {
      string schema_name;
      string query_sql(
          "select schema_name from information_schema.SCHEMATA where "
          "schema_name='");
      query_sql.append(spark_dst_schema);
      query_sql.append("'");
      conn = ds->get_connection(get_session());
      if (!conn)
        throw Error("Fail to get conn for Statement::init_spark_schema");
      conn->query_for_one_value(query_sql.c_str(), schema_name, 0);
      conn->get_pool()->add_back_to_free(conn);
      conn = NULL;
      if (schema_name.size() == 0) {
        string sql = "CREATE DATABASE IF NOT EXISTS ";
        sql.append(spark_dst_schema);
        ds->execute_one_modify_sql(sql.c_str());
      }
    } catch (...) {
      LOG_ERROR("Fail to initialize dbscale_spark_tmp databases.\n");
      if (conn) conn->get_pool()->add_back_to_dead(conn);
      throw;
    }
    backend->set_inited_spark_schema();
  }
}
string Statement::get_spark_dst_jdbc_url(DataSpace *ds) {
  string dst_url("jdbc:mysql://");
  if (ds->is_normal()) {
    DataSource *data_source = ds->get_data_source();
    if (data_source) {
      DataServer *data_server = data_source->get_master_server();
      if (data_server) {
        string host_ip = data_server->get_host_ip();
        unsigned int port = data_server->get_port();
        dst_url.append(host_ip);
        dst_url.append(":");
        dst_url.append(SSTR(port));
      }
    }
  }
  return dst_url;
}

DataSpace *Statement::get_subquery_table_dataspace(DataSpace *local_space,
                                                   const char *key_name,
                                                   bool is_dup) {
  Backend *backend = Backend::instance();
  Schema *join_schema = backend->find_schema(TMP_TABLE_SCHEMA);
  Table *tab = NULL;

  local_space->acquire_tmp_table_read_mutex();
  string table_name = local_space->get_tmp_table_name(key_name);
  local_space->release_tmp_table_mutex();
  if (table_name.empty()) {
    local_space->acquire_tmp_table_write_mutex();
    table_name = local_space->get_tmp_table_name(key_name);
    if (table_name.empty()) {
      string key_str;
      if (key_name)
        key_str.assign(key_name);
      else
        key_str.assign("");
      JoinTableInfo table_info = {key_str, "", "", "", ""};
      try {
        tab = (Table *)(local_space->init_join_table_space(
            table_name, &table_info, is_dup, true));
      } catch (...) {
        LOG_ERROR("Got error when init table sub query tmp table dataspace.\n");
        local_space->release_tmp_table_mutex();
        throw;
      }
      backend->add_join_table_spaces(local_space, (DataSpace *)tab);
      local_space->set_tmp_table_name(key_name, table_name);
    }
    local_space->release_tmp_table_mutex();
  }

  // The tmp table dataspace may not exists.
  if (!tab) tab = join_schema->get_table(table_name.c_str());

  return (DataSpace *)tab;
}

bool Statement::is_spark_one_partition_sql() {
  if (st.type == STMT_SELECT && st.execute_on_partition &&
      st.exe_comment_partition_id && st.exe_comment_partition_num &&
      st.exe_comment_schema && st.exe_comment_table) {
    return true;
  }
  return false;
}

bool Statement::handle_executable_comments_after_cnj(ExecutePlan *plan) {
  Backend *backend = Backend::instance();
  if (is_spark_one_partition_sql()) {
    int id = atoi(st.exe_comment_partition_id);
    int num = atoi(st.exe_comment_partition_num);
    DataSpace *ds = backend->get_data_space_for_table(st.exe_comment_schema,
                                                      st.exe_comment_table);
    if (ds && ds->is_partitioned()) {
      PartitionedTable *par_ds = (PartitionedTable *)ds;
      int partition_num = par_ds->get_real_partition_num();
      if (partition_num < num) construct_sql_function(par_ds, num, id);
      re_parser_stmt(&(st.scanner), &(plan->statement), sql);
      Partition *partition = par_ds->get_partition(id % partition_num);
      if (partition) {
        assemble_direct_exec_plan(plan, partition);
        return true;
      }
    } else {
      LOG_ERROR("Execution on partition, should use partition table.\n");
      throw Error("Execution on partition, should use partition table.\n");
    }
  }
  return false;
}

/* the bool return value is to indicate weather the generate execution plan
 * should finished. */
bool Statement::handle_executable_comments_before_cnj(ExecutePlan *plan) {
  Backend *backend = Backend::instance();
  if (st.execute_max_count_certain) {
    get_execution_plan_for_max_count_certain(plan);
    return true;
  }

  if (st.execute_on_datasource) {
    table_link *table = st.table_list_head;
    const char *schema_name = NULL;
    const char *table_name = NULL;
    while (table) {
      schema_name = table->join->schema_name;
      table_name = table->join->table_name;
      if (table_name && !schema_name) {
        throw Error(
            "execute on datasource directly should specify the database name "
            "in statement");
      }
      table = table->next;
    }
    DataSource *datasource =
        backend->find_data_source(st.execute_on_datasource);
    if (!datasource) {
      throw Error("execute on datasource directly but datasource not exists");
    }
    DataSpace *space = datasource->get_one_space();
    if (!space) {
      throw Error(
          "execute on datasource directly but datasource has no related "
          "dataspace");
    }
    assemble_direct_exec_plan(plan, space);
    return true;
  }

  if (st.execute_on_auth_master)
    if (st.type == STMT_ALTER_TABLE || st.type == STMT_CREATE_FUNCTION ||
        st.type == STMT_DROP_FUNCTION || st.type == STMT_CREATE_PROCEDURE ||
        st.type == STMT_DROP_PROC || st.type == STMT_CREATE_TB ||
        st.type == STMT_DROP_DB || st.type == STMT_CREATE_SELECT ||
        st.type == STMT_CREATE_LIKE || st.type == STMT_DROP_TB ||
        st.type == STMT_CREATE_DB || st.type == STMT_ALTER_DB ||
        st.type == STMT_RENAME_TABLE) {
      DataSpace *dataspace = backend->get_auth_data_space();
      assemble_direct_exec_plan(plan, dataspace);
      return true;
    }

  if (st.type == STMT_SELECT && st.dbscale_group) {
    generate_dbscale_wise_group_plan(plan);
    return true;
  }
  if (st.type == STMT_SELECT && st.dbscale_pages) {
    generate_dbscale_pages_plan(plan);
    return true;
  }
  return false;
}

void Statement::construct_sql_function(PartitionedTable *par_ds,
                                       unsigned int partition_num,
                                       unsigned int partition_id) {
  string constructed_sql;
  vector<const char *> *key_vector = par_ds->get_key_names();

  PartitionMethod *part_method = par_ds->get_partition_method();
  string partition_sql = part_method->get_partition_sql(
      key_vector->at(0), partition_num, partition_id);
  record_scan *rs = st.scanner;

  if (rs->opt_where_start_pos && rs->opt_where_end_pos &&
      rs->opt_where_start_pos != rs->opt_where_end_pos) {
    unsigned int after_where_end_pos = 0;
    if (strlen(sql) > rs->opt_where_end_pos + 1)
      after_where_end_pos = rs->opt_where_end_pos + 1;
    constructed_sql.append(sql, rs->start_pos - 1,
                           rs->opt_where_start_pos - rs->start_pos);
    constructed_sql.append(sql, rs->opt_where_start_pos - 1, 6);
    constructed_sql.append(" (");
    constructed_sql.append(
        sql, rs->opt_where_start_pos - 1 + 6,
        rs->opt_where_end_pos - rs->opt_where_start_pos + 1 - 6);
    constructed_sql.append(") AND ");
    constructed_sql.append(partition_sql);
    if (after_where_end_pos)
      constructed_sql.append(sql, after_where_end_pos,
                             rs->end_pos - after_where_end_pos + 1);
  } else {
    unsigned int where_add_pos;
    if (rs->group_start_pos)
      where_add_pos = rs->group_start_pos - 1;
    else if (rs->having_pos)
      where_add_pos = rs->having_pos - 1;
    else if (rs->order_pos)
      where_add_pos = rs->order_pos;
    else if (rs->limit_pos)
      where_add_pos = rs->limit_pos;
    else
      where_add_pos = rs->end_pos;
    constructed_sql.append(sql, rs->start_pos - 1,
                           where_add_pos - rs->start_pos + 1);
    constructed_sql.append(" WHERE ");
    constructed_sql.append(partition_sql);
    constructed_sql.append(sql, where_add_pos, where_add_pos - rs->end_pos);
  }

  LOG_DEBUG("The constructed sql for spark partition is [%s]\n",
            constructed_sql.c_str());
  sql_tmp = constructed_sql;
  sql = sql_tmp.c_str();
}

void Statement::generate_dbscale_pages_plan(ExecutePlan *plan) {
  int page_size = 0;
  if (st.page_size) page_size = atoi(st.page_size);
  if (page_size <= 1) {
    LOG_ERROR("DBScale page size should bigger than 1.\n");
    throw Error("DBScale page size should bigger than 1.");
  }
  if (st.scanner->is_contain_star) {
    LOG_ERROR("Not support '*' in DBScale page size.\n");
    throw Error("Not support '*' in DBScale page size.");
  }
  if (!st.scanner->order_by_list) {
    LOG_ERROR("DBScale page size should have one order by field.\n");
    throw Error("DBScale page size should have one order by field.");
  }
  string new_sql;
  list<AggregateDesc> aggr_list;
  map<int, pair<int, string> > replace_items;
  get_dbscale_page_items(replace_items, aggr_list);

  generate_new_sql(replace_items, new_sql);
  assemble_dbscale_pages_plan(plan, aggr_list, new_sql, page_size);
}

void Statement::assemble_dbscale_pages_plan(ExecutePlan *plan,
                                            list<AggregateDesc> &aggr_list,
                                            string new_sql, int page_size) {
  record_scan *new_rs;
  Statement *new_stmt = NULL;
  re_parser_stmt(&new_rs, &new_stmt, new_sql.c_str());
  add_order_item_list(new_rs->order_by_list, order_by_list, new_rs);

  Backend *backend = Backend::instance();
  list<ExecuteNode *> nodes;
  table_link *table = st.table_list_head;
  const char *schema_name =
      table->join->schema_name ? table->join->schema_name : schema;
  const char *table_name = table->join->table_name;
  DataSpace *space = backend->get_data_space_for_table(schema_name, table_name);
  if (space->is_partitioned()) {
    PartitionedTable *par_table = (PartitionedTable *)space;
    unsigned int partition_num = par_table->get_real_partition_num();
    for (unsigned int i = 0; i < partition_num; ++i) {
      DataSpace *ds = par_table->get_partition(i);
      const char *used_sql = adjust_stmt_sql_for_shard(ds, new_sql.c_str());
      ExecuteNode *fetch_node = plan->get_fetch_node(ds, used_sql);
      nodes.push_back(fetch_node);
    }
  } else if (space->is_normal()) {
    ExecuteNode *fetch_node = plan->get_fetch_node(space, new_sql.c_str());
    nodes.push_back(fetch_node);
  } else {
    LOG_ERROR("Not support space type.\n");
    throw Error("Not suppport space type.");
  }

  ExecuteNode *send_node = plan->get_send_node();
  ExecuteNode *page_node =
      plan->get_dbscale_pages_node(&order_by_list, aggr_list, page_size);
  ExecuteNode *sort_node = plan->get_sort_node(&order_by_list);
  list<ExecuteNode *>::iterator it = nodes.begin();
  for (; it != nodes.end(); ++it) {
    sort_node->add_child(*it);
  }
  page_node->add_child(sort_node);
  send_node->add_child(page_node);
  plan->set_start_node(send_node);
}

void Statement::generate_dbscale_wise_group_plan(ExecutePlan *plan) {
  if (!st.dbscale_group_order || !st.dbscale_group_item) {
    LOG_ERROR("Fail to get dbscale wise group order list or aggr items.\n");
    throw Error("Fail to get dbscale wise group order list or aggr items.");
  }
  if (st.scanner->children_begin) {
    LOG_ERROR("Not support subquery in dbscale wise group.\n");
    throw Error("Not support subquery in dbscale wise group.");
  }
  if (!st.table_list_head || st.table_list_head->next) {
    LOG_ERROR("Only support one table in dbscale wise group.\n");
    throw Error("Only support one table in dbscale wise group.");
  }
  if (st.scanner->is_contain_star) {
    LOG_ERROR("Not support '*' in dbscale wise group.\n");
    throw Error("Not support '*' in dbscale wise group");
  }
  if (st.scanner->order_by_list || !st.scanner->group_by_list ||
      st.scanner->is_with_rollup) {
    LOG_ERROR(
        "DBScale wise group should have no order by field and have group by "
        "field without rollup.\n");
    throw Error(
        "DBScale wise group should have no order by field and have group by "
        "field without rollup.");
  }
  string new_sql;
  list<AggregateDesc> aggr_list;
  map<int, pair<int, string> > replace_items;
  set<string> item_alias;
  get_dbscale_group_items(replace_items, item_alias, aggr_list);

  group_aggr_item *aggr_item = st.dbscale_group_item->next;
  do {
    if (!item_alias.count(string(aggr_item->aggr_field))) {
      LOG_DEBUG("DBScale wise group items not in select list.\n");
      throw Error("DBScale wise group items not in select list");
    }
    aggr_item = aggr_item->next;
  } while (aggr_item != st.dbscale_group_item->next);

  generate_new_sql(replace_items, new_sql);
  assemble_dbscale_wise_group_plan(plan, aggr_list, new_sql);
}

void Statement::assemble_dbscale_wise_group_plan(ExecutePlan *plan,
                                                 list<AggregateDesc> &aggr_list,
                                                 string new_sql) {
  add_group_item_list(st.scanner->group_by_list, group_by_list, st.scanner);
  record_scan *new_rs;
  Statement *new_stmt = NULL;
  re_parser_stmt(&new_rs, &new_stmt, new_sql.c_str());
  add_order_item_list(new_rs->order_by_list, order_by_list, new_rs);

  Backend *backend = Backend::instance();
  list<ExecuteNode *> nodes;
  table_link *table = st.table_list_head;
  const char *schema_name =
      table->join->schema_name ? table->join->schema_name : schema;
  const char *table_name = table->join->table_name;
  DataSpace *space = backend->get_data_space_for_table(schema_name, table_name);
  if (space->is_partitioned()) {
    PartitionedTable *par_table = (PartitionedTable *)space;
    unsigned int partition_num = par_table->get_real_partition_num();
    for (unsigned int i = 0; i < partition_num; ++i) {
      DataSpace *ds = par_table->get_partition(i);
      const char *used_sql = adjust_stmt_sql_for_shard(ds, new_sql.c_str());
      ExecuteNode *fetch_node = plan->get_fetch_node(ds, used_sql);
      nodes.push_back(fetch_node);
    }
  } else if (space->is_normal()) {
    ExecuteNode *fetch_node = plan->get_fetch_node(space, new_sql.c_str());
    nodes.push_back(fetch_node);
  } else {
#if DEBUG
    ACE_ASSERT(0);  // Should not be here
#endif
  }

  ExecuteNode *send_node = plan->get_send_node();
  ExecuteNode *group_node =
      plan->get_dbscale_wise_group_node(&group_by_list, aggr_list);
  ExecuteNode *sort_node = plan->get_sort_node(&order_by_list);
  list<ExecuteNode *>::iterator it = nodes.begin();
  for (; it != nodes.end(); it++) {
    sort_node->add_child(*it);
  }
  group_node->add_child(sort_node);
  send_node->add_child(group_node);
  plan->set_start_node(send_node);
}

void Statement::get_dbscale_page_items(
    map<int, pair<int, string> > &replace_items,
    list<AggregateDesc> &aggr_list) {
  AggregateDesc desc;
  CurrentStatementFunctionType type;
  int column_index = 0;
  field_item *field = st.scanner->field_list_head;
  set<CurrentStatementFunctionType> agg_func;
  while (field) {
    if (field->field_expr) {
      type = field->field_expr->get_cur_func_type();
      if (type != AGGREGATE_TYPE_NON) {
        if (type != AGGREGATE_TYPE_MAX && type != AGGREGATE_TYPE_MIN &&
            type != AGGREGATE_TYPE_COUNT) {
          LOG_ERROR("Only support MAX/MIN/COUNT for dbscale pages.\n");
          throw Error("Only support MAX/MIN/COUNT for dbscale pages.");
        }
        if (agg_func.count(type)) {
          LOG_ERROR("Only support one MAX/MIN/COUNT for dbscale pages.\n");
          throw Error("nly support one MAX/MIN/COUNT for dbscale pages.");
        } else {
          agg_func.insert(type);
        }
        string column;
        if (type == AGGREGATE_TYPE_COUNT) {
          column.assign("1");
        } else {
          string value, schema_name, table_name, column_name;
          Expression *expr =
              ((FunctionExpression *)field->field_expr)->param_list;
          expr->to_string(value);
          if (((ListExpression *)expr)->expr_list_head->expr->type ==
              EXPR_STR) {
            split_column_expr(value, schema_name, table_name, column_name);
            column.assign("`");
            if (!schema_name.empty()) {
              column.append(schema_name);
              column.append("`.`");
            }
            if (!table_name.empty()) {
              column.append(table_name);
              column.append("`.`");
            }
            column.append(column_name);
            column.append("`");
          } else {
            column.append(value);
          }
        }
        replace_items[field->field_expr->start_pos] =
            make_pair(field->field_expr->end_pos, column);
        desc.column_index = column_index;
        desc.type = type;
        aggr_list.push_back(desc);
      }
    }
    ++column_index;
    field = field->next;
  }
}

void Statement::get_dbscale_group_items(
    map<int, pair<int, string> > &replace_items, set<string> &item_alias,
    list<AggregateDesc> &aggr_list) {
  AggregateDesc desc;
  CurrentStatementFunctionType type;
  int column_index = 0;
  field_item *field = st.scanner->field_list_head;
  while (field) {
    if (field->field_expr) {
      type = field->field_expr->get_cur_func_type();
      if (type != AGGREGATE_TYPE_NON) {
        if (type != AGGREGATE_TYPE_MAX && type != AGGREGATE_TYPE_MIN &&
            type != AGGREGATE_TYPE_COUNT) {
          LOG_ERROR("Only support MAX/MIN/COUNT for dbscale wise group now.\n");
          throw Error("Only support MAX/MIN/COUNT for dbscale wise group now.");
        }
        if (!field->alias) {
          LOG_ERROR(
              "Aggregate function in dbscale wise group should has a alias.\n");
          throw Error(
              "Aggregate function in dbscale wise group should has a alias.");
        }
        string name(field->alias);
        if (item_alias.count(name)) {
          LOG_ERROR(
              "Not support the same field alias in dbscale wise group.\n");
          throw Error(
              "Not support the same field alias in dbscale wise group.");
        }
        item_alias.insert(name);
        string column;
        if (type == AGGREGATE_TYPE_COUNT) {
          column.assign("1");
        } else {
          string value, schema_name, table_name, column_name;
          Expression *expr =
              ((FunctionExpression *)field->field_expr)->param_list;
          expr->to_string(value);
          if (((ListExpression *)expr)->expr_list_head->expr->type ==
              EXPR_STR) {
            split_column_expr(value, schema_name, table_name, column_name);
            column.assign("`");
            if (!schema_name.empty()) {
              column.append(schema_name);
              column.append("`.`");
            }
            if (!table_name.empty()) {
              column.append(table_name);
              column.append("`.`");
            }
            column.append(column_name);
            column.append("`");
          } else {
            column.append(value);
          }
        }
        replace_items[field->field_expr->start_pos] =
            make_pair(field->field_expr->end_pos, column);
        desc.column_index = column_index;
        desc.type = type;
        aggr_list.push_back(desc);
      }
    }
    ++column_index;
    field = field->next;
  }

  set<string> order_alias;
  string order("ORDER BY ");
  order_item *item = st.scanner->group_by_list;
  do {
    string str;
    Expression *expr = item->field->field_expr;
    int column_index = item_in_select_fields_pos(expr, st.scanner);
    if (column_index == COLUMN_INDEX_UNDEF) {
      LOG_ERROR(
          "Select list should contain group by field in dbscale wise group.\n");
      throw Error(
          "Select list should contain group by field in dbscale wise gruop.");
    }
    string value, schema_name, table_name, column_name;
    expr->to_string(value);
    if (item != st.scanner->group_by_list) order.append(", ");
    if (expr->type == EXPR_STR) {
      split_column_expr(value, schema_name, table_name, column_name);
      str.assign("`");
      if (!schema_name.empty()) {
        str.append(schema_name);
        str.append("`.`");
      }
      if (!table_name.empty()) {
        str.append(table_name);
        str.append("`.`");
      }
      str.append(column_name);
      str.append("`");
    } else {
      str.append(value);
    }
    order.append(str);
    if (item->order == ORDER_ASC) {
      order.append(" ASC");
    } else if (item->order == ORDER_DESC) {
      order.append(" DESC");
    }
    order_alias.insert(str);
    item = item->next;
  } while (item != st.scanner->group_by_list);

  group_order_item *order_item = st.dbscale_group_order->next;
  do {
    order.append(", ");
    order.append("`");
    order.append(order_item->order_field);
    order.append("`");
    if (order_item->order == ORDER_ASC) {
      order.append(" ASC");
    } else if (order_item->order == ORDER_DESC) {
      order.append(" DESC");
    }
    if (order_alias.count(string(order_item->order_field))) {
      LOG_ERROR("Group by fields should not contain wise order items.\n");
      throw Error("Group by fields should not contain wise order items.");
    }
    order_item = order_item->next;
  } while (order_item != st.dbscale_group_order->next);
  replace_items[st.scanner->group_start_pos] =
      make_pair(st.scanner->group_end_pos, order);
}

void Statement::generate_new_sql(map<int, pair<int, string> > replace_items,
                                 string &new_sql) {
  int last_pos = 0;
  map<int, pair<int, string> >::iterator it = replace_items.begin();
  for (; it != replace_items.end(); it++) {
    new_sql.append(sql + last_pos, it->first - last_pos - 1);
    new_sql.append(it->second.second);
    last_pos = it->second.first;
  }
  new_sql.append(sql + last_pos, strlen(sql) - last_pos);
  LOG_DEBUG("New sql after replace group items [%s].\n", new_sql.c_str());
}

void Statement::assemble_create_trigger_plan(ExecutePlan *plan) {
  const char *schema_name = NULL;
  if (st.routine_d && st.routine_d->tr_node) {
    schema_name = st.routine_d->tr_node->schema_name;
  }
  DataSpace *dataspace = Backend::instance()->get_data_space_for_table(
      schema_name ? schema_name : schema, NULL);
  assemble_direct_exec_plan(plan, dataspace);
}

void Statement::handle_create_or_drop_event() {
  Backend *backend = Backend::instance();
  const char *schema_name = st.sql->event_oper->schema_name
                                ? st.sql->event_oper->schema_name
                                : schema;
  const char *event_name = st.sql->event_oper->event_name;
  DataSpace *dataspace = backend->get_data_space_for_table(schema_name, NULL);
  DataSource *datasource = dataspace->get_data_source();
  if (!datasource) {
#ifdef DEBUG
    ACE_ASSERT(0);
#endif
    LOG_ERROR("Can not get datasource for schema %s\n.", schema_name);
    throw Error("Fail to get datasource.");
  }
  int group_id = datasource->get_group_id();
  char id[32];
  sprintf(id, "%d", group_id);
  if (st.type == STMT_CREATE_EVENT) {
    string query_sql(
        "REPLACE INTO dbscale.events(event_name, group_id) VALUES('");
    query_sql.append(schema_name);
    query_sql.append(".");
    query_sql.append(event_name);
    query_sql.append("', ");
    query_sql.append(id);
    query_sql.append(")");
    try {
      dataspace->execute_one_modify_sql(query_sql.c_str());
    } catch (...) {
      LOG_ERROR("Fail to store event infomation on server.\n");
      throw;
    }
  } else if (st.type == STMT_DROP_EVENT) {
    string query_sql("DELETE FROM dbscale.events WHERE event_name ='");
    query_sql.append(schema_name);
    query_sql.append(".");
    query_sql.append(event_name);
    query_sql.append("'");
    try {
      dataspace->execute_one_modify_sql(query_sql.c_str());
    } catch (...) {
      LOG_ERROR("Fail to remove event infomation on server.\n");
      throw;
    }
  }
}

bool Statement::check_simple_select(record_scan *rs) {
  // rs should not contain distinct
  if (rs->options & SQL_OPT_DISTINCT) return false;

  // rs should not contain aggregate function
  field_item *field = rs->field_list_head;
  try {
    while (field) {
      if (field->field_expr) {
        if (is_expression_contain_aggr(field->field_expr, rs, false)) {
          return false;
        }
      }
      field = field->next;
    }
  } catch (...) {
    return false;
  }

  // rs should not contain group by, having, limit, order by
  if (rs->group_by_list || rs->having || rs->limit || rs->order_by_list)
    return false;

  return true;
}
bool Statement::check_rs_no_invalid_aggr(record_scan *rs) {
  if (rs->options & SQL_OPT_DISTINCT) return false;
  field_item *field = rs->field_list_head;
  try {
    while (field) {
      if (field->field_expr) {
        is_expression_contain_aggr(field->field_expr, rs, false);
      }
      field = field->next;
    }
  } catch (...) {
    return false;
  }
  return true;
}
table_link *Statement::get_one_table_link(record_scan *rs, DataSpace *space) {
  if (!rs || !space) return NULL;
  Backend *backend = Backend::instance();

  table_link *tmp_tb = rs->first_table;
  while (tmp_tb) {
    if (tmp_tb->join->cur_rec_scan == rs) {
      const char *schema_name =
          tmp_tb->join->schema_name ? tmp_tb->join->schema_name : schema;
      if (!strcasecmp(schema_name, "dbscale_tmp")) {
        tmp_tb = tmp_tb->next;
        continue;
      }
      const char *table_name = tmp_tb->join->table_name;
      DataSpace *tmp_space =
          backend->get_data_space_for_table(schema_name, table_name);
      if (space == tmp_space) return tmp_tb;
    }
    tmp_tb = tmp_tb->next;
  }
  return NULL;
}
bool Statement::check_rs_contains_table(record_scan *rs) {
  table_link *tmp_tb = rs->first_table;
  while (tmp_tb) {
    if (tmp_tb->join->cur_rec_scan == rs) return true;
    tmp_tb = tmp_tb->next;
  }
  return false;
}
int Statement::get_child_rs_num(record_scan *rs) {
  int ret = 0;
  record_scan *tmp = rs->children_begin;
  for (; tmp; tmp = tmp->next) ret++;
  return ret;
}
void Statement::refresh_tables_for_merged_subquery(record_scan *old_rs,
                                                   record_scan *new_rs) {
  DataSpace *space = get_record_scan_table_subquery_space(old_rs);
  table_link *table = get_one_table_link(new_rs, space);
  if (!table) {
    LOG_ERROR(
        "Do not find table link for merge child with no table parent "
        "situation.\n");
    throw Error("Do not support sql, can not find table link for subquery.");
  }
  spaces.clear();
  spaces.push_back(space);
  if (space->get_data_source()) {
    par_table_num = 0;
    par_tables.clear();
  } else {
    par_table_num = 1;
    par_tables.clear();
    par_tables.push_back(table);
  }
  record_scan_one_space_tmp_map[st.scanner] = space;
  record_scan_all_table_spaces[table] = space;
  record_scan_par_table_map[table->join->cur_rec_scan] = table;
}

string Statement::remove_schema_from_full_columns() {
  string query_sql(sql);
  LOG_DEBUG("Query before remove schema from columns [%s].\n",
            query_sql.c_str());
  expr_list_item *str_expr_list_head =
      get_latest_stmt_node()->full_column_list_head;
  expr_list_item *str_expr_list_end =
      get_latest_stmt_node()->full_column_list_end;

  set<Expression *, exprposcmp> expression_set;
  for (expr_list_item *str_expr_item = str_expr_list_head;
       str_expr_item != str_expr_list_end; str_expr_item = str_expr_item->next)
    expression_set.insert(str_expr_item->expr);

  if (str_expr_list_end) expression_set.insert(str_expr_list_end->expr);

  set<Expression *>::iterator it = expression_set.begin();
  for (; it != expression_set.end(); ++it) {
    if ((*it)->type == EXPR_STR) {
      StrExpression *str_expr = (StrExpression *)(*it);
      string replace_sql = str_expr->get_no_schema_column();
      if (!replace_sql.empty() && str_expr->start_pos && str_expr->end_pos) {
        query_sql.replace(str_expr->start_pos - 1,
                          str_expr->end_pos - str_expr->start_pos + 1,
                          replace_sql);
      }
    }
  }
  LOG_DEBUG("Query after remove schema from columns [%s].\n",
            query_sql.c_str());
  return query_sql;
}

void Statement::assemble_dbscale_show_create_oracle_seq_plan(
    ExecutePlan *plan) {
  ExecuteNode *node = plan->get_dbscale_show_create_oracle_seq_node(
      st.sql->seq->schema_name, st.sql->seq->sequence_name);
  plan->set_start_node(node);
}

void Statement::assemble_dbscale_show_seq_status_plan(ExecutePlan *plan) {
  ExecuteNode *node = plan->get_dbscale_show_seq_status_node(
      st.sql->seq->schema_name, st.sql->seq->sequence_name);
  plan->set_start_node(node);
}

void Statement::assemble_dbscale_show_fetchnode_buffer_usage_plan(
    ExecutePlan *plan) {
  ExecuteNode *node = plan->get_dbscale_show_fetchnode_buffer_usage_node();
  plan->set_start_node(node);
}
void Statement::assemble_dbscale_show_trx_block_info_plan(ExecutePlan *plan,
                                                          bool is_local) {
  ExecuteNode *node = plan->get_dbscale_show_trx_block_info_node(is_local);
  plan->set_start_node(node);
}

void Statement::assemble_dbscale_internal_set_plan(ExecutePlan *plan) {
  ExecuteNode *node = plan->get_dbscale_internal_set_node();
  plan->set_start_node(node);
}

void Statement::assemble_dbscale_show_prepare_cache_status_plan(
    ExecutePlan *plan) {
  ExecuteNode *node = plan->get_dbscale_show_prepare_cache_status_node();
  plan->set_start_node(node);
}

void Statement::assemble_dbscale_flush_prepare_cache_hit_plan(
    ExecutePlan *plan) {
  ExecuteNode *node = plan->get_dbscale_flush_prepare_cache_hit_node();
  plan->set_start_node(node);
}

ExecuteNode *Statement::try_assemble_bridge_mark_sql_plan(ExecutePlan *plan,
                                                          table_link *table) {
  if (table->join && table->join->schema_name &&
      strcasecmp("retl", table->join->schema_name) == 0 &&
      table->join->table_name &&
      strcasecmp("retl_mark", table->join->table_name) == 0) {
    if (st.type != STMT_UPDATE) {
      LOG_INFO(
          "bridge mark sql is not type of UPDATE, ignore it, the sql is [%s]\n",
          sql);
      return NULL;
    }
    plan->session->set_bridge_mark_sql(
        sql, st.sql->update_oper->update_table_end_pos);
    return plan->get_return_ok_node();
  }
  return NULL;
}

bool check_table_and_column_name_for_par_key(const char *table_name,
                                             const char *table_alias,
                                             const char *par_key,
                                             const char *str) {
  string field_item_str = str;
  vector<string> names;
  boost::split(names, field_item_str, boost::is_any_of("."));
  if (names.size() == 1) {
    // if no table name specified, we check par_key only,
    // if the par_key != names[0] then we shouldn't push down the modify_select
    return strcasecmp(par_key, names[0].c_str()) == 0;
  } else if (names.size() == 2) {
    return strcasecmp(table_alias ? table_alias : table_name,
                      names[0].c_str()) == 0 &&
           strcasecmp(par_key, names[1].c_str()) == 0;
  }
  return false;
}

/**
 * if this modify_select sql can pushdown to backend server(s), return true
 * if modify_table is a normal_table:
 *   then select_nodes must has the same DataSpace as the modify table
 * if modify_table is a par_table:
 *   then 3 things need to check:
 *   1, all select_nodes are fetch nodes(to exclude distinct, aggr funcs etc.)
 *   2, select table, modify table has the same part rules
 *   3, par_key_index in select_field_list == par_key_index in modify_field_list
 *   4, par_key of select table in select field list can't be complex expr
 *
 * TODO sql: insert into t1 select t2.pk, t2.c2 from t2,t3 where t2,pk = t3.pk;
 *    t1, t2 ,t3 have the same part_scheme.
 *    we only support that the select fields belong to table t2 which is the
 *    first table in the join tables, cause the select_par_table is t2 here
 *    should support when the sql is:
 *     insert into t1 select t3.pk, t3.c2 from t2,t3 where t2.pk = t3.pk;
 * */
bool Statement::check_can_pushdown_for_modify_select(
    PartitionedTable &select_par_table, record_scan *select_rs,
    vector<ExecuteNode *> &select_nodes, PartitionedTable *modify_par_table,
    DataSpace *modify_space) {
  if (hex_pos.size() > 0) return false;
  if (is_cross_node_join() || select_par_table.is_tmp_table_space() ||
      (modify_space && modify_space->is_tmp_table_space()) ||
      (modify_par_table && modify_par_table->is_tmp_table_space()))
    return false;
  vector<ExecuteNode *>::iterator n_it = select_nodes.begin();
  if (!modify_par_table && modify_space) {
    for (; n_it != select_nodes.end(); ++n_it) {
      ExecuteNode *node = *n_it;
      if (!node->is_fetch_node()) return false;
      DataSource *source1 = node->get_dataspace()->get_data_source();
      DataSource *source2 = modify_space->get_data_source();
      if (!is_share_same_server(source1->get_master_server(),
                                source2->get_master_server())) {
        return false;
      }
    }
    return true;
  }
  if (!modify_par_table || modify_space) return false;

  // 1, to check all select nodes are fetch nodes
  n_it = select_nodes.begin();
  for (; n_it != select_nodes.end(); ++n_it) {
    if (!(*n_it)->is_fetch_node()) return false;
  }
  // 2, to check part rules
  // TODO check partition_scheme contents instead of pointers
  if (select_par_table.get_partition_scheme() !=
      modify_par_table->get_partition_scheme())
    return false;
  // 3, to check par key position, and select_tb_pk_field
  const char *modify_tb_pk_name = modify_par_table->get_key_names()->at(0);
  const char *select_tb_pk_name = select_par_table.get_key_names()->at(0);
  int modify_tb_pk_pos = 0, select_tb_pk_pos = 0;
  name_item *modify_field_list = st.sql->insert_oper->insert_column_list;
  field_item *select_field_list = select_rs->field_list_head;
  if (!select_field_list || !select_field_list->field_expr) {
    return false;
  }
  bool is_star_expr = false;
  const char *tb_schema = NULL, *tb_name = NULL;
  if (select_field_list->field_expr->get_str_value()) {
    vector<string> names;
    tb_name = select_par_table.get_name();
    string full_field_name = select_field_list->field_expr->get_str_value();
    boost::split(names, full_field_name, boost::is_any_of("."));
    switch (names.size()) {
      case 1:
        break;
      case 2: {
        const char *table_alias = select_rs->first_table->join->alias;
        tb_name = table_alias ? table_alias : tb_name;
        if (strcasecmp(names[0].c_str(), tb_name) != 0) return false;
      } break;
      default:
        return false;
    }
    is_star_expr = strcmp(names.back().c_str(), "*") == 0;
  }
  string full_table_name;
  vector<unsigned int> key_pos;
  int tmp_pos = 1;
  if (modify_field_list) {
    do {
      vector<string> names;
      string full_field_name = modify_field_list->name;
      boost::split(names, full_field_name, boost::is_any_of("."));
      if (strcasecmp(names.back().c_str(), modify_tb_pk_name) == 0) {
        modify_tb_pk_pos = tmp_pos;
        break;
      }
      modify_field_list = modify_field_list->next;
      ++tmp_pos;
    } while (modify_field_list != st.sql->insert_oper->insert_column_list);
  } else {
    tb_schema = modify_par_table->get_schema()->get_name();
    tb_name = modify_par_table->get_name();
    splice_full_table_name(tb_schema, tb_name, full_table_name);
    modify_par_table->get_key_pos_vec(tb_schema, tb_name, key_pos,
                                      get_session());
    modify_tb_pk_pos = key_pos[0] + 1;
  }
  if (!is_star_expr) {
    select_tb_pk_pos = modify_tb_pk_pos;
    tmp_pos = 1;
    while (tmp_pos != modify_tb_pk_pos && select_field_list) {
      ++tmp_pos;
      select_field_list = select_field_list->next;
    }
    if (select_field_list && select_field_list->field_expr->get_str_value()) {
      tb_name = select_par_table.get_name();
      join_node *first_table = select_rs->first_table->join;
      if (!check_table_and_column_name_for_par_key(
              tb_name, first_table ? first_table->alias : NULL,
              select_tb_pk_name,
              select_field_list->field_expr->get_str_value())) {
        return false;
      }
    } else {
      return false;
    }
  } else {
    tb_schema = select_par_table.get_schema_name();
    tb_name = select_par_table.get_name();
    full_table_name.clear();
    splice_full_table_name(tb_schema, tb_name, full_table_name);
    key_pos.clear();
    select_par_table.get_key_pos_vec(tb_schema, tb_name, key_pos,
                                     get_session());
    select_tb_pk_pos = key_pos[0] + 1;
    if (select_tb_pk_pos != modify_tb_pk_pos) {
      // insert into t2 (c2,c3,c1) select *, c1 from t1
      if (!select_field_list->next) return false;
      TableInfoCollection *tic = TableInfoCollection::instance();
      TableInfo *ti = tic->get_table_info_for_read(full_table_name);
      vector<TableColumnInfo> *column_info_vec;
      try {
        column_info_vec =
            ti->element_table_column->get_element_vector_table_column_info(
                get_session());
      } catch (...) {
        LOG_ERROR(
            "Error occured when try to get table info "
            "column(vector_table_column_info) of table [%s]\n",
            full_table_name.c_str());
        ti->release_table_info_lock();
        throw;
      }
      ti->release_table_info_lock();
      tmp_pos = 1;
      modify_tb_pk_pos -= column_info_vec->size();
      select_field_list = select_field_list->next;
      while (select_field_list && tmp_pos != modify_tb_pk_pos) {
        select_field_list = select_field_list->next;
        ++tmp_pos;
      }
      if (select_field_list && select_field_list->field_expr->get_str_value()) {
        vector<string> names;
        string full_field_name = select_field_list->field_expr->get_str_value();
        join_node *first_table = select_rs->first_table->join;
        tb_name = select_par_table.get_name();
        if (!check_table_and_column_name_for_par_key(
                tb_name, first_table ? first_table->alias : NULL,
                select_tb_pk_name, full_field_name.c_str())) {
          return false;
        }
      } else {
        return false;
      }
    }
  }
  return true;
}

void Statement::assemble_modify_node_according_to_fetch_node(
    ExecutePlan &plan, const vector<ExecuteNode *> &fetch_nodes,
    PartitionedTable *modify_par_table) {
  vector<ExecuteNode *>::const_iterator node_it = fetch_nodes.begin();
  map<DataSpace *, const char *> spaces_map;
  map<DataSpace *, int> sql_count_map;
  for (; node_it != fetch_nodes.end(); ++node_it) {
    DataSpace *space = (*node_it)->get_dataspace();
    if (!space->get_virtual_machine_id()) {
      spaces_map[space] = sql;
      sql_count_map[space] = 1;
    } else {
      string new_sql_tmp;
      adjust_virtual_machine_schema(
          space->get_virtual_machine_id(), space->get_partition_id(), sql,
          modify_par_table->get_schema_name(), get_latest_stmt_node(),
          record_scan_all_table_spaces, new_sql_tmp);
      record_shard_sql_str(new_sql_tmp);
      spaces_map[space] = get_last_shard_sql();
      sql_count_map[space] = 1;
    }
  }
  ExecuteNode *node =
      plan.get_mul_modify_node(spaces_map, false, &sql_count_map);
  if (spaces_map.size() > 1)
    plan.session->set_affected_servers(AFFETCED_MUL_SERVER);
  plan.set_start_node(node);
}

bool Statement::is_partition_table_contain_partition_key(const char *par_key) {
  if (!par_key || st.type != STMT_CREATE_TB) return false;
  if (!st.sql || !st.sql->create_tb_oper) {
    string error_msg;
    error_msg.assign("sql statement node memory error.");
    throw Error(error_msg.c_str());
  }
  name_item *col_list = st.sql->create_tb_oper->column_name_list;
  while (col_list) {
    const char *col = col_list->name;
    if (strcasecmp(col, par_key) == 0) return true;
    col_list = col_list->next;
  }
  return false;
}

// replace ROWNUM compare INTNUM to "1"
void Statement::do_replace_rownum(record_scan *rs, string &old_sql) {
  where_rownum_expr *re = rs->where_rownum_expr;
  if (re) {
    if (rs->subquerytype == SUB_SELECT_ONE_COLUMN ||
        rs->subquerytype == SUB_SELECT_MUL_COLUMN ||
        rs->subquerytype == SUB_SELECT_ONE_COLUMN_MIN ||
        rs->subquerytype == SUB_SELECT_ONE_COLUMN_MAX ||
        rs->subquerytype == SUB_SELECT_UNSUPPORT) {
      LOG_ERROR("Not yet support 'ROWNUM & IN/ALL/ANY/SOME subquery'\n");
      throw Error("Not yet support 'ROWNUM & IN/ALL/ANY/SOME subquery'");
    }
    while (re) {
      string replace_str = "1";
      unsigned int start_pos = re->start_pos;
      unsigned int end_pos = re->end_pos;
      if (start_pos < rs->opt_where_start_pos ||
          end_pos > rs->opt_where_end_pos) {
        LOG_ERROR("Only support 'ROWNUM' in where condition.\n");
        throw Error("Only support 'ROWNUM' in where condition.");
      }
      ConditionAndOr and_or_type = is_and_or_condition(re->cond, rs);
      if (and_or_type == CONDITION_NO_COND || and_or_type == CONDITION_OR) {
        LOG_ERROR("Only support 'ROWNUM' in 'AND' Expression.\n");
        throw Error("Only support 'ROWNUM' in 'AND' Expression.");
      }
      for (unsigned int j = 0; j < end_pos - start_pos; ++j)
        replace_str = replace_str + " ";
      old_sql.replace(start_pos - 1, end_pos - start_pos + 1, replace_str);
      re = re->next;
    }
  }
  LOG_DEBUG("do_replace_rownum completed! sql is %s.\n", old_sql.c_str());
}

// add limit n after where
unsigned int Statement::do_add_limit_after_where(record_scan *rs,
                                                 string &old_sql,
                                                 unsigned int add_pos) {
  LOG_DEBUG("start to do_add_limit_after_where.\n");
  string add_limit_str = " limit ";
  uint64_t limit_n = (unsigned long)(~0);
  where_rownum_expr *re = rs->where_rownum_expr;

  if (!re) return 0;
  while (re) {
    uint64_t n = 0;
    switch (re->type) {
      case COMPARE_EQ:
        if (re->val == 1)
          n = 1;
        else
          n = 0;
        break;
      case COMPARE_NE:
        n = re->val - 1;
        break;
      case COMPARE_GE:
        if (re->val == 0 || re->val == 1)
          n = (unsigned long)(~0);
        else
          n = 0;
        break;
      case COMPARE_GT:
        if (re->val == 0)
          n = (unsigned long)(~0);
        else
          n = 0;
        break;
      case COMPARE_LE:
        n = re->val;
        break;
      case COMPARE_LT:
        if (re->val == 0)
          n = 0;
        else
          n = re->val - 1;
        break;
      default:
        LOG_ERROR("Find unsuport 'ROWNUM' compare type.\n");
        throw Error("Find unsuport 'ROWNUM' compare type.");
    }
    limit_n = limit_n < n ? limit_n : n;
    re = re->next;
  }

  char tmp_c[30];
  sprintf(tmp_c, "%lu", limit_n);
  add_limit_str = add_limit_str + string(tmp_c);
  old_sql.replace(rs->end_pos + add_pos, 0, add_limit_str);
  return add_limit_str.length();
}
void Statement::replace_rownum(record_scan *rs, string &old_sql) {
  do_replace_rownum(rs, old_sql);
  record_scan *rs_it = rs->children_begin;
  if (!rs_it) return;
  for (rs_it = rs->children_begin;; rs_it = rs_it->next) {
    replace_rownum(rs_it, old_sql);
    if (rs_it == rs->children_end) return;
  }
}
unsigned int Statement::add_limit_after_where(record_scan *rs, string &old_sql,
                                              unsigned int add_pos) {
  unsigned int tmp_add = do_add_limit_after_where(rs, old_sql, add_pos);
  record_scan *rs_it = rs->children_begin;
  if (!rs_it) return tmp_add;
  for (rs_it = rs->children_begin;; rs_it = rs_it->next) {
    add_pos = add_pos + add_limit_after_where(rs_it, old_sql, add_pos);
    if (rs_it == rs->children_end) return add_pos;
  }
}
void Statement::replace_rownum_add_limit() {
  rownum_tmp_sql = string(sql);
  string tmp_str = rownum_tmp_sql;
  boost::to_lower(tmp_str);
  if (tmp_str.find("limit") != string::npos) {
    LOG_ERROR("Not yet suport 'ROWNUM & LIMIT'\n");
    throw Error("Not yet suport 'ROWNUM & LIMIT'");
  }
  record_scan *rs = st.cur_rec_scan;
  replace_rownum(rs, rownum_tmp_sql);
  add_limit_after_where(rs, rownum_tmp_sql, 0);
  sql = rownum_tmp_sql.c_str();
  LOG_DEBUG("replace rownum add limit sql is: %s.\n", sql);
}

void Statement::assemble_move_table_to_recycle_bin_plan(ExecutePlan *plan) {
  string from_schema, from_table, to_table;
  table_link *table = st.table_list_head;
  from_schema = table->join->schema_name ? table->join->schema_name : schema;
  from_table = table->join->table_name;
  to_table = from_table + RECYCLE_TABLE_POSTFIX;
  LOG_INFO(
      "assemble move table to recycle bin plan from table: [%s.%s] to table: "
      "[%s.%s]\n",
      from_schema.c_str(), from_table.c_str(), from_schema.c_str(),
      to_table.c_str());
  // precheck
  if (get_table_from_recycle_bin(from_schema.c_str(), from_table.c_str()) !=
      NULL) {
    throw Error(
        "Can't drop/truncate this table, should clean it from recycle bin "
        "first");
  }
  // create tmp dataspace
  DataSpace *tmp_dspace = create_tmp_dataspace_from_table(
      from_schema.c_str(), from_table.c_str(), to_table.c_str());

  Schema *the_schema = Backend::instance()->find_schema(from_schema.c_str());
  // assemble move to recycle bin plan
  ExecuteNode *ok_merge_node = plan->get_ok_merge_node();
  ExecuteNode *ok_node = plan->get_ok_node();
  try {
    ExecuteNode *rename_ok_merge_node = NULL;
    const char *rename_sql_format = "RENAME TABLE `%s`.`%s` TO `%s`.`%s`";
    string rename_sql = (boost::format(rename_sql_format) % from_schema %
                         from_table % from_schema % to_table)
                            .str();
    DataSpace *dspace = Backend::instance()->get_data_space_for_table(
        from_schema.c_str(), from_table.c_str());
    if (dspace->is_partitioned()) {
      PartitionedTable *pt = (PartitionedTable *)dspace;
      rename_ok_merge_node = assemble_modify_node_for_partitioned_table(
          plan, rename_sql.c_str(), pt);
    } else {
      rename_ok_merge_node = plan->get_ok_merge_node();
      ExecuteNode *rename_node =
          plan->get_modify_node(dspace, rename_sql.c_str(), false);
      rename_ok_merge_node->add_child(rename_node);
    }
    // on auth server
    DataSpace *auth_space = Backend::instance()->get_auth_data_space();
    ExecuteNode *rename_on_auth_node =
        plan->get_modify_node(auth_space, rename_sql.c_str(), false);
    rename_ok_merge_node->add_child(rename_on_auth_node);

    ok_merge_node->add_child(rename_ok_merge_node);
    const char *recycle_type = NULL;
    switch (st.type) {
      case STMT_TRUNCATE: {
        ExecuteNode *create_tb_ok_merge_node = NULL;
        const char *create_tb_sql_format =
            "CREATE TABLE `%s`.`%s` LIKE `%s`.`%s`";
        string create_tb_sql =
            (boost::format(create_tb_sql_format) % from_schema % from_table %
             from_schema % to_table)
                .str();
        if (dspace->is_partitioned()) {
          PartitionedTable *pt = (PartitionedTable *)dspace;
          create_tb_ok_merge_node = assemble_modify_node_for_partitioned_table(
              plan, create_tb_sql.c_str(), pt);
        } else {
          create_tb_ok_merge_node = plan->get_ok_merge_node();
          ExecuteNode *create_tb_node =
              plan->get_modify_node(dspace, create_tb_sql.c_str(), false);
          create_tb_ok_merge_node->add_child(create_tb_node);
        }
        // on auth server
        ExecuteNode *create_tb_on_auth_node =
            plan->get_modify_node(auth_space, create_tb_sql.c_str(), false);
        create_tb_ok_merge_node->add_child(create_tb_on_auth_node);
        ok_merge_node->add_child(create_tb_ok_merge_node);
      }
        recycle_type = RECYCLE_TYPE_TRUNCATE;
        break;
      case STMT_DROP_TB:
        recycle_type = RECYCLE_TYPE_DROP;
        break;
      default:
        throw Error(
            "move table to recycle bin should only used for truncate/drop "
            "table");
    }
    DataSpace *auth_dspace = Backend::instance()->get_auth_data_space();
    const char *recycle_bin_sql_format =
        "INSERT INTO dbscale.table_recycle_bin(original_table_name, "
        "new_table_name, recycle_type) VALUES('%s.%s', '%s.%s', '%s')";
    boost::format f = boost::format(recycle_bin_sql_format) % from_schema %
                      from_table % from_schema % to_table % recycle_type;
    ExecuteNode *insert_recycle_bin_node =
        plan->get_modify_node(auth_dspace, f.str().c_str(), false);
    ok_merge_node->add_child(insert_recycle_bin_node);
  } catch (...) {
    destroy_tmp_dataspace(tmp_dspace, the_schema, to_table.c_str());
    throw;
  }
  destroy_tmp_dataspace(tmp_dspace, the_schema, to_table.c_str());
  ok_node->add_child(ok_merge_node);
  plan->set_start_node(ok_node);
}

const char *Statement::get_table_from_recycle_bin(const char *from_schema,
                                                  const char *from_table) {
  DataSpace *auth_dspace = Backend::instance()->get_auth_data_space();
  Connection *conn = NULL;
  string recycle_type;
  try {
    conn = auth_dspace->get_connection(NULL, "dbscale", true);
    if (!conn) {
      throw Error(
          "can not get connection from auth to get_table_from_recycle_bin");
    }
    string full_tb_name = from_schema;
    full_tb_name.append(".").append(from_table);
    const char *check_sql_format =
        "SELECT recycle_type FROM dbscale.table_recycle_bin WHERE "
        "original_table_name ='%s.%s'";
    string check_sql =
        (boost::format(check_sql_format) % from_schema % from_table).str();
    conn->query_for_one_value(check_sql.c_str(), recycle_type, 0);
    conn->get_pool()->add_back_to_free(conn);
  } catch (...) {
    if (conn) conn->get_pool()->add_back_to_dead(conn);
    throw;
  }
  LOG_DEBUG("get table: [%s.%s] from table_recycle_bin, recycle_type: [%s]\n",
            from_schema, from_table, recycle_type.c_str());
  if (recycle_type.empty()) {
    return 0;
  } else if (recycle_type == RECYCLE_TYPE_TRUNCATE) {
    return RECYCLE_TYPE_TRUNCATE;
  } else if (recycle_type == RECYCLE_TYPE_DROP) {
    return RECYCLE_TYPE_DROP;
  } else {
    throw Error("dbscale.table_recycle_bin crupted");
  }
}

DataSpace *Statement::create_tmp_dataspace_from_table(const char *from_schema,
                                                      const char *from_table,
                                                      const char *to_table) {
  LOG_DEBUG("create tmp dataspace from table: [%s.%s] to table: [%s.%s]\n",
            from_schema, from_table, from_schema, to_table);
  Backend *backend = Backend::instance();
  Schema *schema = backend->find_schema(from_schema);
  if (!schema) {
    LOG_DEBUG("Can't find schema: [%s] using catalog\n", from_schema);
    return backend->get_catalog();
  }
  Table *t = backend->get_table_by_name(from_schema, from_table);
  if (!t) {
    LOG_DEBUG("Can't find table: [%s.%s] using catalog\n", from_schema,
              from_table);
    return backend->get_catalog();
  }
  Table *ret = NULL;
  DataSource *ds = t->get_data_source();
  if (!t->is_partitioned()) {
    ret = new Table(to_table, ds, schema, t->is_independent(), NULL);
  } else {
    PartitionedTable *pt = (PartitionedTable *)t;
    const char *p_key = pt->get_key_names()->operator[](0);
    size_t len_key1 = strlen(p_key);
    char *tmp = new char[len_key1 + 1];
    strncpy(tmp, p_key, len_key1 + 1);
    add_dynamic_str(tmp);
    PartitionScheme *p_scheme = pt->get_partition_scheme();
    PartitionType p_type = p_scheme->get_type();
    PartitionedTable *new_pt = new PartitionedTable(
        to_table, tmp, p_scheme, schema, p_type, pt->is_independent(), NULL,
        pt->get_virtual_times());
    new_pt->set_virtual_map(pt->get_virtual_map());
    ret = new_pt;
  }
  schema->add_table(ret);
  backend->add_data_space(ret);
  return ret;
}

void Statement::destroy_tmp_dataspace(DataSpace *dspace, Schema *the_schema,
                                      const char *table_name) {
  if (dspace != Backend::instance()->get_catalog()) {
    Backend::instance()->remove_data_space(dspace);
    the_schema->remove_table(table_name);
    delete dspace;
  }
}

void Statement::assemble_dbscale_restore_table_plan(ExecutePlan *plan,
                                                    const char *schema,
                                                    const char *table) {
  LOG_DEBUG("assemble_dbscale_restore_table_plan for table [%s.%s]\n", schema,
            table);
  string from_schema, from_table, to_table;
  from_schema = schema;
  to_table = table;
  from_table = to_table + RECYCLE_TABLE_POSTFIX;
  // precheck
  const char *recycle_type = get_table_from_recycle_bin(schema, table);
  if (recycle_type == NULL) {
    throw Error("table not found to restore");
  }
  string recycle_type_str = recycle_type;
  DataSpace *dspace =
      Backend::instance()->get_data_space_for_table(schema, table);
  ExecuteNode *ok_node = plan->get_ok_node();
  ExecuteNode *ok_merge_node = plan->get_ok_merge_node();

  ExecuteNode *precheck_ok_merge_node = plan->get_ok_merge_node();
  ExecuteNode *restore_precheck_node =
      plan->get_restore_recycle_table_precheck_node(
          schema, from_table.c_str(), table, recycle_type, dspace);
  precheck_ok_merge_node->add_child(restore_precheck_node);
  ok_merge_node->add_child(precheck_ok_merge_node);

  // create tmp dataspace
  DataSpace *tmp_dspace = create_tmp_dataspace_from_table(
      from_schema.c_str(), to_table.c_str(), from_table.c_str());

  Schema *the_schema = Backend::instance()->find_schema(from_schema.c_str());
  DataSpace *auth_space = Backend::instance()->get_auth_data_space();
  try {
    if (recycle_type_str == RECYCLE_TYPE_TRUNCATE) {
      ExecuteNode *drop_table_ok_merge_node = NULL;
      const char *drop_tb_sql_format = "DROP TABLE `%s`.`%s`";
      string drop_tb_sql =
          (boost::format(drop_tb_sql_format) % from_schema % to_table).str();
      if (dspace->is_partitioned()) {
        PartitionedTable *pt = (PartitionedTable *)dspace;
        drop_table_ok_merge_node = assemble_modify_node_for_partitioned_table(
            plan, drop_tb_sql.c_str(), pt);
      } else {
        drop_table_ok_merge_node = plan->get_ok_merge_node();
        ExecuteNode *drop_tb_node =
            plan->get_modify_node(dspace, drop_tb_sql.c_str(), false);
        drop_table_ok_merge_node->add_child(drop_tb_node);
      }
      ExecuteNode *drop_tb_on_auth_node =
          plan->get_modify_node(auth_space, drop_tb_sql.c_str(), false);
      drop_table_ok_merge_node->add_child(drop_tb_on_auth_node);
      ok_merge_node->add_child(drop_table_ok_merge_node);
    }

    ExecuteNode *rename_ok_merge_node = NULL;
    const char *rename_sql_format = "RENAME TABLE `%s`.`%s` TO `%s`.`%s`";
    string rename_sql = (boost::format(rename_sql_format) % from_schema %
                         from_table % from_schema % to_table)
                            .str();
    if (dspace->is_partitioned()) {
      PartitionedTable *pt = (PartitionedTable *)dspace;
      rename_ok_merge_node = assemble_modify_node_for_partitioned_table(
          plan, rename_sql.c_str(), pt);
    } else {
      rename_ok_merge_node = plan->get_ok_merge_node();
      ExecuteNode *rename_node =
          plan->get_modify_node(dspace, rename_sql.c_str(), false);
      rename_ok_merge_node->add_child(rename_node);
    }
    ExecuteNode *rename_tb_on_auth =
        plan->get_modify_node(auth_space, rename_sql.c_str(), false);
    rename_ok_merge_node->add_child(rename_tb_on_auth);
    ok_merge_node->add_child(rename_ok_merge_node);

    DataSpace *auth_dspace = Backend::instance()->get_auth_data_space();
    const char *recycle_bin_sql_format =
        "DELETE FROM dbscale.table_recycle_bin WHERE original_table_name = "
        "'%s.%s'";
    boost::format f =
        boost::format(recycle_bin_sql_format) % from_schema % to_table;
    ExecuteNode *delete_recycle_bin_node =
        plan->get_modify_node(auth_dspace, f.str().c_str(), false);
    ok_merge_node->add_child(delete_recycle_bin_node);
  } catch (...) {
    destroy_tmp_dataspace(tmp_dspace, the_schema, from_table.c_str());
    throw;
  }
  destroy_tmp_dataspace(tmp_dspace, the_schema, from_table.c_str());
  ok_node->add_child(ok_merge_node);
  plan->set_start_node(ok_node);
}

void Statement::assemble_dbscale_clean_recycle_table_plan(ExecutePlan *plan,
                                                          const char *schema,
                                                          const char *table) {
  DataSpace *dspace =
      Backend::instance()->get_data_space_for_table(schema, table);
  const char *recycle_type = get_table_from_recycle_bin(schema, table);
  if (recycle_type == NULL) {
    throw Error("Can't find this table to clean");
  }
  string recycle_type_str = recycle_type;
  string from_schema, from_table, to_table;
  from_schema = schema;
  from_table = table;
  to_table = from_table + RECYCLE_TABLE_POSTFIX;

  // create tmp dataspace
  DataSpace *tmp_dspace = create_tmp_dataspace_from_table(
      from_schema.c_str(), from_table.c_str(), to_table.c_str());
  Schema *the_schema = Backend::instance()->find_schema(schema);

  ExecuteNode *ok_node = plan->get_ok_node();
  ExecuteNode *ok_merge_node = NULL;
  const char *drop_recycle_tb_format = "DROP TABLE `%s`.`%s`";
  string drop_sql =
      (boost::format(drop_recycle_tb_format) % schema % to_table).str();
  try {
    if (dspace->is_partitioned()) {
      PartitionedTable *pt = (PartitionedTable *)dspace;
      ok_merge_node = assemble_modify_node_for_partitioned_table(
          plan, drop_sql.c_str(), pt);
    } else {
      ok_merge_node = plan->get_ok_merge_node();
      ExecuteNode *drop_tb_node =
          plan->get_modify_node(dspace, drop_sql.c_str(), false);
      ok_merge_node->add_child(drop_tb_node);
    }

    DataSpace *auth_dspace = Backend::instance()->get_auth_data_space();
    ExecuteNode *drop_tb_node_on_auth =
        plan->get_modify_node(auth_dspace, drop_sql.c_str(), false);
    ok_merge_node->add_child(drop_tb_node_on_auth);

    const char *clean_recycle_bin_table_format =
        "DELETE FROM dbscale.table_recycle_bin where original_table_name = "
        "'%s.%s'";
    string clean_recycle_bin_sql =
        (boost::format(clean_recycle_bin_table_format) % schema % table).str();
    ExecuteNode *clean_recycle_table_ok_merge_node = plan->get_ok_merge_node();
    ExecuteNode *clean_recycle_node = plan->get_modify_node(
        auth_dspace, clean_recycle_bin_sql.c_str(), false);
    clean_recycle_table_ok_merge_node->add_child(clean_recycle_node);
    ok_merge_node->add_child(clean_recycle_table_ok_merge_node);

    ok_node->add_child(ok_merge_node);
  } catch (...) {
    destroy_tmp_dataspace(tmp_dspace, the_schema, to_table.c_str());
    throw;
  }
  destroy_tmp_dataspace(tmp_dspace, the_schema, to_table.c_str());
  plan->set_start_node(ok_node);
}

ExecuteNode *Statement::assemble_modify_node_for_partitioned_table(
    ExecutePlan *plan, const char *sql, PartitionedTable *pt) {
  ExecuteNode *ok_merge_node = plan->get_ok_merge_node();
  for (unsigned int i = 0; i < pt->get_real_partition_num(); ++i) {
    ExecuteNode *modify_node =
        plan->get_modify_node(pt->get_partition(i), sql, true);
    ok_merge_node->add_child(modify_node);
  }
  return ok_merge_node;
}

}  // namespace sql
}  // namespace dbscale
